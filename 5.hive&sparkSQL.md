# Hive & SparkSQL 参考指南

## 目录
- [1. SQL 函数与语法](#1-sql-函数与语法)
  - [1.1 日期与时间处理](#11-日期与时间处理)
  - [1.2 字符串处理](#12-字符串处理)
  - [1.3 聚合函数](#13-聚合函数)
  - [1.4 数组与集合操作](#14-数组与集合操作)
  - [1.5 JSON 处理](#15-json-处理)
  - [1.6 条件与判断](#16-条件与判断)
  - [1.7 分组与聚合](#17-分组与聚合)
  - [1.8 窗口函数](#18-窗口函数)
  - [1.9 WITH 语句](#19-with-语句)
  - [1.10 JOIN 操作](#110-join-操作)
- [2. 表操作](#2-表操作)
  - [2.1 建表与压缩格式](#21-建表与压缩格式)
  - [2.2 分桶与分区](#22-分桶与分区)
  - [2.3 优化配置](#23-优化配置)
  - [2.4 实用查询示例](#24-实用查询示例)
- [3. UDF与自定义函数](#3-udf与自定义函数)
  - [3.1 地理位置函数](#31-地理位置函数)
  - [3.2 数据验证函数](#32-数据验证函数)
  - [3.3 字符处理函数](#33-字符处理函数)
  - [3.4 布隆过滤器](#34-布隆过滤器)
- [4. 参考链接与外部资源](#4-参考链接与外部资源)

## 1. SQL 函数与语法

### 1.1 日期与时间处理

```sql
-- 获取10位时间戳
unix_timestamp()  

-- 转换数据类型
cast(create_time as bigint)  

-- 10位时间戳转为指定格式
from_unixtime(create_time,'yyyyMMdd') 

-- 日期加减操作
date_add(date_column, number_of_days) -- 加几天
date_sub(date_column, number_of_days) -- 减几天

-- 复杂日期操作示例
unix_timestamp(date_sub(from_unixtime(unix_timestamp(),30),'yyyy-MM-dd')*1000

-- 计算两个日期差距(天数)
abs(datediff(to_date(logdate, 'yyyyMMdd'), to_date(day, 'yyyyMMdd'))) > 180
```

### 1.2 字符串处理

```sql
-- 判断字符串是否包含
instr(extend_detail, 'name') > 0  

-- col1 在 col2 里面，不在返回0
instr(col2, col1) > 0  

-- 正则表达式提取
regexp_extract(string subject, string pattern, int index)

-- 正则表达式替换
regexp_replace(extend_detail, '#[a-z0-9]{12}#', '##')

-- 子字符串提取 (start从1开始)
substr(string str, int start, int length)
```

### 1.3 聚合函数

```sql
-- 不同COUNT用法比较
```

| 写法 | 作用说明 | NULL行是否计入 |
|------|---------|--------------|
| COUNT(a) | 统计 a 字段非 NULL 的个数 | ❌ 不计入 |
| COUNT(*) | 统计总行数（包括 NULL） | ✅ 计入 |
| COUNT(DISTINCT a) | 去重后的非 NULL 个数 | ❌ 不计入 |

```sql
-- DataFrame 多字段统计示例
df.agg(count(lit(1)), count(when(length(col("geohash")) > 0, 1))).show(false)

-- 聚合多字段
raw.agg(count(when(length(col("ottlist")) > 0, 1)), count(when(length(col("essid")) > 0, 1))).show(false)
```

### 1.4 数组与集合操作

```sql
-- 判断两个数组有交集
arrays_overlap(split(month_app_name, ' '), split(day_app_name, ' '))

-- 数组展开（行转列）
lateral view explode(split(gid_detail, ',')) tmp as single_gid

-- 数组转换与合并
-- 转换数组并合并去重
df.withColumn("geos", expr("array_union(transform(split(geo_list,','), x->substr(x,1,6)), array(substr(geohash,1,6)))")).show

-- 数组去重判断
size(array_distinct(transform(wifimac, x->split(x,',')[0]))) = 1

-- 获取所有geohash6去重
essid_geo.withColumn("geohashs", expr("array_union(array_distinct(transform(split(detail,'\u0001'), x->substr(split(x,'#')[1],1,6))), array(substr(last_geohash,1,6)))")).show(false)

-- 过滤数组元素
df.withColumn("size", expr("filter(split(geo_list,','), x->length(x)=8)"))
```

#### ClickHouse数组操作示例

```sql
-- 获取每个geohash6最后汇报时间
select geohash6, max(day) last_day from 
(select day, arrayJoin(arrayDistinct(arrayConcat([substr(geohash,1,6)],
arrayMap(x->substr(splitByChar('#',x)[1],1,6), splitByChar(',',geo_list))))) geohash6 
from essid_info where essid='94a4f9079302') t1 group by geohash6 order by geohash6;

-- 查看wifi_wide detail中共同geohash6
select *, arrayFilter(x->length(x)>0, arrayDistinct(arrayMap(x->substr(splitByChar('#',x)[2],1,6), splitByString('\u001',detail)))) geos 
from glab_gid_info.wifi_wide_v2_local_old where type='0' and length(geos)=1 limit 5 \G
```

### 1.5 JSON 处理

```sql
-- 解析JSON字段
spark.sql("select get_json_object(split(value,'\\\\|')[5],'$.data') from logs").show(false)
```

### 1.6 条件与判断

```sql
-- COUNT性能比较
count(1)  -- 非NULL的行，计算快
count(*)  -- 计算所有行，包括NULL值的行。扫描整表，算的慢

-- HAVING与WHERE区别
-- 1、where在分组前过滤，having在分组后使用，应用于每一组数据
-- 2、having可以使用select里面定义的别名
select job, count(1) cnt from emp group by job having cnt > 2;
```

### 1.7 分组与聚合

```sql
-- 分组聚合示例
-- 见上面的HAVING示例和窗口函数部分
```

### 1.8 窗口函数

```sql
-- over()获取整个表的总条数  
select "oaid" id_type, oaid_number id_number, count(1) over() gid_sum 
from gid_to_id
where query_id in (select query_id from gid_to_id group by query_id having count(1)=2) 
  and rpc_type='rpc_end'
```

**排序窗口函数比较**

| 函数名 | 含义 | 相同排序值时的处理 | 是否有跳跃 | 示例排名（当成绩并列） |
|-------|-----|-----------------|---------|-------------------|
| row_number() | 严格唯一编号 | 每行都有唯一编号 | ✅ 有跳跃 | 1, 2, 3, 4, 5 |
| rank() | 排名（相同值同名次） | 相同值→相同名次，跳过下个名次 | ✅ 有跳跃 | 1, 2, 2, 4, 5 |
| dense_rank() | 密集排名（不跳号） | 相同值→相同名次，不跳下个名次 | ❌ 无跳跃 | 1, 2, 2, 3, 4 |

### 1.9 WITH 语句

```sql
-- WITH子句用法（公用表表达式CTE）
WITH t1 AS (SELECT * FROM carinfo), 
     t2 AS (SELECT * FROM car_blacklist)   -- 不用写分号
SELECT *
FROM t1, t2
```

### 1.10 JOIN 操作

```sql
-- 广播JOIN（小表广播到大表所在节点）
val fill = raw.join(broadcast(uuid2), raw("wifimac") === uuid2("uuid"))

-- DataFrame按照字段repartition（优化JOIN前的数据分布）
df.repartition(2000, col("uuid"))
```

## 2. 表操作

### 2.1 建表与压缩格式

```sql
-- 建表示例（ORC格式，无压缩）
create table log_orc_none(
  track_time string,
  url string,
  session_id string,
  referer string,
  ip string,
  end_user_id string,
  city_id string
)
row format delimited fields terminated by '\t'
stored as orc tblproperties ("orc.compress"="NONE");
```

### 2.2 分桶与分区

```sql
-- 分桶表创建
CREATE TABLE bucketed_table (
    id INT,
    name STRING
) CLUSTERED BY (id) INTO 10 BUCKETS;

-- 启用分桶
SET hive.enforce.bucketing=true;

-- 动态分区设置
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
```

### 2.3 优化配置

```sql
-- MapJoin配置（小表加载到内存）
SET hive.auto.convert.join=true;
SELECT /*+ MAPJOIN(small_table) */ *
FROM large_table
JOIN small_table ON large_table.id = small_table.id;

-- 压缩中间结果
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress=true;
SET mapreduce.map.output.compress=true;
SET mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;

-- 基于成本的优化器(CBO)
SET hive.cbo.enable=true;

-- MapReduce内存配置
SET mapreduce.map.memory.mb=4096;
SET mapreduce.reduce.memory.mb=8192;
```

### 2.4 实用查询示例

```sql
-- 查看信标历史geohash6最后出现时间
-- 见ClickHouse数组操作示例部分
```

## 3. UDF与自定义函数

### 3.1 地理位置函数

```scala
-- 经纬度转geohash 
-- jars hdfs://gt-ga-xs/tmp/wuxl/udf/ys-bi-udf-hive-function-0.0.0.jar
import com.ys.axe.saas.comn.util.GeohashUtil
spark.udf.register("encode", GeohashUtil.encode _)

-- 是否在国内判断
geohash_in_china('wku7wp7147f6')
```

### 3.2 数据验证函数

```scala
-- 合法wifi判断
import com.ys.axe.saas.comn.util.LegalMacUtil;
val isLegalMac = udf((arg: String) => LegalMacUtil.isLegalMac(arg))
spark.udf.register("isLegalMac", isLegalMac);
```

### 3.3 字符处理函数

```scala
-- 不可见字符检测
import org.apache.spark.sql.{SparkSession, functions => F}
import org.apache.spark.sql.expressions.UserDefinedFunction

/** 判断是否为不可见字符 */
def isInvisible(code: Int): Boolean = {
  (code >= 0x00 && code <= 0x1F) || // 控制字符
  code == 0x7F ||                   // DEL
  code == 0xA0 ||                   // 不换行空格
  code == 0x200B ||                 // 零宽空格
  code == 0xFEFF                    // BOM
}

/** 检测不可见字符并打印位置、Unicode等 */
def detectInvisible(input: String): String = {
  if (input == null || input.isEmpty) return "[空]"
  input.zipWithIndex.flatMap {
    case (ch, idx) =>
      val code = ch.toInt
      if (isInvisible(code)) {
        Some(f"[pos=$idx, U+${code}%04X, ASCII=$code]")
      } else None
  }.mkString(", ")
}

// 注册 UDF
val detectInvisibleUDF: UserDefinedFunction = F.udf((input: String) => detectInvisible(input))
spark.udf.register("detectInvisible", detectInvisibleUDF)
```

### 3.4 布隆过滤器

```scala
// 创建布隆过滤器
val smallBF = smallDF.stat.bloomFilter("user_id", expectedNumItems = 1000000, fpp = 0.01)

// 广播布隆过滤器
val broadcastBF = spark.sparkContext.broadcast(smallBF)

// 用 UDF 过滤主表
import org.apache.spark.sql.functions.udf
val mightExist = udf((id: String) => broadcastBF.value.mightContain(id))
val result = largeDF.filter("mightExist(user_id)")
```

## 4. 参考链接与外部资源

- [UDF 函数文档](https://cf.cloudglab.cn/pages/viewpage.action?pageId=245273514)
