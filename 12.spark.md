# 12. Spark

## ç›®å½•

- [12. Spark](#12-spark)
  - [ç›®å½•](#ç›®å½•)
  - [Spark æ¦‚è¿°ä¸ç¯å¢ƒ](#spark-æ¦‚è¿°ä¸ç¯å¢ƒ)
    - [Sparkç®€ä»‹](#sparkç®€ä»‹)
      - [Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿](#sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿)
      - [Spark vs Hadoop MapReduce](#spark-vs-hadoop-mapreduce)
      - [Sparkåº”ç”¨åœºæ™¯](#sparkåº”ç”¨åœºæ™¯)
    - [Sparkç”Ÿæ€ç³»ç»Ÿ](#sparkç”Ÿæ€ç³»ç»Ÿ)
      - [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
      - [ç”Ÿæ€ç»„ä»¶](#ç”Ÿæ€ç»„ä»¶)
    - [Sparkç¯å¢ƒæ­å»º](#sparkç¯å¢ƒæ­å»º)
      - [æœ¬åœ°æ¨¡å¼](#æœ¬åœ°æ¨¡å¼)
      - [é›†ç¾¤æ¨¡å¼](#é›†ç¾¤æ¨¡å¼)
      - [å¸¸ç”¨é…ç½®](#å¸¸ç”¨é…ç½®)
  - [Spark æ ¸å¿ƒæ¦‚å¿µ â­](#spark-æ ¸å¿ƒæ¦‚å¿µ-)
    - [RDDæ ¸å¿ƒæ¦‚å¿µ](#rddæ ¸å¿ƒæ¦‚å¿µ)
      - [RDDç‰¹æ€§](#rddç‰¹æ€§)
      - [RDDæ“ä½œåˆ†ç±»](#rddæ“ä½œåˆ†ç±»)
      - [RDDä¾èµ–å…³ç³»](#rddä¾èµ–å…³ç³»)
    - [DataFrameä¸Dataset](#dataframeä¸dataset)
      - [DataFrameæ¦‚å¿µ](#dataframeæ¦‚å¿µ)
      - [Datasetæ¦‚å¿µ](#datasetæ¦‚å¿µ)
      - [ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥](#ä¸‰è€…å¯¹æ¯”åˆ†æ-)
    - [åˆ†åŒºæœºåˆ¶](#åˆ†åŒºæœºåˆ¶)
      - [åˆ†åŒºç­–ç•¥](#åˆ†åŒºç­–ç•¥)
      - [åˆ†åŒºè°ƒä¼˜](#åˆ†åŒºè°ƒä¼˜)
  - [Spark æ¶æ„ä¸åŸç† â­â­](#spark-æ¶æ„ä¸åŸç†-)
    - [Sparkæ•´ä½“æ¶æ„](#sparkæ•´ä½“æ¶æ„)
      - [é›†ç¾¤æ¶æ„ç»„ä»¶](#é›†ç¾¤æ¶æ„ç»„ä»¶)
      - [åº”ç”¨ç¨‹åºæ¶æ„](#åº”ç”¨ç¨‹åºæ¶æ„)
    - [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [SparkContext](#sparkcontext)
      - [Driver Program](#driver-program)
      - [Cluster Manager](#cluster-manager)
      - [Executor](#executor)
    - [ä»»åŠ¡è°ƒåº¦åŸç†](#ä»»åŠ¡è°ƒåº¦åŸç†)
      - [DAGSchedulerè°ƒåº¦](#dagschedulerè°ƒåº¦)
      - [TaskSchedulerè°ƒåº¦](#taskschedulerè°ƒåº¦)
    - [å­˜å‚¨ç®¡ç†æœºåˆ¶](#å­˜å‚¨ç®¡ç†æœºåˆ¶)
      - [BlockManagerç»„ä»¶](#blockmanagerç»„ä»¶)
      - [å†…å­˜æ¨¡å‹](#å†…å­˜æ¨¡å‹)
      - [å†…å­˜åˆ†é…ç­–ç•¥](#å†…å­˜åˆ†é…ç­–ç•¥)
    - [ShuffleåŸç†  â­â­â­](#shuffleåŸç†--)
      - [Shuffleå®ç°æœºåˆ¶](#shuffleå®ç°æœºåˆ¶)
      - [Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£](#shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜](#shuffle-ä¼˜åŒ–ä¸è°ƒä¼˜)
      - [Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
  - [Spark SQLä¸Catalyst â­â­](#spark-sqlä¸catalyst-)
    - [Spark SQLæ¦‚è¿°](#spark-sqlæ¦‚è¿°)
      - [ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
      - [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
    - [Catalystä¼˜åŒ–å™¨ ğŸ”¥](#catalystä¼˜åŒ–å™¨-)
      - [ä¼˜åŒ–æµç¨‹](#ä¼˜åŒ–æµç¨‹)
      - [ä¼˜åŒ–è§„åˆ™](#ä¼˜åŒ–è§„åˆ™)
      - [ä»£ç ç”Ÿæˆ](#ä»£ç ç”Ÿæˆ)
    - [SparkSQL å®ç”¨å‡½æ•°ä¸è¯­æ³•](#sparksql-å®ç”¨å‡½æ•°ä¸è¯­æ³•)
      - [æ—¥æœŸä¸æ—¶é—´å¤„ç†](#æ—¥æœŸä¸æ—¶é—´å¤„ç†)
      - [å­—ç¬¦ä¸²å¤„ç†](#å­—ç¬¦ä¸²å¤„ç†)
      - [æ•°ç»„ä¸é›†åˆæ“ä½œ](#æ•°ç»„ä¸é›†åˆæ“ä½œ)
      - [JSONå¤„ç†](#jsonå¤„ç†)
      - [æ¡ä»¶ä¸åˆ¤æ–­](#æ¡ä»¶ä¸åˆ¤æ–­)
      - [çª—å£å‡½æ•°](#çª—å£å‡½æ•°)
      - [èšåˆå‡½æ•°](#èšåˆå‡½æ•°)
      - [å®ç”¨æŸ¥è¯¢ç¤ºä¾‹](#å®ç”¨æŸ¥è¯¢ç¤ºä¾‹)
    - [æ•°æ®æºæ”¯æŒ](#æ•°æ®æºæ”¯æŒ)
      - [å†…ç½®æ•°æ®æº](#å†…ç½®æ•°æ®æº)
      - [å¤–éƒ¨æ•°æ®æº](#å¤–éƒ¨æ•°æ®æº)
  - [æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ– â­â­â­](#æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ–-)
    - [æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–](#æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–)
    - [Joinä¼˜åŒ–](#joinä¼˜åŒ–)
    - [ç¼“å­˜ä¸æŒä¹…åŒ–](#ç¼“å­˜ä¸æŒä¹…åŒ–)
    - [ä»£ç å±‚é¢ä¼˜åŒ–](#ä»£ç å±‚é¢ä¼˜åŒ–)
    - [ç½‘ç»œä¸I/Oä¼˜åŒ–](#ç½‘ç»œä¸ioä¼˜åŒ–)
    - [å¸¸è§æ€§èƒ½é—®é¢˜](#å¸¸è§æ€§èƒ½é—®é¢˜)
    - [ç›‘æ§ä¸è¯Šæ–­](#ç›‘æ§ä¸è¯Šæ–­)
  - [Spark Streaming â­](#spark-streaming-)
    - [æµå¤„ç†æ¦‚å¿µ](#æµå¤„ç†æ¦‚å¿µ)
      - [å¾®æ‰¹æ¬¡å¤„ç†](#å¾®æ‰¹æ¬¡å¤„ç†)
      - [DStreamæ¦‚å¿µ](#dstreamæ¦‚å¿µ)
    - [Structured Streaming](#structured-streaming)
      - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
      - [è¾“å‡ºæ¨¡å¼](#è¾“å‡ºæ¨¡å¼)
      - [çª—å£æ“ä½œ](#çª—å£æ“ä½œ)
    - [å®¹é”™æœºåˆ¶](#å®¹é”™æœºåˆ¶)
      - [Checkpointæœºåˆ¶](#checkpointæœºåˆ¶)
      - [WALæœºåˆ¶](#walæœºåˆ¶)
  - [Sparkä¼ä¸šçº§åº”ç”¨å®æˆ˜ ğŸ¢](#sparkä¼ä¸šçº§åº”ç”¨å®æˆ˜-)
    - [æ‰¹å¤„ç†åº”ç”¨](#æ‰¹å¤„ç†åº”ç”¨)
      - [ETLæ•°æ®å¤„ç†](#etlæ•°æ®å¤„ç†)
      - [æ•°æ®åˆ†ææ¡ˆä¾‹](#æ•°æ®åˆ†ææ¡ˆä¾‹)
    - [æµå¤„ç†åº”ç”¨](#æµå¤„ç†åº”ç”¨)
      - [å®æ—¶æ•°æ®å¤„ç†](#å®æ—¶æ•°æ®å¤„ç†)
      - [æœºå™¨å­¦ä¹ æµæ°´çº¿](#æœºå™¨å­¦ä¹ æµæ°´çº¿)
    - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
      - [å¼€å‘è§„èŒƒ](#å¼€å‘è§„èŒƒ)
      - [éƒ¨ç½²ç­–ç•¥](#éƒ¨ç½²ç­–ç•¥)
      - [è¿ç»´ç®¡ç†](#è¿ç»´ç®¡ç†)
  - [å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ)
    - [å†…å­˜ç›¸å…³é”™è¯¯](#å†…å­˜ç›¸å…³é”™è¯¯)
    - [ç½‘ç»œç›¸å…³é”™è¯¯](#ç½‘ç»œç›¸å…³é”™è¯¯)
    - [åºåˆ—åŒ–ç›¸å…³é”™è¯¯](#åºåˆ—åŒ–ç›¸å…³é”™è¯¯)
    - [èµ„æºç›¸å…³é”™è¯¯](#èµ„æºç›¸å…³é”™è¯¯)
    - [æ•°æ®ç›¸å…³é”™è¯¯](#æ•°æ®ç›¸å…³é”™è¯¯)
    - [è°ƒè¯•å’Œè¯Šæ–­å·¥å…·](#è°ƒè¯•å’Œè¯Šæ–­å·¥å…·)
    - [é¢„é˜²æªæ–½](#é¢„é˜²æªæ–½)
  - [å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿ âš™ï¸](#å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿-ï¸)
    - [èµ„æºç›¸å…³å‚æ•°](#èµ„æºç›¸å…³å‚æ•°)
    - [JVMç›¸å…³å‚æ•°](#jvmç›¸å…³å‚æ•°)
    - [æ€§èƒ½ä¼˜åŒ–å‚æ•°](#æ€§èƒ½ä¼˜åŒ–å‚æ•°)
    - [é…ç½®æ¨¡æ¿](#é…ç½®æ¨¡æ¿)
  - [Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥](#sparké«˜é¢‘é¢è¯•é¢˜-)
    - [åŸºç¡€æ¦‚å¿µé¢˜](#åŸºç¡€æ¦‚å¿µé¢˜)
    - [æ¶æ„åŸç†é¢˜](#æ¶æ„åŸç†é¢˜)
    - [æ€§èƒ½è°ƒä¼˜é¢˜](#æ€§èƒ½è°ƒä¼˜é¢˜)
    - [å®æˆ˜åº”ç”¨é¢˜](#å®æˆ˜åº”ç”¨é¢˜)
    - [æ·±åº¦æŠ€æœ¯åŸç†é¢˜](#æ·±åº¦æŠ€æœ¯åŸç†é¢˜)

---

## Spark æ¦‚è¿°ä¸ç¯å¢ƒ

### Sparkç®€ä»‹

**Apache Spark** æ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡ã€‚å®ƒæä¾›äº†é«˜çº§APIï¼ˆJavaã€Scalaã€Pythonã€Rï¼‰ï¼Œå¹¶æ”¯æŒç”¨äºSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¤„ç†çš„ä¼˜åŒ–å¼•æ“ã€‚

#### Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š

- **é€Ÿåº¦å¿«**ï¼šå†…å­˜è®¡ç®—æ¯”Hadoop MapReduceå¿«100å€ï¼Œç£ç›˜è®¡ç®—å¿«10å€
- **æ˜“ç”¨æ€§**ï¼šæä¾›å¤šç§è¯­è¨€APIï¼Œæ”¯æŒ80å¤šç§é«˜çº§ç®—å­
- **é€šç”¨æ€§**ï¼šæ”¯æŒSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
- **å…¼å®¹æ€§**ï¼šå¯è¿è¡Œåœ¨Hadoopã€Mesosã€Kubernetesã€standaloneç­‰é›†ç¾¤ä¸Š

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š


| ç‰¹æ€§         | Spark               | Hadoop MapReduce |
| -------------- | --------------------- | ------------------ |
| **è®¡ç®—æ¨¡å¼** | å†…å­˜è®¡ç®— + ç£ç›˜å­˜å‚¨ | ç£ç›˜è®¡ç®—         |
| **æ•°æ®å…±äº«** | RDDå†…å­˜å…±äº«         | ç£ç›˜æ–‡ä»¶ç³»ç»Ÿ     |
| **è¿­ä»£è®¡ç®—** | æ”¯æŒé«˜æ•ˆè¿­ä»£        | æ•ˆç‡ä½           |
| **å®æ—¶å¤„ç†** | æ”¯æŒæµå¤„ç†          | ä»…æ‰¹å¤„ç†         |
| **å®¹é”™æœºåˆ¶** | RDDè¡€ç»Ÿæ¢å¤         | æ•°æ®å‰¯æœ¬         |
| **å¼€å‘æ•ˆç‡** | ä»£ç ç®€æ´            | ä»£ç å¤æ‚         |

#### Spark vs Hadoop MapReduce

```mermaid
graph TD
    A[æ•°æ®è¾“å…¥] --> B{è®¡ç®—å¼•æ“}
  
    B -->|MapReduce| C[Mapé˜¶æ®µ]
    C --> D[Shuffleå†™ç£ç›˜]
    D --> E[Reduceé˜¶æ®µ]
    E --> F[ç»“æœå†™HDFS]
  
    B -->|Spark| G[RDDè½¬æ¢]
    G --> H[å†…å­˜è®¡ç®—]
    H --> I[ç»“æœè¾“å‡º]
  
    style C fill:#ffcccb
    style D fill:#ffcccb
    style E fill:#ffcccb
    style F fill:#ffcccb
    style G fill:#90EE90
    style H fill:#90EE90
    style I fill:#90EE90
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

- **å†…å­˜è®¡ç®—**ï¼šSparkåœ¨å†…å­˜ä¸­ç¼“å­˜æ•°æ®ï¼Œé¿å…é‡å¤I/O
- **DAGæ‰§è¡Œ**ï¼šSparkå°†ä½œä¸šæ„å»ºä¸ºDAGï¼Œä¼˜åŒ–æ‰§è¡Œè®¡åˆ’
- **Pipelining**ï¼šSparkæ”¯æŒç®—å­æµæ°´çº¿ï¼Œå‡å°‘ä¸­é—´æ•°æ®å­˜å‚¨
- **ä»£ç ç”Ÿæˆ**ï¼šCatalystä¼˜åŒ–å™¨ç”Ÿæˆé«˜æ•ˆçš„Javaä»£ç 

#### Sparkåº”ç”¨åœºæ™¯

**å…¸å‹åº”ç”¨é¢†åŸŸ**ï¼š


| åœºæ™¯           | æè¿°                       | ä¼˜åŠ¿                       |
| ---------------- | ---------------------------- | ---------------------------- |
| **æ•°æ®ETL**    | å¤§è§„æ¨¡æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åŠ è½½ | å¤„ç†é€Ÿåº¦å¿«ï¼Œæ”¯æŒå¤šç§æ•°æ®æº |
| **å®æ—¶æµå¤„ç†** | å®æ—¶æ•°æ®åˆ†æã€ç›‘æ§å‘Šè­¦     | ä½å»¶è¿Ÿï¼Œé«˜ååé‡           |
| **æœºå™¨å­¦ä¹ **   | å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ     | MLlibç”Ÿæ€ï¼Œè¿­ä»£è®¡ç®—ä¼˜åŠ¿    |
| **äº¤äº’å¼æŸ¥è¯¢** | å³å¸­æŸ¥è¯¢ã€æ•°æ®æ¢ç´¢         | SQLæ”¯æŒï¼Œå“åº”é€Ÿåº¦å¿«        |
| **å›¾è®¡ç®—**     | ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿ     | GraphXå›¾å¤„ç†èƒ½åŠ›           |

### Sparkç”Ÿæ€ç³»ç»Ÿ

#### æ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    A[Spark Core] --> B[Spark SQL]
    A --> C[Spark Streaming]
    A --> D[MLlib]
    A --> E[GraphX]
  
    B --> F[DataFrames & Datasets]
    B --> G[Catalyst Optimizer]
  
    C --> H[DStreams]
    C --> I[Structured Streaming]
  
    D --> J[Classification]
    D --> K[Clustering] 
    D --> L[Collaborative Filtering]
  
    E --> M[Graph Processing]
    E --> N[Graph Algorithms]
  
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#e8f5e8
    style E fill:#fce4ec
```

**ç»„ä»¶è¯¦è§£**ï¼š

1. **Spark Core**ï¼š

   - åŸºç¡€è¿è¡Œæ—¶å¼•æ“
   - RDDæŠ½è±¡
   - ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†ã€å®¹é”™æ¢å¤
2. **Spark SQL**ï¼š

   - ç»“æ„åŒ–æ•°æ®å¤„ç†
   - DataFrame/Dataset API
   - JDBC/ODBCè¿æ¥å™¨
3. **Spark Streaming**ï¼š

   - æµæ•°æ®å¤„ç†
   - å¾®æ‰¹æ¬¡å¤„ç†æ¨¡å‹
   - ä¸æ‰¹å¤„ç†ä»£ç ç»Ÿä¸€
4. **MLlib**ï¼š

   - æœºå™¨å­¦ä¹ åº“
   - åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤
   - ç®¡é“API
5. **GraphX**ï¼š

   - å›¾è®¡ç®—æ¡†æ¶
   - å›¾ç®—æ³•åº“
   - å›¾å¹¶è¡Œè®¡ç®—

#### ç”Ÿæ€ç»„ä»¶

**å¤–éƒ¨é›†æˆ**ï¼š


| ç»„ä»¶ç±»å‹     | ç»„ä»¶åç§°              | ç”¨é€”         |
| -------------- | ----------------------- | -------------- |
| **èµ„æºç®¡ç†** | YARNã€Mesosã€K8s      | é›†ç¾¤èµ„æºç®¡ç† |
| **å­˜å‚¨ç³»ç»Ÿ** | HDFSã€S3ã€HBase       | æ•°æ®å­˜å‚¨     |
| **æ•°æ®æ ¼å¼** | Parquetã€Avroã€JSON   | æ•°æ®åºåˆ—åŒ–   |
| **æµæ•°æ®**   | Kafkaã€Flumeã€Kinesis | æ•°æ®é‡‡é›†     |
| **ç›‘æ§å·¥å…·** | Gangliaã€Nagios       | é›†ç¾¤ç›‘æ§     |

### Sparkç¯å¢ƒæ­å»º

#### æœ¬åœ°æ¨¡å¼

**ä¸‹è½½å®‰è£…**ï¼š

```bash
# ä¸‹è½½Spark
wget https://downloads.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

# è§£å‹
tar -xzf spark-3.4.0-bin-hadoop3.tgz
cd spark-3.4.0-bin-hadoop3

# è®¾ç½®ç¯å¢ƒå˜é‡
export SPARK_HOME=/path/to/spark-3.4.0-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
```

**å¯åŠ¨æœ¬åœ°æ¨¡å¼**ï¼š

```bash
# å¯åŠ¨Spark Shell (Scala)
spark-shell --master local[2]

# å¯åŠ¨PySpark Shell (Python)
pyspark --master local[2]

# æäº¤åº”ç”¨ç¨‹åº
spark-submit \
  --master local[2] \
  --class org.apache.spark.examples.SparkPi \
  examples/jars/spark-examples_2.12-3.4.0.jar \
  10
```

#### é›†ç¾¤æ¨¡å¼

**Standaloneæ¨¡å¼éƒ¨ç½²**ï¼š

```bash
# 1. é…ç½®slavesæ–‡ä»¶
echo "worker1" >> conf/slaves
echo "worker2" >> conf/slaves

# 2. å¯åŠ¨Master
./sbin/start-master.sh

# 3. å¯åŠ¨Workers
./sbin/start-slaves.sh

# 4. æäº¤åº”ç”¨åˆ°é›†ç¾¤
spark-submit \
  --master spark://master:7077 \
  --deploy-mode cluster \
  --class MainClass \
  --conf spark.sql.adaptive.enabled=true \
  app.jar
```

**YARNæ¨¡å¼éƒ¨ç½²**ï¼š

```bash
# é…ç½®Hadoopç¯å¢ƒ
export HADOOP_HOME=/path/to/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# æäº¤åˆ°YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class MainClass \
  app.jar
```

#### å¸¸ç”¨é…ç½®

**æ ¸å¿ƒé…ç½®å‚æ•°**ï¼š

```properties
# spark-defaults.conf

# åº”ç”¨ç¨‹åºé…ç½®
spark.app.name                MySparkApp
spark.master                  yarn
spark.submit.deployMode       cluster

# èµ„æºé…ç½®
spark.driver.memory           2g
spark.driver.cores            1
spark.executor.memory         4g
spark.executor.cores          2
spark.executor.instances      10

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled           true
spark.dynamicAllocation.minExecutors      2
spark.dynamicAllocation.maxExecutors      20
spark.dynamicAllocation.initialExecutors  5

# Shuffleé…ç½®
spark.sql.adaptive.enabled                true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled       true

# åºåˆ—åŒ–
spark.serializer              org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired  false

# å‹ç¼©
spark.sql.parquet.compression.codec  snappy
spark.sql.orc.compression.codec      snappy
```

**æ—¥å¿—é…ç½®**ï¼š

```properties
# log4j.properties
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# è®¾ç½®Sparkæ—¥å¿—çº§åˆ«
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.springframework.core.env.ConfigUtils=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.util.Utils=WARN
```

---

## Spark æ ¸å¿ƒæ¦‚å¿µ â­

### RDDæ ¸å¿ƒæ¦‚å¿µ

**RDD (Resilient Distributed Dataset)** æ˜¯Sparkçš„æ ¸å¿ƒæŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€å¯å¹¶è¡Œè®¡ç®—çš„æ•°æ®é›†åˆã€‚

#### RDDç‰¹æ€§

```mermaid
graph TD
    A[RDDæ ¸å¿ƒç‰¹æ€§] --> B[ä¸å¯å˜æ€§<br/>Immutable]
    A --> C[åˆ†å¸ƒå¼<br/>Distributed]
    A --> D[å¼¹æ€§å®¹é”™<br/>Resilient]
    A --> E[æƒ°æ€§æ±‚å€¼<br/>Lazy Evaluation]
    A --> F[åˆ†åŒºè®¡ç®—<br/>Partitioned]
  
    B --> B1[æ•°æ®ä¸€æ—¦åˆ›å»ºä¸å¯ä¿®æ”¹]
    C --> C1[æ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤å¤šä¸ªèŠ‚ç‚¹]
    D --> D1[é€šè¿‡è¡€ç»Ÿä¿¡æ¯è‡ªåŠ¨å®¹é”™]
    E --> E1[Transformæ“ä½œå»¶è¿Ÿæ‰§è¡Œ]
    F --> F1[æ”¯æŒå¹¶è¡Œè®¡ç®—]
  
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#ffebee
    style E fill:#f3e5f5
    style F fill:#fce4ec
```

**RDDçš„äº”å¤§ç‰¹æ€§**ï¼š


| ç‰¹æ€§         | æè¿°                    | æ„ä¹‰             |
| -------------- | ------------------------- | ------------------ |
| **åˆ†åŒºåˆ—è¡¨** | RDDç”±å¤šä¸ªåˆ†åŒºç»„æˆ       | æ”¯æŒå¹¶è¡Œè®¡ç®—     |
| **è®¡ç®—å‡½æ•°** | æ¯ä¸ªåˆ†åŒºéƒ½æœ‰è®¡ç®—å‡½æ•°    | å®šä¹‰æ•°æ®å¤„ç†é€»è¾‘ |
| **ä¾èµ–å…³ç³»** | RDDä¹‹é—´çš„ä¾èµ–å…³ç³»       | æ”¯æŒå®¹é”™æ¢å¤     |
| **åˆ†åŒºå™¨**   | Key-Value RDDçš„åˆ†åŒºç­–ç•¥ | ä¼˜åŒ–æ•°æ®åˆ†å¸ƒ     |
| **ä½ç½®åå¥½** | è®¡ç®—åˆ†åŒºçš„æœ€ä½³ä½ç½®      | æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–   |

#### RDDæ“ä½œåˆ†ç±»

**Transformation vs Action**ï¼š

```mermaid
graph LR
    A[RDDæ“ä½œ] --> B[Transformation<br/>è½¬æ¢æ“ä½œ]
    A --> C[Action<br/>è¡ŒåŠ¨æ“ä½œ]
  
    B --> D[æƒ°æ€§æ‰§è¡Œ<br/>ä¸ç«‹å³è®¡ç®—]
    B --> E[è¿”å›æ–°RDD]
    B --> F[æ„å»ºè®¡ç®—å›¾]
  
    C --> G[ç«‹å³æ‰§è¡Œ<br/>è§¦å‘è®¡ç®—]
    C --> H[è¿”å›ç»“æœå€¼]
    C --> I[æäº¤ä½œä¸š]
  
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**å¸¸ç”¨Transformationæ“ä½œ**ï¼š

```scala
// åˆ›å»ºRDD
val rdd = sc.parallelize(1 to 100, 4)

// mapï¼šä¸€å¯¹ä¸€è½¬æ¢
val mapRDD = rdd.map(x => x * 2)

// filterï¼šè¿‡æ»¤æ•°æ®
val filterRDD = rdd.filter(x => x % 2 == 0)

// flatMapï¼šä¸€å¯¹å¤šè½¬æ¢
val flatMapRDD = rdd.flatMap(x => 1 to x)

// groupByKeyï¼šæŒ‰é”®åˆ†ç»„
val kvRDD = rdd.map(x => (x % 10, x))
val groupedRDD = kvRDD.groupByKey()

// reduceByKeyï¼šæŒ‰é”®èšåˆ
val reducedRDD = kvRDD.reduceByKey(_ + _)

// joinï¼šè¿æ¥æ“ä½œ
val rdd2 = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c")))
val joinedRDD = kvRDD.join(rdd2)
```

**å¸¸ç”¨Actionæ“ä½œ**ï¼š

```scala
// collectï¼šæ”¶é›†æ‰€æœ‰å…ƒç´ åˆ°Driver
val result = rdd.collect()

// countï¼šè®¡ç®—å…ƒç´ æ•°é‡
val cnt = rdd.count()

// firstï¼šè·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
val firstElement = rdd.first()

// takeï¼šè·å–å‰nä¸ªå…ƒç´ 
val firstN = rdd.take(10)

// reduceï¼šèšåˆæ‰€æœ‰å…ƒç´ 
val sum = rdd.reduce(_ + _)

// foreachï¼šéå†æ¯ä¸ªå…ƒç´ 
rdd.foreach(println)

// saveAsTextFileï¼šä¿å­˜åˆ°æ–‡ä»¶
rdd.saveAsTextFile("hdfs://output/path")
```

#### RDDä¾èµ–å…³ç³»

**ä¾èµ–ç±»å‹**ï¼š

```mermaid
graph TD
    A[RDDä¾èµ–å…³ç³»] --> B[çª„ä¾èµ–<br/>Narrow Dependency]
    A --> C[å®½ä¾èµ–<br/>Wide Dependency]
  
    B --> D[ä¸€å¯¹ä¸€æ˜ å°„<br/>1:1 Mapping]
    B --> E[åŒä¸€Stageå†…<br/>Pipelineæ‰§è¡Œ]
    B --> F[å±€éƒ¨å¤±è´¥æ¢å¤]
  
    C --> G[ä¸€å¯¹å¤šæ˜ å°„<br/>1:N Mapping]
    C --> H[éœ€è¦Shuffle<br/>è·¨Stageæ‰§è¡Œ]
    C --> I[å…¨é‡é‡æ–°è®¡ç®—]
  
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**çª„ä¾èµ–ç¤ºä¾‹**ï¼š

```scala
// map, filter, unionç­‰æ“ä½œäº§ç”Ÿçª„ä¾èµ–
val rdd1 = sc.parallelize(1 to 10, 2)
val rdd2 = rdd1.map(_ * 2)        // çª„ä¾èµ–
val rdd3 = rdd2.filter(_ > 10)    // çª„ä¾èµ–
```

**å®½ä¾èµ–ç¤ºä¾‹**ï¼š

```scala
// groupByKey, reduceByKey, joinç­‰æ“ä½œäº§ç”Ÿå®½ä¾èµ–
val rdd1 = sc.parallelize(Seq((1, "a"), (2, "b"), (1, "c")), 2)
val rdd2 = rdd1.groupByKey()      // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
val rdd3 = rdd1.reduceByKey(_ + _) // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
```

### DataFrameä¸Dataset

#### DataFrameæ¦‚å¿µ

**DataFrame** æ˜¯Spark SQLçš„æ ¸å¿ƒæŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªä»¥å‘½ååˆ—æ–¹å¼ç»„ç»‡çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºå…³ç³»æ•°æ®åº“ä¸­çš„è¡¨ã€‚

**DataFrameç‰¹ç‚¹**ï¼š

- **ç»“æ„åŒ–æ•°æ®**ï¼šå…·æœ‰æ˜ç¡®çš„Schemaå®šä¹‰
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šä½¿ç”¨Catalystä¼˜åŒ–å™¨
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šScalaã€Javaã€Pythonã€R
- **ä¸°å¯ŒAPI**ï¼šSQLé£æ ¼å’Œå‡½æ•°å¼API

**DataFrameåˆ›å»º**ï¼š

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("DataFrameExample")
  .getOrCreate()

import spark.implicits._

// æ–¹å¼1ï¼šä»RDDåˆ›å»º
val rdd = sc.parallelize(Seq(("Alice", 25), ("Bob", 30), ("Charlie", 35)))
val df1 = rdd.toDF("name", "age")

// æ–¹å¼2ï¼šä»åºåˆ—åˆ›å»º
val df2 = Seq(("Alice", 25), ("Bob", 30)).toDF("name", "age")

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val df3 = spark.read.json("path/to/file.json")
val df4 = spark.read.parquet("path/to/file.parquet")

// æ–¹å¼4ï¼šé€šè¿‡Schemaåˆ›å»º
val schema = StructType(Seq(
  StructField("name", StringType, nullable = true),
  StructField("age", IntegerType, nullable = true)
))
val df5 = spark.createDataFrame(rdd, schema)
```

#### Datasetæ¦‚å¿µ

**Dataset** æ˜¯DataFrameçš„æ‰©å±•ï¼Œæä¾›äº†ç±»å‹å®‰å…¨çš„é¢å‘å¯¹è±¡ç¼–ç¨‹æ¥å£ã€‚

**Datasetç‰¹ç‚¹**ï¼š

- **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šäº«å—Catalystä¼˜åŒ–å™¨
- **å‡½æ•°å¼API**ï¼šæ”¯æŒlambdaè¡¨è¾¾å¼
- **ç¼–ç å™¨æ”¯æŒ**ï¼šè‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–

**Datasetåˆ›å»º**ï¼š

```scala
// å®šä¹‰æ ·ä¾‹ç±»
case class Person(name: String, age: Int, city: String)

// æ–¹å¼1ï¼šä»åºåˆ—åˆ›å»º
val ds1 = Seq(
  Person("Alice", 25, "Beijing"),
  Person("Bob", 30, "Shanghai")
).toDS()

// æ–¹å¼2ï¼šä»DataFrameè½¬æ¢
val ds2 = df.as[Person]

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val ds3 = spark.read.json("path/to/file.json").as[Person]
```

#### ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥

**RDD vs DataFrame vs Dataset å…¨é¢å¯¹æ¯”**ï¼š


| ç‰¹æ€§           | RDD                    | DataFrame           | Dataset            |
| ---------------- | ------------------------ | --------------------- | -------------------- |
| **æ•°æ®æŠ½è±¡**   | åˆ†å¸ƒå¼å¯¹è±¡é›†åˆ         | ç»“æ„åŒ–æ•°æ®è¡¨        | ç±»å‹å®‰å…¨çš„æ•°æ®è¡¨   |
| **ç¼–è¯‘æ—¶æ£€æŸ¥** | âŒ è¿è¡Œæ—¶é”™è¯¯          | âŒ è¿è¡Œæ—¶é”™è¯¯       | âœ… ç¼–è¯‘æ—¶é”™è¯¯      |
| **æ‰§è¡Œä¼˜åŒ–**   | âŒ æ— ä¼˜åŒ–              | âœ… Catalystä¼˜åŒ–     | âœ… Catalystä¼˜åŒ–    |
| **ä»£ç ç”Ÿæˆ**   | âŒ æ—                   | âœ… æœ‰               | âœ… æœ‰              |
| **åºåˆ—åŒ–**     | Java/Kryoåºåˆ—åŒ–        | TungstenäºŒè¿›åˆ¶æ ¼å¼  | TungstenäºŒè¿›åˆ¶æ ¼å¼ |
| **APIé£æ ¼**    | å‡½æ•°å¼                 | SQL + å‡½æ•°å¼        | ç±»å‹å®‰å…¨å‡½æ•°å¼     |
| **æ€§èƒ½**       | ä½                     | é«˜                  | é«˜                 |
| **æ˜“ç”¨æ€§**     | å¤æ‚                   | ç®€å•                | ä¸­ç­‰               |
| **é€‚ç”¨åœºæ™¯**   | ä½çº§æ“ä½œã€éç»“æ„åŒ–æ•°æ® | SQLæŸ¥è¯¢ã€ç»“æ„åŒ–æ•°æ® | ç±»å‹å®‰å…¨è¦æ±‚é«˜     |

**æ€§èƒ½å¯¹æ¯”**ï¼š

```scala
// æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
import org.apache.spark.sql.functions._

// RDDæ–¹å¼ - æ€§èƒ½è¾ƒä½
val rddResult = rdd.filter(_.age > 25)
  .map(p => (p.city, 1))
  .reduceByKey(_ + _)
  .collect()

// DataFrameæ–¹å¼ - æ€§èƒ½ä¼˜åŒ–
val dfResult = df.filter($"age" > 25)
  .groupBy("city")
  .count()
  .collect()

// Datasetæ–¹å¼ - ç±»å‹å®‰å…¨ + æ€§èƒ½ä¼˜åŒ–
val dsResult = ds.filter(_.age > 25)
  .groupByKey(_.city)
  .count()
  .collect()
```

**é€‰æ‹©å»ºè®®**ï¼š

```mermaid
graph TD
    A[é€‰æ‹©æ•°æ®æŠ½è±¡] --> B{æ•°æ®ç±»å‹}
    B -->|éç»“æ„åŒ–| C[ä½¿ç”¨RDD]
    B -->|ç»“æ„åŒ–| D{ç±»å‹å®‰å…¨è¦æ±‚}
    D -->|ä¸éœ€è¦| E[ä½¿ç”¨DataFrame]
    D -->|éœ€è¦| F[ä½¿ç”¨Dataset]
  
    C --> G[å¤æ‚æ•°æ®å¤„ç†<br/>åº•å±‚æ§åˆ¶]
    E --> H[SQLæŸ¥è¯¢<br/>é«˜æ€§èƒ½è¦æ±‚]
    F --> I[ç±»å‹å®‰å…¨<br/>ç¼–è¯‘æ—¶æ£€æŸ¥]
  
    style C fill:#ffebee
    style E fill:#e8f5e8
    style F fill:#e1f5fe
```

### åˆ†åŒºæœºåˆ¶

#### åˆ†åŒºç­–ç•¥

**åˆ†åŒºçš„é‡è¦æ€§**ï¼š

- **å¹¶è¡Œåº¦æ§åˆ¶**ï¼šåˆ†åŒºæ•°å†³å®šä»»åŠ¡å¹¶è¡Œåº¦
- **æ•°æ®æœ¬åœ°æ€§**ï¼šå‡å°‘ç½‘ç»œä¼ è¾“
- **è´Ÿè½½å‡è¡¡**ï¼šé¿å…æ•°æ®å€¾æ–œ
- **èµ„æºåˆ©ç”¨**ï¼šå……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æº

**åˆ†åŒºå™¨ç±»å‹**ï¼š


| åˆ†åŒºå™¨               | é€‚ç”¨æ•°æ®ç±»å‹          | åˆ†åŒºç­–ç•¥                  | ä½¿ç”¨åœºæ™¯     |
| ---------------------- | ----------------------- | --------------------------- | -------------- |
| **HashPartitioner**  | Key-Value RDD         | Hash(key) % numPartitions | å‡åŒ€åˆ†å¸ƒçš„é”® |
| **RangePartitioner** | å¯æ’åºçš„Key-Value RDD | æŒ‰é”®å€¼èŒƒå›´åˆ†åŒº            | æœ‰åºæ•°æ®æŸ¥è¯¢ |
| **è‡ªå®šä¹‰åˆ†åŒºå™¨**     | ä»»æ„ç±»å‹              | ç”¨æˆ·å®šä¹‰é€»è¾‘              | ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚ |

**åˆ†åŒºæ“ä½œç¤ºä¾‹**ï¼š

```scala
// åˆ›å»ºå¸¦åˆ†åŒºçš„RDD
val rdd = sc.parallelize(1 to 100, 4)  // 4ä¸ªåˆ†åŒº

// æŸ¥çœ‹åˆ†åŒºä¿¡æ¯
println(s"åˆ†åŒºæ•°: ${rdd.getNumPartitions}")
println(s"åˆ†åŒºå†…å®¹: ${rdd.glom().collect().map(_.toList).toList}")

// é‡æ–°åˆ†åŒº
val repartitionedRDD = rdd.repartition(8)  // å¢åŠ åˆ†åŒºæ•°
val coalescedRDD = rdd.coalesce(2)         // å‡å°‘åˆ†åŒºæ•°

// Key-Value RDDåˆ†åŒº
val kvRDD = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c"), (4, "d")), 2)

// ä½¿ç”¨HashPartitioner
val hashPartitioned = kvRDD.partitionBy(new HashPartitioner(3))

// ä½¿ç”¨RangePartitioner
val rangePartitioned = kvRDD.partitionBy(new RangePartitioner(3, kvRDD))
```

**è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼š

```scala
import org.apache.spark.Partitioner

// è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼šæŒ‰ç”¨æˆ·IDçš„åœ°åŒºåˆ†åŒº
class RegionPartitioner(regions: Array[String]) extends Partitioner {
  
  override def numPartitions: Int = regions.length
  
  override def getPartition(key: Any): Int = {
    val userId = key.asInstanceOf[String]
    val region = getUserRegion(userId)
    math.abs(regions.indexOf(region)) % numPartitions
  }
  
  private def getUserRegion(userId: String): String = {
    // æ ¹æ®ç”¨æˆ·IDç¡®å®šåœ°åŒºçš„ä¸šåŠ¡é€»è¾‘
    userId.substring(0, 2) match {
      case "01" | "02" => "North"
      case "03" | "04" => "South"
      case "05" | "06" => "East"
      case _ => "West"
    }
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
val regions = Array("North", "South", "East", "West")
val customPartitioner = new RegionPartitioner(regions)
val customPartitioned = kvRDD.partitionBy(customPartitioner)
```

#### åˆ†åŒºè°ƒä¼˜

**åˆ†åŒºæ•°ä¼˜åŒ–**ï¼š

```scala
// åˆ†åŒºæ•°è®¾ç½®åŸåˆ™
val totalCores = 16  // é›†ç¾¤æ€»æ ¸å¿ƒæ•°
val optimalPartitions = totalCores * 2  // æ¨èåˆ†åŒºæ•°ä¸ºæ ¸å¿ƒæ•°çš„2-3å€

// åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
def getOptimalPartitions(dataSize: Long): Int = {
  val targetPartitionSize = 128 * 1024 * 1024  // 128MB per partition
  math.max(1, (dataSize / targetPartitionSize).toInt)
}

// åˆ†åŒºå€¾æ–œæ£€æµ‹
def detectPartitionSkew(rdd: RDD[_]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  val maxSize = partitionSizes.map(_._2).max
  val skewRatio = maxSize.toDouble / avgSize
  
  if (skewRatio > 2.0) {
    println(s"è­¦å‘Šï¼šæ£€æµ‹åˆ°åˆ†åŒºå€¾æ–œï¼Œå€¾æ–œæ¯”ä¾‹: $skewRatio")
    partitionSizes.foreach { case (index, size) =>
      println(s"åˆ†åŒº $index: $size æ¡è®°å½•")
    }
  }
}
```

**åˆ†åŒºä¼˜åŒ–ç­–ç•¥**ï¼š

1. **é¢„åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æ ¹æ®æ•°æ®ç‰¹å¾é¢„åˆ†åŒº
val userRDD = sc.textFile("hdfs://users/*")
  .map(parseUser)
  .partitionBy(new HashPartitioner(numPartitions))
  .cache()  // ç¼“å­˜é¢„åˆ†åŒºçš„æ•°æ®
```

2. **Coalesce vs Repartition**ï¼š

```scala
// Coalesceï¼šå‡å°‘åˆ†åŒºï¼Œé¿å…å…¨é‡Shuffle
val reducedRDD = largeRDD.coalesce(10)

// Repartitionï¼šé‡æ–°åˆ†åŒºï¼Œä¼šè¿›è¡Œå…¨é‡Shuffle
val reshuffledRDD = largeRDD.repartition(20)

// æ¡ä»¶åˆ†åŒºè°ƒæ•´
def smartRepartition[T](rdd: RDD[T], targetPartitions: Int): RDD[T] = {
  val currentPartitions = rdd.getNumPartitions
  if (targetPartitions < currentPartitions) {
    rdd.coalesce(targetPartitions)
  } else {
    rdd.repartition(targetPartitions)
  }
}
```

3. **åˆ†åŒºä¿æŒç­–ç•¥**ï¼š

```scala
// ä½¿ç”¨mapPartitionsä¿æŒåˆ†åŒºç»“æ„
val optimizedRDD = rdd.mapPartitions { iter =>
  // åˆ†åŒºå†…å¤„ç†é€»è¾‘
  iter.map(processRecord)
}

// é¿å…ç ´ååˆ†åŒºçš„æ“ä½œ
val goodRDD = partitionedRDD.mapValues(_ * 2)  // ä¿æŒåˆ†åŒº
val badRDD = partitionedRDD.map(x => (x._1, x._2 * 2))  // å¯èƒ½ç ´ååˆ†åŒº
```

---

## Spark æ¶æ„ä¸åŸç† â­â­

### Sparkæ•´ä½“æ¶æ„

#### é›†ç¾¤æ¶æ„ç»„ä»¶

```mermaid
graph TB
    subgraph "Driver Program"
        A[SparkContext]
        A1[DAGScheduler]
        A2[TaskScheduler]
        A3[BackendScheduler]
    end
  
    subgraph "Cluster Manager"
        B[Resource Manager]
        B1[Application Master]
    end
  
    subgraph "Worker Node 1"
        C[Executor 1]
        C1[Task]
        C2[BlockManager]
        C3[Cache]
    end
  
    subgraph "Worker Node 2"
        D[Executor 2]
        D1[Task]
        D2[BlockManager] 
        D3[Cache]
    end
  
    A --> B
    B --> C
    B --> D
    A1 --> A2
    A2 --> A3
    A3 --> C1
    A3 --> D1
    C2 <--> D2
  
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#e8f5e8
    style D fill:#e8f5e8
```

**æ¶æ„ç»„ä»¶è¯¦è§£**ï¼š


| ç»„ä»¶                | èŒè´£                       | è¿è¡Œä½ç½®         |
| --------------------- | ---------------------------- | ------------------ |
| **Driver Program**  | åº”ç”¨ç¨‹åºå…¥å£ï¼ŒåŒ…å«mainå‡½æ•° | å®¢æˆ·ç«¯æˆ–é›†ç¾¤èŠ‚ç‚¹ |
| **SparkContext**    | Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹      | Driver           |
| **Cluster Manager** | é›†ç¾¤èµ„æºç®¡ç†å™¨             | ç‹¬ç«‹èŠ‚ç‚¹         |
| **Worker Node**     | å·¥ä½œèŠ‚ç‚¹ï¼Œè¿è¡ŒExecutor     | é›†ç¾¤èŠ‚ç‚¹         |
| **Executor**        | ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œè¿è¡ŒTask       | Worker Node      |

#### åº”ç”¨ç¨‹åºæ¶æ„

**Sparkåº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸ**ï¼š

```mermaid
sequenceDiagram
    participant Client
    participant Driver
    participant ClusterManager as Cluster Manager
    participant Worker
    participant Executor
  
    Client->>Driver: 1. å¯åŠ¨åº”ç”¨ç¨‹åº
    Driver->>ClusterManager: 2. ç”³è¯·èµ„æº
    ClusterManager->>Worker: 3. å¯åŠ¨Executor
    Worker->>Executor: 4. åˆ›å»ºExecutorè¿›ç¨‹
    Executor->>Driver: 5. æ³¨å†Œåˆ°Driver
    Driver->>Driver: 6. æ„å»ºDAG
    Driver->>Executor: 7. åˆ†å‘Task
    Executor->>Executor: 8. æ‰§è¡ŒTask
    Executor->>Driver: 9. è¿”å›ç»“æœ
    Driver->>Client: 10. åº”ç”¨ç¨‹åºå®Œæˆ
```

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### SparkContext

**SparkContext** æ˜¯Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ä¸é›†ç¾¤å»ºç«‹è¿æ¥ã€‚

```scala
// SparkContextæ ¸å¿ƒåŠŸèƒ½
class SparkContext(config: SparkConf) extends Logging {
  
  // 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
  private val env = SparkEnv.createDriverEnv(conf, isLocal, listenerBus, numCores, mockOutputCommitCoordinator)
  private val statusTracker = new SparkStatusTracker(this, sparkUI)
  private val taskScheduler = createTaskScheduler(this, master, deployMode)
  private val dagScheduler = new DAGScheduler(this)
  
  // 2. åˆ›å»ºRDD
  def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = {
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
  
  def textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = {
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions)
      .map(pair => pair._2.toString)
  }
  
  // 3. æäº¤ä½œä¸š
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
  }
  
  // 4. èµ„æºç®¡ç†
  def stop(): Unit = {
    dagScheduler.stop()
    taskScheduler.stop()
    env.stop()
  }
}
```

#### Driver Program

**Driver** æ˜¯è¿è¡Œåº”ç”¨ç¨‹åºmainå‡½æ•°çš„è¿›ç¨‹ï¼Œè´Ÿè´£ï¼š

- **åˆ›å»ºSparkContext**ï¼šåˆå§‹åŒ–Sparkåº”ç”¨ç¨‹åº
- **æ„å»ºé€»è¾‘è®¡åˆ’**ï¼šå°†ç”¨æˆ·ç¨‹åºè½¬æ¢ä¸ºDAG
- **ä»»åŠ¡è°ƒåº¦**ï¼šå°†DAGåˆ†è§£ä¸ºStageå’ŒTask
- **ç»“æœæ”¶é›†**ï¼šæ”¶é›†Executorè¿”å›çš„ç»“æœ

```scala
// Driverç¨‹åºç¤ºä¾‹
object WordCount {
  def main(args: Array[String]): Unit = {
    // 1. åˆ›å»ºSparkContext
    val conf = new SparkConf().setAppName("WordCount")
    val sc = new SparkContext(conf)
  
    try {
      // 2. åˆ›å»ºRDDå¹¶å®šä¹‰è½¬æ¢æ“ä½œ
      val lines = sc.textFile(args(0))
      val words = lines.flatMap(_.split("\\s+"))
      val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
    
      // 3. è§¦å‘Actionï¼Œæäº¤ä½œä¸š
      wordCounts.saveAsTextFile(args(1))
    
    } finally {
      // 4. åœæ­¢SparkContext
      sc.stop()
    }
  }
}
```

#### Cluster Manager

**é›†ç¾¤ç®¡ç†å™¨ç±»å‹**ï¼š


| ç±»å‹           | ç‰¹ç‚¹                | é€‚ç”¨åœºæ™¯             |
| ---------------- | --------------------- | ---------------------- |
| **Standalone** | Sparkå†…ç½®ï¼Œç®€å•æ˜“ç”¨ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡é›†ç¾¤ |
| **YARN**       | Hadoopç”Ÿæ€é›†æˆ      | ä¼ä¸šçº§Hadoopç¯å¢ƒ     |
| **Mesos**      | é€šç”¨èµ„æºç®¡ç†å™¨      | å¤šæ¡†æ¶å…±äº«é›†ç¾¤       |
| **Kubernetes** | å®¹å™¨åŒ–éƒ¨ç½²          | äº‘åŸç”Ÿç¯å¢ƒ           |

**YARNæ¨¡å¼è¯¦è§£**ï¼š

```scala
// YARN Clientæ¨¡å¼
spark-submit \
  --master yarn \
  --deploy-mode client \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar

// YARN Clusteræ¨¡å¼  
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar
```

#### Executor

**Executor** æ˜¯è¿è¡Œåœ¨WorkerèŠ‚ç‚¹ä¸Šçš„JVMè¿›ç¨‹ï¼Œè´Ÿè´£æ‰§è¡ŒTaskã€‚

```scala
// Executoræ ¸å¿ƒç»„ä»¶
class Executor(
    executorId: String,
    executorHostname: String,
    env: SparkEnv,
    userClassPath: Seq[URL] = Nil,
    isLocal: Boolean = false)
  extends Logging {

  // 1. çº¿ç¨‹æ± ç®¡ç†
  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(
    "Executor task launch worker", sparkConf.get(EXECUTOR_CORES), 60)
  
  // 2. å†…å­˜ç®¡ç†
  private val memoryManager = env.memoryManager
  
  // 3. å­˜å‚¨ç®¡ç†
  private val blockManager = env.blockManager
  
  // 4. ä»»åŠ¡æ‰§è¡Œ
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
    val tr = new TaskRunner(context, taskDescription)
    runningTasks.put(taskDescription.taskId, tr)
    threadPool.execute(tr)
  }
  
  // 5. ä»»åŠ¡è¿è¡Œå™¨
  class TaskRunner(
      execBackend: ExecutorBackend,
      private val taskDescription: TaskDescription)
    extends Runnable {
  
    override def run(): Unit = {
      try {
        // ååºåˆ—åŒ–ä»»åŠ¡
        val task = ser.deserialize[Task[Any]](taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)
      
        // æ‰§è¡Œä»»åŠ¡
        val value = task.run(
          taskAttemptId = taskDescription.taskId,
          attemptNumber = taskDescription.attemptNumber,
          metricsSystem = env.metricsSystem)
      
        // è¿”å›ç»“æœ
        execBackend.statusUpdate(taskDescription.taskId, TaskState.FINISHED, ser.serialize(value))
      
      } catch {
        case e: Exception =>
          execBackend.statusUpdate(taskDescription.taskId, TaskState.FAILED, ser.serialize(TaskFailedReason))
      }
    }
  }
}
```

### ä»»åŠ¡è°ƒåº¦åŸç†

#### DAGSchedulerè°ƒåº¦

**DAGScheduler** æ˜¯Sparkä½œä¸šè°ƒåº¦çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†ç”¨æˆ·æäº¤çš„RDD DAGåˆ†è§£ä¸ºå¤šä¸ªStageï¼Œå¹¶æŒ‰ç…§ä¾èµ–å…³ç³»é¡ºåºæäº¤ç»™TaskScheduleræ‰§è¡Œã€‚å®ƒæ˜¯è¿æ¥é«˜å±‚RDDæ“ä½œå’Œåº•å±‚ä»»åŠ¡æ‰§è¡Œçš„å…³é”®æ¡¥æ¢ã€‚

**ä¸»è¦åŠŸèƒ½**ï¼š

- **DAGæ„å»ºä¸åˆ†æ**ï¼šå°†RDDçš„è½¬æ¢æ“ä½œæ„å»ºæˆæœ‰å‘æ— ç¯å›¾
- **Stageåˆ’åˆ†**ï¼šæ ¹æ®å®½ä¾èµ–è¾¹ç•Œå°†DAGåˆ’åˆ†ä¸ºå¤šä¸ªStage
- **ä»»åŠ¡è°ƒåº¦**ï¼šæŒ‰ç…§Stageä¾èµ–å…³ç³»è¿›è¡Œæ‹“æ‰‘æ’åºå’Œè°ƒåº¦
- **å®¹é”™å¤„ç†**ï¼šå¤„ç†ä»»åŠ¡å¤±è´¥ã€Stageé‡è¯•ç­‰å®¹é”™é€»è¾‘
- **èµ„æºç®¡ç†**ï¼šä¸TaskScheduleråè°ƒè¿›è¡Œèµ„æºåˆ†é…

```mermaid
graph TD
    A[ç”¨æˆ·è°ƒç”¨Action] --> B[SparkContext.runJob]
    B --> C[DAGScheduler.runJob]
    C --> D[åˆ›å»ºActiveJob]
    D --> E[æäº¤JobSubmittedäº‹ä»¶]
    E --> F[handleJobSubmitted]
    F --> G[åˆ›å»ºResultStage]
    G --> H[getOrCreateParentStages]
    H --> I[é€’å½’åˆ†æRDDä¾èµ–]
    I --> J{ä¾èµ–ç±»å‹åˆ¤æ–­}
    J -->|çª„ä¾èµ–| K[ç»§ç»­å‘ä¸Šéå†]
    J -->|å®½ä¾èµ–| L[åˆ›å»ºShuffleMapStage]
    K --> I
    L --> M[submitStage]
    M --> N[getMissingParentStages]
    N --> O{çˆ¶Stageæ˜¯å¦å®Œæˆ}
    O -->|æœªå®Œæˆ| P[é€’å½’æäº¤çˆ¶Stage]
    O -->|å·²å®Œæˆ| Q[submitMissingTasks]
    P --> M
    Q --> R[åˆ›å»ºTaskSet]
    R --> S[TaskScheduler.submitTasks]
    S --> T[ä»»åŠ¡åˆ†å‘ä¸æ‰§è¡Œ]
    T --> U[Stageå®Œæˆ]
    U --> V[æ£€æŸ¥åç»­Stage]
    V --> W[Jobå®Œæˆ]
  
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style L fill:#ffebee
    style W fill:#c8e6c9
```

**DAGScheduleræ¶æ„ç»„ä»¶**ï¼š


| ç»„ä»¶             | ç±»å                                      | ä¸»è¦èŒè´£            | å…³é”®æ–¹æ³•                       |
| ------------------ | ------------------------------------------- | --------------------- | -------------------------------- |
| **DAGScheduler** | `DAGScheduler`                            | ä½œä¸šè°ƒåº¦å’ŒStageåˆ’åˆ† | `submitJob`, `submitStage`     |
| **EventLoop**    | `DAGSchedulerEventProcessLoop`            | äº‹ä»¶å¤„ç†å¾ªç¯        | `post`, `onReceive`            |
| **Stage**        | `Stage`, `ResultStage`, `ShuffleMapStage` | StageæŠ½è±¡           | `findMissingPartitions`        |
| **Job**          | `ActiveJob`                               | ä½œä¸šæŠ½è±¡            | `numFinished`, `numPartitions` |

**äº‹ä»¶å¤„ç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[DAGScheduleräº‹ä»¶] --> B[EventProcessLoop]
    B --> C{äº‹ä»¶ç±»å‹}
    C -->|JobSubmitted| D[handleJobSubmitted]
    C -->|StageCompleted| E[handleStageCompleted]
    C -->|TaskCompleted| F[handleTaskCompleted]
    C -->|TaskFailed| G[handleTaskFailed]
    C -->|ExecutorLost| H[handleExecutorLost]
  
    D --> I[åˆ›å»ºStage]
    E --> J[æ£€æŸ¥åç»­Stage]
    F --> K[æ›´æ–°StageçŠ¶æ€]
    G --> L[é‡è¯•æˆ–å¤±è´¥å¤„ç†]
    H --> M[é‡æ–°æäº¤å—å½±å“Stage]
  
    I --> N[æäº¤Stage]
    J --> N
    K --> O[Stageå®Œæˆæ£€æŸ¥]
    L --> P[å®¹é”™å¤„ç†]
    M --> N
  
    style B fill:#e1f5fe
    style N fill:#e8f5e8
    style P fill:#ffebee
```

```scala
// DAGScheduleräº‹ä»¶ç±»å‹å®šä¹‰
sealed trait DAGSchedulerEvent

case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) extends DAGSchedulerEvent

case class StageCompleted(stage: Stage) extends DAGSchedulerEvent
case class TaskCompleted(task: Task[_], reason: TaskEndReason) extends DAGSchedulerEvent
case class TaskFailed(taskId: Long, reason: TaskFailedReason) extends DAGSchedulerEvent
case class ExecutorLost(execId: String, reason: ExecutorLossReason) extends DAGSchedulerEvent

// äº‹ä»¶å¤„ç†å¾ªç¯å®ç°
class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    val timerContext = dagScheduler.metricsSource.messageProcessingTimer.time()
    try {
      doOnReceive(event)
    } finally {
      timerContext.stop()
    }
  }
  
  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
    
    case StageCompleted(stage) =>
      dagScheduler.handleStageCompleted(stage)
    
    case TaskCompleted(task, reason) =>
      dagScheduler.handleTaskCompleted(task, reason)
    
    case TaskFailed(taskId, reason) =>
      dagScheduler.handleTaskFailed(taskId, reason)
    
    case ExecutorLost(execId, reason) =>
      dagScheduler.handleExecutorLost(execId, reason)
  }
}
```

**DAGSchedulerä¸»è¦æºç **ï¼š

```scala
// DAGScheduler.scala - æ ¸å¿ƒè°ƒåº¦é€»è¾‘
class DAGScheduler(
    private[scheduler] val sc: SparkContext,
    private[scheduler] val taskScheduler: TaskScheduler,
    listenerBus: LiveListenerBus,
    mapOutputTracker: MapOutputTrackerMaster,
    blockManagerMaster: BlockManagerMaster,
    env: SparkEnv,
    clock: Clock = new SystemClock())
  extends Logging {

  // äº‹ä»¶å¤„ç†å¾ªç¯
  private val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  
  // Stageå’ŒJobç®¡ç†
  private val stageIdToStage = new HashMap[Int, Stage]
  private val shuffleIdToMapStage = new HashMap[Int, ShuffleMapStage]
  private val jobIdToActiveJob = new HashMap[Int, ActiveJob]
  private val activeJobs = new HashSet[ActiveJob]
  
  // ç­‰å¾…å’Œè¿è¡Œä¸­çš„Stage
  private val waitingStages = new HashSet[Stage]
  private val runningStages = new HashSet[Stage]
  private val failedStages = new HashSet[Stage]
  
  // 1. æäº¤ä½œä¸šçš„æ ¸å¿ƒæ–¹æ³•
  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): Unit = {
  
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    ThreadUtils.awaitReady(waiter, Duration.Inf)
  
    waiter.value.get match {
      case scala.util.Success(_) =>
        logInfo("Job %d finished: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =>
        logInfo("Job %d failed: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        throw exception
    }
  }
  
  // 2. æäº¤ä½œä¸š
  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
  
    // æ£€æŸ¥åˆ†åŒºæœ‰æ•ˆæ€§
    val maxPartitions = rdd.partitions.length
    partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>
      throw new IllegalArgumentException(
        "Attempting to access a non-existent partition: " + p + ". " +
          "Total number of partitions: " + maxPartitions)
    }
  
    val jobId = nextJobId.getAndIncrement()
    if (partitions.size == 0) {
      // ç©ºåˆ†åŒºç›´æ¥è¿”å›
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }
  
    assert(partitions.size > 0)
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
  
    // æäº¤JobSubmittedäº‹ä»¶
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter, properties))
    waiter
  }
  
  // 3. å¤„ç†ä½œä¸šæäº¤
  private[scheduler] def handleJobSubmitted(jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties): Unit = {
  
    var finalStage: ResultStage = null
    try {
      // åˆ›å»ºResultStage
      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
    } catch {
      case e: Exception =>
        logWarning("Creating new stage failed due to exception - job: " + jobId, e)
        listener.jobFailed(e)
        return
    }
  
    // åˆ›å»ºActiveJob
    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
    clearCacheLocs()
  
    logInfo("Got job %s (%s) with %d output partitions".format(
      job.jobId, callSite.shortForm, partitions.length))
    logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
    logInfo("Parents of final stage: " + finalStage.parents)
    logInfo("Missing parents: " + getMissingParentStages(finalStage))
  
    val jobSubmissionTime = clock.getTimeMillis()
    jobIdToActiveJob(jobId) = job
    activeJobs += job
    finalStage.setActiveJob(job)
  
    val stageIds = jobIdToStageIds(jobId)
    val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))
    listenerBus.post(
      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))
  
    // æäº¤Stage
    submitStage(finalStage)
  }
  
  // 4. Stageåˆ’åˆ†æ ¸å¿ƒç®—æ³•
  private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
    val parents = new ArrayBuffer[Stage]()
    val visited = new HashSet[RDD[_]]
  
    def visit(r: RDD[_]): Unit = {
      if (!visited(r)) {
        visited += r
        for (dep <- r.dependencies) {
          dep match {
            case shufDep: ShuffleDependency[_, _, _] =>
              // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
              parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
            case _ =>
              // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
              visit(dep.rdd)
          }
        }
      }
    }
  
    visit(rdd)
    parents.toList
  }
  
  // 5. åˆ›å»ºæˆ–è·å–ShuffleMapStage
  private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
  
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      case Some(stage) =>
        stage
      
      case None =>
        // é€’å½’åˆ›å»ºçˆ¶Stage
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }
  
  // 6. æäº¤Stage
  private def submitStage(stage: Stage): Unit = {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug("submitStage(" + stage + ")")
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug("missing: " + missing)
        if (missing.isEmpty) {
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent <- missing) {
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, "No active job for stage " + stage.id, None)
    }
  }
  
  // 7. æäº¤ç¼ºå¤±çš„ä»»åŠ¡
  private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
    logDebug("submitMissingTasks(" + stage + ")")
  
    // è·å–ç¼ºå¤±çš„åˆ†åŒº
    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
  
    // æ·»åŠ åˆ°è¿è¡Œä¸­çš„Stage
    runningStages += stage
  
    // åˆ›å»ºä»»åŠ¡
    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = stage.rdd.partitions(id)
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
              taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, serializedTaskMetrics)
          }
        
        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = stage.rdd.partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptId,
              taskBinary, part, locs, id, properties, serializedTaskMetrics)
          }
      }
    } catch {
      case NonFatal(e) =>
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }
  
    if (tasks.size > 0) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${partitionsToCompute.take(15).mkString(", ")})")
    
      // æäº¤TaskSetç»™TaskScheduler
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
    } else {
      // æ²¡æœ‰ä»»åŠ¡éœ€è¦è¿è¡Œï¼Œæ ‡è®°Stageå®Œæˆ
      markStageAsFinished(stage, None)
    
      val debugString = stage match {
        case stage: ShuffleMapStage =>
          s"Stage ${stage} is actually done; " +
            s"(available: ${stage.isAvailable}," +
            s"available outputs: ${stage.numAvailableOutputs}," +
            s"partitions: ${stage.numPartitions})"
        case stage : ResultStage =>
          s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})"
      }
      logDebug(debugString)
    
      submitWaitingChildStages(stage)
    }
  }
  
  // 8. å¤„ç†ä»»åŠ¡å®Œæˆ
  def handleTaskCompletion(event: CompletionEvent): Unit = {
    val task = event.task
    val taskId = event.taskInfo.taskId
    val stageId = task.stageId
    val taskType = Utils.getFormattedClassName(task)
  
    // æ›´æ–°ç´¯åŠ å™¨
    event.accumUpdates.foreach { case (id, partialValue) =>
      val acc = AccumulatorContext.get(id)
      if (acc != null) {
        acc.asInstanceOf[AccumulatorV2[Any, Any]].merge(partialValue.asInstanceOf[Any])
      }
    }
  
    // å¤„ç†ä¸åŒçš„ä»»åŠ¡ç»“æœ
    event.reason match {
      case Success =>
        task match {
          case rt: ResultTask[_, _] =>
            // ResultTaskå®Œæˆ
            val resultStage = stage.asInstanceOf[ResultStage]
            resultStage.activeJob match {
              case Some(job) =>
                if (!job.finished(rt.outputId)) {
                  job.finished(rt.outputId) = true
                  job.numFinished += 1
                
                  // è°ƒç”¨ç»“æœå¤„ç†å™¨
                  job.listener.taskSucceeded(rt.outputId, event.result)
                
                  // æ£€æŸ¥Jobæ˜¯å¦å®Œæˆ
                  if (job.numFinished == job.numPartitions) {
                    markStageAsFinished(resultStage)
                    cleanupStateForJobAndIndependentStages(job)
                    listenerBus.post(SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded))
                  }
                }
              case None =>
                logInfo("Ignoring result from " + rt + " because its job has finished")
            }
          
          case smt: ShuffleMapTask =>
            // ShuffleMapTaskå®Œæˆ
            val shuffleStage = stage.asInstanceOf[ShuffleMapStage]
            shuffleStage.addOutputLoc(smt.partitionId, event.result.asInstanceOf[MapStatus])
          
            if (runningStages.contains(shuffleStage) && shuffleStage.pendingPartitions.isEmpty) {
              markStageAsFinished(shuffleStage)
              logInfo("looking for newly runnable stages")
              logInfo("running: " + runningStages)
              logInfo("waiting: " + waitingStages)
              logInfo("failed: " + failedStages)
            
              // æäº¤ç­‰å¾…ä¸­çš„å­Stage
              submitWaitingChildStages(shuffleStage)
            }
        }
      
      case _: TaskFailedException =>
        // ä»»åŠ¡å¤±è´¥å¤„ç†
        handleTaskFailure(task, event.reason.asInstanceOf[TaskFailedException])
    }
  }
}
```

**Stageåˆ’åˆ†ç®—æ³•è¯¦è§£**

**Stageåˆ’åˆ†åŸç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[å¼€å§‹Stageåˆ’åˆ†] --> B[ä»æœ€ç»ˆRDDå¼€å§‹]
    B --> C[éå†RDDä¾èµ–]
    C --> D{ä¾èµ–ç±»å‹}
    D -->|çª„ä¾èµ–| E[åŠ å…¥å½“å‰Stage]
    D -->|å®½ä¾èµ–| F[åˆ›å»ºæ–°Stageè¾¹ç•Œ]
    E --> G[ç»§ç»­éå†çˆ¶RDD]
    F --> H[åˆ›å»ºShuffleMapStage]
    G --> C
    H --> I[é€’å½’å¤„ç†çˆ¶RDD]
    I --> C
    C --> J{æ˜¯å¦è¿˜æœ‰æœªå¤„ç†RDD}
    J -->|æ˜¯| C
    J -->|å¦| K[Stageåˆ’åˆ†å®Œæˆ]
  
    style A fill:#e1f5fe
    style F fill:#ffebee
    style H fill:#fff3e0
    style K fill:#e8f5e8
```

**Stageåˆ’åˆ†ä¸ä¾èµ–ç®¡ç†æºç **ï¼š

```scala
// Stageåˆ’åˆ†æ ¸å¿ƒé€»è¾‘
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}

// æŸ¥æ‰¾ç¼ºå¤±çš„çˆ¶ä¾èµ–
private def getMissingAncestorShuffleDependencies(
    rdd: RDD[_]): ArrayStack[ShuffleDependency[_, _, _]] = {
  val ancestors = new ArrayStack[ShuffleDependency[_, _, _]]
  val visited = new HashSet[RDD[_]]
  val waitingForVisit = new ArrayStack[RDD[_]]
  
  waitingForVisit.push(rdd)
  while (waitingForVisit.nonEmpty) {
    val toVisit = waitingForVisit.pop()
    if (!visited(toVisit)) {
      visited += toVisit
      toVisit.dependencies.foreach {
        case shuffleDep: ShuffleDependency[_, _, _] =>
          if (!shuffleIdToMapStage.contains(shuffleDep.shuffleId)) {
            ancestors.push(shuffleDep)
            waitingForVisit.push(shuffleDep.rdd)
          }
        case narrowDep: NarrowDependency[_] =>
          waitingForVisit.push(narrowDep.rdd)
      }
    }
  }
  ancestors
}

// åˆ›å»ºResultStage
private def createResultStage(
    rdd: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    jobId: Int,
    callSite: CallSite): ResultStage = {
  
  val parents = getOrCreateParentStages(rdd, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
  stageIdToStage(id) = stage
  updateJobIdStageIdMaps(jobId, stage)
  stage
}

// åˆ›å»ºShuffleMapStage
private def createShuffleMapStage(shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage = {
  val rdd = shuffleDep.rdd
  val numTasks = rdd.partitions.length
  val parents = getOrCreateParentStages(rdd, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ShuffleMapStage(id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep)
  
  stageIdToStage(id) = stage
  shuffleIdToMapStage(shuffleDep.shuffleId) = stage
  updateJobIdStageIdMaps(jobId, stage)
  
  if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
    logInfo("Registering RDD " + rdd.id + " (" + rdd.getCreationSite + ") as input to " +
      "shuffle " + shuffleDep.shuffleId)
    mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)
  }
  stage
}
```

**å®¹é”™å¤„ç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[Taskæ‰§è¡Œå¤±è´¥] --> B{å¤±è´¥åŸå› åˆ†æ}
    B -->|FetchFailed| C[Shuffleæ•°æ®è·å–å¤±è´¥]
    B -->|TaskKilled| D[ä»»åŠ¡è¢«æ€æ­»]
    B -->|ExceptionFailure| E[ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸]
    B -->|ExecutorLostFailure| F[Executorä¸¢å¤±]
  
    C --> G[æ ‡è®°çˆ¶Stageå¤±è´¥]
    G --> H[é‡æ–°æäº¤çˆ¶Stage]
    H --> I[é‡æ–°è®¡ç®—Shuffleæ•°æ®]
  
    D --> J[æ£€æŸ¥é‡è¯•æ¬¡æ•°]
    E --> J
    J --> K{æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•}
    K -->|å¦| L[é‡æ–°è°ƒåº¦Task]
    K -->|æ˜¯| M[æ ‡è®°Stageå¤±è´¥]
  
    F --> N[ç§»é™¤ä¸¢å¤±çš„Executor]
    N --> O[é‡æ–°åˆ†é…èµ„æº]
    O --> P[é‡æ–°æäº¤æ‰€æœ‰Task]
  
    L --> Q[Taské‡æ–°æ‰§è¡Œ]
    I --> Q
    P --> Q
    M --> R[Jobå¤±è´¥]
    Q --> S[æ‰§è¡ŒæˆåŠŸ]
  
    style A fill:#ffebee
    style C fill:#fff3e0
    style R fill:#ffcdd2
    style S fill:#e8f5e8
```

```mermaid
graph TD
    A[RDD DAG] --> B[DAGScheduler]
    B --> C[Stageåˆ’åˆ†]
    C --> D[Stage 0<br/>ShuffleMapStage]
    C --> E[Stage 1<br/>ShuffleMapStage]  
    C --> F[Stage 2<br/>ResultStage]
  
    D --> G[Task 0-1]
    D --> H[Task 0-2]
    E --> I[Task 1-1]
    E --> J[Task 1-2]
    F --> K[Task 2-1]
    F --> L[Task 2-2]
  
    style B fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#e8f5e8
```

**DAGScheduleræ¶æ„ç»„ä»¶**ï¼š


| ç»„ä»¶             | ç±»å                                      | ä¸»è¦èŒè´£            | å…³é”®æ–¹æ³•                       |
| ------------------ | ------------------------------------------- | --------------------- | -------------------------------- |
| **DAGScheduler** | `DAGScheduler`                            | ä½œä¸šè°ƒåº¦å’ŒStageåˆ’åˆ† | `submitJob`, `submitStage`     |
| **EventLoop**    | `DAGSchedulerEventProcessLoop`            | äº‹ä»¶å¤„ç†å¾ªç¯        | `post`, `onReceive`            |
| **Stage**        | `Stage`, `ResultStage`, `ShuffleMapStage` | StageæŠ½è±¡           | `findMissingPartitions`        |
| **Job**          | `ActiveJob`                               | ä½œä¸šæŠ½è±¡            | `numFinished`, `numPartitions` |

**DAGScheduleräº‹ä»¶å¤„ç†**ï¼š

```scala
// DAGScheduleräº‹ä»¶ç±»å‹
sealed trait DAGSchedulerEvent

case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) extends DAGSchedulerEvent

case class StageCompleted(stage: Stage) extends DAGSchedulerEvent
case class TaskCompleted(task: Task[_], reason: TaskEndReason) extends DAGSchedulerEvent

// äº‹ä»¶å¤„ç†å¾ªç¯
class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    event match {
      case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
        dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
      case StageCompleted(stage) =>
        dagScheduler.handleStageCompletion(stage)
      case TaskCompleted(task, reason) =>
        dagScheduler.handleTaskCompletion(task, reason)
    }
  }
}
```

**Stageåˆ’åˆ†ä¸ä¾èµ–ç®¡ç†**ï¼š

```scala
// Stageåˆ’åˆ†æ ¸å¿ƒé€»è¾‘
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}
```

#### TaskSchedulerè°ƒåº¦

**TaskScheduler** è´Ÿè´£å°†Taskåˆ†å‘åˆ°Executoræ‰§è¡Œï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ã€‚

```mermaid
graph TD
    A[DAGScheduleræäº¤TaskSet] --> B[TaskSchedulerImpl.submitTasks]
    B --> C[åˆ›å»ºTaskSetManager]
    C --> D[æ·»åŠ åˆ°è°ƒåº¦æ± Pool]
    D --> E[SchedulerBackend.reviveOffers]
    E --> F[æ”¶é›†Executorèµ„æºä¿¡æ¯]
    F --> G[TaskSchedulerImpl.resourceOffers]
    G --> H[æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…Task]
    H --> I[åˆ›å»ºTaskDescription]
    I --> J[SchedulerBackend.launchTasks]
    J --> K[å‘é€Taskåˆ°Executor]
    K --> L[Executoræ‰§è¡ŒTask]
    L --> M[è¿”å›æ‰§è¡Œç»“æœ]
    M --> N[TaskScheduler.statusUpdate]
    N --> O[æ›´æ–°TaskçŠ¶æ€]
    O --> P{TaskSetæ˜¯å¦å®Œæˆ}
    P -->|å¦| Q[ç»§ç»­è°ƒåº¦å‰©ä½™Task]
    P -->|æ˜¯| R[é€šçŸ¥DAGScheduler]
    Q --> G
    R --> S[Stageå®Œæˆ]
  
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style L fill:#e8f5e8
    style S fill:#c8e6c9
```

**TaskScheduleræ¶æ„ç»„ä»¶**


| ç»„ä»¶                 | ç±»å                            | ä¸»è¦èŒè´£         | å…³é”®ç‰¹æ€§               |
| ---------------------- | --------------------------------- | ------------------ | ------------------------ |
| **TaskScheduler**    | `TaskSchedulerImpl`             | ä»»åŠ¡è°ƒåº¦å’Œåˆ†å‘   | æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥       |
| **SchedulerBackend** | `CoarseGrainedSchedulerBackend` | ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡ | èµ„æºåˆ†é…å’ŒExecutorç®¡ç† |
| **TaskSetManager**   | `TaskSetManager`                | ç®¡ç†TaskSetæ‰§è¡Œ  | ä»»åŠ¡é‡è¯•ã€æ¨æµ‹æ‰§è¡Œ     |
| **Pool**             | `Pool`                          | è°ƒåº¦æ± ç®¡ç†       | å…¬å¹³è°ƒåº¦ã€FIFOè°ƒåº¦     |

TaskSchedulerImplæ ¸å¿ƒæºç å®ç°

```scala
// TaskSchedulerImpl.scala - æ ¸å¿ƒä»»åŠ¡è°ƒåº¦å™¨å®ç°
class TaskSchedulerImpl(
    val sc: SparkContext,
    val maxTaskFailures: Int,
    isLocal: Boolean = false)
  extends TaskScheduler with Logging {

  // è°ƒåº¦åç«¯ï¼Œè´Ÿè´£ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡
  var backend: SchedulerBackend = null
  
  // æ ¹è°ƒåº¦æ± ï¼Œç®¡ç†æ‰€æœ‰TaskSetManager
  val rootPool: Pool = new Pool("", SchedulingMode.FIFO, 0, 0)
  
  // è°ƒåº¦æ„å»ºå™¨ï¼Œè´Ÿè´£æ„å»ºè°ƒåº¦æ ‘
  var schedulableBuilder: SchedulableBuilder = null
  
  // æ­£åœ¨è¿è¡Œçš„TaskSetç®¡ç†å™¨
  private val taskSetsByStageIdAndAttempt = new HashMap[Int, HashMap[Int, TaskSetManager]]
  
  // 1. æäº¤TaskSetçš„æ ¸å¿ƒæ–¹æ³•
  override def submitTasks(taskSet: TaskSet): Unit = synchronized {
    val tasks = taskSet.tasks
    logInfo(s"Adding task set ${taskSet.id} with ${tasks.length} tasks")
  
    // åˆ›å»ºTaskSetManageræ¥ç®¡ç†è¿™ä¸ªTaskSet
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    val stage = taskSet.stageId
    val stageTaskSets = taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
  
    // æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„TaskSet
    stageTaskSets.foreach { case (_, ts) =>
      ts.isZombie = true
    }
    stageTaskSets(taskSet.stageAttemptId) = manager
  
    // å°†TaskSetManageræ·»åŠ åˆ°è°ƒåº¦æ± 
    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
  
    // å¦‚æœä¸æ˜¯æœ¬åœ°æ¨¡å¼ï¼Œè§¦å‘èµ„æºåˆ†é…
    if (!isLocal && manager.tasks.length > 0) {
      backend.reviveOffers()
    }
  }
  
  // 2. èµ„æºåˆ†é…çš„æ ¸å¿ƒæ–¹æ³•
  def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
    // æ ‡è®°æ‰€æœ‰Executorä¸ºæ´»è·ƒçŠ¶æ€
    var newExecAvail = false
    for (o <- offers) {
      if (!hostToExecutors.contains(o.host)) {
        hostToExecutors(o.host) = new HashSet[String]()
      }
      if (!executorIdToRunningTaskIds.contains(o.executorId)) {
        hostToExecutors(o.host) += o.executorId
        executorAdded(o.executorId, o.host)
        executorIdToHost(o.executorId) = o.host
        executorIdToRunningTaskIds(o.executorId) = HashSet[Long]()
        newExecAvail = true
      }
    }
  
    // éšæœºæ‰“ä¹±offersï¼Œé¿å…æ€»æ˜¯åœ¨åŒä¸€ä¸ªExecutorä¸Šåˆ†é…ä»»åŠ¡
    val shuffledOffers = Random.shuffle(offers)
  
    // ä¸ºæ¯ä¸ªofferåˆ›å»ºä»»åŠ¡åˆ—è¡¨
    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK))
    val availableCpus = shuffledOffers.map(o => o.cores).toArray
  
    // è·å–æ’åºåçš„TaskSeté˜Ÿåˆ—
    val sortedTaskSets = rootPool.getSortedTaskSetQueue
  
    // æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…ä»»åŠ¡
    for (taskSet <- sortedTaskSets) {
      var launchedAnyTask = false
    
      // éå†æ‰€æœ‰æœ¬åœ°æ€§çº§åˆ«ï¼šPROCESS_LOCAL -> NODE_LOCAL -> NO_PREF -> RACK_LOCAL -> ANY
      for (currentMaxLocality <- taskSet.myLocalityLevels) {
        do {
          launchedAnyTask = resourceOfferSingleTaskSet(
            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)
        } while (launchedAnyTask)
      }
    }
  
    // å¦‚æœæœ‰æ–°çš„Executorå¯ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ¨æµ‹æ‰§è¡Œçš„ä»»åŠ¡
    if (newExecAvail) {
      for (taskSet <- sortedTaskSets) {
        taskSet.executorAdded()
      }
    }
  
    return tasks
  }
  
  // 3. å•ä¸ªTaskSetçš„èµ„æºåˆ†é…
  private def resourceOfferSingleTaskSet(
      taskSet: TaskSetManager,
      maxLocality: TaskLocality,
      shuffledOffers: Seq[WorkerOffer],
      availableCpus: Array[Int],
      tasks: IndexedSeq[ArrayBuffer[TaskDescription]]): Boolean = {
  
    var launchedTask = false
  
    // éå†æ‰€æœ‰å¯ç”¨çš„Executor
    for (i <- 0 until shuffledOffers.length) {
      val execId = shuffledOffers(i).executorId
      val host = shuffledOffers(i).host
    
      // æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„CPUæ ¸å¿ƒ
      if (availableCpus(i) >= CPUS_PER_TASK) {
        try {
          // å°è¯•åœ¨å½“å‰Executorä¸Šå¯åŠ¨ä»»åŠ¡
          for (task <- taskSet.resourceOffer(execId, host, maxLocality)) {
            tasks(i) += task
            val tid = task.taskId
            taskIdToTaskSetManager(tid) = taskSet
            taskIdToExecutorId(tid) = execId
            executorIdToRunningTaskIds(execId) += tid
            availableCpus(i) -= CPUS_PER_TASK
            assert(availableCpus(i) >= 0)
            launchedTask = true
          }
        } catch {
          case e: TaskNotSerializableException =>
            logError(s"Resource offer failed, task set ${taskSet.name} was not serializable")
            taskSet.abort("TaskSet %s was not serializable".format(taskSet.name))
            return launchedTask
        }
      }
    }
    return launchedTask
  }
  
  // 4. ä»»åŠ¡çŠ¶æ€æ›´æ–°å¤„ç†
  def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer): Unit = {
    var failedExecutor: Option[String] = None
    var reason: Option[TaskFailedReason] = None
  
    synchronized {
      try {
        taskIdToTaskSetManager.get(tid) match {
          case Some(taskSet) =>
            if (state == TaskState.LOST) {
              // ä»»åŠ¡ä¸¢å¤±ï¼Œå¯èƒ½æ˜¯Executorå¤±è´¥
              val execId = taskIdToExecutorId.getOrElse(tid, throw new IllegalStateException(
                "taskIdToTaskSetManager.contains(tid) <=> taskIdToExecutorId.contains(tid)"))
              if (executorIdToRunningTaskIds.contains(execId)) {
                reason = Some(ExecutorLostFailure(execId, exitCausedByApp = false,
                  Some("Task $tid was lost, so marking the executor as lost as well.")))
                removeExecutor(execId, reason.get)
                failedExecutor = Some(execId)
              }
            }
          
            if (TaskState.isFinished(state)) {
              cleanupTaskState(tid)
              taskSet.removeRunningTask(tid)
              if (state == TaskState.FINISHED) {
                taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)
              } else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {
                taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)
              }
            }
          case None =>
            logError(
              ("Ignoring update with state %s for TID %s because its task set is gone (this is " +
               "likely the result of receiving duplicate task finished status updates) or its " +
               "executor has been marked as failed.")
                .format(state, tid))
        }
      } catch {
        case e: Exception => logError("Exception in statusUpdate", e)
      }
    }
  
    // å¦‚æœæœ‰Executorå¤±è´¥ï¼Œæ›´æ–°DAGScheduler
    if (failedExecutor.isDefined) {
      assert(reason.isDefined)
      dagScheduler.executorLost(failedExecutor.get, reason.get)
      backend.reviveOffers()
    }
  }
  
  // 5. åˆ›å»ºTaskSetManager
  private def createTaskSetManager(
      taskSet: TaskSet,
      maxTaskFailures: Int): TaskSetManager = {
    new TaskSetManager(this, taskSet, maxTaskFailures, blacklistTrackerOpt)
  }
  
  // 6. æ¸…ç†ä»»åŠ¡çŠ¶æ€
  private def cleanupTaskState(tid: Long): Unit = {
    taskIdToTaskSetManager.remove(tid)
    taskIdToExecutorId.remove(tid).foreach { execId =>
      executorIdToRunningTaskIds.get(execId).foreach { _.remove(tid) }
    }
  }
  
  // 7. ç§»é™¤å¤±è´¥çš„Executor
  private def removeExecutor(executorId: String, reason: ExecutorLossReason): Unit = {
    executorIdToRunningTaskIds.remove(executorId).foreach { taskIds =>
      logDebug("Cleaning up TaskScheduler state for tasks " +
        s"${taskIds.mkString("[", ",", "]")} on failed executor $executorId")
      // å°†è¿è¡Œåœ¨å¤±è´¥Executorä¸Šçš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
      taskIds.foreach { tid =>
        val taskSetMgr = taskIdToTaskSetManager(tid)
        if (taskSetMgr != null) {
          taskSetMgr.executorLost(executorId, tid, reason)
        }
      }
    }
  
    val host = executorIdToHost(executorId)
    val execs = hostToExecutors.getOrElse(host, new HashSet)
    execs -= executorId
    if (execs.isEmpty) {
      hostToExecutors -= host
    }
    executorIdToHost -= executorId
  
    rootPool.executorLost(executorId, host, reason)
  }
}
```

**TaskSetManageræºç å®ç°**

```scala
// TaskSetManager.scala - ç®¡ç†å•ä¸ªTaskSetçš„æ‰§è¡Œ
private[spark] class TaskSetManager(
    sched: TaskSchedulerImpl,
    val taskSet: TaskSet,
    val maxTaskFailures: Int,
    blacklistTracker: Option[BlacklistTracker] = None)
  extends Schedulable with Logging {

  // TaskSetä¸­çš„æ‰€æœ‰ä»»åŠ¡
  val tasks = taskSet.tasks
  val numTasks = tasks.length
  
  // ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
  private val copiesRunning = new Array[Int](numTasks)
  private val successful = new Array[Boolean](numTasks)
  private val numFailures = new Array[Int](numTasks)
  
  // æœ¬åœ°æ€§çº§åˆ«ç®¡ç†
  private val myLocalityLevels = computeValidLocalityLevels()
  private val localityWaits = myLocalityLevels.map(getLocalityWait)
  
  // å¾…è°ƒåº¦ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†ç»„ï¼‰
  private val pendingTasksForExecutor = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksForHost = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksForRack = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksWithNoPrefs = new ArrayBuffer[Int]
  private val allPendingTasks = new ArrayBuffer[Int]
  
  // 1. èµ„æºåˆ†é…çš„æ ¸å¿ƒæ–¹æ³•
  def resourceOffer(
      execId: String,
      host: String,
      maxLocality: TaskLocality): Option[TaskDescription] = {
  
    val offerBlacklisted = blacklistTracker.exists(_.isExecutorBlacklisted(execId)) ||
      blacklistTracker.exists(_.isNodeBlacklisted(host))
  
    if (!isZombie && !offerBlacklisted) {
      val curTime = clock.getTimeMillis()
    
      var allowedLocality = maxLocality
    
      if (maxLocality != TaskLocality.NO_PREF) {
        allowedLocality = getAllowedLocalityLevel(curTime)
        if (allowedLocality > maxLocality) {
          // å¦‚æœå…è®¸çš„æœ¬åœ°æ€§çº§åˆ«æ¯”æä¾›çš„çº§åˆ«æ›´å®½æ¾ï¼Œåˆ™ä½¿ç”¨æä¾›çš„çº§åˆ«
          allowedLocality = maxLocality
        }
      }
    
      dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
        // æ‰¾åˆ°äº†å¯ä»¥è°ƒåº¦çš„ä»»åŠ¡
        val task = tasks(index)
        val taskId = sched.newTaskId()
      
        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
        copiesRunning(index) += 1
        successful(index) = false
      
        val attemptNum = taskAttempts(index).size
        val info = new TaskInfo(taskId, index, attemptNum, curTime,
          execId, host, taskLocality, speculative)
        taskInfos(taskId) = info
        taskAttempts(index) = info :: taskAttempts(index)
      
        // æ›´æ–°è¿è¡Œä»»åŠ¡ç»Ÿè®¡
        if (maxLocality == TaskLocality.NO_PREF) {
          stats.incNoPrefs(1)
        } else {
          stats.incLocality(taskLocality)
        }
      
        // åºåˆ—åŒ–ä»»åŠ¡
        val serializedTask: ByteBuffer = try {
          ser.serialize(task)
        } catch {
          case NonFatal(e) =>
            val msg = s"Failed to serialize task $taskId, not attempting to retry it."
            logError(msg, e)
            abort(s"$msg Exception during serialization: $e")
            throw new TaskNotSerializableException(e)
        }
      
        if (serializedTask.limit() > TaskSetManager.TASK_SIZE_TO_WARN_KB * 1024 &&
            !emittedTaskSizeWarning) {
          emittedTaskSizeWarning = true
          logWarning(s"Stage ${task.stageId} contains a task of very large size " +
            s"(${serializedTask.limit() / 1024} KB). The maximum recommended task size is " +
            s"${TaskSetManager.TASK_SIZE_TO_WARN_KB} KB.")
        }
      
        addRunningTask(taskId)
      
        // åˆ›å»ºTaskDescription
        new TaskDescription(
          taskId = taskId,
          attemptNumber = attemptNum,
          execId,
          task.name,
          index,
          task.partitionId,
          addedFiles,
          addedJars,
          task.localProperties,
          serializedTask)
      }
    } else {
      None
    }
  }
  
  // 2. ä»»åŠ¡å‡ºé˜Ÿé€»è¾‘
  private def dequeueTask(execId: String, host: String, maxLocality: TaskLocality)
    : Option[(Int, TaskLocality, Boolean)] = {
  
    // æŒ‰æœ¬åœ°æ€§çº§åˆ«ä¾æ¬¡å°è¯•è·å–ä»»åŠ¡
    for (locality <- Array(TaskLocality.PROCESS_LOCAL, TaskLocality.NODE_LOCAL,
                          TaskLocality.NO_PREF, TaskLocality.RACK_LOCAL, TaskLocality.ANY)) {
      if (locality <= maxLocality) {
        val taskSetIndex = locality match {
          case TaskLocality.PROCESS_LOCAL => dequeueTaskFromList(execId, host, 
            pendingTasksForExecutor.getOrElse(execId, ArrayBuffer()))
          case TaskLocality.NODE_LOCAL => dequeueTaskFromList(execId, host,
            pendingTasksForHost.getOrElse(host, ArrayBuffer()))
          case TaskLocality.NO_PREF => dequeueTaskFromList(execId, host, pendingTasksWithNoPrefs)
          case TaskLocality.RACK_LOCAL => dequeueTaskFromList(execId, host,
            pendingTasksForRack.getOrElse(sched.getRackForHost(host).orNull, ArrayBuffer()))
          case TaskLocality.ANY => dequeueTaskFromList(execId, host, allPendingTasks)
        }
      
        if (taskSetIndex.isDefined) {
          return Some((taskSetIndex.get, locality, false))
        }
      }
    }
    None
  }
  
  // 3. ä»ä»»åŠ¡åˆ—è¡¨ä¸­å‡ºé˜Ÿ
  private def dequeueTaskFromList(
      execId: String,
      host: String,
      list: ArrayBuffer[Int]): Option[Int] = {
    var indexOffset = list.size
    while (indexOffset > 0) {
      indexOffset -= 1
      val index = list(indexOffset)
      if (copiesRunning(index) == 0 && !successful(index)) {
        // æ‰¾åˆ°å¯è¿è¡Œçš„ä»»åŠ¡
        list.remove(indexOffset)
        if (pendingTasksForExecutor.contains(execId)) {
          pendingTasksForExecutor(execId) -= index
        }
        if (pendingTasksForHost.contains(host)) {
          pendingTasksForHost(host) -= index
        }
        val rack = sched.getRackForHost(host)
        if (rack.isDefined && pendingTasksForRack.contains(rack.get)) {
          pendingTasksForRack(rack.get) -= index
        }
        pendingTasksWithNoPrefs -= index
        allPendingTasks -= index
        return Some(index)
      }
    }
    None
  }
  
  // 4. ä»»åŠ¡å®Œæˆå¤„ç†
  def handleSuccessfulTask(tid: Long, result: DirectTaskResult[_]): Unit = {
    val info = taskInfos(tid)
    val index = info.index
  
    // æ ‡è®°ä»»åŠ¡æˆåŠŸ
    successful(index) = true
    removalPendingTask(index)
  
    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    tasksSuccessful += 1
    logInfo(s"Finished task ${info.id} in stage ${taskSet.stageId} (TID $tid) in" +
      s" ${info.duration} ms on ${info.host} (executor ${info.executorId})" +
      s" ($tasksSuccessful/$numTasks)")
  
    // æ£€æŸ¥TaskSetæ˜¯å¦å®Œæˆ
    if (tasksSuccessful == numTasks) {
      isZombie = true
    }
  
    // é€šçŸ¥DAGSchedulerä»»åŠ¡å®Œæˆ
    sched.dagScheduler.taskEnded(tasks(index), Success, result.value(), result.accumUpdates, info)
  }
  
  // 5. ä»»åŠ¡å¤±è´¥å¤„ç†
  def handleFailedTask(tid: Long, state: TaskState, reason: TaskFailedReason): Unit = {
    val info = taskInfos(tid)
    if (info != null && !successful(info.index)) {
      val index = info.index
      copiesRunning(index) -= 1
    
      reason match {
        case fetchFailed: FetchFailed =>
          logWarning(s"Lost task ${info.id} in stage ${taskSet.stageId} (TID $tid, ${info.host}," +
            s" executor ${info.executorId}): ${reason.toErrorString}")
          if (!successful(index)) {
            successful(index) = true
            tasksSuccessful += 1
          }
          isZombie = true
        
        case ef: ExceptionFailure =>
          // ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸
          numFailures(index) += 1
          if (numFailures(index) >= maxTaskFailures) {
            logError(s"Task ${info.id} in stage ${taskSet.stageId} failed $maxTaskFailures times; aborting job")
            abort(s"Task $index in stage ${taskSet.stageId} failed $maxTaskFailures times, " +
              s"most recent failure: ${ef.description}")
            return
          } else {
            // é‡æ–°è°ƒåº¦ä»»åŠ¡
            addPendingTask(index)
          }
        
        case TaskKilled(_) =>
          logWarning(s"Task ${info.id} in stage ${taskSet.stageId} was killed")
        
        case _ =>
          logWarning(s"Lost task ${info.id} in stage ${taskSet.stageId} (TID $tid) on " +
            s"executor ${info.executorId}: ${reason.toErrorString}")
          addPendingTask(index)
      }
    }
  }
  
  // 6. è®¡ç®—æœ¬åœ°æ€§çº§åˆ«
  private def computeValidLocalityLevels(): Array[TaskLocality.TaskLocality] = {
    import TaskLocality._
    val levels = new ArrayBuffer[TaskLocality.TaskLocality]
  
    if (!pendingTasksForExecutor.isEmpty &&
        pendingTasksForExecutor.values.exists(_.nonEmpty)) {
      levels += PROCESS_LOCAL
    }
    if (!pendingTasksForHost.isEmpty &&
        pendingTasksForHost.values.exists(_.nonEmpty)) {
      levels += NODE_LOCAL
    }
    if (!pendingTasksWithNoPrefs.isEmpty) {
      levels += NO_PREF
    }
    if (!pendingTasksForRack.isEmpty &&
        pendingTasksForRack.values.exists(_.nonEmpty)) {
      levels += RACK_LOCAL
    }
    levels += ANY
  
    logDebug("Valid locality levels for " + taskSet + ": " + levels.mkString(", "))
    levels.toArray
  }
}
```

**SchedulerBackendæºç å®ç°**

```scala
// CoarseGrainedSchedulerBackend.scala - ç²—ç²’åº¦è°ƒåº¦åç«¯
private[spark] class CoarseGrainedSchedulerBackend(
    scheduler: TaskSchedulerImpl,
    val rpcEnv: RpcEnv)
  extends ExecutorAllocationClient with SchedulerBackend with Logging {

  // Executorä¿¡æ¯ç®¡ç†
  private val executorDataMap = new HashMap[String, ExecutorData]
  private val addressToExecutorId = new HashMap[RpcAddress, String]
  
  // èµ„æºç®¡ç†
  private val totalCoreCount = new AtomicInteger(0)
  private val totalRegisteredExecutors = new AtomicInteger(0)
  private val maxRpcMessageSize = RpcUtils.maxMessageSizeBytes(conf)
  
  // RPCç«¯ç‚¹å¼•ç”¨
  var driverEndpoint: RpcEndpointRef = null
  
  // 1. å¯åŠ¨è°ƒåº¦åç«¯
  override def start(): Unit = {
    val properties = Seq[(String, String)](
      ("spark.scheduler.mode", scheduler.schedulingMode.toString),
      ("spark.starvation.timeout", starvationTimer.toString),
      ("spark.rpc.askTimeout", askTimeout.toString)
    ) ++ scheduler.applicationAttemptId().map(id => ("spark.app.attempt.id", id.toString))
  
    driverEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))
  }
  
  // 2. åˆ›å»ºDriverç«¯ç‚¹
  protected def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {
    new DriverEndpoint(rpcEnv, properties)
  }
  
  // 3. Driverç«¯ç‚¹å®ç°
  class DriverEndpoint(override val rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])
    extends ThreadSafeRpcEndpoint with Logging {
  
    // å¤„ç†Executoræ³¨å†Œ
    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
      case RegisterExecutor(executorId, executorRef, hostname, cores, logUrls) =>
        if (executorDataMap.contains(executorId)) {
          executorRef.send(RegisterExecutorFailed("Duplicate executor ID: " + executorId))
          context.reply(true)
        } else if (scheduler.nodeBlacklist.contains(hostname)) {
          logInfo(s"Rejecting $executorId as it has been blacklisted.")
          executorRef.send(RegisterExecutorFailed(s"Executor is blacklisted: $executorId"))
          context.reply(true)
        } else {
          // æ³¨å†Œæ–°çš„Executor
          val executorAddress = if (executorRef.address != null) {
            executorRef.address
          } else {
            context.senderAddress
          }
        
          logInfo(s"Registered executor $executorRef ($executorAddress) with ID $executorId")
          addressToExecutorId(executorAddress) = executorId
          totalCoreCount.addAndGet(cores)
          totalRegisteredExecutors.addAndGet(1)
        
          val data = new ExecutorData(executorRef, executorAddress, hostname,
            cores, cores, logUrls)
        
          // å°†Executorä¿¡æ¯å­˜å‚¨åˆ°æ˜ å°„ä¸­
          CoarseGrainedSchedulerBackend.this.synchronized {
            executorDataMap.put(executorId, data)
            if (currentExecutorIdCounter < executorId.toInt) {
              currentExecutorIdCounter = executorId.toInt
            }
            if (numPendingExecutors > 0) {
              numPendingExecutors -= 1
              logDebug(s"Decremented number of pending executors ($numPendingExecutors left)")
            }
          }
        
          executorRef.send(RegisteredExecutor)
          context.reply(true)
          listenerBus.post(
            SparkListenerExecutorAdded(System.currentTimeMillis(), executorId, data))
          makeOffers()
        }
      
      case StopDriver =>
        context.reply(true)
        stop()
      
      case StopExecutors =>
        logInfo("Asking each executor to shut down")
        for ((_, executorData) <- executorDataMap) {
          executorData.executorEndpoint.send(StopExecutor)
        }
        context.reply(true)
    }
  
    // å¤„ç†ExecutorçŠ¶æ€æ›´æ–°
    override def receive: PartialFunction[Any, Unit] = {
      case StatusUpdate(executorId, taskId, state, data) =>
        scheduler.statusUpdate(taskId, state, data.value)
        if (TaskState.isFinished(state)) {
          executorDataMap.get(executorId) match {
            case Some(executorInfo) =>
              executorInfo.freeCores += scheduler.CPUS_PER_TASK
              makeOffers(executorId)
            case None =>
              logWarning(s"Ignored task status update ($taskId state $state) " +
                s"from unknown executor with ID $executorId")
          }
        }
      
      case ReviveOffers =>
        makeOffers()
      
      case KillTask(taskId, executorId, interruptThread, reason) =>
        executorDataMap.get(executorId) match {
          case Some(executorInfo) =>
            executorInfo.executorEndpoint.send(
              KillTask(taskId, executorId, interruptThread, reason))
          case None =>
            logWarning(s"Attempted to kill task $taskId for unknown executor $executorId.")
        }
    }
  
    // 4. èµ„æºåˆ†é…æ ¸å¿ƒæ–¹æ³•
    private def makeOffers(): Unit = {
      // è¿‡æ»¤å‡ºæ´»è·ƒçš„Executor
      val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
      val workOffers = activeExecutors.map {
        case (id, executorData) =>
          new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
      }.toIndexedSeq
    
      launchTasks(scheduler.resourceOffers(workOffers))
    }
  
    private def makeOffers(executorId: String): Unit = {
      // ä¸ºç‰¹å®šExecutoråˆ›å»ºèµ„æºoffer
      if (executorIsAlive(executorId)) {
        val executorData = executorDataMap(executorId)
        val workOffers = IndexedSeq(
          new WorkerOffer(executorId, executorData.executorHost, executorData.freeCores))
        launchTasks(scheduler.resourceOffers(workOffers))
      }
    }
  
    // 5. å¯åŠ¨ä»»åŠ¡
    private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = {
      for (i <- tasks.indices) {
        val execId = tasks(i).head.executorId
        val executorData = executorDataMap(execId)
        val executorOffers = tasks(i)
      
        if (executorOffers.nonEmpty) {
          // å‡å°‘å¯ç”¨æ ¸å¿ƒæ•°
          executorData.freeCores -= executorOffers.size * scheduler.CPUS_PER_TASK
        
          logDebug(s"Launching ${executorOffers.size} tasks on executor $execId")
        
          // åºåˆ—åŒ–ä»»åŠ¡å¹¶å‘é€ç»™Executor
          val serializedTasks = executorOffers.map { task =>
            ser.serialize(task)
          }
        
          if (serializedTasks.nonEmpty) {
            executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(
              Utils.serialize(serializedTasks))))
          }
        }
      }
    }
  }
  
  // 6. åœæ­¢è°ƒåº¦åç«¯
  override def stop(): Unit = {
    reviveThread.shutdownNow()
    try {
      if (driverEndpoint != null) {
        driverEndpoint.askSync[Boolean](StopDriver)
      }
    } catch {
      case e: Exception =>
        logWarning("Exception during shutdown", e)
    }
  }
}
```

**Taskåˆ†å‘å’Œæ‰§è¡Œå®Œæ•´æµç¨‹å›¾**

```mermaid
graph TD
    A[DAGScheduleræäº¤TaskSet] --> B[TaskScheduler.submitTasks]
    B --> C[åˆ›å»ºTaskSetManager]
    C --> D[æ·»åŠ åˆ°è°ƒåº¦æ± ]
    D --> E[SchedulerBackend.reviveOffers]
    E --> F[æ”¶é›†Executorèµ„æºä¿¡æ¯]
    F --> G[åˆ›å»ºWorkerOfferåˆ—è¡¨]
    G --> H[TaskScheduler.resourceOffers]
    H --> I[éå†TaskSetManageré˜Ÿåˆ—]
    I --> J[æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…]
    J --> K{PROCESS_LOCALå¯ç”¨?}
    K -->|æ˜¯| L[åˆ†é…åˆ°æŒ‡å®šExecutor]
    K -->|å¦| M{NODE_LOCALå¯ç”¨?}
    M -->|æ˜¯| N[åˆ†é…åˆ°æŒ‡å®šHost]
    M -->|å¦| O{RACK_LOCALå¯ç”¨?}
    O -->|æ˜¯| P[åˆ†é…åˆ°æŒ‡å®šRack]
    O -->|å¦| Q[ANYçº§åˆ«åˆ†é…]
    L --> R[åˆ›å»ºTaskDescription]
    N --> R
    P --> R
    Q --> R
    R --> S[åºåˆ—åŒ–Task]
    S --> T[SchedulerBackend.launchTasks]
    T --> U[å‘é€LaunchTaskæ¶ˆæ¯]
    U --> V[Executoræ¥æ”¶ä»»åŠ¡]
    V --> W[åˆ›å»ºTaskRunner]
    W --> X[æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ]
    X --> Y[Task.runæ‰§è¡Œ]
    Y --> Z[è¿”å›æ‰§è¡Œç»“æœ]
    Z --> AA[StatusUpdateæ¶ˆæ¯]
    AA --> BB[TaskScheduler.statusUpdate]
    BB --> CC[æ›´æ–°TaskSetManagerçŠ¶æ€]
    CC --> DD{TaskSetå®Œæˆ?}
    DD -->|å¦| E
    DD -->|æ˜¯| EE[é€šçŸ¥DAGScheduler]
  
    style A fill:#e1f5fe
    style H fill:#fff3e0
    style Y fill:#e8f5e8
    style EE fill:#c8e6c9
```

**Stageåˆ’åˆ†æœºåˆ¶ ğŸ”¥**

**Stageåˆ’åˆ†åŸåˆ™**ï¼š

- **å®½ä¾èµ–è¾¹ç•Œ**ï¼šé‡åˆ°å®½ä¾èµ–ï¼ˆShuffleï¼‰åˆ’åˆ†æ–°Stage
- **çª„ä¾èµ–åˆå¹¶**ï¼šçª„ä¾èµ–çš„RDDåœ¨åŒä¸€Stageå†…Pipelineæ‰§è¡Œ
- **Stageç±»å‹**ï¼šShuffleMapStageå’ŒResultStage

```mermaid
graph TD
    A[textFile] --> B[flatMap]
    B --> C[map]
    C --> D[reduceByKey]
    D --> E[map]
    E --> F[collect]
  
    subgraph "Stage 0 (ShuffleMapStage)"
        A
        B
        C
    end
  
    subgraph "Stage 1 (ResultStage)"
        E
        F
    end
  
    C -.->|Shuffle| E
  
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#e8f5e8
    style E fill:#e1f5fe
    style F fill:#e1f5fe
```

### å­˜å‚¨ç®¡ç†æœºåˆ¶

#### BlockManagerç»„ä»¶

**BlockManager** æ˜¯Sparkä¸­è´Ÿè´£æ•°æ®å­˜å‚¨å’Œç®¡ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œç»Ÿä¸€ç®¡ç†å†…å­˜å’Œç£ç›˜ä¸Šçš„æ•°æ®å—ã€‚

**æ“ä½œæ—¶åºå›¾**

```mermaid
sequenceDiagram
    participant Driver
    participant BlockManagerMaster
    participant Executor
    participant BlockManager
    participant MemoryStore
    participant DiskStore
  
    Driver->>BlockManagerMaster: 1. æ³¨å†ŒBlockManager
    Executor->>BlockManager: 2. åˆå§‹åŒ–BlockManager
    BlockManager->>BlockManagerMaster: 3. æ³¨å†Œåˆ°Master
    BlockManager->>MemoryStore: 4. åˆå§‹åŒ–å†…å­˜å­˜å‚¨
    BlockManager->>DiskStore: 5. åˆå§‹åŒ–ç£ç›˜å­˜å‚¨
  
    Note over BlockManager,DiskStore: æ•°æ®å­˜å‚¨æµç¨‹
    BlockManager->>MemoryStore: 6. å°è¯•å†…å­˜å­˜å‚¨
    alt å†…å­˜è¶³å¤Ÿ
        MemoryStore-->>BlockManager: 7a. å­˜å‚¨æˆåŠŸ
    else å†…å­˜ä¸è¶³
        BlockManager->>DiskStore: 7b. ç£ç›˜å­˜å‚¨
        DiskStore-->>BlockManager: 7c. å­˜å‚¨æˆåŠŸ
    end
  
    BlockManager->>BlockManagerMaster: 8. æŠ¥å‘Šå­˜å‚¨çŠ¶æ€
```

**BlockManageræ ¸å¿ƒç»„ä»¶è¯¦è§£**

**1. BlockManageræ¶æ„ç»„ä»¶**


| ç»„ä»¶                   | ç±»å                 | ä¸»è¦èŒè´£           | å­˜å‚¨ä»‹è´¨   |
| ------------------------ | ---------------------- | -------------------- | ------------ |
| **BlockManager**       | `BlockManager`       | æ•°æ®å—ç®¡ç†æ€»æ§åˆ¶å™¨ | å†…å­˜+ç£ç›˜  |
| **MemoryStore**        | `MemoryStore`        | å†…å­˜æ•°æ®å­˜å‚¨       | JVMå †å†…å­˜  |
| **DiskStore**          | `DiskStore`          | ç£ç›˜æ•°æ®å­˜å‚¨       | æœ¬åœ°ç£ç›˜   |
| **BlockManagerMaster** | `BlockManagerMaster` | å…ƒæ•°æ®ç®¡ç†         | Driverå†…å­˜ |
| **BlockInfoManager**   | `BlockInfoManager`   | Blockä¿¡æ¯ç®¡ç†      | å†…å­˜ç´¢å¼•   |

**2. BlockManageråˆ›å»ºä¸åˆå§‹åŒ–**

```scala
class BlockManager(
    executorId: String,
    rpcEnv: RpcEnv,
    master: BlockManagerMaster,
    serializerManager: SerializerManager,
    conf: SparkConf,
    memoryManager: MemoryManager,
    mapOutputTracker: MapOutputTracker)
  extends BlockDataManager with BlockEvictionHandler with Logging {

  // æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
  private[spark] val diskBlockManager = new DiskBlockManager(conf, deleteFilesOnStop = true)
  private[spark] val blockInfoManager = new BlockInfoManager
  
  // åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
  private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager)
  private[spark] val diskStore = new DiskStore(conf, diskBlockManager)
  
  // æ³¨å†Œåˆ°Master
  master.registerBlockManager(blockManagerId, maxMemory, slaveEndpoint)
}
```

**3. æ•°æ®å—å­˜å‚¨æµç¨‹**

```scala
// æ•°æ®å—å­˜å‚¨çš„æ ¸å¿ƒæ–¹æ³•
def putBlockData(
    blockId: BlockId,
    data: BlockData,
    level: StorageLevel,
    tellMaster: Boolean = true): Boolean = {
  
  // 1. æ£€æŸ¥å­˜å‚¨çº§åˆ«
  if (level.useMemory) {
    // 2. å°è¯•å­˜å‚¨åˆ°å†…å­˜
    val putSucceeded = memoryStore.putBytes(blockId, data, level)
    if (putSucceeded) {
      // 3. é€šçŸ¥Master
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, 0))
      }
      return true
    }
  }
  
  // 4. å†…å­˜ä¸è¶³ï¼Œå­˜å‚¨åˆ°ç£ç›˜
  if (level.useDisk) {
    val putSucceeded = diskStore.putBytes(blockId, data)
    if (putSucceeded) {
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, data.size))
      }
      return true
    }
  }
  
  false
}
```

**4. æ•°æ®å—è·å–æµç¨‹**

```scala
// æ•°æ®å—è·å–çš„æ ¸å¿ƒæ–¹æ³•
def get[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. æ£€æŸ¥æœ¬åœ°å†…å­˜
  memoryStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 2. æ£€æŸ¥æœ¬åœ°ç£ç›˜
  diskStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 3. ä»è¿œç¨‹è·å–
  getRemote(blockId)
}

def getRemote[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. ä»Masterè·å–blockä½ç½®
  val locations = master.getLocations(blockId)
  
  // 2. ä»è¿œç¨‹èŠ‚ç‚¹è·å–
  for (location <- locations) {
    val blockResult = blockTransferService.fetchBlockSync(
      location.host, location.port, location.executorId, blockId.toString)
    if (blockResult.isDefined) {
      return blockResult
    }
  }
  
  None
}
```

#### å†…å­˜æ¨¡å‹

**Sparkå†…å­˜åˆ†åŒºæ¶æ„**ï¼š

```mermaid
graph TD
    A[JVMå†…å­˜] --> B[å †å†…å†…å­˜<br/>On-Heap]
    A --> C[å †å¤–å†…å­˜<br/>Off-Heap]
  
    B --> D[å­˜å‚¨å†…å­˜<br/>Storage Memory]
    B --> E[æ‰§è¡Œå†…å­˜<br/>Execution Memory]
    B --> F[å…¶ä»–å†…å­˜<br/>Other Memory]
  
    D --> G[RDDç¼“å­˜<br/>å¹¿æ’­å˜é‡]
    E --> H[Shuffle<br/>Joinèšåˆ]
    F --> I[ç”¨æˆ·æ•°æ®ç»“æ„<br/>Sparkå†…éƒ¨å¯¹è±¡]
  
    C --> J[å †å¤–å­˜å‚¨<br/>Off-Heap Storage]
    C --> K[å †å¤–æ‰§è¡Œ<br/>Off-Heap Execution]
  
    style B fill:#e8f5e8
    style C fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#ffebee
```

**1. å†…å­˜ç®¡ç†æ¶æ„ç»„ä»¶**


| ç»„ä»¶                    | ç±»å                   | ä¸»è¦èŒè´£       | ç®¡ç†èŒƒå›´      |
| ------------------------- | ------------------------ | ---------------- | --------------- |
| **MemoryManager**       | `UnifiedMemoryManager` | ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨ | å †å†…+å †å¤–å†…å­˜ |
| **StorageMemoryPool**   | `StorageMemoryPool`    | å­˜å‚¨å†…å­˜æ±      | ç¼“å­˜æ•°æ®å†…å­˜  |
| **ExecutionMemoryPool** | `ExecutionMemoryPool`  | æ‰§è¡Œå†…å­˜æ±      | ä»»åŠ¡æ‰§è¡Œå†…å­˜  |
| **MemoryStore**         | `MemoryStore`          | å†…å­˜å­˜å‚¨ç®¡ç†   | ç¼“å­˜æ•°æ®å­˜å‚¨  |
| **TaskMemoryManager**   | `TaskMemoryManager`    | ä»»åŠ¡å†…å­˜ç®¡ç†   | å•ä¸ªä»»åŠ¡å†…å­˜  |

**2. MemoryStoreç¼“å­˜ç®¡ç†**

```scala
// MemoryStoreæ ¸å¿ƒå®ç°
class MemoryStore(
    conf: SparkConf,
    blockInfoManager: BlockInfoManager)
  extends BlockStore(BlockStore.MEMORY) with BlockEvictionHandler with Logging {

  // å†…å­˜æ˜ å°„è¡¨
  private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]](32, 0.75f, true)
  
  // å½“å‰å†…å­˜ä½¿ç”¨é‡
  private var _currentMemory = 0L
  
  def putBytes[T](
      blockId: BlockId,
      size: Long,
      memoryMode: MemoryMode,
      _bytes: () => ChunkedByteBuffer): Boolean = {
  
    // 1. æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
    if (!memoryManager.acquireStorageMemory(blockId, size, memoryMode)) {
      return false
    }
  
    // 2. åˆ†é…å†…å­˜å¹¶å­˜å‚¨æ•°æ®
    val bytes = _bytes()
    val entry = new SerializedMemoryEntry[T](bytes, memoryMode, implicitly[ClassTag[T]])
    entries.synchronized {
      entries.put(blockId, entry)
      _currentMemory += size
    }
  
    true
  }
  
  def get[T](blockId: BlockId): Option[BlockResult[T]] = {
    entries.synchronized {
      entries.get(blockId) match {
        case entry: SerializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case entry: DeserializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case _ => None
      }
    }
  }
}
```

**3. TaskMemoryManagerä»»åŠ¡å†…å­˜ç®¡ç†**

```scala
// TaskMemoryManageræ ¸å¿ƒå®ç°
class TaskMemoryManager(
    memoryManager: MemoryManager,
    taskAttemptId: Long)
  extends MemoryManager with Logging {

  // ä»»åŠ¡å†…å­˜æ˜ å°„è¡¨
  private val memoryForTask = new mutable.HashMap[Long, Long]()
  
  // å†…å­˜åˆ†é…æ–¹æ³•
  def acquireExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Long = {
  
    // 1. å°è¯•ä»æ‰§è¡Œå†…å­˜æ± åˆ†é…
    val acquired = memoryManager.acquireExecutionMemory(numBytes, taskAttemptId, memoryMode)
  
    // 2. è®°å½•åˆ†é…çš„å†…å­˜
    if (acquired > 0) {
      memoryForTask.synchronized {
        memoryForTask(taskAttemptId) = memoryForTask.getOrElse(taskAttemptId, 0L) + acquired
      }
    }
  
    acquired
  }
  
  // é‡Šæ”¾å†…å­˜
  def releaseExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Unit = {
  
    memoryManager.releaseExecutionMemory(numBytes, taskAttemptId, memoryMode)
  
    memoryForTask.synchronized {
      val current = memoryForTask.getOrElse(taskAttemptId, 0L)
      val newTotal = math.max(0L, current - numBytes)
      if (newTotal == 0) {
        memoryForTask.remove(taskAttemptId)
      } else {
        memoryForTask(taskAttemptId) = newTotal
      }
    }
  }
}
```

#### å†…å­˜åˆ†é…ç­–ç•¥

**ç»Ÿä¸€å†…å­˜ç®¡ç†**ï¼š

```scala
class UnifiedMemoryManager(
    conf: SparkConf,
    val maxHeapMemory: Long,
    onHeapStorageRegionSize: Long,
    numCores: Int)
  extends MemoryManager(conf, numCores, onHeapStorageRegionSize, maxHeapMemory) {

  // å†…å­˜æ± é…ç½®
  private val maxPoolSize = maxHeapMemory - reservedMemory
  private val poolSize = maxPoolSize * memoryFraction
  
  // åŠ¨æ€å†…å­˜åˆ†é…
  override def acquireStorageMemory(
      blockId: BlockId,
      numBytes: Long,
      memoryMode: MemoryMode): Boolean = synchronized {
  
    val (executionPool, storagePool, maxMemory) = memoryMode match {
      case MemoryMode.ON_HEAP => (
        onHeapExecutionMemoryPool,
        onHeapStorageMemoryPool,
        maxOnHeapStorageMemory)
      case MemoryMode.OFF_HEAP => (
        offHeapExecutionMemoryPool,
        offHeapStorageMemoryPool,
        maxOffHeapStorageMemory)
    }
  
    if (numBytes > maxMemory) {
      return false
    }
  
    if (numBytes > storagePool.memoryFree) {
      // å°è¯•ä»æ‰§è¡Œå†…å­˜æ± å€Ÿç”¨
      val memoryBorrowedFromExecution = math.min(
        executionPool.memoryFree, 
        numBytes - storagePool.memoryFree)
    
      executionPool.decrementPoolSize(memoryBorrowedFromExecution)
      storagePool.incrementPoolSize(memoryBorrowedFromExecution)
    }
  
    storagePool.acquireMemory(blockId, numBytes)
  }
}
```

### ShuffleåŸç†  â­â­â­

**Shuffle** æ˜¯Sparkä¸­æ•°æ®é‡æ–°åˆ†å¸ƒçš„è¿‡ç¨‹ï¼Œå‘ç”Ÿåœ¨éœ€è¦è·¨åˆ†åŒºè¿›è¡Œæ•°æ®äº¤æ¢çš„æ“ä½œä¸­ã€‚

**è§¦å‘Shuffleçš„æ“ä½œ**ï¼š

```scala
// 1. èšåˆæ“ä½œ
val grouped = rdd.groupByKey()        // è§¦å‘Shuffle
val reduced = rdd.reduceByKey(_ + _)  // è§¦å‘Shuffle

// 2. è¿æ¥æ“ä½œ
val joined = rdd1.join(rdd2)          // è§¦å‘Shuffle

// 3. é‡åˆ†åŒºæ“ä½œ
val repartitioned = rdd.repartition(10)  // è§¦å‘Shuffle
```

**Shuffleç±»å‹å¯¹æ¯” ğŸ”¥**


| Shuffleç±»å‹       | ç‰¹ç‚¹                                  | ä¼˜ç¼ºç‚¹               |
| ------------------- | --------------------------------------- | ---------------------- |
| **Hash Shuffle**  | æ¯ä¸ªMap Taskä¸ºæ¯ä¸ªReduce Taskåˆ›å»ºæ–‡ä»¶ | æ–‡ä»¶æ•°è¿‡å¤šï¼Œå½±å“æ€§èƒ½ |
| **Sort Shuffle**  | æ¯ä¸ªMap Taskåˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼ŒæŒ‰åˆ†åŒºæ’åº  | å‡å°‘æ–‡ä»¶æ•°ï¼Œæé«˜æ€§èƒ½ |
| **Tungsten Sort** | ä½¿ç”¨å †å¤–å†…å­˜ï¼Œä¼˜åŒ–æ’åºæ€§èƒ½            | å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ       |

#### Shuffleå®ç°æœºåˆ¶

**Hash Shuffle**

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[File 1-1]
        A --> E[File 1-2]
        A --> F[File 1-3]
      
        B[Map Task 2] --> G[File 2-1]
        B --> H[File 2-2] 
        B --> I[File 2-3]
      
        C[Map Task 3] --> J[File 3-1]
        C --> K[File 3-2]
        C --> L[File 3-3]
    end
  
    subgraph "Reduceé˜¶æ®µ"
        D --> M[Reduce Task 1]
        G --> M
        J --> M
      
        E --> N[Reduce Task 2]
        H --> N
        K --> N
      
        F --> O[Reduce Task 3]
        I --> O
        L --> O
    end
  
    style A fill:#ffebee
    style B fill:#ffebee
    style C fill:#ffebee
```

**Hash Shuffleé—®é¢˜**ï¼š

- **æ–‡ä»¶æ•°çˆ†ç‚¸**ï¼šMä¸ªMap Task Ã— Nä¸ªReduce Task = MÃ—Nä¸ªæ–‡ä»¶
- **éšæœºI/O**ï¼šå¤§é‡å°æ–‡ä»¶å¯¼è‡´éšæœºI/O
- **å†…å­˜å‹åŠ›**ï¼šéœ€è¦ä¸ºæ¯ä¸ªæ–‡ä»¶ç»´æŠ¤ç¼“å†²åŒº

**Sort Shuffle**

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[æ’åºç¼“å†²åŒº]
        D --> E[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        E --> F[ç´¢å¼•æ–‡ä»¶]
      
        B[Map Task 2] --> G[æ’åºç¼“å†²åŒº]
        G --> H[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        H --> I[ç´¢å¼•æ–‡ä»¶]
    end
  
    subgraph "Reduceé˜¶æ®µ"
        F --> J[Reduce Task 1]
        I --> J
      
        F --> K[Reduce Task 2]
        I --> K
    end
  
    style D fill:#fff3e0
    style G fill:#fff3e0
    style E fill:#e8f5e8
    style H fill:#e8f5e8
```

**Sort Shuffleä¼˜åŠ¿**ï¼š

- **æ–‡ä»¶æ•°å‡å°‘**ï¼šæ¯ä¸ªMap Taskåªäº§ç”Ÿä¸€ä¸ªæ•°æ®æ–‡ä»¶å’Œä¸€ä¸ªç´¢å¼•æ–‡ä»¶
- **é¡ºåºI/O**ï¼šæ•°æ®æŒ‰åˆ†åŒºIDæ’åºå†™å…¥ï¼Œæé«˜I/Oæ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨å¤–éƒ¨æ’åºï¼Œæ”¯æŒspillåˆ°ç£ç›˜

**Tungsten Sort Shuffle**

**Tungstenä¼˜åŒ–**ï¼š

- **å †å¤–å†…å­˜ç®¡ç†**ï¼šå‡å°‘GCå‹åŠ›
- **ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„**ï¼šæé«˜CPUç¼“å­˜å‘½ä¸­ç‡
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ç”Ÿæˆä¼˜åŒ–çš„å­—èŠ‚ç 

```scala
// Tungsten Sortå®ç°
class UnsafeShuffleWriter[K, V](
    blockManager: BlockManager,
    shuffleBlockResolver: IndexShuffleBlockResolver,
    taskMemoryManager: TaskMemoryManager,
    handle: SerializedShuffleHandle[K, V],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val partitioner = handle.dependency.partitioner
  private val numPartitions = partitioner.numPartitions
  private var sorter: UnsafeShuffleExternalSorter = _
  
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    // ä½¿ç”¨Tungstenå†…å­˜ç®¡ç†
    val taskContext = context.asInstanceOf[TaskContextImpl]
    sorter = UnsafeShuffleExternalSorter.create(
      taskContext.taskMemoryManager(),
      blockManager,
      context,
      numPartitions,
      shouldCompress = true)

    // åºåˆ—åŒ–å¹¶æ’å…¥è®°å½•
    while (records.hasNext) {
      insertRecordIntoSorter(records.next())
    }
  
    // å†™å‡ºæ’åºç»“æœ
    val outputFile = shuffleBlockResolver.getDataFile(handle.shuffleId, mapId)
    val partitionLengths = sorter.closeAndGetSpills.map(_.file)
      .foldLeft(Array.fill[Long](numPartitions)(0)) { (lengths, file) =>
        // åˆå¹¶spillæ–‡ä»¶
        mergeSpillsWithTransferTo(file, outputFile, lengths)
      }
  
    shuffleBlockResolver.writeIndexFileAndCommit(handle.shuffleId, mapId, partitionLengths, outputFile)
  }
}

```

**Hash Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant HashWriter
    participant FileSystem
    participant ReduceTask
    participant HashReader
  
    MapTask->>HashWriter: å†™å…¥è®°å½•
    HashWriter->>FileSystem: ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºæ–‡ä»¶
    Note over FileSystem: MÃ—Nä¸ªæ–‡ä»¶åˆ›å»º
    HashWriter->>FileSystem: å†™å…¥æ•°æ®åˆ°å¯¹åº”æ–‡ä»¶
  
    ReduceTask->>HashReader: å¼€å§‹è¯»å–
    HashReader->>FileSystem: è¯»å–ç›¸å…³åˆ†åŒºæ–‡ä»¶
    FileSystem-->>HashReader: è¿”å›æ•°æ®
    HashReader-->>ReduceTask: èšåˆåæ•°æ®
```

**Sort Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant SortWriter
    participant ExternalSorter
    participant FileSystem
    participant ReduceTask
    participant SortReader
  
    MapTask->>SortWriter: å†™å…¥è®°å½•
    SortWriter->>ExternalSorter: ç¼“å­˜å¹¶æ’åº
    ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
    ExternalSorter->>FileSystem: å†™å…¥å•ä¸ªæ•°æ®æ–‡ä»¶
    SortWriter->>FileSystem: å†™å…¥ç´¢å¼•æ–‡ä»¶
  
    ReduceTask->>SortReader: å¼€å§‹è¯»å–
    SortReader->>FileSystem: æ ¹æ®ç´¢å¼•è¯»å–æ•°æ®
    FileSystem-->>SortReader: è¿”å›åˆ†åŒºæ•°æ®
    SortReader-->>ReduceTask: èšåˆåæ•°æ®
```

#### Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. ShuffleManageræ¶æ„ç»„ä»¶**


| ç»„ä»¶                   | ç±»å                                       | ä¸»è¦èŒè´£           | é€‚ç”¨åœºæ™¯         |
| ------------------------ | -------------------------------------------- | -------------------- | ------------------ |
| **SortShuffleManager** | `SortShuffleManager`                       | Sort Shuffleç®¡ç†å™¨ | é»˜è®¤Shuffleæ–¹å¼  |
| **HashShuffleManager** | `HashShuffleManager`                       | Hash Shuffleç®¡ç†å™¨ | å·²åºŸå¼ƒ           |
| **ShuffleWriter**      | `SortShuffleWriter`, `UnsafeShuffleWriter` | Shuffleå†™å…¥å™¨      | Mapç«¯æ•°æ®å†™å…¥    |
| **ShuffleReader**      | `BlockStoreShuffleReader`                  | Shuffleè¯»å–å™¨      | Reduceç«¯æ•°æ®è¯»å– |

**2. ShuffleWriteræ ¸å¿ƒå®ç°**

```scala
// Sort Shuffleå®ç°æ ¸å¿ƒ
class SortShuffleWriter[K, V, C](
    shuffleBlockResolver: IndexShuffleBlockResolver,
    handle: BaseShuffleHandle[K, V, C],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val dep = handle.dependency
  private val blockManager = SparkEnv.get.blockManager
  private val sorter: ExternalSorter[K, V, _] = {
    if (dep.mapSideCombine) {
      new ExternalSorter[K, V, C](
        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    } else {
      new ExternalSorter[K, V, V](
        context, aggregator = None, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    }
  }

  // å†™å…¥æ•°æ®
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    sorter.insertAll(records)
  
    // è·å–è¾“å‡ºæ–‡ä»¶
    val outputFile = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
  
    // å†™å…¥æ’åºåçš„æ•°æ®
    val partitionLengths = sorter.writePartitionedFile(blockId, outputFile)
  
    // å†™å…¥ç´¢å¼•æ–‡ä»¶
    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, outputFile)
  }
}
```

**3. ExternalSorterå†…å­˜ç®¡ç†**

```scala
// ExternalSorteræ ¸å¿ƒå®ç°
class ExternalSorter[K, V, C](
    context: TaskContext,
    aggregator: Option[Aggregator[K, V, C]] = None,
    partitioner: Option[Partitioner] = None,
    ordering: Option[Ordering[K]] = None,
    serializer: Serializer = SparkEnv.get.serializer)
  extends Spillable[WritablePartitionedPairCollection[K, C]](context.taskMemoryManager())
  with Logging {

  // å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
  private var map = new PartitionedAppendOnlyMap[K, C]
  private val buffer = new PartitionedPairBuffer[K, C]

  // æ’å…¥æ•°æ®
  def insertAll(records: Iterator[Product2[K, V]]): Unit = {
    val shouldCombine = aggregator.isDefined
  
    if (shouldCombine) {
      // éœ€è¦èšåˆçš„æƒ…å†µ
      val mergeValue = aggregator.get.mergeValue
      val createCombiner = aggregator.get.createCombiner
      var kv: Product2[K, V] = null
    
      val update = (hadValue: Boolean, oldValue: C) => {
        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
      }
    
      while (records.hasNext) {
        addElementsRead()
        kv = records.next()
        map.changeValue((getPartition(kv._1), kv._1), update)
        maybeSpillCollection(usingMap = true)
      }
    } else {
      // ä¸éœ€è¦èšåˆçš„æƒ…å†µ
      while (records.hasNext) {
        addElementsRead()
        val kv = records.next()
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)
      }
    }
  }

  // Spillåˆ°ç£ç›˜
  override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): SpilledFile = {
    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)
    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)
    collection.clear()
    spillFile
  }
}
```

**4. ShuffleReaderæ•°æ®è¯»å–**

```scala
// ShuffleReaderæ ¸å¿ƒå®ç°
class BlockStoreShuffleReader[K, C](
    handle: BaseShuffleHandle[K, _, C],
    startPartition: Int,
    endPartition: Int,
    context: TaskContext,
    serializerManager: SerializerManager = SparkEnv.get.serializerManager,
    blockManager: BlockManager = SparkEnv.get.blockManager,
    mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker)
  extends ShuffleReader[K, C] with Logging {

  private val dep = handle.dependency

  override def read(): Iterator[Product2[K, C]] = {
    // 1. è·å–Shuffleæ•°æ®å—ä½ç½®
    val blocksByAddress = mapOutputTracker.getMapSizesByExecutorId(
      handle.shuffleId, startPartition, endPartition)
  
    // 2. è¯»å–æ•°æ®å—
    val blockFetcherItr = new ShuffleBlockFetcherIterator(
      context,
      blockManager.blockTransferService,
      blockManager,
      blocksByAddress,
      serializerManager.wrapStream(blockId, _),
      // æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨serializerManageræ¥è·å–å‹ç¼©å’ŒåŠ å¯†åŒ…è£…å™¨
      maxBytesInFlight = SparkEnv.get.conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024,
      maxReqsInFlight = SparkEnv.get.conf.getInt("spark.reducer.maxReqsInFlight", Int.MaxValue),
      maxBlocksInFlightPerAddress = SparkEnv.get.conf.getInt(
        "spark.reducer.maxBlocksInFlightPerAddress", Int.MaxValue),
      maxReqSizeShuffleToMem = SparkEnv.get.conf.getSizeAsBytes(
        "spark.reducer.maxReqSizeShuffleToMem", Long.MaxValue),
      detectCorrupt = SparkEnv.get.conf.getBoolean("spark.shuffle.detectCorrupt", true))

    // 3. ååºåˆ—åŒ–å¹¶èšåˆ
    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
      if (dep.mapSideCombine) {
        // Mapç«¯å·²ç»èšåˆï¼ŒReduceç«¯ç»§ç»­èšåˆ
        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
      } else {
        // Mapç«¯æœªèšåˆï¼ŒReduceç«¯è¿›è¡Œèšåˆ
        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, V)]]
        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
      }
    } else {
      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
    }

    // 4. æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
    dep.keyOrdering match {
      case Some(keyOrd: Ordering[K]) =>
        // åˆ›å»ºExternalSorterè¿›è¡Œæ’åº
        val sorter = new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
        sorter.insertAll(aggregatedIter)
        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
      case None =>
        aggregatedIter
    }
  }
}
```

**5. ShuffleBlockResolveræ–‡ä»¶ç®¡ç†**

```scala
// ShuffleBlockResolveræ ¸å¿ƒå®ç°
class IndexShuffleBlockResolver(conf: SparkConf, _blockManager: BlockManager = null)
  extends ShuffleBlockResolver with Logging {

  // è·å–æ•°æ®æ–‡ä»¶
  def getDataFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.data")
  }
  
  // è·å–ç´¢å¼•æ–‡ä»¶
  def getIndexFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.index")
  }
  
  // å†™å…¥ç´¢å¼•æ–‡ä»¶å¹¶æäº¤
  def writeIndexFileAndCommit(
      shuffleId: Int,
      mapId: Long,
      lengths: Array[Long],
      dataTmp: File): Unit = {
  
    val indexFile = getIndexFile(shuffleId, mapId)
    val indexTmp = new File(indexFile.getAbsolutePath + ".tmp")
  
    try {
      val out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexTmp)))
      Utils.tryWithSafeFinally {
        // å†™å…¥åç§»é‡
        var offset = 0L
        out.writeLong(offset)
        for (length <- lengths) {
          offset += length
          out.writeLong(offset)
        }
      } {
        out.close()
      }
    
      // åŸå­æ€§é‡å‘½å
      val dataFile = getDataFile(shuffleId, mapId)
      if (dataTmp.exists() && !dataTmp.renameTo(dataFile)) {
        throw new IOException("Failed to rename data file")
      }
      if (!indexTmp.renameTo(indexFile)) {
        throw new IOException("Failed to rename index file")
      }
    } catch {
      case e: Exception =>
        indexTmp.delete()
        throw e
    }
  }
}
```

**6. Shuffleæ•°æ®æµç»„ä»¶äº¤äº’**

```mermaid
sequenceDiagram
  participant MapTask
  participant ExternalSorter
  participant ShuffleWriter
  participant ShuffleBlockResolver
  participant Disk
  participant ReduceTask
  participant ShuffleReader
  participant BlockManager

  MapTask->>ExternalSorter: insertAll(records)
  ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
  ExternalSorter->>ShuffleWriter: writePartitionedFile()
  ShuffleWriter->>ShuffleBlockResolver: writeIndexFileAndCommit()
  ShuffleBlockResolver->>Disk: å†™å…¥data/indexæ–‡ä»¶
  
  ReduceTask->>ShuffleReader: read()
  ShuffleReader->>BlockManager: è·å–blockä½ç½®
  BlockManager->>Disk: è¯»å–Shuffleæ–‡ä»¶
  Disk-->>ShuffleReader: è¿”å›æ•°æ®
  ShuffleReader->>ReduceTask: èšåˆ/æ’åºç»“æœ
```

**7. Shuffleæ€§èƒ½ç›‘æ§ç»„ä»¶**

```scala
// Shuffleæ€§èƒ½æŒ‡æ ‡æ”¶é›†
class ShuffleWriteMetrics extends TaskMetrics {
  // å†™å…¥å­—èŠ‚æ•°
  private var _bytesWritten: Long = 0L
  // å†™å…¥è®°å½•æ•°
  private var _recordsWritten: Long = 0L
  // å†™å…¥æ—¶é—´
  private var _writeTime: Long = 0L
  
  def bytesWritten: Long = _bytesWritten
  def recordsWritten: Long = _recordsWritten
  def writeTime: Long = _writeTime
}

class ShuffleReadMetrics extends TaskMetrics {
  // è¯»å–å­—èŠ‚æ•°
  private var _bytesRead: Long = 0L
  // è¯»å–è®°å½•æ•°
  private var _recordsRead: Long = 0L
  // è¯»å–æ—¶é—´
  private var _readTime: Long = 0L
  // è¿œç¨‹è¯»å–å­—èŠ‚æ•°
  private var _remoteBytesRead: Long = 0L
  
  def bytesRead: Long = _bytesRead
  def recordsRead: Long = _recordsRead
  def readTime: Long = _readTime
  def remoteBytesRead: Long = _remoteBytesRead
}
```

#### Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜

**ä¸»è¦ä¼˜åŒ–ç­–ç•¥**ï¼š

- **å‹ç¼©**ï¼š`spark.shuffle.compress`ï¼Œå‡å°‘ç½‘ç»œä¼ è¾“é‡
- **åˆç†è®¾ç½®åˆ†åŒºæ•°**ï¼š`spark.sql.shuffle.partitions`ï¼Œé¿å…åˆ†åŒºè¿‡å¤šæˆ–è¿‡å°‘
- **ä½¿ç”¨æœ¬åœ°åŒ–Shuffle**ï¼šå‡å°‘ç½‘ç»œI/O
- **å¯ç”¨spillæœºåˆ¶**ï¼šå†…å­˜ä¸è¶³æ—¶æº¢å†™ç£ç›˜ï¼Œé˜²æ­¢OOM
- **èšåˆç¼“å†²åŒº**ï¼šMapç«¯æœ¬åœ°èšåˆï¼Œå‡å°‘ä¼ è¾“æ•°æ®é‡

**1. åˆ†åŒºä¼˜åŒ–ç­–ç•¥**

```properties
# æ¨èè®¾ç½®ï¼ˆæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
spark.sql.shuffle.partitions=200
spark.default.parallelism=200

# åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
```

**2. åŠ¨æ€èµ„æºåˆ†é…**

```properties
# å¯ç”¨åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.initialExecutors=2

# èµ„æºåˆ†é…ç­–ç•¥
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=120s
```

**3. å‹ç¼©ä¸åºåˆ—åŒ–ä¼˜åŒ–**


| é…ç½®é¡¹                            | æ¨èå€¼           | è¯´æ˜            |
| ----------------------------------- | ------------------ | ----------------- |
| `spark.shuffle.compress`          | `true`           | å¯ç”¨Shuffleå‹ç¼© |
| `spark.shuffle.compress.codec`    | `snappy`         | å‹ç¼©ç®—æ³•é€‰æ‹©    |
| `spark.serializer`                | `KryoSerializer` | åºåˆ—åŒ–å™¨é€‰æ‹©    |
| `spark.kryo.registrationRequired` | `false`          | æ˜¯å¦è¦æ±‚æ³¨å†Œç±»  |

**4. æœ¬åœ°åŒ–Shuffleä¼˜åŒ–**

```properties
# æœ¬åœ°åŒ–é…ç½®
spark.locality.wait=3s
spark.locality.wait.process=3s
spark.locality.wait.node=3s
spark.locality.wait.rack=3s
```

**5. é«˜çº§ä¼˜åŒ–æŠ€å·§**

**Mapç«¯èšåˆ**ï¼š

```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val result = rdd.reduceByKey(_ + _)  // æ¨è
// val result = rdd.groupByKey().mapValues(_.sum)  // ä¸æ¨è
```

**å¹¿æ’­å˜é‡ä¼˜åŒ–**ï¼š

```scala
// å°è¡¨å¹¿æ’­ï¼Œé¿å…Shuffle
val smallTable = spark.table("small_table").collect()
val broadcastVar = spark.sparkContext.broadcast(smallTable)
```

#### Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**1. æ•°æ®å€¾æ–œé—®é¢˜**

**ç°è±¡**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´Taskæ‰§è¡Œæ—¶é—´å·®å¼‚å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š

```scala
// æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†
val skewedRDD = rdd.map(x => {
  val key = x._1
  val value = x._2
  if (isSkewedKey(key)) {
    (key + "_" + Random.nextInt(10), value)
  } else {
    (key, value)
  }
})

// æ–¹æ¡ˆ2ï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨
class SkewPartitioner(numPartitions: Int) extends Partitioner {
  override def numPartitions: Int = numPartitions
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val rawKey = key.toString.split("_")(0)
    math.abs(rawKey.hashCode) % numPartitions
  }
}
```

**2. Shuffleæ–‡ä»¶è¿‡å¤šé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­äº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼Œå½±å“æ€§èƒ½

**è§£å†³æ–¹æ¡ˆ**ï¼š

```properties
# åˆå¹¶å°æ–‡ä»¶
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200
```

**3. å†…å­˜æº¢å‡ºé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­å‡ºç°OOM

**è§£å†³æ–¹æ¡ˆ**ï¼š

```properties
# å¯ç”¨Spillæœºåˆ¶
spark.shuffle.spill=true
spark.shuffle.spill.compress=true

# è°ƒæ•´å†…å­˜é…ç½®
spark.executor.memory=4g
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3
```

---

## Spark SQLä¸Catalyst â­â­

### Spark SQLæ¦‚è¿°

**Spark SQL** æ˜¯Sparkç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®çš„æ¨¡å—ï¼Œæä¾›äº†DataFrameå’ŒDataset APIã€‚

#### ä¸»è¦ç‰¹æ€§

- **ç»Ÿä¸€æ•°æ®è®¿é—®**ï¼šæ”¯æŒå¤šç§æ•°æ®æº
- **Hiveå…¼å®¹æ€§**ï¼šå®Œå…¨å…¼å®¹Hive SQL
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šCatalystä¼˜åŒ–å™¨
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ä»£ç ç”Ÿæˆ

#### ä½¿ç”¨æ–¹å¼

```scala
// åˆ›å»ºSparkSession
val spark = SparkSession.builder()
  .appName("SparkSQLExample")
  .config("spark.sql.adaptive.enabled", "true")
  .getOrCreate()

// è¯»å–æ•°æ®
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// SQLæŸ¥è¯¢
df.createOrReplaceTempView("people")
val result = spark.sql("""
  SELECT age, count(*) as count
  FROM people 
  WHERE age > 21
  GROUP BY age
  ORDER BY age
""")

// DataFrame API
val result2 = df
  .filter($"age" > 21)
  .groupBy("age")
  .count()
  .orderBy("age")
```

### Catalystä¼˜åŒ–å™¨ ğŸ”¥

**Catalyst** æ˜¯Spark SQLçš„æŸ¥è¯¢ä¼˜åŒ–æ¡†æ¶ï¼ŒåŸºäºScalaçš„å‡½æ•°å¼ç¼–ç¨‹æ„å»ºã€‚

#### ä¼˜åŒ–æµç¨‹

```mermaid
graph LR
    A[SQL/DataFrame] --> B[è§£æå™¨<br/>Parser]
    B --> C[é€»è¾‘è®¡åˆ’<br/>Logical Plan]
    C --> D[ä¼˜åŒ–å™¨<br/>Optimizer]
    D --> E[ç‰©ç†è®¡åˆ’<br/>Physical Plan]
    E --> F[ä»£ç ç”Ÿæˆ<br/>CodeGen]
    F --> G[æ‰§è¡Œ<br/>Execution]
  
    style B fill:#e8f5e8
    style D fill:#e1f5fe
    style F fill:#fff3e0
```

**ä¼˜åŒ–é˜¶æ®µ**ï¼š

1. **é€»è¾‘è®¡åˆ’ä¼˜åŒ–**ï¼šè°“è¯ä¸‹æ¨ã€æŠ•å½±ä¸‹æ¨ã€å¸¸é‡æŠ˜å 
2. **ç‰©ç†è®¡åˆ’ç”Ÿæˆ**ï¼šé€‰æ‹©æœ€ä¼˜çš„ç‰©ç†æ‰§è¡Œç­–ç•¥
3. **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆé«˜æ•ˆçš„Javaå­—èŠ‚ç 

#### ä¼˜åŒ–è§„åˆ™

**ä¸»è¦ä¼˜åŒ–è§„åˆ™**ï¼š

```scala
// è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT * FROM (SELECT * FROM table WHERE col1 > 10) WHERE col2 = 'value'
// ä¼˜åŒ–å  
SELECT * FROM table WHERE col1 > 10 AND col2 = 'value'

// æŠ•å½±ä¸‹æ¨ï¼ˆProjection Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT col1 FROM (SELECT col1, col2, col3 FROM table)
// ä¼˜åŒ–å
SELECT col1 FROM table

// å¸¸é‡æŠ˜å ï¼ˆConstant Foldingï¼‰
// ä¼˜åŒ–å‰
SELECT col1 + 1 + 2 FROM table
// ä¼˜åŒ–å
SELECT col1 + 3 FROM table
```

#### ä»£ç ç”Ÿæˆ

**Whole-Stage Code Generation**ï¼š

```scala
// ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
```

### SparkSQL å®ç”¨å‡½æ•°ä¸è¯­æ³•

#### æ—¥æœŸä¸æ—¶é—´å¤„ç†

```sql
-- è·å–å½“å‰æ—¶é—´æˆ³
SELECT current_timestamp() as current_time,
       unix_timestamp() as unix_timestamp;

-- æ—¶é—´æˆ³è½¬æ¢
SELECT from_unixtime(1640995200) as datetime,
       unix_timestamp('2022-01-01 00:00:00') as timestamp;

-- æ—¥æœŸåŠ å‡æ“ä½œ
SELECT date_add('2022-01-01', 7) as next_week,
       date_sub('2022-01-01', 7) as last_week,
       add_months('2022-01-01', 1) as next_month;

-- æ—¥æœŸå·®è®¡ç®—
SELECT datediff('2022-01-15', '2022-01-01') as days_diff,
       months_between('2022-03-01', '2022-01-01') as months_diff;

-- æ—¥æœŸæ ¼å¼åŒ–
SELECT date_format('2022-01-01', 'yyyy-MM-dd') as formatted_date,
       to_date('2022-01-01') as date_type;
```

#### å­—ç¬¦ä¸²å¤„ç†

```sql
-- å­—ç¬¦ä¸²è¿æ¥
SELECT concat('Hello', ' ', 'World') as greeting,
       concat_ws('-', '2022', '01', '01') as date_str;

-- å­—ç¬¦ä¸²æˆªå–
SELECT substring('Hello World', 1, 5) as substring,
       substr('Hello World', 7) as substr_from_7;

-- å­—ç¬¦ä¸²æ›¿æ¢
SELECT replace('Hello World', 'World', 'Spark') as replaced,
       regexp_replace('Hello123World', '\\d+', '') as no_digits;

-- æ­£åˆ™è¡¨è¾¾å¼æå–
SELECT regexp_extract('Hello123World456', '(\\d+)', 1) as first_number,
       regexp_extract('email@example.com', '([^@]+)@([^@]+)', 1) as username;

-- å­—ç¬¦ä¸²åˆ†å‰²
SELECT split('a,b,c,d', ',') as split_array,
       size(split('a,b,c,d', ',')) as array_size;
```

#### æ•°ç»„ä¸é›†åˆæ“ä½œ

```sql
-- æ•°ç»„åˆ›å»º
SELECT array(1, 2, 3) as simple_array,
       array_contains(array(1, 2, 3), 2) as contains_2;

-- æ•°ç»„å±•å¼€ï¼ˆè¡Œè½¬åˆ—ï¼‰
SELECT explode(array(1, 2, 3)) as exploded_value;

-- æ•°ç»„èšåˆï¼ˆåˆ—è½¬è¡Œï¼‰
SELECT collect_list(column_name) as list_agg,
       collect_set(column_name) as set_agg;

-- æ•°ç»„äº¤é›†
SELECT arrays_overlap(array(1, 2, 3), array(2, 3, 4)) as has_overlap;

-- å¤æ‚æ•°ç»„æ“ä½œ
SELECT 
    id,
    explode(split(tags, ',')) as tag
FROM user_tags;
```

#### JSONå¤„ç†

```sql
-- JSONè§£æ
SELECT get_json_object('{"name": "John", "age": 30}', '$.name') as name,
       get_json_object('{"name": "John", "age": 30}', '$.age') as age;

-- JSONæ•°ç»„å¤„ç†
SELECT json_array_length('[1, 2, 3, 4]') as array_length;

-- è½¬æ¢ä¸ºJSON
SELECT to_json(struct('John' as name, 30 as age)) as json_string;

-- å¤æ‚JSONæ“ä½œ
SELECT 
    user_id,
    get_json_object(profile, '$.email') as email,
    get_json_object(profile, '$.address.city') as city
FROM user_profiles;
```

#### æ¡ä»¶ä¸åˆ¤æ–­

```sql
-- CASE WHENè¯­å¥
SELECT 
    name,
    age,
    CASE 
        WHEN age < 18 THEN 'æœªæˆå¹´'
        WHEN age < 30 THEN 'é’å¹´'
        WHEN age < 50 THEN 'ä¸­å¹´'
        ELSE 'è€å¹´'
    END as age_group
FROM users;

-- IFå‡½æ•°
SELECT 
    name,
    IF(age >= 18, 'æˆå¹´', 'æœªæˆå¹´') as adult_status,
    IFNULL(email, 'æ— é‚®ç®±') as email_info
FROM users;

-- COALESCEå‡½æ•°
SELECT 
    user_id,
    COALESCE(nickname, real_name, 'Unknown') as display_name
FROM user_info;
```

#### çª—å£å‡½æ•°

```sql
-- ROW_NUMBER() è¡Œå·
SELECT 
    name,
    salary,
    ROW_NUMBER() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- RANK() æ’åï¼ˆç›¸åŒå€¼ç›¸åŒæ’åï¼Œè·³è¿‡ï¼‰
SELECT 
    name,
    salary,
    RANK() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- DENSE_RANK() å¯†é›†æ’åï¼ˆç›¸åŒå€¼ç›¸åŒæ’åï¼Œä¸è·³è¿‡ï¼‰
SELECT 
    name,
    salary,
    DENSE_RANK() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- LAG/LEAD å‰åå€¼
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) as prev_sales,
    LEAD(sales, 1) OVER (ORDER BY date) as next_sales
FROM daily_sales;

-- åˆ†åŒºçª—å£å‡½æ•°
SELECT 
    department,
    name,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank
FROM employees;
```

#### èšåˆå‡½æ•°

```sql
-- åŸºæœ¬èšåˆ
SELECT 
    department,
    COUNT(*) as employee_count,
    AVG(salary) as avg_salary,
    SUM(salary) as total_salary,
    MIN(salary) as min_salary,
    MAX(salary) as max_salary
FROM employees
GROUP BY department;

-- æ¡ä»¶èšåˆ
SELECT 
    department,
    COUNT(CASE WHEN gender = 'M' THEN 1 END) as male_count,
    COUNT(CASE WHEN gender = 'F' THEN 1 END) as female_count,
    AVG(CASE WHEN age > 30 THEN salary END) as senior_avg_salary
FROM employees
GROUP BY department;

-- å»é‡èšåˆ
SELECT 
    department,
    COUNT(DISTINCT employee_id) as unique_employees
FROM employees
GROUP BY department;
```

#### å®ç”¨æŸ¥è¯¢ç¤ºä¾‹

```sql
-- ç”¨æˆ·ç•™å­˜åˆ†æ
WITH user_activity AS (
    SELECT 
        user_id,
        date,
        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY date) as visit_order
    FROM user_visits
)
SELECT 
    first_visit.date as cohort_date,
    COUNT(DISTINCT first_visit.user_id) as cohort_size,
    COUNT(DISTINCT CASE WHEN second_visit.user_id IS NOT NULL THEN first_visit.user_id END) as retained_users,
    COUNT(DISTINCT CASE WHEN second_visit.user_id IS NOT NULL THEN first_visit.user_id END) / 
        COUNT(DISTINCT first_visit.user_id) as retention_rate
FROM user_activity first_visit
LEFT JOIN user_activity second_visit 
    ON first_visit.user_id = second_visit.user_id 
    AND second_visit.visit_order = 2
WHERE first_visit.visit_order = 1
GROUP BY first_visit.date;

-- æ¼æ–—åˆ†æ
SELECT 
    step_name,
    COUNT(DISTINCT user_id) as users,
    LAG(COUNT(DISTINCT user_id)) OVER (ORDER BY step_order) as prev_step_users,
    COUNT(DISTINCT user_id) / LAG(COUNT(DISTINCT user_id)) OVER (ORDER BY step_order) as conversion_rate
FROM funnel_events
GROUP BY step_name, step_order
ORDER BY step_order;

-- æ—¶é—´åºåˆ—åˆ†æ
SELECT 
    date,
    sales,
    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d,
    SUM(sales) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) as rolling_sum_30d
FROM daily_sales
ORDER BY date;

// åŸå§‹æŸ¥è¯¢ï¼šSELECT sum(x + y) FROM table WHERE z > 10
class GeneratedIterator extends Iterator[InternalRow] {
  private var sum: Long = 0L
  
  def processNext(): Unit = {
    while (input.hasNext) {
      val row = input.next()
      val z = row.getLong(2)
      if (z > 10) {  // è°“è¯è®¡ç®—
        val x = row.getLong(0)
        val y = row.getLong(1)
        sum += (x + y)  // èšåˆè®¡ç®—
      }
    }
    // è¿”å›æœ€ç»ˆç»“æœ
    result.setLong(0, sum)
  }
}
```

### æ•°æ®æºæ”¯æŒ

#### å†…ç½®æ•°æ®æº

**æ”¯æŒçš„æ•°æ®æ ¼å¼**ï¼š

- **Parquet**ï¼šåˆ—å¼å­˜å‚¨ï¼Œé«˜å‹ç¼©æ¯”
- **JSON**ï¼šåŠç»“æ„åŒ–æ•°æ®
- **CSV**ï¼šæ–‡æœ¬æ ¼å¼
- **ORC**ï¼šä¼˜åŒ–çš„è¡Œåˆ—å­˜å‚¨
- **Avro**ï¼šæ¨¡å¼æ¼”åŒ–æ”¯æŒ

```scala
// è¯»å–ä¸åŒæ ¼å¼æ•°æ®
val parquetDF = spark.read.parquet("path/to/data.parquet")
val jsonDF = spark.read.json("path/to/data.json")
val csvDF = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// å†™å…¥æ•°æ®
df.write
  .mode("overwrite")
  .option("compression", "snappy")
  .parquet("output/path")
```

#### å¤–éƒ¨æ•°æ®æº

**å¸¸ç”¨å¤–éƒ¨æ•°æ®æº**ï¼š

```scala
// JDBCæ•°æ®æº
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "username")
  .option("password", "password")
  .load()

// Kafkaæ•°æ®æº
val kafkaDF = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// Hiveè¡¨
val hiveDF = spark.sql("SELECT * FROM hive_table")
```

---

---

## æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ– â­â­â­

### æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–

**1ã€å­˜å‚¨æ ¼å¼ä¼˜åŒ–**

**æ¨èå­˜å‚¨æ ¼å¼å¯¹æ¯”**ï¼š


| æ ¼å¼        | å‹ç¼©æ¯” | æŸ¥è¯¢é€Ÿåº¦ | å†™å…¥é€Ÿåº¦ | é€‚ç”¨åœºæ™¯             |
| ------------- | -------- | ---------- | ---------- | ---------------------- |
| **Parquet** | é«˜     | å¿«       | ä¸­ç­‰     | åˆ†ææŸ¥è¯¢ï¼Œåˆ—å¼å­˜å‚¨   |
| **ORC**     | å¾ˆé«˜   | å¾ˆå¿«     | å¿«       | Hiveé›†æˆï¼Œé«˜å‹ç¼©æ¯”   |
| **Avro**    | ä¸­ç­‰   | ä¸­ç­‰     | å¿«       | è¡Œå¼å­˜å‚¨ï¼ŒSchemaæ¼”è¿› |
| **JSON**    | ä½     | æ…¢       | å¿«       | å¼€å‘è°ƒè¯•ï¼Œçµæ´»æ€§é«˜   |

**Parquetæ ¼å¼ä¼˜åŒ–**ï¼š

```scala
// æ¨èä½¿ç”¨Parquetæ ¼å¼
df.write.mode("overwrite").parquet("data.parquet")
val optimizedDF = spark.read.parquet("data.parquet")

// é…ç½®å‹ç¼©
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

// è®¾ç½®åˆ—å¼è¯»å–æ‰¹æ¬¡å¤§å°
spark.conf.set("spark.sql.parquet.columnarReaderBatchSize", "10000")

// å¯ç”¨å‘é‡åŒ–è¯»å–
spark.conf.set("spark.sql.parquet.enableVectorizedReader", "true")
```

**ORCæ ¼å¼ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨ORCæ ¼å¼
df.write.format("orc").mode("overwrite").save("data.orc")

// ORCä¼˜åŒ–é…ç½®
spark.conf.set("spark.sql.orc.compression.codec", "snappy")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.orc.enableVectorizedReader", "true")
```

**2ã€åˆ†åŒºç­–ç•¥ä¼˜åŒ–**

**æ—¶é—´åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æŒ‰æ—¶é—´åˆ†åŒºï¼ˆæ¨èï¼‰
df.write
  .partitionBy("year", "month", "day")
  .parquet("time_partitioned_data")

// é¿å…è¿‡åº¦åˆ†åŒº
val dailyData = df.withColumn("date", to_date($"timestamp"))
dailyData.write
  .partitionBy("date")
  .parquet("daily_partitioned_data")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "true")
```

**ä¸šåŠ¡åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æŒ‰ä¸šåŠ¡å­—æ®µåˆ†åŒº
df.write
  .partitionBy("region", "category")
  .parquet("business_partitioned_data")

// åˆ†åŒºæ•°æ§åˆ¶
val numPartitions = spark.conf.get("spark.sql.shuffle.partitions", "200").toInt
val optimalPartitions = Math.max(100, Math.min(numPartitions, 1000))

df.repartition(optimalPartitions, $"partition_key")
  .write
  .partitionBy("partition_key")
  .parquet("optimized_data")
```

**åˆ†åŒºè£å‰ªä¼˜åŒ–**ï¼š

```scala
// å¯ç”¨åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

// æŸ¥è¯¢æ—¶ä½¿ç”¨åˆ†åŒºè¿‡æ»¤
val result = spark.read.parquet("partitioned_data")
  .filter($"year" === 2023 && $"month" >= 6)  // æœ‰æ•ˆåˆ†åŒºè£å‰ª
  .select("id", "name", "value")
```

**3ã€è°“è¯ä¸‹æ¨ä¼˜åŒ–**

**å¯ç”¨è°“è¯ä¸‹æ¨**ï¼š

```scala
// å¯ç”¨å„ç§æ•°æ®æºçš„è°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.json.filterPushdown", "true")

// JDBCè°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.pushDownPredicate", "true")
```

**ä¼˜åŒ–ç¤ºä¾‹**ï¼š

```scala
// åŸå§‹æŸ¥è¯¢ï¼ˆæœªä¼˜åŒ–ï¼‰
val df = spark.read.parquet("large_dataset.parquet")
val result = df.select("*").filter($"age" > 18 && $"city" === "Beijing")

// ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆè°“è¯ä¸‹æ¨ï¼‰
val result = spark.read.parquet("large_dataset.parquet")
  .filter($"age" > 18)  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .filter($"city" === "Beijing")  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .select("id", "name", "age")  // åˆ—è£å‰ª
```

**å¤æ‚è°“è¯ä¼˜åŒ–**ï¼š

```scala
// ç»„åˆæ¡ä»¶ä¼˜åŒ–
val complexFilter = ($"age".between(18, 65)) && 
                   ($"salary" > 50000) && 
                   ($"department".isin("IT", "Finance"))

val result = spark.read.parquet("employee_data.parquet")
  .filter(complexFilter)  // å¤æ‚è°“è¯ä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–å’Œä¸‹æ¨
  .select("id", "name", "salary")
```

### Joinä¼˜åŒ–

1. **Joinç­–ç•¥é€‰æ‹©**

**Joinç±»å‹å¯¹æ¯”**ï¼š


| Joinç±»å‹              | é€‚ç”¨åœºæ™¯     | ä¼˜åŠ¿                | åŠ£åŠ¿               | è§¦å‘æ¡ä»¶    |
| ----------------------- | -------------- | --------------------- | -------------------- | ------------- |
| **Broadcast Join**    | å°è¡¨Joinå¤§è¡¨ | æ— Shuffleï¼Œæ€§èƒ½æœ€å¥½ | å°è¡¨å¿…é¡»èƒ½æ”¾å…¥å†…å­˜ | å°è¡¨ < 10MB |
| **Sort Merge Join**   | å¤§è¡¨Joinå¤§è¡¨ | å†…å­˜å‹å¥½ï¼Œç¨³å®š      | éœ€è¦Shuffle        | é»˜è®¤Join    |
| **Shuffle Hash Join** | ä¸­ç­‰è¡¨Join   | å†…å­˜ä½¿ç”¨é€‚ä¸­        | éœ€è¦Shuffle        | ä¸­ç­‰æ•°æ®é‡  |
| **Cartesian Join**    | ç¬›å¡å°”ç§¯     | ç®€å•                | æ€§èƒ½æå·®           | æ— Joiné”®    |

2. **å¹¿æ’­Joinä¼˜åŒ–**

**è‡ªåŠ¨å¹¿æ’­é…ç½®**ï¼š

```scala
// è®¾ç½®è‡ªåŠ¨å¹¿æ’­é˜ˆå€¼
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

// å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
```

**æ‰‹åŠ¨å¹¿æ’­ä¼˜åŒ–**ï¼š

```scala
// æ‰‹åŠ¨å¹¿æ’­å°è¡¨
val smallTable = spark.table("small_table")
val broadcastDF = broadcast(smallTable)
val result = largeTable.join(broadcastDF, "id")

// å¼ºåˆ¶å¹¿æ’­ï¼ˆå³ä½¿è¶…è¿‡é˜ˆå€¼ï¼‰
val result = largeTable.join(broadcast(mediumTable), "id")

// å¹¿æ’­å˜é‡ä¼˜åŒ–
val lookupMap = smallTable.collect()
  .map(row => row.getAs[String]("key") -> row.getAs[String]("value"))
  .toMap
val broadcastMap = spark.sparkContext.broadcast(lookupMap)

val enrichedData = largeTable.map { row =>
  val key = row.getAs[String]("key")
  val enrichValue = broadcastMap.value.getOrElse(key, "unknown")
  (row, enrichValue)
}
```

3. **Sort Merge Joinä¼˜åŒ–**

**é¢„æ’åºä¼˜åŒ–**ï¼š

```scala
// é¢„å…ˆæ’åºå‡å°‘Shuffleæˆæœ¬
val sortedTable1 = table1.sort("join_key")
val sortedTable2 = table2.sort("join_key")
val result = sortedTable1.join(sortedTable2, "join_key")

// ä½¿ç”¨åˆ†æ¡¶è¡¨
table1.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table1")

table2.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table2")

// åˆ†æ¡¶è¡¨Joinï¼ˆæ— Shuffleï¼‰
val result = spark.table("bucketed_table1")
  .join(spark.table("bucketed_table2"), "join_key")
```

4. **æ•°æ®å€¾æ–œå¤„ç†**

**å€¾æ–œæ£€æµ‹**ï¼š

```scala
// æ£€æµ‹Joiné”®åˆ†å¸ƒ
val keyDistribution = largeTable
  .groupBy("join_key")
  .count()
  .orderBy($"count".desc)

keyDistribution.show(20)  // æŸ¥çœ‹top 20çš„é”®åˆ†å¸ƒ

// ç»Ÿè®¡åˆ†æ
val stats = keyDistribution.agg(
  avg("count").as("avg_count"),
  max("count").as("max_count"),
  min("count").as("min_count"),
  stddev("count").as("stddev_count")
)
stats.show()
```

**å€¾æ–œè§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†**ï¼š

```scala
// åŠ ç›Join
import scala.util.Random

// ç»™å°è¡¨åŠ ç›
val saltedSmallTable = smallTable.flatMap { row =>
  (0 until 10).map { salt =>
    Row.fromSeq(row.toSeq :+ salt)
  }
}

// ç»™å¤§è¡¨çš„å€¾æ–œé”®åŠ éšæœºç›
val saltedLargeTable = largeTable.map { row =>
  val key = row.getAs[String]("join_key")
  val salt = if (isSkewedKey(key)) Random.nextInt(10) else 0
  Row.fromSeq(row.toSeq :+ salt)
}

// æ‰§è¡ŒJoin
val result = saltedLargeTable.join(saltedSmallTable, 
  Seq("join_key", "salt_column"))
```

**æ–¹æ¡ˆ2ï¼šå€¾æ–œé”®å•ç‹¬å¤„ç†**ï¼š

```scala
// è¯†åˆ«å€¾æ–œé”®
val skewedKeys = Set("skewed_key_1", "skewed_key_2")

// åˆ†ç¦»å€¾æ–œæ•°æ®å’Œæ­£å¸¸æ•°æ®
val normalData = largeTable.filter(!$"join_key".isin(skewedKeys.toSeq:_*))
val skewedData = largeTable.filter($"join_key".isin(skewedKeys.toSeq:_*))

// æ­£å¸¸æ•°æ®æ­£å¸¸Join
val normalResult = normalData.join(smallTable, "join_key")

// å€¾æ–œæ•°æ®ä½¿ç”¨å¹¿æ’­Join
val skewedResult = skewedData.join(broadcast(smallTable), "join_key")

// åˆå¹¶ç»“æœ
val finalResult = normalResult.union(skewedResult)
```

**æ–¹æ¡ˆ3ï¼šä¸¤é˜¶æ®µèšåˆ**ï¼š

```scala
// é¢„èšåˆé˜¶æ®µ
val preAggregated = largeTable
  .withColumn("salt", (rand() * 10).cast("int"))
  .withColumn("salted_key", concat($"join_key", lit("_"), $"salt"))
  .groupBy("salted_key")
  .agg(sum("value").as("partial_sum"))

// æœ€ç»ˆèšåˆé˜¶æ®µ
val finalAggregated = preAggregated
  .withColumn("original_key", split($"salted_key", "_").getItem(0))
  .groupBy("original_key")
  .agg(sum("partial_sum").as("final_sum"))
```

### ç¼“å­˜ä¸æŒä¹…åŒ–

1. **å­˜å‚¨çº§åˆ«é€‰æ‹©**

**å­˜å‚¨çº§åˆ«å¯¹æ¯”**ï¼š


| å­˜å‚¨çº§åˆ«            | å†…å­˜ä½¿ç”¨ | ç£ç›˜ä½¿ç”¨ | åºåˆ—åŒ– | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯             |
| --------------------- | ---------- | ---------- | -------- | --------- | ---------------------- |
| **MEMORY_ONLY**     | é«˜       | æ—        | æ—      | ä½      | å°æ•°æ®é›†ï¼Œé¢‘ç¹è®¿é—®   |
| **MEMORY_AND_DISK** | ä¸­ç­‰     | æœ‰       | æ—      | ä½      | å¤§æ•°æ®é›†ï¼Œå†…å­˜ä¸è¶³æ—¶ |
| **MEMORY_ONLY_SER** | ä½       | æ—        | æœ‰     | é«˜      | å†…å­˜ç´§å¼ ï¼ŒCPUå……è¶³    |
| **DISK_ONLY**       | æ—        | é«˜       | æœ‰     | ä¸­ç­‰    | å¤§æ•°æ®é›†ï¼Œå†…å­˜ç´§å¼    |
| **OFF_HEAP**        | å †å¤–     | æ—        | æœ‰     | ä¸­ç­‰    | å‡å°‘GCå‹åŠ›           |

2. **ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**

**æ™ºèƒ½ç¼“å­˜ç­–ç•¥**ï¼š

```scala
// æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©å­˜å‚¨çº§åˆ«
def selectStorageLevel(dataSize: Long, memoryAvailable: Long): StorageLevel = {
  val ratio = dataSize.toDouble / memoryAvailable
  
  if (ratio < 0.3) {
    StorageLevel.MEMORY_ONLY  // å†…å­˜å……è¶³
  } else if (ratio < 0.8) {
    StorageLevel.MEMORY_ONLY_SER  // å†…å­˜ç´§å¼ ï¼Œåºåˆ—åŒ–èŠ‚çœç©ºé—´
  } else {
    StorageLevel.MEMORY_AND_DISK_SER  // å†…å­˜ä¸è¶³ï¼Œæº¢å‡ºåˆ°ç£ç›˜
  }
}

// åº”ç”¨ç¼“å­˜ç­–ç•¥
val dataSize = rdd.map(_.toString.length).sum()
val storageLevel = selectStorageLevel(dataSize, availableMemory)
rdd.persist(storageLevel)
```

**ç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

```scala
// ç¼“å­˜ç®¡ç†å™¨
class CacheManager {
  private val cachedRDDs = mutable.Map[String, RDD[_]]()
  
  def cache[T](name: String, rdd: RDD[T], level: StorageLevel = StorageLevel.MEMORY_AND_DISK): RDD[T] = {
    // æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
    if (!cachedRDDs.contains(name)) {
      rdd.persist(level)
      cachedRDDs(name) = rdd
      println(s"Cached RDD: $name")
    }
    rdd
  }
  
  def uncache(name: String): Unit = {
    cachedRDDs.get(name).foreach { rdd =>
      rdd.unpersist()
      cachedRDDs.remove(name)
      println(s"Uncached RDD: $name")
    }
  }
  
  def clearAll(): Unit = {
    cachedRDDs.values.foreach(_.unpersist())
    cachedRDDs.clear()
    println("Cleared all cached RDDs")
  }
}
```

3. **Checkpointä¼˜åŒ–**

**Checkpointç­–ç•¥**ï¼š

```scala
// è®¾ç½®checkpointç›®å½•
spark.sparkContext.setCheckpointDir("hdfs://namenode:8020/checkpoint")

// æ™ºèƒ½checkpoint
def smartCheckpoint[T](rdd: RDD[T], lineageDepth: Int = 10): RDD[T] = {
  // è®¡ç®—è¡€ç¼˜æ·±åº¦
  def getLineageDepth(rdd: RDD[_]): Int = {
    if (rdd.dependencies.isEmpty) {
      1
    } else {
      1 + rdd.dependencies.map(_.rdd).map(getLineageDepth).max
    }
  }
  
  val currentDepth = getLineageDepth(rdd)
  if (currentDepth > lineageDepth) {
    println(s"Checkpointing RDD with lineage depth: $currentDepth")
    rdd.checkpoint()
  }
  rdd
}

// ä½¿ç”¨ç¤ºä¾‹
val deepRDD = rdd1.map(transform1)
  .filter(filter1)
  .join(rdd2)
  .map(transform2)
  .filter(filter2)

val checkpointedRDD = smartCheckpoint(deepRDD)
```

### ä»£ç å±‚é¢ä¼˜åŒ–

1. **ç®—å­é€‰æ‹©ä¼˜åŒ–**

**é«˜æ•ˆç®—å­ä½¿ç”¨**ï¼š

```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val wordCounts = words.map(word => (word, 1))
  .reduceByKey(_ + _)  // æ¨èï¼šMapç«¯é¢„èšåˆ

// è€Œä¸æ˜¯
val wordCounts = words.map(word => (word, 1))
  .groupByKey()  // ä¸æ¨èï¼šæ‰€æœ‰æ•°æ®éƒ½è¦Shuffle
  .mapValues(_.sum)

// ä½¿ç”¨mapPartitionså‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
val result = rdd.mapPartitions { partition =>
  // åœ¨åˆ†åŒºçº§åˆ«åˆå§‹åŒ–èµ„æº
  val connection = createConnection()
  val buffer = new ArrayBuffer[ProcessedRecord]()
  
  try {
    partition.foreach { record =>
      buffer += processWithConnection(record, connection)
    }
    buffer.iterator
  } finally {
    connection.close()
  }
}

// ä½¿ç”¨aggregateByKeyè¿›è¡Œå¤æ‚èšåˆ
val result = rdd.aggregateByKey((0, 0))(
  // åˆ†åŒºå†…èšåˆ
  (acc, value) => (acc._1 + value, acc._2 + 1),
  // åˆ†åŒºé—´èšåˆ
  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
).mapValues { case (sum, count) => sum.toDouble / count }
```

2. **æ•°æ®ç»“æ„ä¼˜åŒ–**

**é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„**ï¼š

```scala
// ä½¿ç”¨åŸå§‹ç±»å‹æ•°ç»„æ›¿ä»£é›†åˆ
class OptimizedProcessor {
  // æ¨èï¼šåŸå§‹ç±»å‹æ•°ç»„
  private val intArray = new Array[Int](1000000)
  
  // ä¸æ¨èï¼šè£…ç®±ç±»å‹é›†åˆ
  private val intList = new ArrayBuffer[Integer]()
  
  // ä½¿ç”¨ä¸“ç”¨çš„æ•°æ®ç»“æ„
  def processLargeDataset(data: RDD[String]): RDD[String] = {
    data.mapPartitions { partition =>
      val bloom = new BloomFilter[String](1000000, 0.01)
      val deduped = new mutable.HashSet[String]()
    
      partition.filter { item =>
        if (bloom.mightContain(item)) {
          if (deduped.contains(item)) {
            false  // é‡å¤é¡¹
          } else {
            deduped += item
            true
          }
        } else {
          bloom.put(item)
          deduped += item
          true
        }
      }
    }
  }
}
```

3. **å†…å­˜ä½¿ç”¨ä¼˜åŒ–**

**å¯¹è±¡å¤ç”¨**ï¼š

```scala
// å¯¹è±¡æ± æ¨¡å¼
class ObjectPool[T](createFunc: () => T, resetFunc: T => Unit) {
  private val pool = new mutable.Queue[T]()
  
  def borrow(): T = {
    pool.synchronized {
      if (pool.nonEmpty) {
        pool.dequeue()
      } else {
        createFunc()
      }
    }
  }
  
  def return(obj: T): Unit = {
    resetFunc(obj)
    pool.synchronized {
      pool.enqueue(obj)
    }
  }
}

// ä½¿ç”¨å¯¹è±¡æ± 
val stringBuilderPool = new ObjectPool[StringBuilder](
  () => new StringBuilder(),
  _.clear()
)

val result = rdd.mapPartitions { partition =>
  partition.map { item =>
    val sb = stringBuilderPool.borrow()
    try {
      sb.append(item).append("_processed").toString
    } finally {
      stringBuilderPool.return(sb)
    }
  }
}
```

4. **å¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ä¼˜åŒ–**

**å¹¿æ’­å˜é‡æœ€ä½³å®è·µ**ï¼š

```scala
// å¤§æŸ¥æ‰¾è¡¨å¹¿æ’­
val lookupTableMap = smallTable.collect()
  .map(row => row.getString(0) -> row.getString(1))
  .toMap

val broadcastLookup = spark.sparkContext.broadcast(lookupTableMap)

val enrichedData = largeRDD.map { record =>
  val enrichValue = broadcastLookup.value.getOrElse(record.key, "unknown")
  EnrichedRecord(record, enrichValue)
}

// è®°ä½åŠæ—¶é‡Šæ”¾
broadcastLookup.destroy()
```

**ç´¯åŠ å™¨ä¼˜åŒ–**ï¼š

```scala
// è‡ªå®šä¹‰ç´¯åŠ å™¨
class HistogramAccumulator extends AccumulatorV2[Double, mutable.Map[String, Long]] {
  private val histogram = mutable.Map[String, Long]()
  
  override def isZero: Boolean = histogram.isEmpty
  
  override def copy(): HistogramAccumulator = {
    val newAcc = new HistogramAccumulator
    newAcc.histogram ++= this.histogram
    newAcc
  }
  
  override def reset(): Unit = histogram.clear()
  
  override def add(value: Double): Unit = {
    val bucket = getBucket(value)
    histogram(bucket) = histogram.getOrElse(bucket, 0L) + 1
  }
  
  override def merge(other: AccumulatorV2[Double, mutable.Map[String, Long]]): Unit = {
    other match {
      case o: HistogramAccumulator =>
        o.histogram.foreach { case (bucket, count) =>
          histogram(bucket) = histogram.getOrElse(bucket, 0L) + count
        }
    }
  }
  
  override def value: mutable.Map[String, Long] = histogram.toMap
  
  private def getBucket(value: Double): String = {
    if (value < 0) "negative"
    else if (value < 10) "0-10"
    else if (value < 100) "10-100"
    else "100+"
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰ç´¯åŠ å™¨
val histogramAcc = new HistogramAccumulator
spark.sparkContext.register(histogramAcc, "histogram")

rdd.foreach(value => histogramAcc.add(value))
println(s"Histogram: ${histogramAcc.value}")
```

### ç½‘ç»œä¸I/Oä¼˜åŒ–

1. **åºåˆ—åŒ–ä¼˜åŒ–**

**Kryoåºåˆ—åŒ–é…ç½®**ï¼š

```scala
// Kryoé…ç½®
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")
spark.conf.set("spark.kryo.unsafe", "true")

// æ³¨å†Œå¸¸ç”¨ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[scala.collection.mutable.WrappedArray.ofRef[_]],
  classOf[org.apache.spark.sql.types.StructType],
  classOf[Array[org.apache.spark.sql.types.StructField]]
))
```

2. **å‹ç¼©ä¼˜åŒ–**

**å‹ç¼©ç®—æ³•é€‰æ‹©**ï¼š

```scala
// æ ¹æ®åœºæ™¯é€‰æ‹©å‹ç¼©ç®—æ³•
spark.conf.set("spark.io.compression.codec", "snappy")  // å¹³è¡¡å‹ç¼©æ¯”å’Œé€Ÿåº¦
// spark.conf.set("spark.io.compression.codec", "lz4")     // æ›´å¿«çš„å‹ç¼©/è§£å‹
// spark.conf.set("spark.io.compression.codec", "gzip")    // æ›´é«˜çš„å‹ç¼©æ¯”

// å¯ç”¨å„ç§å‹ç¼©
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")
spark.conf.set("spark.shuffle.spill.compress", "true")
spark.conf.set("spark.rdd.compress", "true")
```

3. **ç½‘ç»œè°ƒä¼˜**

**ç½‘ç»œå‚æ•°ä¼˜åŒ–**ï¼š

```bash
# ç½‘ç»œè¶…æ—¶è®¾ç½®
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.rpc.askTimeout", "800s")
spark.conf.set("spark.rpc.lookupTimeout", "800s")

# ç½‘ç»œè¿æ¥ä¼˜åŒ–
spark.conf.set("spark.rpc.netty.dispatcher.numThreads", "8")
spark.conf.set("spark.shuffle.io.numConnectionsPerPeer", "3")
spark.conf.set("spark.shuffle.io.preferDirectBufs", "true")

# ä¼ è¾“ä¼˜åŒ–
spark.conf.set("spark.reducer.maxSizeInFlight", "96m")
spark.conf.set("spark.reducer.maxReqsInFlight", "Int.MaxValue")
```

### å¸¸è§æ€§èƒ½é—®é¢˜

**å†…å­˜æº¢å‡ºé—®é¢˜ ğŸ”¥**

**OOMé—®é¢˜è¯Šæ–­**ï¼š

```scala
// 1. Driver OOM
// åŸå› ï¼šcollect()æ“ä½œæ•°æ®é‡è¿‡å¤§
val result = largeRDD.collect()  // å±é™©æ“ä½œ

// è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨take()æˆ–åˆ†æ‰¹å¤„ç†
val sample = largeRDD.take(1000)
largeRDD.foreachPartition { partition =>
  // åˆ†åŒºå†…å¤„ç†ï¼Œé¿å…å°†æ‰€æœ‰æ•°æ®æ‹‰åˆ°Driver
}

// 2. Executor OOM  
// åŸå› ï¼šå•ä¸ªåˆ†åŒºæ•°æ®è¿‡å¤§
// è§£å†³æ–¹æ¡ˆï¼šå¢åŠ åˆ†åŒºæ•°
val repartitionedRDD = rdd.repartition(numPartitions * 2)
```

**æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ ğŸ”¥**

**å€¾æ–œæ£€æµ‹å’Œè§£å†³**ï¼š

```scala
// æ£€æµ‹å€¾æ–œ
def detectSkew[T](rdd: RDD[T]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val maxSize = partitionSizes.maxBy(_._2)
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  
  if (maxSize._2 > avgSize * 3) {
    println(s"æ•°æ®å€¾æ–œè­¦å‘Šï¼šåˆ†åŒº${maxSize._1}æœ‰${maxSize._2}æ¡è®°å½•ï¼Œå¹³å‡${avgSize}æ¡")
  }
}
```

### ç›‘æ§ä¸è¯Šæ–­

Spark UIç›‘æ§

**å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š

- **Jobsé¡µé¢**ï¼šä½œä¸šæ‰§è¡Œæ—¶é—´ã€Stageä¿¡æ¯
- **Stagesé¡µé¢**ï¼šStageæ‰§è¡Œè¯¦æƒ…ã€ä»»åŠ¡åˆ†å¸ƒ
- **Storageé¡µé¢**ï¼šRDDç¼“å­˜ä½¿ç”¨æƒ…å†µ
- **Executorsé¡µé¢**ï¼šExecutorèµ„æºä½¿ç”¨æƒ…å†µ
- **SQLé¡µé¢**ï¼šSQLæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’

æ€§èƒ½æŒ‡æ ‡

**æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡**ï¼š

```bash
# æŸ¥çœ‹åº”ç”¨ç¨‹åºæŒ‡æ ‡
curl http://driver-host:4040/api/v1/applications
curl http://driver-host:4040/api/v1/applications/[app-id]/jobs
curl http://driver-host:4040/api/v1/applications/[app-id]/stages
curl http://driver-host:4040/api/v1/applications/[app-id]/executors
```

---

---

## Spark Streaming â­

### æµå¤„ç†æ¦‚å¿µ

#### å¾®æ‰¹æ¬¡å¤„ç†

**Spark Streaming** åŸºäºå¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰å¤„ç†æ¨¡å‹ï¼Œå°†è¿ç»­çš„æ•°æ®æµåˆ’åˆ†ä¸ºå°çš„æ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

```mermaid
graph LR
    A[å®æ—¶æ•°æ®æµ] --> B[æ‰¹æ¬¡1<br/>1-2ç§’]
    A --> C[æ‰¹æ¬¡2<br/>2-3ç§’]
    A --> D[æ‰¹æ¬¡3<br/>3-4ç§’]
  
    B --> E[RDD1]
    C --> F[RDD2]
    D --> G[RDD3]
  
    E --> H[å¤„ç†ç»“æœ1]
    F --> I[å¤„ç†ç»“æœ2]
    G --> J[å¤„ç†ç»“æœ3]
  
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

#### DStreamæ¦‚å¿µ

**DStream (Discretized Stream)** æ˜¯Spark Streamingçš„åŸºæœ¬æŠ½è±¡ï¼Œä»£è¡¨è¿ç»­çš„æ•°æ®æµã€‚

```scala
// DStreamåŸºæœ¬ä½¿ç”¨
val conf = new SparkConf().setAppName("StreamingExample")
val ssc = new StreamingContext(conf, Seconds(2))

// åˆ›å»ºDStream
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)

// è¾“å‡ºç»“æœ
wordCounts.print()

// å¯åŠ¨æµå¤„ç†
ssc.start()
ssc.awaitTermination()
```

### Structured Streaming

#### æ ¸å¿ƒæ¦‚å¿µ

**Structured Streaming** æ˜¯Spark 2.0å¼•å…¥çš„æ–°æµå¤„ç†å¼•æ“ï¼ŒåŸºäºSpark SQLæ„å»ºã€‚

```scala
// Structured Streamingç¤ºä¾‹
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("StructuredStreamingExample")
  .getOrCreate()

// å®šä¹‰Schema
val schema = StructType(
  StructField("timestamp", TimestampType, true) ::
  StructField("value", StringType, true) :: Nil
)

// åˆ›å»ºæµæ•°æ®æº
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// å¤„ç†æ•°æ®
val words = df
  .selectExpr("CAST(value AS STRING) as message")
  .flatMap(_.split(" "))
  .groupBy("word")
  .count()

// è¾“å‡ºåˆ°æ§åˆ¶å°
val query = words.writeStream
  .outputMode("complete")
  .format("console")
  .trigger(Trigger.ProcessingTime("2 seconds"))
  .start()

query.awaitTermination()
```

#### è¾“å‡ºæ¨¡å¼


| è¾“å‡ºæ¨¡å¼     | æè¿°           | é€‚ç”¨åœºæ™¯           |
| -------------- | ---------------- | -------------------- |
| **Complete** | è¾“å‡ºå®Œæ•´ç»“æœè¡¨ | èšåˆæŸ¥è¯¢           |
| **Append**   | åªè¾“å‡ºæ–°å¢è¡Œ   | æ— èšåˆçš„æŸ¥è¯¢       |
| **Update**   | è¾“å‡ºæ›´æ–°çš„è¡Œ   | èšåˆæŸ¥è¯¢çš„å¢é‡æ›´æ–° |

#### çª—å£æ“ä½œ

```scala
// çª—å£èšåˆ
val windowedCounts = df
  .withWatermark("timestamp", "10 minutes")
  .groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  )
  .count()

// ä¼šè¯çª—å£
val sessionCounts = df
  .groupBy(
    session_window($"timestamp", "30 minutes"),
    $"userId"
  )
  .count()
```

### å®¹é”™æœºåˆ¶

#### Checkpointæœºåˆ¶

**Checkpoint** æä¾›å®¹é”™æ¢å¤èƒ½åŠ›ï¼Œä¿å­˜DStreamçš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚

```scala
// è®¾ç½®Checkpoint
ssc.checkpoint("hdfs://checkpoint/path")

// æœ‰çŠ¶æ€æ“ä½œéœ€è¦Checkpoint
val runningCounts = words.updateStateByKey { (values: Seq[Int], state: Option[Int]) =>
  val currentCount = values.sum
  val previousCount = state.getOrElse(0)
  Some(currentCount + previousCount)
}
```

#### WALæœºåˆ¶

**Write-Ahead Logs** ç¡®ä¿æ¥æ”¶åˆ°çš„æ•°æ®åœ¨å¤„ç†å‰å…ˆå†™å…¥å¯é å­˜å‚¨ã€‚

```scala
// å¯ç”¨WAL
spark.conf.set("spark.streaming.receiver.writeAheadLog.enable", "true")
spark.conf.set("spark.streaming.driver.writeAheadLog.closeFileAfterWrite", "true")
```

---

---

## Sparkä¼ä¸šçº§åº”ç”¨å®æˆ˜ ğŸ¢

### æ‰¹å¤„ç†åº”ç”¨

#### ETLæ•°æ®å¤„ç†

**å¤§è§„æ¨¡æ•°æ®æ¸…æ´—æ¡ˆä¾‹**ï¼š

```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// 1. æ•°æ®è¯»å–ä¸Schemaå®šä¹‰
val schema = StructType(Array(
  StructField("user_id", LongType, true),
  StructField("timestamp", TimestampType, true),
  StructField("event_type", StringType, true),
  StructField("page_url", StringType, true),
  StructField("ip_address", StringType, true)
))

val rawData = spark.read
  .option("header", "true")
  .schema(schema)
  .csv("hdfs://data/raw/user_events/*")

// 2. æ•°æ®æ¸…æ´—ä¸è½¬æ¢
val cleanedData = rawData
  .filter($"user_id".isNotNull && $"timestamp".isNotNull)
  .filter($"ip_address".rlike("^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$"))
  .withColumn("date", to_date($"timestamp"))
  .withColumn("hour", hour($"timestamp"))
  .withColumn("domain", regexp_extract($"page_url", "https?://([^/]+)", 1))

// 3. æ•°æ®èšåˆåˆ†æ
val userBehavior = cleanedData
  .groupBy("user_id", "date", "event_type")
  .agg(
    count("*").as("event_count"),
    countDistinct("page_url").as("unique_pages"),
    min("timestamp").as("first_event_time"),
    max("timestamp").as("last_event_time")
  )

// 4. ç»“æœä¿å­˜
userBehavior
  .coalesce(100)
  .write
  .mode("overwrite")
  .partitionBy("date")
  .parquet("hdfs://data/processed/user_behavior")
```

#### æ•°æ®åˆ†ææ¡ˆä¾‹

**ç”¨æˆ·è¡Œä¸ºåˆ†æ**ï¼š

```scala
// æ¼æ–—åˆ†æ
val funnelAnalysis = spark.sql("""
  WITH user_events AS (
    SELECT user_id, event_type, timestamp,
           ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as event_order
    FROM cleaned_data
    WHERE date >= '2023-01-01'
  ),
  funnel_steps AS (
    SELECT 
      user_id,
      SUM(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as step1_count,
      SUM(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as step2_count,
      SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as step3_count
    FROM user_events
    GROUP BY user_id
  )
  SELECT 
    COUNT(DISTINCT user_id) as total_users,
    COUNT(DISTINCT CASE WHEN step1_count > 0 THEN user_id END) as step1_users,
    COUNT(DISTINCT CASE WHEN step2_count > 0 THEN user_id END) as step2_users,
    COUNT(DISTINCT CASE WHEN step3_count > 0 THEN user_id END) as step3_users
  FROM funnel_steps
""")

// ç”¨æˆ·ç•™å­˜åˆ†æ
val retentionAnalysis = spark.sql("""
  WITH user_first_visit AS (
    SELECT user_id, MIN(date) as first_visit_date
    FROM cleaned_data
    GROUP BY user_id
  ),
  user_visits AS (
    SELECT u.user_id, u.first_visit_date, c.date as visit_date,
           DATEDIFF(c.date, u.first_visit_date) as days_since_first_visit
    FROM user_first_visit u
    JOIN cleaned_data c ON u.user_id = c.user_id
  )
  SELECT 
    first_visit_date,
    COUNT(DISTINCT user_id) as new_users,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 1 THEN user_id END) as day1_retention,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 7 THEN user_id END) as day7_retention,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 30 THEN user_id END) as day30_retention
  FROM user_visits
  GROUP BY first_visit_date
  ORDER BY first_visit_date
""")
```

### æµå¤„ç†åº”ç”¨

#### å®æ—¶æ•°æ®å¤„ç†

**å®æ—¶æ¨èç³»ç»Ÿ**ï¼š

```scala
// å®æ—¶ç”¨æˆ·è¡Œä¸ºæµå¤„ç†
val kafkaStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "user_behavior")
  .option("startingOffsets", "latest")
  .load()

// è§£æJSONæ•°æ®
val userEvents = kafkaStream
  .select(from_json($"value".cast("string"), schema).as("data"))
  .select("data.*")
  .withWatermark("timestamp", "10 minutes")

// å®æ—¶ç‰¹å¾è®¡ç®—
val userFeatures = userEvents
  .groupBy(
    $"user_id",
    window($"timestamp", "10 minutes", "1 minute")
  )
  .agg(
    count("*").as("event_count"),
    countDistinct("page_url").as("unique_pages"),
    collect_list("event_type").as("event_sequence")
  )

// å®æ—¶æ¨èç”Ÿæˆ
val recommendations = userFeatures.map { row =>
  val userId = row.getLong("user_id")
  val features = extractFeatures(row)
  val recommendations = recommendationModel.predict(features)
  RecommendationResult(userId, recommendations, System.currentTimeMillis())
}

// è¾“å‡ºåˆ°Kafka
val query = recommendations.writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("topic", "recommendations")
  .option("checkpointLocation", "/path/to/checkpoint")
  .outputMode("append")
  .start()
```

#### æœºå™¨å­¦ä¹ æµæ°´çº¿

**å®æ—¶æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹**ï¼š

```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification.RandomForestClassifier

// æ„å»ºML Pipeline
val assembler = new VectorAssembler()
  .setInputCols(Array("feature1", "feature2", "feature3"))
  .setOutputCol("features")

val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")

val rf = new RandomForestClassifier()
  .setFeaturesCol("scaledFeatures")
  .setLabelCol("label")
  .setNumTrees(100)

val pipeline = new Pipeline()
  .setStages(Array(assembler, scaler, rf))

// æµå¼æ¨¡å‹è®­ç»ƒ
val trainingStream = spark.readStream
  .format("delta")
  .option("path", "/path/to/training/data")
  .load()

val model = pipeline.fit(trainingStream)

// æµå¼é¢„æµ‹
val predictionStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "prediction_requests")
  .load()

val predictions = model.transform(predictionStream)

predictions.writeStream
  .format("console")
  .outputMode("append")
  .start()
```

### æœ€ä½³å®è·µ

#### å¼€å‘è§„èŒƒ

**ä»£ç ç»„ç»‡ç»“æ„**ï¼š

```scala
// é…ç½®ç®¡ç†
object SparkConfig {
  def getSparkSession(appName: String): SparkSession = {
    SparkSession.builder()
      .appName(appName)
      .config("spark.sql.adaptive.enabled", "true")
      .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .getOrCreate()
  }
}

// æ•°æ®å¤„ç†å·¥å…·ç±»
object DataProcessor {
  def cleanUserData(spark: SparkSession, inputPath: String): DataFrame = {
    import spark.implicits._
  
    spark.read.parquet(inputPath)
      .filter($"user_id".isNotNull)
      .filter($"timestamp".isNotNull)
      .dropDuplicates("user_id", "timestamp")
  }
  
  def validateData(df: DataFrame): DataFrame = {
    df.filter("user_id > 0")
      .filter("timestamp IS NOT NULL")
  }
}

// ä¸»åº”ç”¨ç¨‹åº
object UserAnalysisApp {
  def main(args: Array[String]): Unit = {
    val spark = SparkConfig.getSparkSession("UserAnalysis")
  
    try {
      val inputPath = args(0)
      val outputPath = args(1)
    
      val cleanData = DataProcessor.cleanUserData(spark, inputPath)
      val validData = DataProcessor.validateData(cleanData)
    
      validData.write
        .mode("overwrite")
        .parquet(outputPath)
      
    } finally {
      spark.stop()
    }
  }
}
```

#### éƒ¨ç½²ç­–ç•¥

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š

```bash
#!/bin/bash
# spark-submitè„šæœ¬

spark-submit \
  --class com.company.UserAnalysisApp \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 50 \
  --executor-memory 8g \
  --executor-cores 4 \
  --driver-memory 4g \
  --driver-cores 2 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=10 \
  --conf spark.dynamicAllocation.maxExecutors=100 \
  --conf spark.sql.execution.arrow.pyspark.enabled=true \
  --conf spark.sql.parquet.columnarReaderBatchSize=10000 \
  user-analysis-app.jar \
  /input/path \
  /output/path
```

#### è¿ç»´ç®¡ç†

**ç›‘æ§è„šæœ¬**ï¼š

```bash
#!/bin/bash
# Sparkåº”ç”¨ç›‘æ§è„šæœ¬

# æ£€æŸ¥åº”ç”¨çŠ¶æ€
check_app_status() {
    local app_id=$1
    local status=$(yarn application -status $app_id | grep "Final-State" | awk '{print $3}')
    echo "Application $app_id status: $status"
    return $status
}

# ç›‘æ§èµ„æºä½¿ç”¨
monitor_resources() {
    local app_id=$1
    yarn top -r $app_id
}

# æ”¶é›†æ—¥å¿—
collect_logs() {
    local app_id=$1
    yarn logs -applicationId $app_id > /logs/spark_${app_id}.log
}

# ä¸»ç›‘æ§å¾ªç¯
main() {
    while true; do
        for app_id in $(yarn application -list -appStates RUNNING | grep SPARK | awk '{print $1}'); do
            check_app_status $app_id
            monitor_resources $app_id
        done
        sleep 30
    done
}

main
```

---

## å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ

### å†…å­˜ç›¸å…³é”™è¯¯

1. OutOfMemoryError: Java heap space

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)
    at java.lang.StringBuilder.append(StringBuilder.java:136)
```

**åŸå› åˆ†æ**ï¼š

- **å †å†…å­˜ä¸è¶³**ï¼šExecutoræˆ–Driverçš„å †å†…å­˜è®¾ç½®è¿‡å°
- **æ•°æ®å€¾æ–œ**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿‡å¤§ï¼Œå¯¼è‡´å•ä¸ªTaskå†…å­˜æº¢å‡º
- **ç¼“å­˜è¿‡å¤š**ï¼šRDDç¼“å­˜å ç”¨è¿‡å¤šå†…å­˜
- **å¯¹è±¡åˆ›å»ºè¿‡å¤š**ï¼šé¢‘ç¹åˆ›å»ºå¤§å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´å†…å­˜é…ç½®**ï¼š

```bash
# å¢åŠ Executorå†…å­˜
spark-submit \
  --conf spark.executor.memory=8g \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memory=4g \
  --conf spark.driver.memoryOverhead=1g \
  your-app.jar

# è°ƒæ•´å†…å­˜æ¯”ä¾‹
spark-submit \
  --conf spark.memory.fraction=0.8 \
  --conf spark.memory.storageFraction=0.3 \
  your-app.jar
```

**2. å¤„ç†æ•°æ®å€¾æ–œ**ï¼š

```scala
// æ–¹æ³•1ï¼šå¢åŠ åˆ†åŒºæ•°
val skewedRDD = originalRDD.repartition(200)

// æ–¹æ³•2ï¼šè‡ªå®šä¹‰åˆ†åŒºç­–ç•¥
val customPartitioner = new Partitioner {
  override def numPartitions: Int = 200
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val hash = key.hashCode()
    Math.abs(hash) % numPartitions
  }
}
val skewedRDD = originalRDD.partitionBy(customPartitioner)

// æ–¹æ³•3ï¼šä¸¤é˜¶æ®µèšåˆ
val stage1RDD = originalRDD
  .map(x => (x._1 + "_" + Random.nextInt(10), x._2))  // æ·»åŠ éšæœºå‰ç¼€
  .reduceByKey(_ + _)
  .map(x => (x._1.split("_")(0), x._2))  // å»æ‰éšæœºå‰ç¼€
  .reduceByKey(_ + _)
```

**3. ä¼˜åŒ–ç¼“å­˜ç­–ç•¥**ï¼š

```scala
// ä½¿ç”¨MEMORY_AND_DISK_SERå‡å°‘å†…å­˜å ç”¨
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

// åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpointå‡å°‘å†…å­˜å‹åŠ›
rdd.checkpoint()
```

**4. ä»£ç ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨mapPartitionså‡å°‘å¯¹è±¡åˆ›å»º
val optimizedRDD = rdd.mapPartitions(iter => {
  val result = new ArrayBuffer[String]()
  while (iter.hasNext) {
    val item = iter.next()
    // å¤„ç†é€»è¾‘
    result += processedItem
  }
  result.iterator
})

// å¤ç”¨å¯¹è±¡
case class User(id: Long, name: String)
val userRDD = rdd.mapPartitions(iter => {
  val user = User(0L, "")  // å¤ç”¨å¯¹è±¡
  iter.map { case (id, name) =>
    user.id = id
    user.name = name
    user.copy()  // è¿”å›å‰¯æœ¬
  }
})
```

1. OutOfMemoryError: Direct buffer memory

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Direct buffer memory
    at java.nio.Bits.reserveMemory(Bits.java:694)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
```

**åŸå› åˆ†æ**ï¼š

- **å †å¤–å†…å­˜ä¸è¶³**ï¼šDirectBufferå†…å­˜è®¾ç½®è¿‡å°
- **ç½‘ç»œä¼ è¾“è¿‡å¤š**ï¼šå¤§é‡æ•°æ®é€šè¿‡ç½‘ç»œä¼ è¾“
- **åºåˆ—åŒ–é—®é¢˜**ï¼šåºåˆ—åŒ–è¿‡ç¨‹ä¸­å ç”¨è¿‡å¤šå †å¤–å†…å­˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ å †å¤–å†…å­˜**ï¼š

```bash
spark-submit \
  --conf spark.executor.memoryOverhead=4g \
  --conf spark.driver.memoryOverhead=2g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  your-app.jar
```

**2. ä¼˜åŒ–ç½‘ç»œä¼ è¾“**ï¼š

```scala
// å¯ç”¨å‹ç¼©
spark.conf.set("spark.io.compression.codec", "snappy")
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")

// è°ƒæ•´ç½‘ç»œç¼“å†²åŒº
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.executor.heartbeatInterval", "60s")
```

**3. ä¼˜åŒ–åºåˆ—åŒ–**ï¼š

```scala
// ä½¿ç”¨Kryoåºåˆ—åŒ–
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")

// æ³¨å†Œè‡ªå®šä¹‰ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(classOf[MyClass]))
```

1. OutOfMemoryError: Metaspace

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Metaspace
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»åŠ è½½è¿‡å¤š**ï¼šåŠ¨æ€ç”Ÿæˆå¤§é‡ç±»
- **Metaspaceè®¾ç½®è¿‡å°**ï¼šJVM Metaspaceç©ºé—´ä¸è¶³
- **UDFä½¿ç”¨è¿‡å¤š**ï¼šå¤§é‡UDFå¯¼è‡´ç±»åŠ è½½

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´JVMå‚æ•°**ï¼š

```bash
spark-submit \
  --conf spark.executor.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  --conf spark.driver.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  your-app.jar
```

**2. ä¼˜åŒ–UDFä½¿ç”¨**ï¼š

```scala
// é¿å…åœ¨UDFä¸­åˆ›å»ºè¿‡å¤šç±»
val optimizedUDF = udf((value: String) => {
  // ä½¿ç”¨ç®€å•é€»è¾‘ï¼Œé¿å…åŠ¨æ€ç±»ç”Ÿæˆ
  value.toUpperCase
})

// å¤ç”¨UDFå®ä¾‹
val myUDF = udf((x: Int) => x * 2)
df.select(myUDF(col("value")))
```

### ç½‘ç»œç›¸å…³é”™è¯¯

1. FetchFailedException

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hostname:7337
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
```

**åŸå› åˆ†æ**ï¼š

- **ç½‘ç»œè¶…æ—¶**ï¼šç½‘ç»œè¿æ¥è¶…æ—¶
- **Executorä¸¢å¤±**ï¼šExecutorè¿›ç¨‹å¼‚å¸¸é€€å‡º
- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³å¯¼è‡´è¿›ç¨‹é€€å‡º
- **ç½‘ç»œä¸ç¨³å®š**ï¼šç½‘ç»œè¿æ¥ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´ç½‘ç»œè¶…æ—¶å‚æ•°**ï¼š

```bash
spark-submit \
  --conf spark.network.timeout=800s \
  --conf spark.executor.heartbeatInterval=60s \
  --conf spark.rpc.askTimeout=800s \
  --conf spark.rpc.lookupTimeout=800s \
  your-app.jar
```

**2. å¢åŠ é‡è¯•æœºåˆ¶**ï¼š

```bash
spark-submit \
  --conf spark.task.maxFailures=8 \
  --conf spark.stage.maxAttempts=4 \
  your-app.jar
```

**3. ä¼˜åŒ–Shuffleé…ç½®**ï¼š

```bash
spark-submit \
  --conf spark.shuffle.io.maxRetries=3 \
  --conf spark.shuffle.io.retryWait=60s \
  --conf spark.shuffle.file.buffer=32k \
  your-app.jar
```

**4. ç›‘æ§ExecutorçŠ¶æ€**ï¼š

```scala
// æ·»åŠ ç›‘æ§ä»£ç 
spark.sparkContext.addSparkListener(new SparkListener {
  override def onExecutorLost(executorLost: SparkListenerExecutorLost): Unit = {
    println(s"Executor lost: ${executorLost.executorId}")
    // è®°å½•æ—¥å¿—æˆ–å‘é€å‘Šè­¦
  }
})
```

1. ConnectionTimeoutException

**é”™è¯¯ç°è±¡**ï¼š

```
java.net.ConnectTimeoutException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
```

**åŸå› åˆ†æ**ï¼š

- **ç½‘ç»œå»¶è¿Ÿ**ï¼šç½‘ç»œå»¶è¿Ÿè¿‡é«˜
- **é˜²ç«å¢™é™åˆ¶**ï¼šé˜²ç«å¢™é˜»æ­¢è¿æ¥
- **ç«¯å£å†²çª**ï¼šç«¯å£è¢«å ç”¨
- **DNSè§£æé—®é¢˜**ï¼šDNSè§£æå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´è¿æ¥è¶…æ—¶**ï¼š

```bash
spark-submit \
  --conf spark.network.timeout=1200s \
  --conf spark.rpc.askTimeout=1200s \
  your-app.jar
```

**2. æ£€æŸ¥ç½‘ç»œé…ç½®**ï¼š

```bash
# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping hostname
telnet hostname port

# æ£€æŸ¥é˜²ç«å¢™
iptables -L
```

**3. ä½¿ç”¨æœ¬åœ°åŒ–ç­–ç•¥**ï¼š

```scala
// å¯ç”¨æ•°æ®æœ¬åœ°åŒ–
spark.conf.set("spark.locality.wait", "30s")
spark.conf.set("spark.locality.wait.process", "30s")
spark.conf.set("spark.locality.wait.node", "30s")
spark.conf.set("spark.locality.wait.rack", "30s")
```

### åºåˆ—åŒ–ç›¸å…³é”™è¯¯

1. NotSerializableException

**é”™è¯¯ç°è±¡**ï¼š

```
java.io.NotSerializableException: com.example.MyClass
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»æœªå®ç°Serializable**ï¼šè‡ªå®šä¹‰ç±»æœªå®ç°Serializableæ¥å£
- **é—­åŒ…é—®é¢˜**ï¼šåœ¨é—­åŒ…ä¸­å¼•ç”¨äº†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
- **é™æ€å˜é‡**ï¼šå¼•ç”¨äº†é™æ€å˜é‡æˆ–å•ä¾‹å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å®ç°Serializableæ¥å£**ï¼š

```scala
// æ–¹æ³•1ï¼šå®ç°Serializable
case class MyClass(id: Int, name: String) extends Serializable

// æ–¹æ³•2ï¼šä½¿ç”¨@transientæ³¨è§£
class MyClass(val id: Int, val name: String) extends Serializable {
  @transient
  private val nonSerializableField = new NonSerializableClass()
}
```

**2. é¿å…é—­åŒ…é—®é¢˜**ï¼š

```scala
// é”™è¯¯ç¤ºä¾‹
val nonSerializableObject = new NonSerializableClass()
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => nonSerializableObject.process(x))  // ä¼šæŠ¥é”™

// æ­£ç¡®ç¤ºä¾‹
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.mapPartitions(iter => {
  val localObject = new NonSerializableClass()  // åœ¨åˆ†åŒºå†…åˆ›å»º
  iter.map(x => localObject.process(x))
})
```

**3. ä½¿ç”¨å¹¿æ’­å˜é‡**ï¼š

```scala
// å¯¹äºåªè¯»çš„å¤§å¯¹è±¡ï¼Œä½¿ç”¨å¹¿æ’­å˜é‡
val largeData = spark.sparkContext.parallelize(1 to 1000000).collect()
val broadcastData = spark.sparkContext.broadcast(largeData)

val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => {
  val data = broadcastData.value  // ä½¿ç”¨å¹¿æ’­å˜é‡
  processWithData(x, data)
})
```

1. KryoSerializationException

**é”™è¯¯ç°è±¡**ï¼š

```
com.esotericsoftware.kryo.KryoException: java.lang.ClassNotFoundException: com.example.MyClass
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»æœªæ³¨å†Œ**ï¼šä½¿ç”¨Kryoåºåˆ—åŒ–æ—¶ç±»æœªæ³¨å†Œ
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **ç‰ˆæœ¬ä¸å…¼å®¹**ï¼šåºåˆ—åŒ–å’Œååºåˆ—åŒ–ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ³¨å†Œè‡ªå®šä¹‰ç±»**ï¼š

```scala
val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[MyOtherClass]
))

val spark = SparkSession.builder()
  .config(conf)
  .getOrCreate()
```

**2. ä½¿ç”¨Kryoæ³¨å†Œå™¨**ï¼š

```scala
class MyKryoRegistrator extends KryoRegistrator {
  override def registerClasses(kryo: Kryo): Unit = {
    kryo.register(classOf[MyClass])
    kryo.register(classOf[MyOtherClass])
  }
}

val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.set("spark.kryo.registrator", "com.example.MyKryoRegistrator")
```

**3. ç¦ç”¨ä¸¥æ ¼æ¨¡å¼**ï¼š

```scala
spark.conf.set("spark.kryo.registrationRequired", "false")
```

### èµ„æºç›¸å…³é”™è¯¯

1. ExecutorLostFailure

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.scheduler.ExecutorLostFailure: Executor 1 lost
    at org.apache.spark.scheduler.TaskSetManager$$anonfun$abortIfCompletelyBlacklisted$1.apply(TaskSetManager.scala:1023)
```

**åŸå› åˆ†æ**ï¼š

- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³è¢«æ€æ­»
- **CPUè¿‡è½½**ï¼šCPUä½¿ç”¨ç‡è¿‡é«˜å¯¼è‡´è¿›ç¨‹å¼‚å¸¸
- **ç£ç›˜ç©ºé—´ä¸è¶³**ï¼šç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´å†™å…¥å¤±è´¥
- **ç½‘ç»œé—®é¢˜**ï¼šç½‘ç»œè¿æ¥é—®é¢˜å¯¼è‡´å¿ƒè·³è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ èµ„æºé…é¢**ï¼š

```bash
spark-submit \
  --executor-memory 8g \
  --executor-cores 4 \
  --num-executors 10 \
  --conf spark.executor.memoryOverhead=2g \
  your-app.jar
```

**2. ç›‘æ§èµ„æºä½¿ç”¨**ï¼š

```scala
// æ·»åŠ èµ„æºç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    println(s"Task ${taskEnd.taskInfo.taskId} completed:")
    println(s"  Duration: ${taskEnd.taskInfo.duration}ms")
    println(s"  Memory: ${metrics.peakExecutionMemory} bytes")
    println(s"  Disk: ${metrics.diskBytesSpilled} bytes spilled")
  }
})
```

**3. ä¼˜åŒ–èµ„æºåˆ†é…**ï¼š

```bash
spark-submit \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=2 \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.dynamicAllocation.initialExecutors=5 \
  your-app.jar
```

1. NoClassDefFoundError

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.NoClassDefFoundError: com.example.MyClass
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š

- **ä¾èµ–ç¼ºå¤±**ï¼šç¼ºå°‘å¿…è¦çš„jaråŒ…
- **ç‰ˆæœ¬å†²çª**ï¼šä¾èµ–ç‰ˆæœ¬å†²çª
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **æ‰“åŒ…é—®é¢˜**ï¼šjaråŒ…æ‰“åŒ…ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ·»åŠ ä¾èµ–jaråŒ…**ï¼š

```bash
spark-submit \
  --jars /path/to/dependency1.jar,/path/to/dependency2.jar \
  --conf spark.executor.extraClassPath=/path/to/dependencies/* \
  your-app.jar
```

**2. ä½¿ç”¨fat jar**ï¼š

```xml
<!-- Mavené…ç½® -->
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.2.4</version>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
    </execution>
  </executions>
</plugin>
```

**3. æ£€æŸ¥ä¾èµ–å†²çª**ï¼š

```bash
# æŸ¥çœ‹ä¾èµ–æ ‘
mvn dependency:tree

# æ’é™¤å†²çªä¾èµ–
<dependency>
  <groupId>com.example</groupId>
  <artifactId>library</artifactId>
  <version>1.0.0</version>
  <exclusions>
    <exclusion>
      <groupId>conflicting.group</groupId>
      <artifactId>conflicting-artifact</artifactId>
    </exclusion>
  </exclusions>
</dependency>
```

### æ•°æ®ç›¸å…³é”™è¯¯

1. FileNotFoundException

**é”™è¯¯ç°è±¡**ï¼š

```
java.io.FileNotFoundException: File does not exist: hdfs://namenode:8020/path/to/file
    at org.apache.hadoop.hdfs.DFSClient.checkPath(DFSClient.java:1274)
    at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1527)
```

**åŸå› åˆ†æ**ï¼š

- **æ–‡ä»¶ä¸å­˜åœ¨**ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶è¢«åˆ é™¤
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è¯»å–æ–‡ä»¶çš„æƒé™
- **è·¯å¾„é”™è¯¯**ï¼šæ–‡ä»¶è·¯å¾„æ ¼å¼é”™è¯¯
- **HDFSé—®é¢˜**ï¼šHDFSæœåŠ¡å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„**ï¼š

```scala
// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
val hadoopConf = spark.sparkContext.hadoopConfiguration
val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
val path = new org.apache.hadoop.fs.Path("/path/to/file")

if (fs.exists(path)) {
  println("File exists")
} else {
  println("File does not exist")
}
```

**2. è®¾ç½®æ–‡ä»¶ç³»ç»Ÿé…ç½®**ï¼š

```scala
// è®¾ç½®HDFSé…ç½®
spark.conf.set("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020")
spark.conf.set("spark.hadoop.dfs.namenode.rpc-address", "namenode:8020")
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š

```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
hdfs dfs -ls /path/to/file

# ä¿®æ”¹æ–‡ä»¶æƒé™
hdfs dfs -chmod 644 /path/to/file

# ä¿®æ”¹æ–‡ä»¶æ‰€æœ‰è€…
hdfs dfs -chown username:group /path/to/file
```

1. DataSourceException

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.sql.AnalysisException: Table or view not found: table_name
    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
```

**åŸå› åˆ†æ**ï¼š

- **è¡¨ä¸å­˜åœ¨**ï¼šæ•°æ®åº“è¡¨ä¸å­˜åœ¨
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è®¿é—®è¡¨çš„æƒé™
- **æ•°æ®åº“è¿æ¥é—®é¢˜**ï¼šæ•°æ®åº“è¿æ¥å¤±è´¥
- **è¡¨åé”™è¯¯**ï¼šè¡¨åæ‹¼å†™é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨**ï¼š

```scala
// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
val tables = spark.catalog.listTables()
tables.filter(_.name == "table_name").show()

// æˆ–è€…ä½¿ç”¨SQL
spark.sql("SHOW TABLES").show()
```

**2. è®¾ç½®æ•°æ®åº“è¿æ¥**ï¼š

```scala
// è®¾ç½®æ•°æ®åº“è¿æ¥
spark.conf.set("spark.sql.warehouse.dir", "/user/hive/warehouse")
spark.conf.set("hive.metastore.uris", "thrift://metastore:9083")

// ä½¿ç”¨Hive
spark.sql("USE database_name")
spark.sql("SHOW TABLES").show()
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š

```sql
-- æˆäºˆæƒé™
GRANT SELECT ON TABLE table_name TO USER username;

-- æ£€æŸ¥æƒé™
SHOW GRANT USER username ON TABLE table_name;
```

### è°ƒè¯•å’Œè¯Šæ–­å·¥å…·

1. Spark Web UI

**è®¿é—®æ–¹å¼**ï¼š

```
http://driver-host:4040  # åº”ç”¨è¿è¡Œæ—¶
http://driver-host:18080 # å†å²æœåŠ¡å™¨
```

**å…³é”®æŒ‡æ ‡**ï¼š

- **Stagesé¡µé¢**ï¼šæŸ¥çœ‹Stageæ‰§è¡Œæƒ…å†µå’Œå¤±è´¥åŸå› 
- **Executorsé¡µé¢**ï¼šæŸ¥çœ‹Executorèµ„æºä½¿ç”¨æƒ…å†µ
- **Storageé¡µé¢**ï¼šæŸ¥çœ‹RDDç¼“å­˜æƒ…å†µ
- **Environmenté¡µé¢**ï¼šæŸ¥çœ‹é…ç½®å‚æ•°

1. æ—¥å¿—åˆ†æ

**æŸ¥çœ‹æ—¥å¿—**ï¼š

```bash
# æŸ¥çœ‹Driveræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-driver-*.log

# æŸ¥çœ‹Executoræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-executor-*.log

# æŸ¥çœ‹YARNæ—¥å¿—
yarn logs -applicationId application_1234567890_0001
```

**å…³é”®æ—¥å¿—æ¨¡å¼**ï¼š

```bash
# æŸ¥æ‰¾é”™è¯¯ä¿¡æ¯
grep -i "error\|exception\|failed" /path/to/logs/*.log

# æŸ¥æ‰¾å†…å­˜ç›¸å…³é”™è¯¯
grep -i "outofmemory\|oom" /path/to/logs/*.log

# æŸ¥æ‰¾ç½‘ç»œç›¸å…³é”™è¯¯
grep -i "timeout\|connection" /path/to/logs/*.log
```

1. æ€§èƒ½åˆ†æå·¥å…·

**JVMåˆ†æ**ï¼š

```bash
# æŸ¥çœ‹JVMå †å†…å­˜ä½¿ç”¨
jstat -gc <pid> 1000

# æŸ¥çœ‹çº¿ç¨‹çŠ¶æ€
jstack <pid>

# æŸ¥çœ‹å†…å­˜dump
jmap -dump:format=b,file=heap.hprof <pid>
```

**ç³»ç»Ÿèµ„æºç›‘æ§**ï¼š

```bash
# æŸ¥çœ‹ç³»ç»Ÿèµ„æºä½¿ç”¨
top -p <pid>
iostat -x 1
netstat -i
```

1. è°ƒè¯•ä»£ç 

**æ·»åŠ è°ƒè¯•ä¿¡æ¯**ï¼š

```scala
// æ·»åŠ æ—¥å¿—
import org.apache.log4j.{Level, Logger}
Logger.getLogger("org.apache.spark").setLevel(Level.DEBUG)

// æ·»åŠ ç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {
    println(s"Task started: ${taskStart.taskInfo.taskId}")
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    println(s"Task ended: ${taskEnd.taskInfo.taskId}, " +
            s"status: ${taskEnd.taskInfo.status}")
  }
})
```

**ä½¿ç”¨Spark Shellè°ƒè¯•**ï¼š

```scala
// å¯åŠ¨Spark Shell
spark-shell --master local[*]

// é€æ­¥è°ƒè¯•
val rdd = sc.textFile("/path/to/file")
rdd.take(10).foreach(println)  // æŸ¥çœ‹æ•°æ®
rdd.count()  // æ£€æŸ¥æ•°æ®é‡
```

### é¢„é˜²æªæ–½

1. é…ç½®ä¼˜åŒ–

**åŸºç¡€é…ç½®**ï¼š

```bash
# å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2g
spark.driver.memory=4g
spark.driver.memoryOverhead=1g

# ç½‘ç»œé…ç½®
spark.network.timeout=800s
spark.executor.heartbeatInterval=60s

# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
```

**æ€§èƒ½é…ç½®**ï¼š

```bash
# Shuffleé…ç½®
spark.shuffle.file.buffer=32k
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
```

1. ä»£ç æœ€ä½³å®è·µ

**å†…å­˜ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨å¹¿æ’­å˜é‡
val broadcastVar = sc.broadcast(largeData)

// åŠæ—¶é‡Šæ”¾ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpoint
rdd.checkpoint()
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š

```scala
// åˆç†è®¾ç½®åˆ†åŒºæ•°
val optimalPartitions = Math.max(rdd.partitions.length, 200)
val repartitionedRDD = rdd.repartition(optimalPartitions)

// ä½¿ç”¨mapPartitions
val optimizedRDD = rdd.mapPartitions(iter => {
  // æ‰¹é‡å¤„ç†é€»è¾‘
  iter.map(processItem)
})
```

1. ç›‘æ§å‘Šè­¦

**è®¾ç½®ç›‘æ§**ï¼š

```scala
// æ·»åŠ ç›‘æ§æŒ‡æ ‡
val metrics = spark.sparkContext.getStatusTracker
val stageInfo = metrics.getStageInfo(stageId)
println(s"Stage $stageId: ${stageInfo.numTasks} tasks, " +
        s"${stageInfo.numCompletedTasks} completed")
```

**å‘Šè­¦é…ç½®**ï¼š

```bash
# è®¾ç½®å‘Šè­¦é˜ˆå€¼
spark.executor.failures.max=3
spark.task.maxFailures=8
spark.stage.maxAttempts=4
```

é€šè¿‡ä»¥ä¸Šè¯¦ç»†çš„é”™è¯¯åˆ†æå’Œè§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆå¤„ç†Sparkä»»åŠ¡ä¸­çš„å¸¸è§é—®é¢˜ï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

---

---

---

## å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿ âš™ï¸

### èµ„æºç›¸å…³å‚æ•°

**å†…å­˜é…ç½®**ï¼š

```properties
# åŸºç¡€å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2048m
spark.driver.memory=4g
spark.driver.memoryOverhead=1024m

# å †å¤–å†…å­˜
spark.memory.offHeap.enabled=true
spark.memory.offHeap.size=4g

# å†…å­˜åˆ†é…æ¯”ä¾‹
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3

# åŠ¨æ€å†…å­˜åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=5
spark.dynamicAllocation.maxExecutors=100
spark.dynamicAllocation.initialExecutors=10
```

**CPUé…ç½®**ï¼š

```properties
# CPUèµ„æºé…ç½®
spark.executor.cores=4
spark.driver.cores=2
spark.default.parallelism=400
spark.sql.shuffle.partitions=400

# ä»»åŠ¡è°ƒåº¦
spark.task.cpus=1
spark.task.maxFailures=8
spark.stage.maxConsecutiveAttempts=4
```

### JVMç›¸å…³å‚æ•°

**åƒåœ¾å›æ”¶é…ç½®**ï¼š

```bash
# G1GCé…ç½®ï¼ˆæ¨èï¼‰
--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC 
-XX:G1HeapRegionSize=16m 
-XX:MaxGCPauseMillis=200 
-XX:+G1PrintRegionRememberedSetInfo 
-XX:+UseCompressedOops 
-XX:+UseCompressedClassPointers
-XX:+PrintHeapAtGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps
-Xloggc:/var/log/spark/gc-executor.log"

--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC 
-XX:MaxGCPauseMillis=200 
-XX:+PrintHeapAtGC
-Xloggc:/var/log/spark/gc-driver.log"
```

**å¹¶å‘GCé…ç½®**ï¼š

```bash
# CMS GCé…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC 
-XX:+CMSParallelRemarkEnabled 
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:CMSInitiatingOccupancyFraction=70
-XX:+PrintGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps"
```

**å†…å­˜è°ƒè¯•å‚æ•°**ï¼š

```bash
# å†…å­˜è°ƒè¯•é…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/log/spark/heapdump
-XX:+TraceClassLoading
-XX:+PrintStringDeduplication"
```

### æ€§èƒ½ä¼˜åŒ–å‚æ•°

**SQLæ‰§è¡Œä¼˜åŒ–**ï¼š

```properties
# è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# æ•°æ®æ ¼å¼ä¼˜åŒ–
spark.sql.parquet.columnarReaderBatchSize=10000
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.binaryAsString=false
spark.sql.execution.arrow.pyspark.enabled=true

# Joinä¼˜åŒ–
spark.sql.autoBroadcastJoinThreshold=100MB
spark.sql.broadcastTimeout=300s
```

**Shuffleä¼˜åŒ–**ï¼š

```properties
# ShuffleåŸºç¡€é…ç½®
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.file.buffer=64k
spark.shuffle.unsafe.file.output.buffer=64k

# Shuffleé«˜çº§é…ç½®
spark.shuffle.sort.bypassMergeThreshold=200
spark.shuffle.consolidateFiles=true
spark.shuffle.memoryFraction=0.2
spark.shuffle.safetyFraction=0.8

# Shuffleç½‘ç»œé…ç½®
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s
spark.shuffle.service.enabled=true
```

**å…¶ä»–å¸¸ç”¨å‚æ•°**ï¼š

```properties
# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryo.unsafe=true

# å‹ç¼©é…ç½®
spark.io.compression.codec=snappy
spark.broadcast.compress=true
spark.rdd.compress=true

# æ–‡ä»¶ç³»ç»Ÿé…ç½®
spark.files.maxPartitionBytes=128m
spark.files.openCostInBytes=4m

# Hadoopå…¼å®¹é…ç½®
spark.hadoop.parquet.enable.summary-metadata=false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
```

### é…ç½®æ¨¡æ¿

**å¼€å‘ç¯å¢ƒé…ç½®**ï¼š

```bash
#!/bin/bash
# å¼€å‘ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master local[*] \
  --deploy-mode client \
  --executor-memory 2g \
  --driver-memory 1g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  your-app.jar
```

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š

```bash
#!/bin/bash
# ç”Ÿäº§ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 50 \
  --executor-memory 8g \
  --executor-cores 4 \
  --driver-memory 4g \
  --driver-cores 2 \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memoryOverhead=1g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=10 \
  --conf spark.dynamicAllocation.maxExecutors=100 \
  --conf spark.shuffle.service.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.execution.arrow.pyspark.enabled=true \
  your-app.jar
```

---

## Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥

### åŸºç¡€æ¦‚å¿µé¢˜

**1. ä»€ä¹ˆæ˜¯RDDï¼Ÿå®ƒæœ‰å“ªäº›ç‰¹æ€§ï¼Ÿ**

RDDï¼ˆResilient Distributed Datasetï¼‰æ˜¯Sparkçš„æ ¸å¿ƒæŠ½è±¡ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

- **ä¸å¯å˜æ€§**ï¼šæ•°æ®ä¸€æ—¦åˆ›å»ºä¸å¯ä¿®æ”¹ï¼Œä¿è¯äº†æ•°æ®çš„ä¸€è‡´æ€§
- **åˆ†å¸ƒå¼**ï¼šæ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤çš„å¤šä¸ªèŠ‚ç‚¹ä¸Š
- **å¼¹æ€§å®¹é”™**ï¼šé€šè¿‡è¡€ç»Ÿä¿¡æ¯è‡ªåŠ¨å®¹é”™æ¢å¤
- **æƒ°æ€§æ±‚å€¼**ï¼šTransformationæ“ä½œå»¶è¿Ÿæ‰§è¡Œï¼Œç›´åˆ°Actionæ“ä½œè§¦å‘
- **åˆ†åŒºè®¡ç®—**ï¼šæ”¯æŒå¹¶è¡Œè®¡ç®—ï¼Œæé«˜å¤„ç†æ•ˆç‡

**2. RDDã€DataFrameã€Datasetä¸‰è€…çš„åŒºåˆ«ï¼Ÿ**


| ç‰¹æ€§           | RDD            | DataFrame       | Dataset          |
| ---------------- | ---------------- | ----------------- | ------------------ |
| **æ•°æ®æŠ½è±¡**   | åˆ†å¸ƒå¼å¯¹è±¡é›†åˆ | ç»“æ„åŒ–æ•°æ®è¡¨    | ç±»å‹å®‰å…¨çš„æ•°æ®è¡¨ |
| **ç¼–è¯‘æ—¶æ£€æŸ¥** | âŒ             | âŒ              | âœ…               |
| **æ‰§è¡Œä¼˜åŒ–**   | âŒ             | âœ… Catalystä¼˜åŒ– | âœ… Catalystä¼˜åŒ–  |
| **åºåˆ—åŒ–**     | Java/Kryo      | TungstenäºŒè¿›åˆ¶  | TungstenäºŒè¿›åˆ¶   |
| **é€‚ç”¨åœºæ™¯**   | åº•å±‚æ“ä½œ       | SQLæŸ¥è¯¢         | ç±»å‹å®‰å…¨è¦æ±‚é«˜   |

### æ¶æ„åŸç†é¢˜

**3. Sparkçš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿå„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

- **Driver Program**ï¼šè¿è¡Œåº”ç”¨ç¨‹åºmainå‡½æ•°ï¼Œåˆ›å»ºSparkContextï¼Œåè°ƒä»»åŠ¡æ‰§è¡Œ
- **SparkContext**ï¼šSparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ä¸é›†ç¾¤å»ºç«‹è¿æ¥
- **Cluster Manager**ï¼šé›†ç¾¤èµ„æºç®¡ç†å™¨ï¼ˆYARNã€Mesosã€Kubernetesç­‰ï¼‰
- **Executor**ï¼šè¿è¡Œåœ¨WorkerèŠ‚ç‚¹ä¸Šçš„JVMè¿›ç¨‹ï¼Œæ‰§è¡Œå…·ä½“çš„Task
- **DAGScheduler**ï¼šå°†ä½œä¸šåˆ†è§£ä¸ºStageï¼ŒæŒ‰ä¾èµ–å…³ç³»è°ƒåº¦
- **TaskScheduler**ï¼šå°†Taskåˆ†å‘åˆ°Executoræ‰§è¡Œ

**4. Sparkçš„ä»»åŠ¡è°ƒåº¦æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ**

1. **ç”¨æˆ·æäº¤Actionæ“ä½œ**
2. **SparkContextåˆ›å»ºä½œä¸š**ï¼Œè°ƒç”¨DAGScheduler.runJob
3. **DAGScheduleråˆ†æä¾èµ–å…³ç³»**ï¼Œåˆ’åˆ†Stage
4. **æŒ‰æ‹“æ‰‘é¡ºåºæäº¤Stage**ï¼Œåˆ›å»ºTaskSet
5. **TaskScheduleræ¥æ”¶TaskSet**ï¼Œåˆ†é…èµ„æº
6. **Taskåˆ†å‘åˆ°Executor**æ‰§è¡Œ
7. **æ”¶é›†æ‰§è¡Œç»“æœ**ï¼Œå®Œæˆä½œä¸š

### æ€§èƒ½è°ƒä¼˜é¢˜

**5. Sparkä¸­å¦‚ä½•å¤„ç†æ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿ**

- **åŠ ç›å¤„ç†**ï¼šä¸ºå€¾æ–œçš„keyæ·»åŠ éšæœºå‰ç¼€ï¼Œåˆ†æ•£åˆ°å¤šä¸ªåˆ†åŒº
- **ä¸¤é˜¶æ®µèšåˆ**ï¼šå…ˆæœ¬åœ°èšåˆï¼Œå†å…¨å±€èšåˆ
- **è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼šæ ¹æ®ä¸šåŠ¡é€»è¾‘è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥
- **å¹¿æ’­Join**ï¼šå°†å°è¡¨å¹¿æ’­ï¼Œé¿å…Shuffle
- **å¢åŠ åˆ†åŒºæ•°**ï¼šæé«˜å¹¶è¡Œåº¦ï¼Œå‡å°‘å•ä¸ªåˆ†åŒºæ•°æ®é‡

**6. Sparkæ€§èƒ½è°ƒä¼˜çš„ä¸»è¦ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ**

- **èµ„æºé…ç½®è°ƒä¼˜**ï¼šåˆç†è®¾ç½®Executorå†…å­˜ã€æ ¸å¿ƒæ•°ã€æ•°é‡
- **ä»£ç ä¼˜åŒ–**ï¼šä½¿ç”¨åˆé€‚çš„ç®—å­ã€é¿å…é‡å¤è®¡ç®—ã€ä½¿ç”¨å¹¿æ’­å˜é‡
- **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†é€‰æ‹©å­˜å‚¨çº§åˆ«ï¼ŒåŠæ—¶é‡Šæ”¾ç¼“å­˜
- **Shuffleä¼˜åŒ–**ï¼šå‡å°‘Shuffleæ“ä½œï¼Œè°ƒæ•´åˆ†åŒºæ•°
- **åºåˆ—åŒ–ä¼˜åŒ–**ï¼šä½¿ç”¨Kryoåºåˆ—åŒ–å™¨
- **åƒåœ¾å›æ”¶ä¼˜åŒ–**ï¼šè°ƒæ•´GCå‚æ•°

### å®æˆ˜åº”ç”¨é¢˜

**7. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•ç›‘æ§Sparkåº”ç”¨ï¼Ÿ**

- **Spark UIç›‘æ§**ï¼šæŸ¥çœ‹Jobsã€Stagesã€Storageã€Executorsé¡µé¢
- **åº”ç”¨ç¨‹åºæ—¥å¿—**ï¼šåˆ†æDriverå’ŒExecutoræ—¥å¿—
- **ç³»ç»ŸæŒ‡æ ‡ç›‘æ§**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œä½¿ç”¨ç‡
- **ç¬¬ä¸‰æ–¹å·¥å…·**ï¼šGangliaã€Nagiosã€Prometheusç­‰
- **è‡ªå®šä¹‰ç›‘æ§**ï¼šé€šè¿‡Spark Listeneræ”¶é›†æŒ‡æ ‡

**8. Spark Streamingä¸Structured Streamingçš„åŒºåˆ«ï¼Ÿ**


| ç‰¹æ€§         | Spark Streaming | Structured Streaming |
| -------------- | ----------------- | ---------------------- |
| **å¤„ç†æ¨¡å‹** | å¾®æ‰¹æ¬¡DStream   | è¿ç»­æµè¡¨             |
| **API**      | DStream API     | DataFrame/SQL API    |
| **ä¼˜åŒ–**     | æ—               | Catalystä¼˜åŒ–å™¨       |
| **å®¹é”™**     | Checkpoint+WAL  | Checkpoint+æ—¥å¿—      |
| **å»¶è¿Ÿ**     | ç§’çº§            | æ¯«ç§’çº§               |

### æ·±åº¦æŠ€æœ¯åŸç†é¢˜

**9. Shuffleçš„å®ç°åŸç†æ˜¯ä»€ä¹ˆï¼Ÿä¸åŒShuffleç±»å‹çš„åŒºåˆ«ï¼Ÿ**

**Shuffleå®ç°åŸç†**ï¼š

- **Mapç«¯**ï¼šå°†æ•°æ®æŒ‰åˆ†åŒºIDå†™å…¥æœ¬åœ°ç£ç›˜æ–‡ä»¶
- **Reduceç«¯**ï¼šä»å„ä¸ªMapç«¯æ‹‰å–å¯¹åº”åˆ†åŒºçš„æ•°æ®
- **åè°ƒæœºåˆ¶**ï¼šé€šè¿‡MapOutputTrackerç®¡ç†å…ƒæ•°æ®

**Shuffleç±»å‹å¯¹æ¯”**ï¼š

- **Hash Shuffle**ï¼šæ¯ä¸ªMap Taskä¸ºæ¯ä¸ªReduce Taskåˆ›å»ºæ–‡ä»¶ï¼Œæ–‡ä»¶æ•°MÃ—N
- **Sort Shuffle**ï¼šæ¯ä¸ªMap Taskåˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼ŒæŒ‰åˆ†åŒºIDæ’åº
- **Tungsten Sort**ï¼šä½¿ç”¨å †å¤–å†…å­˜ï¼Œä¼˜åŒ–æ’åºæ€§èƒ½

**10. Sparkçš„å†…å­˜ç®¡ç†æœºåˆ¶æ˜¯æ€æ ·çš„ï¼Ÿ**

Sparkä½¿ç”¨ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼ˆUnified Memory Managerï¼‰ï¼š

- **å­˜å‚¨å†…å­˜**ï¼šç”¨äºRDDç¼“å­˜ã€å¹¿æ’­å˜é‡
- **æ‰§è¡Œå†…å­˜**ï¼šç”¨äºShuffleã€Joinã€èšåˆç­‰æ“ä½œ
- **åŠ¨æ€è°ƒæ•´**ï¼šå­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜å¯ä»¥ç›¸äº’å€Ÿç”¨
- **å†…å­˜åˆ†åŒº**ï¼šå †å†…å†…å­˜å’Œå †å¤–å†…å­˜åˆ†åˆ«ç®¡ç†
