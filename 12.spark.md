# 12. Spark

## ç›®å½•

- [12. Spark](#12-spark)
  - [ç›®å½•](#ç›®å½•)
  - [Spark æ¦‚è¿°ä¸ç¯å¢ƒ](#spark-æ¦‚è¿°ä¸ç¯å¢ƒ)
    - [Sparkç®€ä»‹](#sparkç®€ä»‹)
      - [Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿](#sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿)
      - [Spark vs Hadoop MapReduce](#spark-vs-hadoop-mapreduce)
      - [Sparkåº”ç”¨åœºæ™¯](#sparkåº”ç”¨åœºæ™¯)
    - [Sparkç”Ÿæ€ç³»ç»Ÿ](#sparkç”Ÿæ€ç³»ç»Ÿ)
      - [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
      - [ç”Ÿæ€ç»„ä»¶](#ç”Ÿæ€ç»„ä»¶)
    - [Sparkç¯å¢ƒæ­å»º](#sparkç¯å¢ƒæ­å»º)
      - [æœ¬åœ°æ¨¡å¼](#æœ¬åœ°æ¨¡å¼)
      - [é›†ç¾¤æ¨¡å¼](#é›†ç¾¤æ¨¡å¼)
      - [å¸¸ç”¨é…ç½®](#å¸¸ç”¨é…ç½®)
  - [Spark æ ¸å¿ƒæ¦‚å¿µ â­](#spark-æ ¸å¿ƒæ¦‚å¿µ-)
    - [RDDæ ¸å¿ƒæ¦‚å¿µ](#rddæ ¸å¿ƒæ¦‚å¿µ)
      - [RDDç‰¹æ€§](#rddç‰¹æ€§)
      - [RDDæ“ä½œåˆ†ç±»](#rddæ“ä½œåˆ†ç±»)
      - [RDDä¾èµ–å…³ç³»](#rddä¾èµ–å…³ç³»)
    - [DataFrameä¸Dataset](#dataframeä¸dataset)
      - [DataFrameæ¦‚å¿µ](#dataframeæ¦‚å¿µ)
      - [Datasetæ¦‚å¿µ](#datasetæ¦‚å¿µ)
      - [ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥](#ä¸‰è€…å¯¹æ¯”åˆ†æ-)
    - [åˆ†åŒºæœºåˆ¶](#åˆ†åŒºæœºåˆ¶)
      - [åˆ†åŒºç­–ç•¥](#åˆ†åŒºç­–ç•¥)
      - [åˆ†åŒºè°ƒä¼˜](#åˆ†åŒºè°ƒä¼˜)
  - [Spark æ¶æ„ä¸åŸç† â­â­](#spark-æ¶æ„ä¸åŸç†-)
    - [Sparkæ•´ä½“æ¶æ„](#sparkæ•´ä½“æ¶æ„)
      - [é›†ç¾¤æ¶æ„ç»„ä»¶](#é›†ç¾¤æ¶æ„ç»„ä»¶)
      - [åº”ç”¨ç¨‹åºæ¶æ„](#åº”ç”¨ç¨‹åºæ¶æ„)
    - [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [SparkContext](#sparkcontext)
      - [Driver Program](#driver-program)
      - [Cluster Manager](#cluster-manager)
      - [Executor](#executor)
    - [ä»»åŠ¡è°ƒåº¦åŸç†](#ä»»åŠ¡è°ƒåº¦åŸç†)
      - [DAGSchedulerè°ƒåº¦](#dagschedulerè°ƒåº¦)
      - [TaskSchedulerè°ƒåº¦](#taskschedulerè°ƒåº¦)
    - [å­˜å‚¨ç®¡ç†æœºåˆ¶](#å­˜å‚¨ç®¡ç†æœºåˆ¶)
      - [BlockManagerç»„ä»¶](#blockmanagerç»„ä»¶)
      - [å†…å­˜æ¨¡å‹](#å†…å­˜æ¨¡å‹)
      - [å†…å­˜åˆ†é…ç­–ç•¥](#å†…å­˜åˆ†é…ç­–ç•¥)
    - [ShuffleåŸç†  â­â­â­](#shuffleåŸç†--)
      - [Shuffleå®ç°æœºåˆ¶](#shuffleå®ç°æœºåˆ¶)
      - [Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£](#shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜](#shuffle-ä¼˜åŒ–ä¸è°ƒä¼˜)
      - [Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
  - [Spark SQLä¸Catalyst â­â­](#spark-sqlä¸catalyst-)
    - [Spark SQLæ¦‚è¿°](#spark-sqlæ¦‚è¿°)
      - [ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
      - [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
    - [Catalystä¼˜åŒ–å™¨ ğŸ”¥](#catalystä¼˜åŒ–å™¨-)
      - [ä¼˜åŒ–æµç¨‹](#ä¼˜åŒ–æµç¨‹)
      - [ä¼˜åŒ–è§„åˆ™](#ä¼˜åŒ–è§„åˆ™)
      - [ä»£ç ç”Ÿæˆ](#ä»£ç ç”Ÿæˆ)
    - [SparkSQL å®ç”¨å‡½æ•°ä¸è¯­æ³•](#sparksql-å®ç”¨å‡½æ•°ä¸è¯­æ³•)
      - [æ—¥æœŸä¸æ—¶é—´å¤„ç†](#æ—¥æœŸä¸æ—¶é—´å¤„ç†)
      - [å­—ç¬¦ä¸²å¤„ç†](#å­—ç¬¦ä¸²å¤„ç†)
      - [æ•°ç»„ä¸é›†åˆæ“ä½œ](#æ•°ç»„ä¸é›†åˆæ“ä½œ)
      - [JSONå¤„ç†](#jsonå¤„ç†)
      - [æ¡ä»¶ä¸åˆ¤æ–­](#æ¡ä»¶ä¸åˆ¤æ–­)
      - [çª—å£å‡½æ•°](#çª—å£å‡½æ•°)
      - [èšåˆå‡½æ•°](#èšåˆå‡½æ•°)
      - [å®ç”¨æŸ¥è¯¢ç¤ºä¾‹](#å®ç”¨æŸ¥è¯¢ç¤ºä¾‹)
    - [æ•°æ®æºæ”¯æŒ](#æ•°æ®æºæ”¯æŒ)
      - [å†…ç½®æ•°æ®æº](#å†…ç½®æ•°æ®æº)
      - [å¤–éƒ¨æ•°æ®æº](#å¤–éƒ¨æ•°æ®æº)
  - [æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ– â­â­â­](#æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ–-)
    - [æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–](#æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–)
    - [Joinä¼˜åŒ–](#joinä¼˜åŒ–)
    - [ç¼“å­˜ä¸æŒä¹…åŒ–](#ç¼“å­˜ä¸æŒä¹…åŒ–)
    - [ä»£ç å±‚é¢ä¼˜åŒ–](#ä»£ç å±‚é¢ä¼˜åŒ–)
    - [ç½‘ç»œä¸I/Oä¼˜åŒ–](#ç½‘ç»œä¸ioä¼˜åŒ–)
    - [å¸¸è§æ€§èƒ½é—®é¢˜](#å¸¸è§æ€§èƒ½é—®é¢˜)
    - [ç›‘æ§ä¸è¯Šæ–­](#ç›‘æ§ä¸è¯Šæ–­)
  - [Spark Streaming â­](#spark-streaming-)
    - [æµå¤„ç†æ¦‚å¿µ](#æµå¤„ç†æ¦‚å¿µ)
      - [å¾®æ‰¹æ¬¡å¤„ç†](#å¾®æ‰¹æ¬¡å¤„ç†)
      - [DStreamæ¦‚å¿µ](#dstreamæ¦‚å¿µ)
    - [Structured Streaming](#structured-streaming)
      - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
      - [è¾“å‡ºæ¨¡å¼](#è¾“å‡ºæ¨¡å¼)
      - [çª—å£æ“ä½œ](#çª—å£æ“ä½œ)
    - [å®¹é”™æœºåˆ¶](#å®¹é”™æœºåˆ¶)
      - [Checkpointæœºåˆ¶](#checkpointæœºåˆ¶)
      - [WALæœºåˆ¶](#walæœºåˆ¶)
  - [å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ)
    - [å†…å­˜ç›¸å…³é”™è¯¯](#å†…å­˜ç›¸å…³é”™è¯¯)
    - [ç½‘ç»œç›¸å…³é”™è¯¯](#ç½‘ç»œç›¸å…³é”™è¯¯)
    - [åºåˆ—åŒ–ç›¸å…³é”™è¯¯](#åºåˆ—åŒ–ç›¸å…³é”™è¯¯)
    - [èµ„æºç›¸å…³é”™è¯¯](#èµ„æºç›¸å…³é”™è¯¯)
    - [æ•°æ®ç›¸å…³é”™è¯¯](#æ•°æ®ç›¸å…³é”™è¯¯)
    - [è°ƒè¯•å’Œè¯Šæ–­å·¥å…·](#è°ƒè¯•å’Œè¯Šæ–­å·¥å…·)
    - [é¢„é˜²æªæ–½](#é¢„é˜²æªæ–½)
  - [å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿ âš™ï¸](#å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿-ï¸)
    - [èµ„æºç›¸å…³å‚æ•°](#èµ„æºç›¸å…³å‚æ•°)
    - [JVMç›¸å…³å‚æ•°](#jvmç›¸å…³å‚æ•°)
    - [æ€§èƒ½ä¼˜åŒ–å‚æ•°](#æ€§èƒ½ä¼˜åŒ–å‚æ•°)
    - [é…ç½®æ¨¡æ¿](#é…ç½®æ¨¡æ¿)
  - [Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥](#sparké«˜é¢‘é¢è¯•é¢˜-)
    - [åŸºç¡€æ¦‚å¿µé¢˜](#åŸºç¡€æ¦‚å¿µé¢˜)
    - [æ¶æ„åŸç†é¢˜](#æ¶æ„åŸç†é¢˜)
    - [æ€§èƒ½è°ƒä¼˜é¢˜](#æ€§èƒ½è°ƒä¼˜é¢˜)
    - [å®æˆ˜åº”ç”¨é¢˜](#å®æˆ˜åº”ç”¨é¢˜)
    - [æ·±åº¦æŠ€æœ¯åŸç†é¢˜](#æ·±åº¦æŠ€æœ¯åŸç†é¢˜)
    - [æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜](#æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜)
    - [é«˜çº§åº”ç”¨é¢˜](#é«˜çº§åº”ç”¨é¢˜)

---

## Spark æ¦‚è¿°ä¸ç¯å¢ƒ

### Sparkç®€ä»‹

**Apache Spark** æ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡ã€‚å®ƒæä¾›äº†é«˜çº§APIï¼ˆJavaã€Scalaã€Pythonã€Rï¼‰ï¼Œå¹¶æ”¯æŒç”¨äºSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¤„ç†çš„ä¼˜åŒ–å¼•æ“ã€‚

#### Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š

- **é€Ÿåº¦å¿«**ï¼šå†…å­˜è®¡ç®—æ¯”Hadoop MapReduceå¿«100å€ï¼Œç£ç›˜è®¡ç®—å¿«10å€
- **æ˜“ç”¨æ€§**ï¼šæä¾›å¤šç§è¯­è¨€APIï¼Œæ”¯æŒ80å¤šç§é«˜çº§ç®—å­
- **é€šç”¨æ€§**ï¼šæ”¯æŒSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
- **å…¼å®¹æ€§**ï¼šå¯è¿è¡Œåœ¨Hadoopã€Mesosã€Kubernetesã€standaloneç­‰é›†ç¾¤ä¸Š

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š


| ç‰¹æ€§         | Spark               | Hadoop MapReduce |
| -------------- | --------------------- | ------------------ |
| **è®¡ç®—æ¨¡å¼** | å†…å­˜è®¡ç®— + ç£ç›˜å­˜å‚¨ | ç£ç›˜è®¡ç®—         |
| **æ•°æ®å…±äº«** | RDDå†…å­˜å…±äº«         | ç£ç›˜æ–‡ä»¶ç³»ç»Ÿ     |
| **è¿­ä»£è®¡ç®—** | æ”¯æŒé«˜æ•ˆè¿­ä»£        | æ•ˆç‡ä½           |
| **å®æ—¶å¤„ç†** | æ”¯æŒæµå¤„ç†          | ä»…æ‰¹å¤„ç†         |
| **å®¹é”™æœºåˆ¶** | RDDè¡€ç»Ÿæ¢å¤         | æ•°æ®å‰¯æœ¬         |
| **å¼€å‘æ•ˆç‡** | ä»£ç ç®€æ´            | ä»£ç å¤æ‚         |

#### Spark vs Hadoop MapReduce

```mermaid
graph TD
    A[æ•°æ®è¾“å…¥] --> B{è®¡ç®—å¼•æ“}
    
    B -->|MapReduce| C[Mapé˜¶æ®µ]
    C --> D[Shuffleå†™ç£ç›˜]
    D --> E[Reduceé˜¶æ®µ]
    E --> F[ç»“æœå†™HDFS]
    
    B -->|Spark| G[RDDè½¬æ¢]
    G --> H[å†…å­˜è®¡ç®—]
    H --> I[ç»“æœè¾“å‡º]
    
    style C fill:#ffcccb
    style D fill:#ffcccb
    style E fill:#ffcccb
    style F fill:#ffcccb
    style G fill:#90EE90
    style H fill:#90EE90
    style I fill:#90EE90
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

- **å†…å­˜è®¡ç®—**ï¼šSparkåœ¨å†…å­˜ä¸­ç¼“å­˜æ•°æ®ï¼Œé¿å…é‡å¤I/O
- **DAGæ‰§è¡Œ**ï¼šSparkå°†ä½œä¸šæ„å»ºä¸ºDAGï¼Œä¼˜åŒ–æ‰§è¡Œè®¡åˆ’
- **Pipelining**ï¼šSparkæ”¯æŒç®—å­æµæ°´çº¿ï¼Œå‡å°‘ä¸­é—´æ•°æ®å­˜å‚¨
- **ä»£ç ç”Ÿæˆ**ï¼šCatalystä¼˜åŒ–å™¨ç”Ÿæˆé«˜æ•ˆçš„Javaä»£ç 

#### Sparkåº”ç”¨åœºæ™¯

**å…¸å‹åº”ç”¨é¢†åŸŸ**ï¼š


| åœºæ™¯           | æè¿°                       | ä¼˜åŠ¿                       |
| ---------------- | ---------------------------- | ---------------------------- |
| **æ•°æ®ETL**    | å¤§è§„æ¨¡æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åŠ è½½ | å¤„ç†é€Ÿåº¦å¿«ï¼Œæ”¯æŒå¤šç§æ•°æ®æº |
| **å®æ—¶æµå¤„ç†** | å®æ—¶æ•°æ®åˆ†æã€ç›‘æ§å‘Šè­¦     | ä½å»¶è¿Ÿï¼Œé«˜ååé‡           |
| **æœºå™¨å­¦ä¹ **   | å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ     | MLlibç”Ÿæ€ï¼Œè¿­ä»£è®¡ç®—ä¼˜åŠ¿    |
| **äº¤äº’å¼æŸ¥è¯¢** | å³å¸­æŸ¥è¯¢ã€æ•°æ®æ¢ç´¢         | SQLæ”¯æŒï¼Œå“åº”é€Ÿåº¦å¿«        |
| **å›¾è®¡ç®—**     | ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿ     | GraphXå›¾å¤„ç†èƒ½åŠ›           |

### Sparkç”Ÿæ€ç³»ç»Ÿ

#### æ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    A[Spark Core] --> B[Spark SQL]
    A --> C[Spark Streaming]
    A --> D[MLlib]
    A --> E[GraphX]
    
    B --> F[DataFrames & Datasets]
    B --> G[Catalyst Optimizer]
    
    C --> H[DStreams]
    C --> I[Structured Streaming]
    
    D --> J[Classification]
    D --> K[Clustering] 
    D --> L[Collaborative Filtering]
    
    E --> M[Graph Processing]
    E --> N[Graph Algorithms]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#e8f5e8
    style E fill:#fce4ec
```

**ç»„ä»¶è¯¦è§£**ï¼š

1. **Spark Core**ï¼š

   - åŸºç¡€è¿è¡Œæ—¶å¼•æ“
   - RDDæŠ½è±¡
   - ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†ã€å®¹é”™æ¢å¤
2. **Spark SQL**ï¼š

   - ç»“æ„åŒ–æ•°æ®å¤„ç†
   - DataFrame/Dataset API
   - JDBC/ODBCè¿æ¥å™¨
3. **Spark Streaming**ï¼š

   - æµæ•°æ®å¤„ç†
   - å¾®æ‰¹æ¬¡å¤„ç†æ¨¡å‹
   - ä¸æ‰¹å¤„ç†ä»£ç ç»Ÿä¸€
4. **MLlib**ï¼š

   - æœºå™¨å­¦ä¹ åº“
   - åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤
   - ç®¡é“API
5. **GraphX**ï¼š

   - å›¾è®¡ç®—æ¡†æ¶
   - å›¾ç®—æ³•åº“
   - å›¾å¹¶è¡Œè®¡ç®—

#### ç”Ÿæ€ç»„ä»¶

**å¤–éƒ¨é›†æˆ**ï¼š


| ç»„ä»¶ç±»å‹     | ç»„ä»¶åç§°              | ç”¨é€”         |
| -------------- | ----------------------- | -------------- |
| **èµ„æºç®¡ç†** | YARNã€Mesosã€K8s      | é›†ç¾¤èµ„æºç®¡ç† |
| **å­˜å‚¨ç³»ç»Ÿ** | HDFSã€S3ã€HBase       | æ•°æ®å­˜å‚¨     |
| **æ•°æ®æ ¼å¼** | Parquetã€Avroã€JSON   | æ•°æ®åºåˆ—åŒ–   |
| **æµæ•°æ®**   | Kafkaã€Flumeã€Kinesis | æ•°æ®é‡‡é›†     |
| **ç›‘æ§å·¥å…·** | Gangliaã€Nagios       | é›†ç¾¤ç›‘æ§     |

### Sparkç¯å¢ƒæ­å»º

#### æœ¬åœ°æ¨¡å¼

**ä¸‹è½½å®‰è£…**ï¼š

```bash
# ä¸‹è½½Spark
wget https://downloads.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

# è§£å‹
tar -xzf spark-3.4.0-bin-hadoop3.tgz
cd spark-3.4.0-bin-hadoop3

# è®¾ç½®ç¯å¢ƒå˜é‡
export SPARK_HOME=/path/to/spark-3.4.0-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
```

**å¯åŠ¨æœ¬åœ°æ¨¡å¼**ï¼š

```bash
# å¯åŠ¨Spark Shell (Scala)
spark-shell --master local[2]

# å¯åŠ¨PySpark Shell (Python)
pyspark --master local[2]

# æäº¤åº”ç”¨ç¨‹åº
spark-submit \
  --master local[2] \
  --class org.apache.spark.examples.SparkPi \
  examples/jars/spark-examples_2.12-3.4.0.jar \
  10
```

#### é›†ç¾¤æ¨¡å¼

**Standaloneæ¨¡å¼éƒ¨ç½²**ï¼š

```bash
# 1. é…ç½®slavesæ–‡ä»¶
echo "worker1" >> conf/slaves
echo "worker2" >> conf/slaves

# 2. å¯åŠ¨Master
./sbin/start-master.sh

# 3. å¯åŠ¨Workers
./sbin/start-slaves.sh

# 4. æäº¤åº”ç”¨åˆ°é›†ç¾¤
spark-submit \
  --master spark://master:7077 \
  --deploy-mode cluster \
  --class MainClass \
  --conf spark.sql.adaptive.enabled=true \
  app.jar
```

**YARNæ¨¡å¼éƒ¨ç½²**ï¼š

```bash
# é…ç½®Hadoopç¯å¢ƒ
export HADOOP_HOME=/path/to/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# æäº¤åˆ°YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class MainClass \
  app.jar
```

#### å¸¸ç”¨é…ç½®

**æ ¸å¿ƒé…ç½®å‚æ•°**ï¼š

```properties
# spark-defaults.conf

# åº”ç”¨ç¨‹åºé…ç½®
spark.app.name                MySparkApp
spark.master                  yarn
spark.submit.deployMode       cluster

# èµ„æºé…ç½®
spark.driver.memory           2g
spark.driver.cores            1
spark.executor.memory         4g
spark.executor.cores          2
spark.executor.instances      10

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled           true
spark.dynamicAllocation.minExecutors      2
spark.dynamicAllocation.maxExecutors      20
spark.dynamicAllocation.initialExecutors  5

# Shuffleé…ç½®
spark.sql.adaptive.enabled                true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled       true

# åºåˆ—åŒ–
spark.serializer              org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired  false

# å‹ç¼©
spark.sql.parquet.compression.codec  snappy
spark.sql.orc.compression.codec      snappy
```

**æ—¥å¿—é…ç½®**ï¼š

```properties
# log4j.properties
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# è®¾ç½®Sparkæ—¥å¿—çº§åˆ«
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.springframework.core.env.ConfigUtils=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.util.Utils=WARN
```

---

## Spark æ ¸å¿ƒæ¦‚å¿µ â­

### RDDæ ¸å¿ƒæ¦‚å¿µ

**RDD (Resilient Distributed Dataset)** æ˜¯Sparkçš„æ ¸å¿ƒæŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€å¯å¹¶è¡Œè®¡ç®—çš„æ•°æ®é›†åˆã€‚

#### RDDç‰¹æ€§

```mermaid
graph TD
    A[RDDæ ¸å¿ƒç‰¹æ€§] --> B[ä¸å¯å˜æ€§<br/>Immutable]
    A --> C[åˆ†å¸ƒå¼<br/>Distributed]
    A --> D[å¼¹æ€§å®¹é”™<br/>Resilient]
    A --> E[æƒ°æ€§æ±‚å€¼<br/>Lazy Evaluation]
    A --> F[åˆ†åŒºè®¡ç®—<br/>Partitioned]
    
    B --> B1[æ•°æ®ä¸€æ—¦åˆ›å»ºä¸å¯ä¿®æ”¹]
    C --> C1[æ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤å¤šä¸ªèŠ‚ç‚¹]
    D --> D1[é€šè¿‡è¡€ç»Ÿä¿¡æ¯è‡ªåŠ¨å®¹é”™]
    E --> E1[Transformæ“ä½œå»¶è¿Ÿæ‰§è¡Œ]
    F --> F1[æ”¯æŒå¹¶è¡Œè®¡ç®—]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#ffebee
    style E fill:#f3e5f5
    style F fill:#fce4ec
```

**RDDçš„äº”å¤§ç‰¹æ€§**ï¼š


| ç‰¹æ€§         | æè¿°                    | æ„ä¹‰             |
| -------------- | ------------------------- | ------------------ |
| **åˆ†åŒºåˆ—è¡¨** | RDDç”±å¤šä¸ªåˆ†åŒºç»„æˆ       | æ”¯æŒå¹¶è¡Œè®¡ç®—     |
| **è®¡ç®—å‡½æ•°** | æ¯ä¸ªåˆ†åŒºéƒ½æœ‰è®¡ç®—å‡½æ•°    | å®šä¹‰æ•°æ®å¤„ç†é€»è¾‘ |
| **ä¾èµ–å…³ç³»** | RDDä¹‹é—´çš„ä¾èµ–å…³ç³»       | æ”¯æŒå®¹é”™æ¢å¤     |
| **åˆ†åŒºå™¨**   | Key-Value RDDçš„åˆ†åŒºç­–ç•¥ | ä¼˜åŒ–æ•°æ®åˆ†å¸ƒ     |
| **ä½ç½®åå¥½** | è®¡ç®—åˆ†åŒºçš„æœ€ä½³ä½ç½®      | æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–   |

#### RDDæ“ä½œåˆ†ç±»

**Transformation vs Action**ï¼š

```mermaid
graph LR
    A[RDDæ“ä½œ] --> B[Transformation<br/>è½¬æ¢æ“ä½œ]
    A --> C[Action<br/>è¡ŒåŠ¨æ“ä½œ]
    
    B --> D[æƒ°æ€§æ‰§è¡Œ<br/>ä¸ç«‹å³è®¡ç®—]
    B --> E[è¿”å›æ–°RDD]
    B --> F[æ„å»ºè®¡ç®—å›¾]
    
    C --> G[ç«‹å³æ‰§è¡Œ<br/>è§¦å‘è®¡ç®—]
    C --> H[è¿”å›ç»“æœå€¼]
    C --> I[æäº¤ä½œä¸š]
    
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**å¸¸ç”¨Transformationæ“ä½œ**ï¼š

```scala
// åˆ›å»ºRDD
val rdd = sc.parallelize(1 to 100, 4)

// mapï¼šä¸€å¯¹ä¸€è½¬æ¢
val mapRDD = rdd.map(x => x * 2)

// filterï¼šè¿‡æ»¤æ•°æ®
val filterRDD = rdd.filter(x => x % 2 == 0)

// flatMapï¼šä¸€å¯¹å¤šè½¬æ¢
val flatMapRDD = rdd.flatMap(x => 1 to x)

// groupByKeyï¼šæŒ‰é”®åˆ†ç»„
val kvRDD = rdd.map(x => (x % 10, x))
val groupedRDD = kvRDD.groupByKey()

// reduceByKeyï¼šæŒ‰é”®èšåˆ
val reducedRDD = kvRDD.reduceByKey(_ + _)

// joinï¼šè¿æ¥æ“ä½œ
val rdd2 = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c")))
val joinedRDD = kvRDD.join(rdd2)
```

**å¸¸ç”¨Actionæ“ä½œ**ï¼š

```scala
// collectï¼šæ”¶é›†æ‰€æœ‰å…ƒç´ åˆ°Driver
val result = rdd.collect()

// countï¼šè®¡ç®—å…ƒç´ æ•°é‡
val cnt = rdd.count()

// firstï¼šè·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
val firstElement = rdd.first()

// takeï¼šè·å–å‰nä¸ªå…ƒç´ 
val firstN = rdd.take(10)

// reduceï¼šèšåˆæ‰€æœ‰å…ƒç´ 
val sum = rdd.reduce(_ + _)

// foreachï¼šéå†æ¯ä¸ªå…ƒç´ 
rdd.foreach(println)

// saveAsTextFileï¼šä¿å­˜åˆ°æ–‡ä»¶
rdd.saveAsTextFile("hdfs://output/path")
```

#### RDDä¾èµ–å…³ç³»

**ä¾èµ–ç±»å‹**ï¼š

```mermaid
graph TD
    A[RDDä¾èµ–å…³ç³»] --> B[çª„ä¾èµ–<br/>Narrow Dependency]
    A --> C[å®½ä¾èµ–<br/>Wide Dependency]
    
    B --> D[ä¸€å¯¹ä¸€æ˜ å°„<br/>1:1 Mapping]
    B --> E[åŒä¸€Stageå†…<br/>Pipelineæ‰§è¡Œ]
    B --> F[å±€éƒ¨å¤±è´¥æ¢å¤]
    
    C --> G[ä¸€å¯¹å¤šæ˜ å°„<br/>1:N Mapping]
    C --> H[éœ€è¦Shuffle<br/>è·¨Stageæ‰§è¡Œ]
    C --> I[å…¨é‡é‡æ–°è®¡ç®—]
    
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**çª„ä¾èµ–ç¤ºä¾‹**ï¼š

```scala
// map, filter, unionç­‰æ“ä½œäº§ç”Ÿçª„ä¾èµ–
val rdd1 = sc.parallelize(1 to 10, 2)
val rdd2 = rdd1.map(_ * 2)        // çª„ä¾èµ–
val rdd3 = rdd2.filter(_ > 10)    // çª„ä¾èµ–
```

**å®½ä¾èµ–ç¤ºä¾‹**ï¼š

```scala
// groupByKey, reduceByKey, joinç­‰æ“ä½œäº§ç”Ÿå®½ä¾èµ–
val rdd1 = sc.parallelize(Seq((1, "a"), (2, "b"), (1, "c")), 2)
val rdd2 = rdd1.groupByKey()      // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
val rdd3 = rdd1.reduceByKey(_ + _) // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
```

### DataFrameä¸Dataset

#### DataFrameæ¦‚å¿µ

**DataFrame** æ˜¯Spark SQLçš„æ ¸å¿ƒæŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªä»¥å‘½ååˆ—æ–¹å¼ç»„ç»‡çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºå…³ç³»æ•°æ®åº“ä¸­çš„è¡¨ã€‚

**DataFrameç‰¹ç‚¹**ï¼š

- **ç»“æ„åŒ–æ•°æ®**ï¼šå…·æœ‰æ˜ç¡®çš„Schemaå®šä¹‰
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šä½¿ç”¨Catalystä¼˜åŒ–å™¨
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šScalaã€Javaã€Pythonã€R
- **ä¸°å¯ŒAPI**ï¼šSQLé£æ ¼å’Œå‡½æ•°å¼API

**DataFrameåˆ›å»º**ï¼š

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("DataFrameExample")
  .getOrCreate()

import spark.implicits._

// æ–¹å¼1ï¼šä»RDDåˆ›å»º
val rdd = sc.parallelize(Seq(("Alice", 25), ("Bob", 30), ("Charlie", 35)))
val df1 = rdd.toDF("name", "age")

// æ–¹å¼2ï¼šä»åºåˆ—åˆ›å»º
val df2 = Seq(("Alice", 25), ("Bob", 30)).toDF("name", "age")

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val df3 = spark.read.json("path/to/file.json")
val df4 = spark.read.parquet("path/to/file.parquet")

// æ–¹å¼4ï¼šé€šè¿‡Schemaåˆ›å»º
val schema = StructType(Seq(
  StructField("name", StringType, nullable = true),
  StructField("age", IntegerType, nullable = true)
))
val df5 = spark.createDataFrame(rdd, schema)
```

#### Datasetæ¦‚å¿µ

**Dataset** æ˜¯DataFrameçš„æ‰©å±•ï¼Œæä¾›äº†ç±»å‹å®‰å…¨çš„é¢å‘å¯¹è±¡ç¼–ç¨‹æ¥å£ã€‚

**Datasetç‰¹ç‚¹**ï¼š

- **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šäº«å—Catalystä¼˜åŒ–å™¨
- **å‡½æ•°å¼API**ï¼šæ”¯æŒlambdaè¡¨è¾¾å¼
- **ç¼–ç å™¨æ”¯æŒ**ï¼šè‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–

**Datasetåˆ›å»º**ï¼š

```scala
// å®šä¹‰æ ·ä¾‹ç±»
case class Person(name: String, age: Int, city: String)

// æ–¹å¼1ï¼šä»åºåˆ—åˆ›å»º
val ds1 = Seq(
  Person("Alice", 25, "Beijing"),
  Person("Bob", 30, "Shanghai")
).toDS()

// æ–¹å¼2ï¼šä»DataFrameè½¬æ¢
val ds2 = df.as[Person]

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val ds3 = spark.read.json("path/to/file.json").as[Person]
```

#### ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥

**RDD vs DataFrame vs Dataset å…¨é¢å¯¹æ¯”**ï¼š


| ç‰¹æ€§           | RDD                    | DataFrame           | Dataset            |
| ---------------- | ------------------------ | --------------------- | -------------------- |
| **æ•°æ®æŠ½è±¡**   | åˆ†å¸ƒå¼å¯¹è±¡é›†åˆ         | ç»“æ„åŒ–æ•°æ®è¡¨        | ç±»å‹å®‰å…¨çš„æ•°æ®è¡¨   |
| **ç¼–è¯‘æ—¶æ£€æŸ¥** | âŒ è¿è¡Œæ—¶é”™è¯¯          | âŒ è¿è¡Œæ—¶é”™è¯¯       | âœ… ç¼–è¯‘æ—¶é”™è¯¯      |
| **æ‰§è¡Œä¼˜åŒ–**   | âŒ æ— ä¼˜åŒ–              | âœ… Catalystä¼˜åŒ–     | âœ… Catalystä¼˜åŒ–    |
| **ä»£ç ç”Ÿæˆ**   | âŒ æ—                   | âœ… æœ‰               | âœ… æœ‰              |
| **åºåˆ—åŒ–**     | Java/Kryoåºåˆ—åŒ–        | TungstenäºŒè¿›åˆ¶æ ¼å¼  | TungstenäºŒè¿›åˆ¶æ ¼å¼ |
| **APIé£æ ¼**    | å‡½æ•°å¼                 | SQL + å‡½æ•°å¼        | ç±»å‹å®‰å…¨å‡½æ•°å¼     |
| **æ€§èƒ½**       | ä½                     | é«˜                  | é«˜                 |
| **æ˜“ç”¨æ€§**     | å¤æ‚                   | ç®€å•                | ä¸­ç­‰               |
| **é€‚ç”¨åœºæ™¯**   | ä½çº§æ“ä½œã€éç»“æ„åŒ–æ•°æ® | SQLæŸ¥è¯¢ã€ç»“æ„åŒ–æ•°æ® | ç±»å‹å®‰å…¨è¦æ±‚é«˜     |

**æ€§èƒ½å¯¹æ¯”**ï¼š

```scala
// æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
import org.apache.spark.sql.functions._

// RDDæ–¹å¼ - æ€§èƒ½è¾ƒä½
val rddResult = rdd.filter(_.age > 25)
  .map(p => (p.city, 1))
  .reduceByKey(_ + _)
  .collect()

// DataFrameæ–¹å¼ - æ€§èƒ½ä¼˜åŒ–
val dfResult = df.filter($"age" > 25)
  .groupBy("city")
  .count()
  .collect()

// Datasetæ–¹å¼ - ç±»å‹å®‰å…¨ + æ€§èƒ½ä¼˜åŒ–
val dsResult = ds.filter(_.age > 25)
  .groupByKey(_.city)
  .count()
  .collect()
```

**é€‰æ‹©å»ºè®®**ï¼š

```mermaid
graph TD
    A[é€‰æ‹©æ•°æ®æŠ½è±¡] --> B{æ•°æ®ç±»å‹}
    B -->|éç»“æ„åŒ–| C[ä½¿ç”¨RDD]
    B -->|ç»“æ„åŒ–| D{ç±»å‹å®‰å…¨è¦æ±‚}
    D -->|ä¸éœ€è¦| E[ä½¿ç”¨DataFrame]
    D -->|éœ€è¦| F[ä½¿ç”¨Dataset]
    
    C --> G[å¤æ‚æ•°æ®å¤„ç†<br/>åº•å±‚æ§åˆ¶]
    E --> H[SQLæŸ¥è¯¢<br/>é«˜æ€§èƒ½è¦æ±‚]
    F --> I[ç±»å‹å®‰å…¨<br/>ç¼–è¯‘æ—¶æ£€æŸ¥]
    
    style C fill:#ffebee
    style E fill:#e8f5e8
    style F fill:#e1f5fe
```

### åˆ†åŒºæœºåˆ¶

#### åˆ†åŒºç­–ç•¥

**åˆ†åŒºçš„é‡è¦æ€§**ï¼š

- **å¹¶è¡Œåº¦æ§åˆ¶**ï¼šåˆ†åŒºæ•°å†³å®šä»»åŠ¡å¹¶è¡Œåº¦
- **æ•°æ®æœ¬åœ°æ€§**ï¼šå‡å°‘ç½‘ç»œä¼ è¾“
- **è´Ÿè½½å‡è¡¡**ï¼šé¿å…æ•°æ®å€¾æ–œ
- **èµ„æºåˆ©ç”¨**ï¼šå……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æº

**åˆ†åŒºå™¨ç±»å‹**ï¼š


| åˆ†åŒºå™¨               | é€‚ç”¨æ•°æ®ç±»å‹          | åˆ†åŒºç­–ç•¥                  | ä½¿ç”¨åœºæ™¯     |
| ---------------------- | ----------------------- | --------------------------- | -------------- |
| **HashPartitioner**  | Key-Value RDD         | Hash(key) % numPartitions | å‡åŒ€åˆ†å¸ƒçš„é”® |
| **RangePartitioner** | å¯æ’åºçš„Key-Value RDD | æŒ‰é”®å€¼èŒƒå›´åˆ†åŒº            | æœ‰åºæ•°æ®æŸ¥è¯¢ |
| **è‡ªå®šä¹‰åˆ†åŒºå™¨**     | ä»»æ„ç±»å‹              | ç”¨æˆ·å®šä¹‰é€»è¾‘              | ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚ |

**åˆ†åŒºæ“ä½œç¤ºä¾‹**ï¼š

```scala
// åˆ›å»ºå¸¦åˆ†åŒºçš„RDD
val rdd = sc.parallelize(1 to 100, 4)  // 4ä¸ªåˆ†åŒº

// æŸ¥çœ‹åˆ†åŒºä¿¡æ¯
println(s"åˆ†åŒºæ•°: ${rdd.getNumPartitions}")
println(s"åˆ†åŒºå†…å®¹: ${rdd.glom().collect().map(_.toList).toList}")

// é‡æ–°åˆ†åŒº
val repartitionedRDD = rdd.repartition(8)  // å¢åŠ åˆ†åŒºæ•°
val coalescedRDD = rdd.coalesce(2)         // å‡å°‘åˆ†åŒºæ•°

// Key-Value RDDåˆ†åŒº
val kvRDD = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c"), (4, "d")), 2)

// ä½¿ç”¨HashPartitioner
val hashPartitioned = kvRDD.partitionBy(new HashPartitioner(3))

// ä½¿ç”¨RangePartitioner
val rangePartitioned = kvRDD.partitionBy(new RangePartitioner(3, kvRDD))
```

**è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼š

```scala
import org.apache.spark.Partitioner

// è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼šæŒ‰ç”¨æˆ·IDçš„åœ°åŒºåˆ†åŒº
class RegionPartitioner(regions: Array[String]) extends Partitioner {
  
  override def numPartitions: Int = regions.length
  
  override def getPartition(key: Any): Int = {
    val userId = key.asInstanceOf[String]
    val region = getUserRegion(userId)
    math.abs(regions.indexOf(region)) % numPartitions
  }
  
  private def getUserRegion(userId: String): String = {
    // æ ¹æ®ç”¨æˆ·IDç¡®å®šåœ°åŒºçš„ä¸šåŠ¡é€»è¾‘
    userId.substring(0, 2) match {
      case "01" | "02" => "North"
      case "03" | "04" => "South"
      case "05" | "06" => "East"
      case _ => "West"
    }
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
val regions = Array("North", "South", "East", "West")
val customPartitioner = new RegionPartitioner(regions)
val customPartitioned = kvRDD.partitionBy(customPartitioner)
```

#### åˆ†åŒºè°ƒä¼˜

**åˆ†åŒºæ•°ä¼˜åŒ–**ï¼š

```scala
// åˆ†åŒºæ•°è®¾ç½®åŸåˆ™
val totalCores = 16  // é›†ç¾¤æ€»æ ¸å¿ƒæ•°
val optimalPartitions = totalCores * 2  // æ¨èåˆ†åŒºæ•°ä¸ºæ ¸å¿ƒæ•°çš„2-3å€

// åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
def getOptimalPartitions(dataSize: Long): Int = {
  val targetPartitionSize = 128 * 1024 * 1024  // 128MB per partition
  math.max(1, (dataSize / targetPartitionSize).toInt)
}

// åˆ†åŒºå€¾æ–œæ£€æµ‹
def detectPartitionSkew(rdd: RDD[_]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  val maxSize = partitionSizes.map(_._2).max
  val skewRatio = maxSize.toDouble / avgSize
  
  if (skewRatio > 2.0) {
    println(s"è­¦å‘Šï¼šæ£€æµ‹åˆ°åˆ†åŒºå€¾æ–œï¼Œå€¾æ–œæ¯”ä¾‹: $skewRatio")
    partitionSizes.foreach { case (index, size) =>
      println(s"åˆ†åŒº $index: $size æ¡è®°å½•")
    }
  }
}
```

**åˆ†åŒºä¼˜åŒ–ç­–ç•¥**ï¼š

1. **é¢„åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æ ¹æ®æ•°æ®ç‰¹å¾é¢„åˆ†åŒº
val userRDD = sc.textFile("hdfs://users/*")
  .map(parseUser)
  .partitionBy(new HashPartitioner(numPartitions))
  .cache()  // ç¼“å­˜é¢„åˆ†åŒºçš„æ•°æ®
```

2. **Coalesce vs Repartition**ï¼š

```scala
// Coalesceï¼šå‡å°‘åˆ†åŒºï¼Œé¿å…å…¨é‡Shuffle
val reducedRDD = largeRDD.coalesce(10)

// Repartitionï¼šé‡æ–°åˆ†åŒºï¼Œä¼šè¿›è¡Œå…¨é‡Shuffle
val reshuffledRDD = largeRDD.repartition(20)

// æ¡ä»¶åˆ†åŒºè°ƒæ•´
def smartRepartition[T](rdd: RDD[T], targetPartitions: Int): RDD[T] = {
  val currentPartitions = rdd.getNumPartitions
  if (targetPartitions < currentPartitions) {
    rdd.coalesce(targetPartitions)
  } else {
    rdd.repartition(targetPartitions)
  }
}
```

3. **åˆ†åŒºä¿æŒç­–ç•¥**ï¼š

```scala
// ä½¿ç”¨mapPartitionsä¿æŒåˆ†åŒºç»“æ„
val optimizedRDD = rdd.mapPartitions { iter =>
  // åˆ†åŒºå†…å¤„ç†é€»è¾‘
  iter.map(processRecord)
}

// é¿å…ç ´ååˆ†åŒºçš„æ“ä½œ
val goodRDD = partitionedRDD.mapValues(_ * 2)  // ä¿æŒåˆ†åŒº
val badRDD = partitionedRDD.map(x => (x._1, x._2 * 2))  // å¯èƒ½ç ´ååˆ†åŒº
```

---

## Spark æ¶æ„ä¸åŸç† â­â­

### Sparkæ•´ä½“æ¶æ„

#### é›†ç¾¤æ¶æ„ç»„ä»¶

```mermaid
graph TB
    subgraph "Driver Program"
        A[SparkContext]
        A1[DAGScheduler]
        A2[TaskScheduler]
        A3[BackendScheduler]
    end
    
    subgraph "Cluster Manager"
        B[Resource Manager]
        B1[Application Master]
    end
    
    subgraph "Worker Node 1"
        C[Executor 1]
        C1[Task]
        C2[BlockManager]
        C3[Cache]
    end
    
    subgraph "Worker Node 2"
        D[Executor 2]
        D1[Task]
        D2[BlockManager] 
        D3[Cache]
    end
    
    A --> B
    B --> C
    B --> D
    A1 --> A2
    A2 --> A3
    A3 --> C1
    A3 --> D1
    C2 <--> D2
    
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#e8f5e8
    style D fill:#e8f5e8
```

**æ¶æ„ç»„ä»¶è¯¦è§£**ï¼š


| ç»„ä»¶                | èŒè´£                       | è¿è¡Œä½ç½®         |
| --------------------- | ---------------------------- | ------------------ |
| **Driver Program**  | åº”ç”¨ç¨‹åºå…¥å£ï¼ŒåŒ…å«mainå‡½æ•° | å®¢æˆ·ç«¯æˆ–é›†ç¾¤èŠ‚ç‚¹ |
| **SparkContext**    | Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹      | Driver           |
| **Cluster Manager** | é›†ç¾¤èµ„æºç®¡ç†å™¨             | ç‹¬ç«‹èŠ‚ç‚¹         |
| **Worker Node**     | å·¥ä½œèŠ‚ç‚¹ï¼Œè¿è¡ŒExecutor     | é›†ç¾¤èŠ‚ç‚¹         |
| **Executor**        | ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œè¿è¡ŒTask       | Worker Node      |

#### åº”ç”¨ç¨‹åºæ¶æ„

**Sparkåº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸ**ï¼š

```mermaid
sequenceDiagram
    participant Client
    participant Driver
    participant ClusterManager as Cluster Manager
    participant Worker
    participant Executor
    
    Client->>Driver: 1. å¯åŠ¨åº”ç”¨ç¨‹åº
    Driver->>ClusterManager: 2. ç”³è¯·èµ„æº
    ClusterManager->>Worker: 3. å¯åŠ¨Executor
    Worker->>Executor: 4. åˆ›å»ºExecutorè¿›ç¨‹
    Executor->>Driver: 5. æ³¨å†Œåˆ°Driver
    Driver->>Driver: 6. æ„å»ºDAG
    Driver->>Executor: 7. åˆ†å‘Task
    Executor->>Executor: 8. æ‰§è¡ŒTask
    Executor->>Driver: 9. è¿”å›ç»“æœ
    Driver->>Client: 10. åº”ç”¨ç¨‹åºå®Œæˆ
```

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### SparkContext

**SparkContext** æ˜¯Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ä¸é›†ç¾¤å»ºç«‹è¿æ¥ã€‚

```scala
// SparkContextæ ¸å¿ƒåŠŸèƒ½
class SparkContext(config: SparkConf) extends Logging {
  
  // 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
  private val env = SparkEnv.createDriverEnv(conf, isLocal, listenerBus, numCores, mockOutputCommitCoordinator)
  private val statusTracker = new SparkStatusTracker(this, sparkUI)
  private val taskScheduler = createTaskScheduler(this, master, deployMode)
  private val dagScheduler = new DAGScheduler(this)
  
  // 2. åˆ›å»ºRDD
  def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = {
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
  
  def textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = {
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions)
      .map(pair => pair._2.toString)
  }
  
  // 3. æäº¤ä½œä¸š
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
  }
  
  // 4. èµ„æºç®¡ç†
  def stop(): Unit = {
    dagScheduler.stop()
    taskScheduler.stop()
    env.stop()
  }
}
```

#### Driver Program

**Driver** æ˜¯è¿è¡Œåº”ç”¨ç¨‹åºmainå‡½æ•°çš„è¿›ç¨‹ï¼Œè´Ÿè´£ï¼š

- **åˆ›å»ºSparkContext**ï¼šåˆå§‹åŒ–Sparkåº”ç”¨ç¨‹åº
- **æ„å»ºé€»è¾‘è®¡åˆ’**ï¼šå°†ç”¨æˆ·ç¨‹åºè½¬æ¢ä¸ºDAG
- **ä»»åŠ¡è°ƒåº¦**ï¼šå°†DAGåˆ†è§£ä¸ºStageå’ŒTask
- **ç»“æœæ”¶é›†**ï¼šæ”¶é›†Executorè¿”å›çš„ç»“æœ

```scala
// Driverç¨‹åºç¤ºä¾‹
object WordCount {
  def main(args: Array[String]): Unit = {
    // 1. åˆ›å»ºSparkContext
    val conf = new SparkConf().setAppName("WordCount")
    val sc = new SparkContext(conf)
    
    try {
      // 2. åˆ›å»ºRDDå¹¶å®šä¹‰è½¬æ¢æ“ä½œ
      val lines = sc.textFile(args(0))
      val words = lines.flatMap(_.split("\\s+"))
      val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
      
      // 3. è§¦å‘Actionï¼Œæäº¤ä½œä¸š
      wordCounts.saveAsTextFile(args(1))
      
    } finally {
      // 4. åœæ­¢SparkContext
      sc.stop()
    }
  }
}
```

#### Cluster Manager

**é›†ç¾¤ç®¡ç†å™¨ç±»å‹**ï¼š


| ç±»å‹           | ç‰¹ç‚¹                | é€‚ç”¨åœºæ™¯             |
| ---------------- | --------------------- | ---------------------- |
| **Standalone** | Sparkå†…ç½®ï¼Œç®€å•æ˜“ç”¨ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡é›†ç¾¤ |
| **YARN**       | Hadoopç”Ÿæ€é›†æˆ      | ä¼ä¸šçº§Hadoopç¯å¢ƒ     |
| **Mesos**      | é€šç”¨èµ„æºç®¡ç†å™¨      | å¤šæ¡†æ¶å…±äº«é›†ç¾¤       |
| **Kubernetes** | å®¹å™¨åŒ–éƒ¨ç½²          | äº‘åŸç”Ÿç¯å¢ƒ           |

**YARNæ¨¡å¼è¯¦è§£**ï¼š

```scala
// YARN Clientæ¨¡å¼
spark-submit \
  --master yarn \
  --deploy-mode client \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar

// YARN Clusteræ¨¡å¼  
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar
```

#### Executor

**Executor** æ˜¯è¿è¡Œåœ¨WorkerèŠ‚ç‚¹ä¸Šçš„JVMè¿›ç¨‹ï¼Œè´Ÿè´£æ‰§è¡ŒTaskã€‚

```scala
// Executoræ ¸å¿ƒç»„ä»¶
class Executor(
    executorId: String,
    executorHostname: String,
    env: SparkEnv,
    userClassPath: Seq[URL] = Nil,
    isLocal: Boolean = false)
  extends Logging {

  // 1. çº¿ç¨‹æ± ç®¡ç†
  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(
    "Executor task launch worker", sparkConf.get(EXECUTOR_CORES), 60)
  
  // 2. å†…å­˜ç®¡ç†
  private val memoryManager = env.memoryManager
  
  // 3. å­˜å‚¨ç®¡ç†
  private val blockManager = env.blockManager
  
  // 4. ä»»åŠ¡æ‰§è¡Œ
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
    val tr = new TaskRunner(context, taskDescription)
    runningTasks.put(taskDescription.taskId, tr)
    threadPool.execute(tr)
  }
  
  // 5. ä»»åŠ¡è¿è¡Œå™¨
  class TaskRunner(
      execBackend: ExecutorBackend,
      private val taskDescription: TaskDescription)
    extends Runnable {
    
    override def run(): Unit = {
      try {
        // ååºåˆ—åŒ–ä»»åŠ¡
        val task = ser.deserialize[Task[Any]](taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)
        
        // æ‰§è¡Œä»»åŠ¡
        val value = task.run(
          taskAttemptId = taskDescription.taskId,
          attemptNumber = taskDescription.attemptNumber,
          metricsSystem = env.metricsSystem)
        
        // è¿”å›ç»“æœ
        execBackend.statusUpdate(taskDescription.taskId, TaskState.FINISHED, ser.serialize(value))
        
      } catch {
        case e: Exception =>
          execBackend.statusUpdate(taskDescription.taskId, TaskState.FAILED, ser.serialize(TaskFailedReason))
      }
    }
  }
}
```

### ä»»åŠ¡è°ƒåº¦åŸç†

#### DAGSchedulerè°ƒåº¦

**DAGScheduler** æ˜¯Sparkä½œä¸šè°ƒåº¦çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†ç”¨æˆ·æäº¤çš„RDD DAGåˆ†è§£ä¸ºå¤šä¸ªStageï¼Œå¹¶æŒ‰ç…§ä¾èµ–å…³ç³»é¡ºåºæäº¤ç»™TaskScheduleræ‰§è¡Œã€‚å®ƒæ˜¯è¿æ¥é«˜å±‚RDDæ“ä½œå’Œåº•å±‚ä»»åŠ¡æ‰§è¡Œçš„å…³é”®æ¡¥æ¢ã€‚

**ä¸»è¦åŠŸèƒ½**ï¼š

- **DAGæ„å»ºä¸åˆ†æ**ï¼šå°†RDDçš„è½¬æ¢æ“ä½œæ„å»ºæˆæœ‰å‘æ— ç¯å›¾
- **Stageåˆ’åˆ†**ï¼šæ ¹æ®å®½ä¾èµ–è¾¹ç•Œå°†DAGåˆ’åˆ†ä¸ºå¤šä¸ªStage
- **ä»»åŠ¡è°ƒåº¦**ï¼šæŒ‰ç…§Stageä¾èµ–å…³ç³»è¿›è¡Œæ‹“æ‰‘æ’åºå’Œè°ƒåº¦
- **å®¹é”™å¤„ç†**ï¼šå¤„ç†ä»»åŠ¡å¤±è´¥ã€Stageé‡è¯•ç­‰å®¹é”™é€»è¾‘
- **èµ„æºç®¡ç†**ï¼šä¸TaskScheduleråè°ƒè¿›è¡Œèµ„æºåˆ†é…

```mermaid
graph TD
    A[ç”¨æˆ·è°ƒç”¨Action] --> B[SparkContext.runJob]
    B --> C[DAGScheduler.runJob]
    C --> D[åˆ›å»ºActiveJob]
    D --> E[æäº¤JobSubmittedäº‹ä»¶]
    E --> F[handleJobSubmitted]
    F --> G[åˆ›å»ºResultStage]
    G --> H[getOrCreateParentStages]
    H --> I[é€’å½’åˆ†æRDDä¾èµ–]
    I --> J{ä¾èµ–ç±»å‹åˆ¤æ–­}
    J -->|çª„ä¾èµ–| K[ç»§ç»­å‘ä¸Šéå†]
    J -->|å®½ä¾èµ–| L[åˆ›å»ºShuffleMapStage]
    K --> I
    L --> M[submitStage]
    M --> N[getMissingParentStages]
    N --> O{çˆ¶Stageæ˜¯å¦å®Œæˆ}
    O -->|æœªå®Œæˆ| P[é€’å½’æäº¤çˆ¶Stage]
    O -->|å·²å®Œæˆ| Q[submitMissingTasks]
    P --> M
    Q --> R[åˆ›å»ºTaskSet]
    R --> S[TaskScheduler.submitTasks]
    S --> T[ä»»åŠ¡åˆ†å‘ä¸æ‰§è¡Œ]
    T --> U[Stageå®Œæˆ]
    U --> V[æ£€æŸ¥åç»­Stage]
    V --> W[Jobå®Œæˆ]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style L fill:#ffebee
    style W fill:#c8e6c9
```

**DAGScheduleræ¶æ„ç»„ä»¶**ï¼š


| ç»„ä»¶             | ç±»å                                      | ä¸»è¦èŒè´£            | å…³é”®æ–¹æ³•                       |
| ------------------ | ------------------------------------------- | --------------------- | -------------------------------- |
| **DAGScheduler** | `DAGScheduler`                            | ä½œä¸šè°ƒåº¦å’ŒStageåˆ’åˆ† | `submitJob`, `submitStage`     |
| **EventLoop**    | `DAGSchedulerEventProcessLoop`            | äº‹ä»¶å¤„ç†å¾ªç¯        | `post`, `onReceive`            |
| **Stage**        | `Stage`, `ResultStage`, `ShuffleMapStage` | StageæŠ½è±¡           | `findMissingPartitions`        |
| **Job**          | `ActiveJob`                               | ä½œä¸šæŠ½è±¡            | `numFinished`, `numPartitions` |

**äº‹ä»¶å¤„ç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[DAGScheduleräº‹ä»¶] --> B[EventProcessLoop]
    B --> C{äº‹ä»¶ç±»å‹}
    C -->|JobSubmitted| D[handleJobSubmitted]
    C -->|StageCompleted| E[handleStageCompleted]
    C -->|TaskCompleted| F[handleTaskCompleted]
    C -->|TaskFailed| G[handleTaskFailed]
    C -->|ExecutorLost| H[handleExecutorLost]
    
    D --> I[åˆ›å»ºStage]
    E --> J[æ£€æŸ¥åç»­Stage]
    F --> K[æ›´æ–°StageçŠ¶æ€]
    G --> L[é‡è¯•æˆ–å¤±è´¥å¤„ç†]
    H --> M[é‡æ–°æäº¤å—å½±å“Stage]
    
    I --> N[æäº¤Stage]
    J --> N
    K --> O[Stageå®Œæˆæ£€æŸ¥]
    L --> P[å®¹é”™å¤„ç†]
    M --> N
    
    style B fill:#e1f5fe
    style N fill:#e8f5e8
    style P fill:#ffebee
```

```scala
// DAGScheduleräº‹ä»¶ç±»å‹å®šä¹‰
sealed trait DAGSchedulerEvent

case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) extends DAGSchedulerEvent

case class StageCompleted(stage: Stage) extends DAGSchedulerEvent
case class TaskCompleted(task: Task[_], reason: TaskEndReason) extends DAGSchedulerEvent
case class TaskFailed(taskId: Long, reason: TaskFailedReason) extends DAGSchedulerEvent
case class ExecutorLost(execId: String, reason: ExecutorLossReason) extends DAGSchedulerEvent

// äº‹ä»¶å¤„ç†å¾ªç¯å®ç°
class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    val timerContext = dagScheduler.metricsSource.messageProcessingTimer.time()
    try {
      doOnReceive(event)
    } finally {
      timerContext.stop()
    }
  }
  
  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
      
    case StageCompleted(stage) =>
      dagScheduler.handleStageCompleted(stage)
      
    case TaskCompleted(task, reason) =>
      dagScheduler.handleTaskCompleted(task, reason)
      
    case TaskFailed(taskId, reason) =>
      dagScheduler.handleTaskFailed(taskId, reason)
      
    case ExecutorLost(execId, reason) =>
      dagScheduler.handleExecutorLost(execId, reason)
  }
}
```

**DAGSchedulerä¸»è¦æºç **ï¼š

```scala
// DAGScheduler.scala - æ ¸å¿ƒè°ƒåº¦é€»è¾‘
class DAGScheduler(
    private[scheduler] val sc: SparkContext,
    private[scheduler] val taskScheduler: TaskScheduler,
    listenerBus: LiveListenerBus,
    mapOutputTracker: MapOutputTrackerMaster,
    blockManagerMaster: BlockManagerMaster,
    env: SparkEnv,
    clock: Clock = new SystemClock())
  extends Logging {

  // äº‹ä»¶å¤„ç†å¾ªç¯
  private val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  
  // Stageå’ŒJobç®¡ç†
  private val stageIdToStage = new HashMap[Int, Stage]
  private val shuffleIdToMapStage = new HashMap[Int, ShuffleMapStage]
  private val jobIdToActiveJob = new HashMap[Int, ActiveJob]
  private val activeJobs = new HashSet[ActiveJob]
  
  // ç­‰å¾…å’Œè¿è¡Œä¸­çš„Stage
  private val waitingStages = new HashSet[Stage]
  private val runningStages = new HashSet[Stage]
  private val failedStages = new HashSet[Stage]
  
  // 1. æäº¤ä½œä¸šçš„æ ¸å¿ƒæ–¹æ³•
  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): Unit = {
    
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    ThreadUtils.awaitReady(waiter, Duration.Inf)
    
    waiter.value.get match {
      case scala.util.Success(_) =>
        logInfo("Job %d finished: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =>
        logInfo("Job %d failed: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        throw exception
    }
  }
  
  // 2. æäº¤ä½œä¸š
  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
    
    // æ£€æŸ¥åˆ†åŒºæœ‰æ•ˆæ€§
    val maxPartitions = rdd.partitions.length
    partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>
      throw new IllegalArgumentException(
        "Attempting to access a non-existent partition: " + p + ". " +
          "Total number of partitions: " + maxPartitions)
    }
    
    val jobId = nextJobId.getAndIncrement()
    if (partitions.size == 0) {
      // ç©ºåˆ†åŒºç›´æ¥è¿”å›
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }
    
    assert(partitions.size > 0)
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
    
    // æäº¤JobSubmittedäº‹ä»¶
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter, properties))
    waiter
  }
  
  // 3. å¤„ç†ä½œä¸šæäº¤
  private[scheduler] def handleJobSubmitted(jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties): Unit = {
    
    var finalStage: ResultStage = null
    try {
      // åˆ›å»ºResultStage
      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
    } catch {
      case e: Exception =>
        logWarning("Creating new stage failed due to exception - job: " + jobId, e)
        listener.jobFailed(e)
        return
    }
    
    // åˆ›å»ºActiveJob
    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
    clearCacheLocs()
    
    logInfo("Got job %s (%s) with %d output partitions".format(
      job.jobId, callSite.shortForm, partitions.length))
    logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
    logInfo("Parents of final stage: " + finalStage.parents)
    logInfo("Missing parents: " + getMissingParentStages(finalStage))
    
    val jobSubmissionTime = clock.getTimeMillis()
    jobIdToActiveJob(jobId) = job
    activeJobs += job
    finalStage.setActiveJob(job)
    
    val stageIds = jobIdToStageIds(jobId)
    val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))
    listenerBus.post(
      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))
    
    // æäº¤Stage
    submitStage(finalStage)
  }
  
  // 4. Stageåˆ’åˆ†æ ¸å¿ƒç®—æ³•
  private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
    val parents = new ArrayBuffer[Stage]()
    val visited = new HashSet[RDD[_]]
    
    def visit(r: RDD[_]): Unit = {
      if (!visited(r)) {
        visited += r
        for (dep <- r.dependencies) {
          dep match {
            case shufDep: ShuffleDependency[_, _, _] =>
              // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
              parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
            case _ =>
              // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
              visit(dep.rdd)
          }
        }
      }
    }
    
    visit(rdd)
    parents.toList
  }
  
  // 5. åˆ›å»ºæˆ–è·å–ShuffleMapStage
  private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
    
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      case Some(stage) =>
        stage
        
      case None =>
        // é€’å½’åˆ›å»ºçˆ¶Stage
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }
  
  // 6. æäº¤Stage
  private def submitStage(stage: Stage): Unit = {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug("submitStage(" + stage + ")")
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug("missing: " + missing)
        if (missing.isEmpty) {
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent <- missing) {
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, "No active job for stage " + stage.id, None)
    }
  }
  
  // 7. æäº¤ç¼ºå¤±çš„ä»»åŠ¡
  private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
    logDebug("submitMissingTasks(" + stage + ")")
    
    // è·å–ç¼ºå¤±çš„åˆ†åŒº
    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
    
    // æ·»åŠ åˆ°è¿è¡Œä¸­çš„Stage
    runningStages += stage
    
    // åˆ›å»ºä»»åŠ¡
    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = stage.rdd.partitions(id)
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
              taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, serializedTaskMetrics)
          }
          
        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = stage.rdd.partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptId,
              taskBinary, part, locs, id, properties, serializedTaskMetrics)
          }
      }
    } catch {
      case NonFatal(e) =>
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }
    
    if (tasks.size > 0) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${partitionsToCompute.take(15).mkString(", ")})")
      
      // æäº¤TaskSetç»™TaskScheduler
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
    } else {
      // æ²¡æœ‰ä»»åŠ¡éœ€è¦è¿è¡Œï¼Œæ ‡è®°Stageå®Œæˆ
      markStageAsFinished(stage, None)
      
      val debugString = stage match {
        case stage: ShuffleMapStage =>
          s"Stage ${stage} is actually done; " +
            s"(available: ${stage.isAvailable}," +
            s"available outputs: ${stage.numAvailableOutputs}," +
            s"partitions: ${stage.numPartitions})"
        case stage : ResultStage =>
          s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})"
      }
      logDebug(debugString)
      
      submitWaitingChildStages(stage)
    }
  }
  
  // 8. å¤„ç†ä»»åŠ¡å®Œæˆ
  def handleTaskCompletion(event: CompletionEvent): Unit = {
    val task = event.task
    val taskId = event.taskInfo.taskId
    val stageId = task.stageId
    val taskType = Utils.getFormattedClassName(task)
    
    // æ›´æ–°ç´¯åŠ å™¨
    event.accumUpdates.foreach { case (id, partialValue) =>
      val acc = AccumulatorContext.get(id)
      if (acc != null) {
        acc.asInstanceOf[AccumulatorV2[Any, Any]].merge(partialValue.asInstanceOf[Any])
      }
    }
    
    // å¤„ç†ä¸åŒçš„ä»»åŠ¡ç»“æœ
    event.reason match {
      case Success =>
        task match {
          case rt: ResultTask[_, _] =>
            // ResultTaskå®Œæˆ
            val resultStage = stage.asInstanceOf[ResultStage]
            resultStage.activeJob match {
              case Some(job) =>
                if (!job.finished(rt.outputId)) {
                  job.finished(rt.outputId) = true
                  job.numFinished += 1
                  
                  // è°ƒç”¨ç»“æœå¤„ç†å™¨
                  job.listener.taskSucceeded(rt.outputId, event.result)
                  
                  // æ£€æŸ¥Jobæ˜¯å¦å®Œæˆ
                  if (job.numFinished == job.numPartitions) {
                    markStageAsFinished(resultStage)
                    cleanupStateForJobAndIndependentStages(job)
                    listenerBus.post(SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded))
                  }
                }
              case None =>
                logInfo("Ignoring result from " + rt + " because its job has finished")
            }
            
          case smt: ShuffleMapTask =>
            // ShuffleMapTaskå®Œæˆ
            val shuffleStage = stage.asInstanceOf[ShuffleMapStage]
            shuffleStage.addOutputLoc(smt.partitionId, event.result.asInstanceOf[MapStatus])
            
            if (runningStages.contains(shuffleStage) && shuffleStage.pendingPartitions.isEmpty) {
              markStageAsFinished(shuffleStage)
              logInfo("looking for newly runnable stages")
              logInfo("running: " + runningStages)
              logInfo("waiting: " + waitingStages)
              logInfo("failed: " + failedStages)
              
              // æäº¤ç­‰å¾…ä¸­çš„å­Stage
              submitWaitingChildStages(shuffleStage)
            }
        }
        
      case _: TaskFailedException =>
        // ä»»åŠ¡å¤±è´¥å¤„ç†
        handleTaskFailure(task, event.reason.asInstanceOf[TaskFailedException])
    }
  }
}
```

**Stageåˆ’åˆ†ç®—æ³•è¯¦è§£**

**Stageåˆ’åˆ†åŸç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[å¼€å§‹Stageåˆ’åˆ†] --> B[ä»æœ€ç»ˆRDDå¼€å§‹]
    B --> C[éå†RDDä¾èµ–]
    C --> D{ä¾èµ–ç±»å‹}
    D -->|çª„ä¾èµ–| E[åŠ å…¥å½“å‰Stage]
    D -->|å®½ä¾èµ–| F[åˆ›å»ºæ–°Stageè¾¹ç•Œ]
    E --> G[ç»§ç»­éå†çˆ¶RDD]
    F --> H[åˆ›å»ºShuffleMapStage]
    G --> C
    H --> I[é€’å½’å¤„ç†çˆ¶RDD]
    I --> C
    C --> J{æ˜¯å¦è¿˜æœ‰æœªå¤„ç†RDD}
    J -->|æ˜¯| C
    J -->|å¦| K[Stageåˆ’åˆ†å®Œæˆ]
    
    style A fill:#e1f5fe
    style F fill:#ffebee
    style H fill:#fff3e0
    style K fill:#e8f5e8
```

**Stageåˆ’åˆ†ä¸ä¾èµ–ç®¡ç†æºç **ï¼š

```scala
// Stageåˆ’åˆ†æ ¸å¿ƒé€»è¾‘
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}

// æŸ¥æ‰¾ç¼ºå¤±çš„çˆ¶ä¾èµ–
private def getMissingAncestorShuffleDependencies(
    rdd: RDD[_]): ArrayStack[ShuffleDependency[_, _, _]] = {
  val ancestors = new ArrayStack[ShuffleDependency[_, _, _]]
  val visited = new HashSet[RDD[_]]
  val waitingForVisit = new ArrayStack[RDD[_]]
  
  waitingForVisit.push(rdd)
  while (waitingForVisit.nonEmpty) {
    val toVisit = waitingForVisit.pop()
    if (!visited(toVisit)) {
      visited += toVisit
      toVisit.dependencies.foreach {
        case shuffleDep: ShuffleDependency[_, _, _] =>
          if (!shuffleIdToMapStage.contains(shuffleDep.shuffleId)) {
            ancestors.push(shuffleDep)
            waitingForVisit.push(shuffleDep.rdd)
          }
        case narrowDep: NarrowDependency[_] =>
          waitingForVisit.push(narrowDep.rdd)
      }
    }
  }
  ancestors
}

// åˆ›å»ºResultStage
private def createResultStage(
    rdd: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    jobId: Int,
    callSite: CallSite): ResultStage = {
  
  val parents = getOrCreateParentStages(rdd, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
  stageIdToStage(id) = stage
  updateJobIdStageIdMaps(jobId, stage)
  stage
}

// åˆ›å»ºShuffleMapStage
private def createShuffleMapStage(shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage = {
  val rdd = shuffleDep.rdd
  val numTasks = rdd.partitions.length
  val parents = getOrCreateParentStages(rdd, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ShuffleMapStage(id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep)
  
  stageIdToStage(id) = stage
  shuffleIdToMapStage(shuffleDep.shuffleId) = stage
  updateJobIdStageIdMaps(jobId, stage)
  
  if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
    logInfo("Registering RDD " + rdd.id + " (" + rdd.getCreationSite + ") as input to " +
      "shuffle " + shuffleDep.shuffleId)
    mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)
  }
  stage
}
```

**å®¹é”™å¤„ç†æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[Taskæ‰§è¡Œå¤±è´¥] --> B{å¤±è´¥åŸå› åˆ†æ}
    B -->|FetchFailed| C[Shuffleæ•°æ®è·å–å¤±è´¥]
    B -->|TaskKilled| D[ä»»åŠ¡è¢«æ€æ­»]
    B -->|ExceptionFailure| E[ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸]
    B -->|ExecutorLostFailure| F[Executorä¸¢å¤±]
    
    C --> G[æ ‡è®°çˆ¶Stageå¤±è´¥]
    G --> H[é‡æ–°æäº¤çˆ¶Stage]
    H --> I[é‡æ–°è®¡ç®—Shuffleæ•°æ®]
    
    D --> J[æ£€æŸ¥é‡è¯•æ¬¡æ•°]
    E --> J
    J --> K{æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•}
    K -->|å¦| L[é‡æ–°è°ƒåº¦Task]
    K -->|æ˜¯| M[æ ‡è®°Stageå¤±è´¥]
    
    F --> N[ç§»é™¤ä¸¢å¤±çš„Executor]
    N --> O[é‡æ–°åˆ†é…èµ„æº]
    O --> P[é‡æ–°æäº¤æ‰€æœ‰Task]
    
    L --> Q[Taské‡æ–°æ‰§è¡Œ]
    I --> Q
    P --> Q
    M --> R[Jobå¤±è´¥]
    Q --> S[æ‰§è¡ŒæˆåŠŸ]
    
    style A fill:#ffebee
    style C fill:#fff3e0
    style R fill:#ffcdd2
    style S fill:#e8f5e8
```

```mermaid
graph TD
    A[RDD DAG] --> B[DAGScheduler]
    B --> C[Stageåˆ’åˆ†]
    C --> D[Stage 0<br/>ShuffleMapStage]
    C --> E[Stage 1<br/>ShuffleMapStage]  
    C --> F[Stage 2<br/>ResultStage]
    
    D --> G[Task 0-1]
    D --> H[Task 0-2]
    E --> I[Task 1-1]
    E --> J[Task 1-2]
    F --> K[Task 2-1]
    F --> L[Task 2-2]
    
    style B fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#e8f5e8
```

**DAGScheduleræ¶æ„ç»„ä»¶**ï¼š


| ç»„ä»¶             | ç±»å                                      | ä¸»è¦èŒè´£            | å…³é”®æ–¹æ³•                       |
| ------------------ | ------------------------------------------- | --------------------- | -------------------------------- |
| **DAGScheduler** | `DAGScheduler`                            | ä½œä¸šè°ƒåº¦å’ŒStageåˆ’åˆ† | `submitJob`, `submitStage`     |
| **EventLoop**    | `DAGSchedulerEventProcessLoop`            | äº‹ä»¶å¤„ç†å¾ªç¯        | `post`, `onReceive`            |
| **Stage**        | `Stage`, `ResultStage`, `ShuffleMapStage` | StageæŠ½è±¡           | `findMissingPartitions`        |
| **Job**          | `ActiveJob`                               | ä½œä¸šæŠ½è±¡            | `numFinished`, `numPartitions` |

**DAGScheduleräº‹ä»¶å¤„ç†**ï¼š

```scala
// DAGScheduleräº‹ä»¶ç±»å‹
sealed trait DAGSchedulerEvent

case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) extends DAGSchedulerEvent

case class StageCompleted(stage: Stage) extends DAGSchedulerEvent
case class TaskCompleted(task: Task[_], reason: TaskEndReason) extends DAGSchedulerEvent

// äº‹ä»¶å¤„ç†å¾ªç¯
class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    event match {
      case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
        dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
      case StageCompleted(stage) =>
        dagScheduler.handleStageCompletion(stage)
      case TaskCompleted(task, reason) =>
        dagScheduler.handleTaskCompletion(task, reason)
    }
  }
}
```

**Stageåˆ’åˆ†ä¸ä¾èµ–ç®¡ç†**ï¼š

```scala
// Stageåˆ’åˆ†æ ¸å¿ƒé€»è¾‘
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}
```

#### TaskSchedulerè°ƒåº¦

**TaskScheduler** è´Ÿè´£å°†Taskåˆ†å‘åˆ°Executoræ‰§è¡Œï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ã€‚

```mermaid
graph TD
    A[DAGScheduleræäº¤TaskSet] --> B[TaskSchedulerImpl.submitTasks]
    B --> C[åˆ›å»ºTaskSetManager]
    C --> D[æ·»åŠ åˆ°è°ƒåº¦æ± Pool]
    D --> E[SchedulerBackend.reviveOffers]
    E --> F[æ”¶é›†Executorèµ„æºä¿¡æ¯]
    F --> G[TaskSchedulerImpl.resourceOffers]
    G --> H[æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…Task]
    H --> I[åˆ›å»ºTaskDescription]
    I --> J[SchedulerBackend.launchTasks]
    J --> K[å‘é€Taskåˆ°Executor]
    K --> L[Executoræ‰§è¡ŒTask]
    L --> M[è¿”å›æ‰§è¡Œç»“æœ]
    M --> N[TaskScheduler.statusUpdate]
    N --> O[æ›´æ–°TaskçŠ¶æ€]
    O --> P{TaskSetæ˜¯å¦å®Œæˆ}
    P -->|å¦| Q[ç»§ç»­è°ƒåº¦å‰©ä½™Task]
    P -->|æ˜¯| R[é€šçŸ¥DAGScheduler]
    Q --> G
    R --> S[Stageå®Œæˆ]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style L fill:#e8f5e8
    style S fill:#c8e6c9
```

**TaskScheduleræ¶æ„ç»„ä»¶**


| ç»„ä»¶                 | ç±»å                            | ä¸»è¦èŒè´£         | å…³é”®ç‰¹æ€§               |
| ---------------------- | --------------------------------- | ------------------ | ------------------------ |
| **TaskScheduler**    | `TaskSchedulerImpl`             | ä»»åŠ¡è°ƒåº¦å’Œåˆ†å‘   | æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥       |
| **SchedulerBackend** | `CoarseGrainedSchedulerBackend` | ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡ | èµ„æºåˆ†é…å’ŒExecutorç®¡ç† |
| **TaskSetManager**   | `TaskSetManager`                | ç®¡ç†TaskSetæ‰§è¡Œ  | ä»»åŠ¡é‡è¯•ã€æ¨æµ‹æ‰§è¡Œ     |
| **Pool**             | `Pool`                          | è°ƒåº¦æ± ç®¡ç†       | å…¬å¹³è°ƒåº¦ã€FIFOè°ƒåº¦     |

TaskSchedulerImplæ ¸å¿ƒæºç å®ç°

```scala
// TaskSchedulerImpl.scala - æ ¸å¿ƒä»»åŠ¡è°ƒåº¦å™¨å®ç°
class TaskSchedulerImpl(
    val sc: SparkContext,
    val maxTaskFailures: Int,
    isLocal: Boolean = false)
  extends TaskScheduler with Logging {

  // è°ƒåº¦åç«¯ï¼Œè´Ÿè´£ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡
  var backend: SchedulerBackend = null
  
  // æ ¹è°ƒåº¦æ± ï¼Œç®¡ç†æ‰€æœ‰TaskSetManager
  val rootPool: Pool = new Pool("", SchedulingMode.FIFO, 0, 0)
  
  // è°ƒåº¦æ„å»ºå™¨ï¼Œè´Ÿè´£æ„å»ºè°ƒåº¦æ ‘
  var schedulableBuilder: SchedulableBuilder = null
  
  // æ­£åœ¨è¿è¡Œçš„TaskSetç®¡ç†å™¨
  private val taskSetsByStageIdAndAttempt = new HashMap[Int, HashMap[Int, TaskSetManager]]
  
  // 1. æäº¤TaskSetçš„æ ¸å¿ƒæ–¹æ³•
  override def submitTasks(taskSet: TaskSet): Unit = synchronized {
    val tasks = taskSet.tasks
    logInfo(s"Adding task set ${taskSet.id} with ${tasks.length} tasks")
    
    // åˆ›å»ºTaskSetManageræ¥ç®¡ç†è¿™ä¸ªTaskSet
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    val stage = taskSet.stageId
    val stageTaskSets = taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
    
    // æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„TaskSet
    stageTaskSets.foreach { case (_, ts) =>
      ts.isZombie = true
    }
    stageTaskSets(taskSet.stageAttemptId) = manager
    
    // å°†TaskSetManageræ·»åŠ åˆ°è°ƒåº¦æ± 
    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
    
    // å¦‚æœä¸æ˜¯æœ¬åœ°æ¨¡å¼ï¼Œè§¦å‘èµ„æºåˆ†é…
    if (!isLocal && manager.tasks.length > 0) {
      backend.reviveOffers()
    }
  }
  
  // 2. èµ„æºåˆ†é…çš„æ ¸å¿ƒæ–¹æ³•
  def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
    // æ ‡è®°æ‰€æœ‰Executorä¸ºæ´»è·ƒçŠ¶æ€
    var newExecAvail = false
    for (o <- offers) {
      if (!hostToExecutors.contains(o.host)) {
        hostToExecutors(o.host) = new HashSet[String]()
      }
      if (!executorIdToRunningTaskIds.contains(o.executorId)) {
        hostToExecutors(o.host) += o.executorId
        executorAdded(o.executorId, o.host)
        executorIdToHost(o.executorId) = o.host
        executorIdToRunningTaskIds(o.executorId) = HashSet[Long]()
        newExecAvail = true
      }
    }
    
    // éšæœºæ‰“ä¹±offersï¼Œé¿å…æ€»æ˜¯åœ¨åŒä¸€ä¸ªExecutorä¸Šåˆ†é…ä»»åŠ¡
    val shuffledOffers = Random.shuffle(offers)
    
    // ä¸ºæ¯ä¸ªofferåˆ›å»ºä»»åŠ¡åˆ—è¡¨
    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK))
    val availableCpus = shuffledOffers.map(o => o.cores).toArray
    
    // è·å–æ’åºåçš„TaskSeté˜Ÿåˆ—
    val sortedTaskSets = rootPool.getSortedTaskSetQueue
    
    // æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…ä»»åŠ¡
    for (taskSet <- sortedTaskSets) {
      var launchedAnyTask = false
      
      // éå†æ‰€æœ‰æœ¬åœ°æ€§çº§åˆ«ï¼šPROCESS_LOCAL -> NODE_LOCAL -> NO_PREF -> RACK_LOCAL -> ANY
      for (currentMaxLocality <- taskSet.myLocalityLevels) {
        do {
          launchedAnyTask = resourceOfferSingleTaskSet(
            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)
        } while (launchedAnyTask)
      }
    }
    
    // å¦‚æœæœ‰æ–°çš„Executorå¯ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ¨æµ‹æ‰§è¡Œçš„ä»»åŠ¡
    if (newExecAvail) {
      for (taskSet <- sortedTaskSets) {
        taskSet.executorAdded()
      }
    }
    
    return tasks
  }
  
  // 3. å•ä¸ªTaskSetçš„èµ„æºåˆ†é…
  private def resourceOfferSingleTaskSet(
      taskSet: TaskSetManager,
      maxLocality: TaskLocality,
      shuffledOffers: Seq[WorkerOffer],
      availableCpus: Array[Int],
      tasks: IndexedSeq[ArrayBuffer[TaskDescription]]): Boolean = {
    
    var launchedTask = false
    
    // éå†æ‰€æœ‰å¯ç”¨çš„Executor
    for (i <- 0 until shuffledOffers.length) {
      val execId = shuffledOffers(i).executorId
      val host = shuffledOffers(i).host
      
      // æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„CPUæ ¸å¿ƒ
      if (availableCpus(i) >= CPUS_PER_TASK) {
        try {
          // å°è¯•åœ¨å½“å‰Executorä¸Šå¯åŠ¨ä»»åŠ¡
          for (task <- taskSet.resourceOffer(execId, host, maxLocality)) {
            tasks(i) += task
            val tid = task.taskId
            taskIdToTaskSetManager(tid) = taskSet
            taskIdToExecutorId(tid) = execId
            executorIdToRunningTaskIds(execId) += tid
            availableCpus(i) -= CPUS_PER_TASK
            assert(availableCpus(i) >= 0)
            launchedTask = true
          }
        } catch {
          case e: TaskNotSerializableException =>
            logError(s"Resource offer failed, task set ${taskSet.name} was not serializable")
            taskSet.abort("TaskSet %s was not serializable".format(taskSet.name))
            return launchedTask
        }
      }
    }
    return launchedTask
  }
  
  // 4. ä»»åŠ¡çŠ¶æ€æ›´æ–°å¤„ç†
  def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer): Unit = {
    var failedExecutor: Option[String] = None
    var reason: Option[TaskFailedReason] = None
    
    synchronized {
      try {
        taskIdToTaskSetManager.get(tid) match {
          case Some(taskSet) =>
            if (state == TaskState.LOST) {
              // ä»»åŠ¡ä¸¢å¤±ï¼Œå¯èƒ½æ˜¯Executorå¤±è´¥
              val execId = taskIdToExecutorId.getOrElse(tid, throw new IllegalStateException(
                "taskIdToTaskSetManager.contains(tid) <=> taskIdToExecutorId.contains(tid)"))
              if (executorIdToRunningTaskIds.contains(execId)) {
                reason = Some(ExecutorLostFailure(execId, exitCausedByApp = false,
                  Some("Task $tid was lost, so marking the executor as lost as well.")))
                removeExecutor(execId, reason.get)
                failedExecutor = Some(execId)
              }
            }
            
            if (TaskState.isFinished(state)) {
              cleanupTaskState(tid)
              taskSet.removeRunningTask(tid)
              if (state == TaskState.FINISHED) {
                taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)
              } else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {
                taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)
              }
            }
          case None =>
            logError(
              ("Ignoring update with state %s for TID %s because its task set is gone (this is " +
               "likely the result of receiving duplicate task finished status updates) or its " +
               "executor has been marked as failed.")
                .format(state, tid))
        }
      } catch {
        case e: Exception => logError("Exception in statusUpdate", e)
      }
    }
    
    // å¦‚æœæœ‰Executorå¤±è´¥ï¼Œæ›´æ–°DAGScheduler
    if (failedExecutor.isDefined) {
      assert(reason.isDefined)
      dagScheduler.executorLost(failedExecutor.get, reason.get)
      backend.reviveOffers()
    }
  }
  
  // 5. åˆ›å»ºTaskSetManager
  private def createTaskSetManager(
      taskSet: TaskSet,
      maxTaskFailures: Int): TaskSetManager = {
    new TaskSetManager(this, taskSet, maxTaskFailures, blacklistTrackerOpt)
  }
  
  // 6. æ¸…ç†ä»»åŠ¡çŠ¶æ€
  private def cleanupTaskState(tid: Long): Unit = {
    taskIdToTaskSetManager.remove(tid)
    taskIdToExecutorId.remove(tid).foreach { execId =>
      executorIdToRunningTaskIds.get(execId).foreach { _.remove(tid) }
    }
  }
  
  // 7. ç§»é™¤å¤±è´¥çš„Executor
  private def removeExecutor(executorId: String, reason: ExecutorLossReason): Unit = {
    executorIdToRunningTaskIds.remove(executorId).foreach { taskIds =>
      logDebug("Cleaning up TaskScheduler state for tasks " +
        s"${taskIds.mkString("[", ",", "]")} on failed executor $executorId")
      // å°†è¿è¡Œåœ¨å¤±è´¥Executorä¸Šçš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
      taskIds.foreach { tid =>
        val taskSetMgr = taskIdToTaskSetManager(tid)
        if (taskSetMgr != null) {
          taskSetMgr.executorLost(executorId, tid, reason)
        }
      }
    }
    
    val host = executorIdToHost(executorId)
    val execs = hostToExecutors.getOrElse(host, new HashSet)
    execs -= executorId
    if (execs.isEmpty) {
      hostToExecutors -= host
    }
    executorIdToHost -= executorId
    
    rootPool.executorLost(executorId, host, reason)
  }
}
```

**TaskSetManageræºç å®ç°**

```scala
// TaskSetManager.scala - ç®¡ç†å•ä¸ªTaskSetçš„æ‰§è¡Œ
private[spark] class TaskSetManager(
    sched: TaskSchedulerImpl,
    val taskSet: TaskSet,
    val maxTaskFailures: Int,
    blacklistTracker: Option[BlacklistTracker] = None)
  extends Schedulable with Logging {

  // TaskSetä¸­çš„æ‰€æœ‰ä»»åŠ¡
  val tasks = taskSet.tasks
  val numTasks = tasks.length
  
  // ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
  private val copiesRunning = new Array[Int](numTasks)
  private val successful = new Array[Boolean](numTasks)
  private val numFailures = new Array[Int](numTasks)
  
  // æœ¬åœ°æ€§çº§åˆ«ç®¡ç†
  private val myLocalityLevels = computeValidLocalityLevels()
  private val localityWaits = myLocalityLevels.map(getLocalityWait)
  
  // å¾…è°ƒåº¦ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†ç»„ï¼‰
  private val pendingTasksForExecutor = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksForHost = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksForRack = new HashMap[String, ArrayBuffer[Int]]
  private val pendingTasksWithNoPrefs = new ArrayBuffer[Int]
  private val allPendingTasks = new ArrayBuffer[Int]
  
  // 1. èµ„æºåˆ†é…çš„æ ¸å¿ƒæ–¹æ³•
  def resourceOffer(
      execId: String,
      host: String,
      maxLocality: TaskLocality): Option[TaskDescription] = {
    
    val offerBlacklisted = blacklistTracker.exists(_.isExecutorBlacklisted(execId)) ||
      blacklistTracker.exists(_.isNodeBlacklisted(host))
    
    if (!isZombie && !offerBlacklisted) {
      val curTime = clock.getTimeMillis()
      
      var allowedLocality = maxLocality
      
      if (maxLocality != TaskLocality.NO_PREF) {
        allowedLocality = getAllowedLocalityLevel(curTime)
        if (allowedLocality > maxLocality) {
          // å¦‚æœå…è®¸çš„æœ¬åœ°æ€§çº§åˆ«æ¯”æä¾›çš„çº§åˆ«æ›´å®½æ¾ï¼Œåˆ™ä½¿ç”¨æä¾›çš„çº§åˆ«
          allowedLocality = maxLocality
        }
      }
      
      dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
        // æ‰¾åˆ°äº†å¯ä»¥è°ƒåº¦çš„ä»»åŠ¡
        val task = tasks(index)
        val taskId = sched.newTaskId()
        
        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
        copiesRunning(index) += 1
        successful(index) = false
        
        val attemptNum = taskAttempts(index).size
        val info = new TaskInfo(taskId, index, attemptNum, curTime,
          execId, host, taskLocality, speculative)
        taskInfos(taskId) = info
        taskAttempts(index) = info :: taskAttempts(index)
        
        // æ›´æ–°è¿è¡Œä»»åŠ¡ç»Ÿè®¡
        if (maxLocality == TaskLocality.NO_PREF) {
          stats.incNoPrefs(1)
        } else {
          stats.incLocality(taskLocality)
        }
        
        // åºåˆ—åŒ–ä»»åŠ¡
        val serializedTask: ByteBuffer = try {
          ser.serialize(task)
        } catch {
          case NonFatal(e) =>
            val msg = s"Failed to serialize task $taskId, not attempting to retry it."
            logError(msg, e)
            abort(s"$msg Exception during serialization: $e")
            throw new TaskNotSerializableException(e)
        }
        
        if (serializedTask.limit() > TaskSetManager.TASK_SIZE_TO_WARN_KB * 1024 &&
            !emittedTaskSizeWarning) {
          emittedTaskSizeWarning = true
          logWarning(s"Stage ${task.stageId} contains a task of very large size " +
            s"(${serializedTask.limit() / 1024} KB). The maximum recommended task size is " +
            s"${TaskSetManager.TASK_SIZE_TO_WARN_KB} KB.")
        }
        
        addRunningTask(taskId)
        
        // åˆ›å»ºTaskDescription
        new TaskDescription(
          taskId = taskId,
          attemptNumber = attemptNum,
          execId,
          task.name,
          index,
          task.partitionId,
          addedFiles,
          addedJars,
          task.localProperties,
          serializedTask)
      }
    } else {
      None
    }
  }
  
  // 2. ä»»åŠ¡å‡ºé˜Ÿé€»è¾‘
  private def dequeueTask(execId: String, host: String, maxLocality: TaskLocality)
    : Option[(Int, TaskLocality, Boolean)] = {
    
    // æŒ‰æœ¬åœ°æ€§çº§åˆ«ä¾æ¬¡å°è¯•è·å–ä»»åŠ¡
    for (locality <- Array(TaskLocality.PROCESS_LOCAL, TaskLocality.NODE_LOCAL,
                          TaskLocality.NO_PREF, TaskLocality.RACK_LOCAL, TaskLocality.ANY)) {
      if (locality <= maxLocality) {
        val taskSetIndex = locality match {
          case TaskLocality.PROCESS_LOCAL => dequeueTaskFromList(execId, host, 
            pendingTasksForExecutor.getOrElse(execId, ArrayBuffer()))
          case TaskLocality.NODE_LOCAL => dequeueTaskFromList(execId, host,
            pendingTasksForHost.getOrElse(host, ArrayBuffer()))
          case TaskLocality.NO_PREF => dequeueTaskFromList(execId, host, pendingTasksWithNoPrefs)
          case TaskLocality.RACK_LOCAL => dequeueTaskFromList(execId, host,
            pendingTasksForRack.getOrElse(sched.getRackForHost(host).orNull, ArrayBuffer()))
          case TaskLocality.ANY => dequeueTaskFromList(execId, host, allPendingTasks)
        }
        
        if (taskSetIndex.isDefined) {
          return Some((taskSetIndex.get, locality, false))
        }
      }
    }
    None
  }
  
  // 3. ä»ä»»åŠ¡åˆ—è¡¨ä¸­å‡ºé˜Ÿ
  private def dequeueTaskFromList(
      execId: String,
      host: String,
      list: ArrayBuffer[Int]): Option[Int] = {
    var indexOffset = list.size
    while (indexOffset > 0) {
      indexOffset -= 1
      val index = list(indexOffset)
      if (copiesRunning(index) == 0 && !successful(index)) {
        // æ‰¾åˆ°å¯è¿è¡Œçš„ä»»åŠ¡
        list.remove(indexOffset)
        if (pendingTasksForExecutor.contains(execId)) {
          pendingTasksForExecutor(execId) -= index
        }
        if (pendingTasksForHost.contains(host)) {
          pendingTasksForHost(host) -= index
        }
        val rack = sched.getRackForHost(host)
        if (rack.isDefined && pendingTasksForRack.contains(rack.get)) {
          pendingTasksForRack(rack.get) -= index
        }
        pendingTasksWithNoPrefs -= index
        allPendingTasks -= index
        return Some(index)
      }
    }
    None
  }
  
  // 4. ä»»åŠ¡å®Œæˆå¤„ç†
  def handleSuccessfulTask(tid: Long, result: DirectTaskResult[_]): Unit = {
    val info = taskInfos(tid)
    val index = info.index
    
    // æ ‡è®°ä»»åŠ¡æˆåŠŸ
    successful(index) = true
    removalPendingTask(index)
    
    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    tasksSuccessful += 1
    logInfo(s"Finished task ${info.id} in stage ${taskSet.stageId} (TID $tid) in" +
      s" ${info.duration} ms on ${info.host} (executor ${info.executorId})" +
      s" ($tasksSuccessful/$numTasks)")
    
    // æ£€æŸ¥TaskSetæ˜¯å¦å®Œæˆ
    if (tasksSuccessful == numTasks) {
      isZombie = true
    }
    
    // é€šçŸ¥DAGSchedulerä»»åŠ¡å®Œæˆ
    sched.dagScheduler.taskEnded(tasks(index), Success, result.value(), result.accumUpdates, info)
  }
  
  // 5. ä»»åŠ¡å¤±è´¥å¤„ç†
  def handleFailedTask(tid: Long, state: TaskState, reason: TaskFailedReason): Unit = {
    val info = taskInfos(tid)
    if (info != null && !successful(info.index)) {
      val index = info.index
      copiesRunning(index) -= 1
      
      reason match {
        case fetchFailed: FetchFailed =>
          logWarning(s"Lost task ${info.id} in stage ${taskSet.stageId} (TID $tid, ${info.host}," +
            s" executor ${info.executorId}): ${reason.toErrorString}")
          if (!successful(index)) {
            successful(index) = true
            tasksSuccessful += 1
          }
          isZombie = true
          
        case ef: ExceptionFailure =>
          // ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸
          numFailures(index) += 1
          if (numFailures(index) >= maxTaskFailures) {
            logError(s"Task ${info.id} in stage ${taskSet.stageId} failed $maxTaskFailures times; aborting job")
            abort(s"Task $index in stage ${taskSet.stageId} failed $maxTaskFailures times, " +
              s"most recent failure: ${ef.description}")
            return
          } else {
            // é‡æ–°è°ƒåº¦ä»»åŠ¡
            addPendingTask(index)
          }
          
        case TaskKilled(_) =>
          logWarning(s"Task ${info.id} in stage ${taskSet.stageId} was killed")
          
        case _ =>
          logWarning(s"Lost task ${info.id} in stage ${taskSet.stageId} (TID $tid) on " +
            s"executor ${info.executorId}: ${reason.toErrorString}")
          addPendingTask(index)
      }
    }
  }
  
  // 6. è®¡ç®—æœ¬åœ°æ€§çº§åˆ«
  private def computeValidLocalityLevels(): Array[TaskLocality.TaskLocality] = {
    import TaskLocality._
    val levels = new ArrayBuffer[TaskLocality.TaskLocality]
    
    if (!pendingTasksForExecutor.isEmpty &&
        pendingTasksForExecutor.values.exists(_.nonEmpty)) {
      levels += PROCESS_LOCAL
    }
    if (!pendingTasksForHost.isEmpty &&
        pendingTasksForHost.values.exists(_.nonEmpty)) {
      levels += NODE_LOCAL
    }
    if (!pendingTasksWithNoPrefs.isEmpty) {
      levels += NO_PREF
    }
    if (!pendingTasksForRack.isEmpty &&
        pendingTasksForRack.values.exists(_.nonEmpty)) {
      levels += RACK_LOCAL
    }
    levels += ANY
    
    logDebug("Valid locality levels for " + taskSet + ": " + levels.mkString(", "))
    levels.toArray
  }
}
```

**SchedulerBackendæºç å®ç°**

```scala
// CoarseGrainedSchedulerBackend.scala - ç²—ç²’åº¦è°ƒåº¦åç«¯
private[spark] class CoarseGrainedSchedulerBackend(
    scheduler: TaskSchedulerImpl,
    val rpcEnv: RpcEnv)
  extends ExecutorAllocationClient with SchedulerBackend with Logging {

  // Executorä¿¡æ¯ç®¡ç†
  private val executorDataMap = new HashMap[String, ExecutorData]
  private val addressToExecutorId = new HashMap[RpcAddress, String]
  
  // èµ„æºç®¡ç†
  private val totalCoreCount = new AtomicInteger(0)
  private val totalRegisteredExecutors = new AtomicInteger(0)
  private val maxRpcMessageSize = RpcUtils.maxMessageSizeBytes(conf)
  
  // RPCç«¯ç‚¹å¼•ç”¨
  var driverEndpoint: RpcEndpointRef = null
  
  // 1. å¯åŠ¨è°ƒåº¦åç«¯
  override def start(): Unit = {
    val properties = Seq[(String, String)](
      ("spark.scheduler.mode", scheduler.schedulingMode.toString),
      ("spark.starvation.timeout", starvationTimer.toString),
      ("spark.rpc.askTimeout", askTimeout.toString)
    ) ++ scheduler.applicationAttemptId().map(id => ("spark.app.attempt.id", id.toString))
    
    driverEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))
  }
  
  // 2. åˆ›å»ºDriverç«¯ç‚¹
  protected def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint = {
    new DriverEndpoint(rpcEnv, properties)
  }
  
  // 3. Driverç«¯ç‚¹å®ç°
  class DriverEndpoint(override val rpcEnv: RpcEnv, sparkProperties: Seq[(String, String)])
    extends ThreadSafeRpcEndpoint with Logging {
    
    // å¤„ç†Executoræ³¨å†Œ
    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
      case RegisterExecutor(executorId, executorRef, hostname, cores, logUrls) =>
        if (executorDataMap.contains(executorId)) {
          executorRef.send(RegisterExecutorFailed("Duplicate executor ID: " + executorId))
          context.reply(true)
        } else if (scheduler.nodeBlacklist.contains(hostname)) {
          logInfo(s"Rejecting $executorId as it has been blacklisted.")
          executorRef.send(RegisterExecutorFailed(s"Executor is blacklisted: $executorId"))
          context.reply(true)
        } else {
          // æ³¨å†Œæ–°çš„Executor
          val executorAddress = if (executorRef.address != null) {
            executorRef.address
          } else {
            context.senderAddress
          }
          
          logInfo(s"Registered executor $executorRef ($executorAddress) with ID $executorId")
          addressToExecutorId(executorAddress) = executorId
          totalCoreCount.addAndGet(cores)
          totalRegisteredExecutors.addAndGet(1)
          
          val data = new ExecutorData(executorRef, executorAddress, hostname,
            cores, cores, logUrls)
          
          // å°†Executorä¿¡æ¯å­˜å‚¨åˆ°æ˜ å°„ä¸­
          CoarseGrainedSchedulerBackend.this.synchronized {
            executorDataMap.put(executorId, data)
            if (currentExecutorIdCounter < executorId.toInt) {
              currentExecutorIdCounter = executorId.toInt
            }
            if (numPendingExecutors > 0) {
              numPendingExecutors -= 1
              logDebug(s"Decremented number of pending executors ($numPendingExecutors left)")
            }
          }
          
          executorRef.send(RegisteredExecutor)
          context.reply(true)
          listenerBus.post(
            SparkListenerExecutorAdded(System.currentTimeMillis(), executorId, data))
          makeOffers()
        }
        
      case StopDriver =>
        context.reply(true)
        stop()
        
      case StopExecutors =>
        logInfo("Asking each executor to shut down")
        for ((_, executorData) <- executorDataMap) {
          executorData.executorEndpoint.send(StopExecutor)
        }
        context.reply(true)
    }
    
    // å¤„ç†ExecutorçŠ¶æ€æ›´æ–°
    override def receive: PartialFunction[Any, Unit] = {
      case StatusUpdate(executorId, taskId, state, data) =>
        scheduler.statusUpdate(taskId, state, data.value)
        if (TaskState.isFinished(state)) {
          executorDataMap.get(executorId) match {
            case Some(executorInfo) =>
              executorInfo.freeCores += scheduler.CPUS_PER_TASK
              makeOffers(executorId)
            case None =>
              logWarning(s"Ignored task status update ($taskId state $state) " +
                s"from unknown executor with ID $executorId")
          }
        }
        
      case ReviveOffers =>
        makeOffers()
        
      case KillTask(taskId, executorId, interruptThread, reason) =>
        executorDataMap.get(executorId) match {
          case Some(executorInfo) =>
            executorInfo.executorEndpoint.send(
              KillTask(taskId, executorId, interruptThread, reason))
          case None =>
            logWarning(s"Attempted to kill task $taskId for unknown executor $executorId.")
        }
    }
    
    // 4. èµ„æºåˆ†é…æ ¸å¿ƒæ–¹æ³•
    private def makeOffers(): Unit = {
      // è¿‡æ»¤å‡ºæ´»è·ƒçš„Executor
      val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
      val workOffers = activeExecutors.map {
        case (id, executorData) =>
          new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
      }.toIndexedSeq
      
      launchTasks(scheduler.resourceOffers(workOffers))
    }
    
    private def makeOffers(executorId: String): Unit = {
      // ä¸ºç‰¹å®šExecutoråˆ›å»ºèµ„æºoffer
      if (executorIsAlive(executorId)) {
        val executorData = executorDataMap(executorId)
        val workOffers = IndexedSeq(
          new WorkerOffer(executorId, executorData.executorHost, executorData.freeCores))
        launchTasks(scheduler.resourceOffers(workOffers))
      }
    }
    
    // 5. å¯åŠ¨ä»»åŠ¡
    private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = {
      for (i <- tasks.indices) {
        val execId = tasks(i).head.executorId
        val executorData = executorDataMap(execId)
        val executorOffers = tasks(i)
        
        if (executorOffers.nonEmpty) {
          // å‡å°‘å¯ç”¨æ ¸å¿ƒæ•°
          executorData.freeCores -= executorOffers.size * scheduler.CPUS_PER_TASK
          
          logDebug(s"Launching ${executorOffers.size} tasks on executor $execId")
          
          // åºåˆ—åŒ–ä»»åŠ¡å¹¶å‘é€ç»™Executor
          val serializedTasks = executorOffers.map { task =>
            ser.serialize(task)
          }
          
          if (serializedTasks.nonEmpty) {
            executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(
              Utils.serialize(serializedTasks))))
          }
        }
      }
    }
  }
  
  // 6. åœæ­¢è°ƒåº¦åç«¯
  override def stop(): Unit = {
    reviveThread.shutdownNow()
    try {
      if (driverEndpoint != null) {
        driverEndpoint.askSync[Boolean](StopDriver)
      }
    } catch {
      case e: Exception =>
        logWarning("Exception during shutdown", e)
    }
  }
}
```

**Taskåˆ†å‘å’Œæ‰§è¡Œå®Œæ•´æµç¨‹å›¾**

```mermaid
graph TD
    A[DAGScheduleræäº¤TaskSet] --> B[TaskScheduler.submitTasks]
    B --> C[åˆ›å»ºTaskSetManager]
    C --> D[æ·»åŠ åˆ°è°ƒåº¦æ± ]
    D --> E[SchedulerBackend.reviveOffers]
    E --> F[æ”¶é›†Executorèµ„æºä¿¡æ¯]
    F --> G[åˆ›å»ºWorkerOfferåˆ—è¡¨]
    G --> H[TaskScheduler.resourceOffers]
    H --> I[éå†TaskSetManageré˜Ÿåˆ—]
    I --> J[æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…]
    J --> K{PROCESS_LOCALå¯ç”¨?}
    K -->|æ˜¯| L[åˆ†é…åˆ°æŒ‡å®šExecutor]
    K -->|å¦| M{NODE_LOCALå¯ç”¨?}
    M -->|æ˜¯| N[åˆ†é…åˆ°æŒ‡å®šHost]
    M -->|å¦| O{RACK_LOCALå¯ç”¨?}
    O -->|æ˜¯| P[åˆ†é…åˆ°æŒ‡å®šRack]
    O -->|å¦| Q[ANYçº§åˆ«åˆ†é…]
    L --> R[åˆ›å»ºTaskDescription]
    N --> R
    P --> R
    Q --> R
    R --> S[åºåˆ—åŒ–Task]
    S --> T[SchedulerBackend.launchTasks]
    T --> U[å‘é€LaunchTaskæ¶ˆæ¯]
    U --> V[Executoræ¥æ”¶ä»»åŠ¡]
    V --> W[åˆ›å»ºTaskRunner]
    W --> X[æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ]
    X --> Y[Task.runæ‰§è¡Œ]
    Y --> Z[è¿”å›æ‰§è¡Œç»“æœ]
    Z --> AA[StatusUpdateæ¶ˆæ¯]
    AA --> BB[TaskScheduler.statusUpdate]
    BB --> CC[æ›´æ–°TaskSetManagerçŠ¶æ€]
    CC --> DD{TaskSetå®Œæˆ?}
    DD -->|å¦| E
    DD -->|æ˜¯| EE[é€šçŸ¥DAGScheduler]
    
    style A fill:#e1f5fe
    style H fill:#fff3e0
    style Y fill:#e8f5e8
    style EE fill:#c8e6c9
```

**Stageåˆ’åˆ†æœºåˆ¶ ğŸ”¥**

**Stageåˆ’åˆ†åŸåˆ™**ï¼š

- **å®½ä¾èµ–è¾¹ç•Œ**ï¼šé‡åˆ°å®½ä¾èµ–ï¼ˆShuffleï¼‰åˆ’åˆ†æ–°Stage
- **çª„ä¾èµ–åˆå¹¶**ï¼šçª„ä¾èµ–çš„RDDåœ¨åŒä¸€Stageå†…Pipelineæ‰§è¡Œ
- **Stageç±»å‹**ï¼šShuffleMapStageå’ŒResultStage

```mermaid
graph TD
    A[textFile] --> B[flatMap]
    B --> C[map]
    C --> D[reduceByKey]
    D --> E[map]
    E --> F[collect]
    
    subgraph "Stage 0 (ShuffleMapStage)"
        A
        B
        C
    end
    
    subgraph "Stage 1 (ResultStage)"
        E
        F
    end
    
    C -.->|Shuffle| E
    
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#e8f5e8
    style E fill:#e1f5fe
    style F fill:#e1f5fe
```

### å­˜å‚¨ç®¡ç†æœºåˆ¶

#### BlockManagerç»„ä»¶

**BlockManager** æ˜¯Sparkä¸­è´Ÿè´£æ•°æ®å­˜å‚¨å’Œç®¡ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œç»Ÿä¸€ç®¡ç†å†…å­˜å’Œç£ç›˜ä¸Šçš„æ•°æ®å—ã€‚

**æ“ä½œæ—¶åºå›¾**

```mermaid
sequenceDiagram
    participant Driver
    participant BlockManagerMaster
    participant Executor
    participant BlockManager
    participant MemoryStore
    participant DiskStore
    
    Driver->>BlockManagerMaster: 1. æ³¨å†ŒBlockManager
    Executor->>BlockManager: 2. åˆå§‹åŒ–BlockManager
    BlockManager->>BlockManagerMaster: 3. æ³¨å†Œåˆ°Master
    BlockManager->>MemoryStore: 4. åˆå§‹åŒ–å†…å­˜å­˜å‚¨
    BlockManager->>DiskStore: 5. åˆå§‹åŒ–ç£ç›˜å­˜å‚¨
    
    Note over BlockManager,DiskStore: æ•°æ®å­˜å‚¨æµç¨‹
    BlockManager->>MemoryStore: 6. å°è¯•å†…å­˜å­˜å‚¨
    alt å†…å­˜è¶³å¤Ÿ
        MemoryStore-->>BlockManager: 7a. å­˜å‚¨æˆåŠŸ
    else å†…å­˜ä¸è¶³
        BlockManager->>DiskStore: 7b. ç£ç›˜å­˜å‚¨
        DiskStore-->>BlockManager: 7c. å­˜å‚¨æˆåŠŸ
    end
    
    BlockManager->>BlockManagerMaster: 8. æŠ¥å‘Šå­˜å‚¨çŠ¶æ€
```

**BlockManageræ ¸å¿ƒç»„ä»¶è¯¦è§£**

**1. BlockManageræ¶æ„ç»„ä»¶**


| ç»„ä»¶                   | ç±»å                 | ä¸»è¦èŒè´£           | å­˜å‚¨ä»‹è´¨   |
| ------------------------ | ---------------------- | -------------------- | ------------ |
| **BlockManager**       | `BlockManager`       | æ•°æ®å—ç®¡ç†æ€»æ§åˆ¶å™¨ | å†…å­˜+ç£ç›˜  |
| **MemoryStore**        | `MemoryStore`        | å†…å­˜æ•°æ®å­˜å‚¨       | JVMå †å†…å­˜  |
| **DiskStore**          | `DiskStore`          | ç£ç›˜æ•°æ®å­˜å‚¨       | æœ¬åœ°ç£ç›˜   |
| **BlockManagerMaster** | `BlockManagerMaster` | å…ƒæ•°æ®ç®¡ç†         | Driverå†…å­˜ |
| **BlockInfoManager**   | `BlockInfoManager`   | Blockä¿¡æ¯ç®¡ç†      | å†…å­˜ç´¢å¼•   |

**2. BlockManageråˆ›å»ºä¸åˆå§‹åŒ–**

```scala
class BlockManager(
    executorId: String,
    rpcEnv: RpcEnv,
    master: BlockManagerMaster,
    serializerManager: SerializerManager,
    conf: SparkConf,
    memoryManager: MemoryManager,
    mapOutputTracker: MapOutputTracker)
  extends BlockDataManager with BlockEvictionHandler with Logging {

  // æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
  private[spark] val diskBlockManager = new DiskBlockManager(conf, deleteFilesOnStop = true)
  private[spark] val blockInfoManager = new BlockInfoManager
  
  // åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
  private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager)
  private[spark] val diskStore = new DiskStore(conf, diskBlockManager)
  
  // æ³¨å†Œåˆ°Master
  master.registerBlockManager(blockManagerId, maxMemory, slaveEndpoint)
}
```

**3. æ•°æ®å—å­˜å‚¨æµç¨‹**

```scala
// æ•°æ®å—å­˜å‚¨çš„æ ¸å¿ƒæ–¹æ³•
def putBlockData(
    blockId: BlockId,
    data: BlockData,
    level: StorageLevel,
    tellMaster: Boolean = true): Boolean = {
  
  // 1. æ£€æŸ¥å­˜å‚¨çº§åˆ«
  if (level.useMemory) {
    // 2. å°è¯•å­˜å‚¨åˆ°å†…å­˜
    val putSucceeded = memoryStore.putBytes(blockId, data, level)
    if (putSucceeded) {
      // 3. é€šçŸ¥Master
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, 0))
      }
      return true
    }
  }
  
  // 4. å†…å­˜ä¸è¶³ï¼Œå­˜å‚¨åˆ°ç£ç›˜
  if (level.useDisk) {
    val putSucceeded = diskStore.putBytes(blockId, data)
    if (putSucceeded) {
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, data.size))
      }
      return true
    }
  }
  
  false
}
```

**4. æ•°æ®å—è·å–æµç¨‹**

```scala
// æ•°æ®å—è·å–çš„æ ¸å¿ƒæ–¹æ³•
def get[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. æ£€æŸ¥æœ¬åœ°å†…å­˜
  memoryStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 2. æ£€æŸ¥æœ¬åœ°ç£ç›˜
  diskStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 3. ä»è¿œç¨‹è·å–
  getRemote(blockId)
}

def getRemote[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. ä»Masterè·å–blockä½ç½®
  val locations = master.getLocations(blockId)
  
  // 2. ä»è¿œç¨‹èŠ‚ç‚¹è·å–
  for (location <- locations) {
    val blockResult = blockTransferService.fetchBlockSync(
      location.host, location.port, location.executorId, blockId.toString)
    if (blockResult.isDefined) {
      return blockResult
    }
  }
  
  None
}
```

#### å†…å­˜æ¨¡å‹

**Sparkå†…å­˜åˆ†åŒºæ¶æ„**ï¼š

```mermaid
graph TD
    A[JVMå†…å­˜] --> B[å †å†…å†…å­˜<br/>On-Heap]
    A --> C[å †å¤–å†…å­˜<br/>Off-Heap]
    
    B --> D[å­˜å‚¨å†…å­˜<br/>Storage Memory]
    B --> E[æ‰§è¡Œå†…å­˜<br/>Execution Memory]
    B --> F[å…¶ä»–å†…å­˜<br/>Other Memory]
    
    D --> G[RDDç¼“å­˜<br/>å¹¿æ’­å˜é‡]
    E --> H[Shuffle<br/>Joinèšåˆ]
    F --> I[ç”¨æˆ·æ•°æ®ç»“æ„<br/>Sparkå†…éƒ¨å¯¹è±¡]
    
    C --> J[å †å¤–å­˜å‚¨<br/>Off-Heap Storage]
    C --> K[å †å¤–æ‰§è¡Œ<br/>Off-Heap Execution]
    
    style B fill:#e8f5e8
    style C fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#ffebee
```

**1. å†…å­˜ç®¡ç†æ¶æ„ç»„ä»¶**


| ç»„ä»¶                    | ç±»å                   | ä¸»è¦èŒè´£       | ç®¡ç†èŒƒå›´      |
| ------------------------- | ------------------------ | ---------------- | --------------- |
| **MemoryManager**       | `UnifiedMemoryManager` | ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨ | å †å†…+å †å¤–å†…å­˜ |
| **StorageMemoryPool**   | `StorageMemoryPool`    | å­˜å‚¨å†…å­˜æ±      | ç¼“å­˜æ•°æ®å†…å­˜  |
| **ExecutionMemoryPool** | `ExecutionMemoryPool`  | æ‰§è¡Œå†…å­˜æ±      | ä»»åŠ¡æ‰§è¡Œå†…å­˜  |
| **MemoryStore**         | `MemoryStore`          | å†…å­˜å­˜å‚¨ç®¡ç†   | ç¼“å­˜æ•°æ®å­˜å‚¨  |
| **TaskMemoryManager**   | `TaskMemoryManager`    | ä»»åŠ¡å†…å­˜ç®¡ç†   | å•ä¸ªä»»åŠ¡å†…å­˜  |

**2. MemoryStoreç¼“å­˜ç®¡ç†**

```scala
// MemoryStoreæ ¸å¿ƒå®ç°
class MemoryStore(
    conf: SparkConf,
    blockInfoManager: BlockInfoManager)
  extends BlockStore(BlockStore.MEMORY) with BlockEvictionHandler with Logging {

  // å†…å­˜æ˜ å°„è¡¨
  private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]](32, 0.75f, true)
  
  // å½“å‰å†…å­˜ä½¿ç”¨é‡
  private var _currentMemory = 0L
  
  def putBytes[T](
      blockId: BlockId,
      size: Long,
      memoryMode: MemoryMode,
      _bytes: () => ChunkedByteBuffer): Boolean = {
    
    // 1. æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
    if (!memoryManager.acquireStorageMemory(blockId, size, memoryMode)) {
      return false
    }
    
    // 2. åˆ†é…å†…å­˜å¹¶å­˜å‚¨æ•°æ®
    val bytes = _bytes()
    val entry = new SerializedMemoryEntry[T](bytes, memoryMode, implicitly[ClassTag[T]])
    entries.synchronized {
      entries.put(blockId, entry)
      _currentMemory += size
    }
    
    true
  }
  
  def get[T](blockId: BlockId): Option[BlockResult[T]] = {
    entries.synchronized {
      entries.get(blockId) match {
        case entry: SerializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case entry: DeserializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case _ => None
      }
    }
  }
}
```

**3. TaskMemoryManagerä»»åŠ¡å†…å­˜ç®¡ç†**

```scala
// TaskMemoryManageræ ¸å¿ƒå®ç°
class TaskMemoryManager(
    memoryManager: MemoryManager,
    taskAttemptId: Long)
  extends MemoryManager with Logging {

  // ä»»åŠ¡å†…å­˜æ˜ å°„è¡¨
  private val memoryForTask = new mutable.HashMap[Long, Long]()
  
  // å†…å­˜åˆ†é…æ–¹æ³•
  def acquireExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Long = {
    
    // 1. å°è¯•ä»æ‰§è¡Œå†…å­˜æ± åˆ†é…
    val acquired = memoryManager.acquireExecutionMemory(numBytes, taskAttemptId, memoryMode)
    
    // 2. è®°å½•åˆ†é…çš„å†…å­˜
    if (acquired > 0) {
      memoryForTask.synchronized {
        memoryForTask(taskAttemptId) = memoryForTask.getOrElse(taskAttemptId, 0L) + acquired
      }
    }
    
    acquired
  }
  
  // é‡Šæ”¾å†…å­˜
  def releaseExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Unit = {
    
    memoryManager.releaseExecutionMemory(numBytes, taskAttemptId, memoryMode)
    
    memoryForTask.synchronized {
      val current = memoryForTask.getOrElse(taskAttemptId, 0L)
      val newTotal = math.max(0L, current - numBytes)
      if (newTotal == 0) {
        memoryForTask.remove(taskAttemptId)
      } else {
        memoryForTask(taskAttemptId) = newTotal
      }
    }
  }
}
```

#### å†…å­˜åˆ†é…ç­–ç•¥

**ç»Ÿä¸€å†…å­˜ç®¡ç†**ï¼š

```scala
class UnifiedMemoryManager(
    conf: SparkConf,
    val maxHeapMemory: Long,
    onHeapStorageRegionSize: Long,
    numCores: Int)
  extends MemoryManager(conf, numCores, onHeapStorageRegionSize, maxHeapMemory) {

  // å†…å­˜æ± é…ç½®
  private val maxPoolSize = maxHeapMemory - reservedMemory
  private val poolSize = maxPoolSize * memoryFraction
  
  // åŠ¨æ€å†…å­˜åˆ†é…
  override def acquireStorageMemory(
      blockId: BlockId,
      numBytes: Long,
      memoryMode: MemoryMode): Boolean = synchronized {
    
    val (executionPool, storagePool, maxMemory) = memoryMode match {
      case MemoryMode.ON_HEAP => (
        onHeapExecutionMemoryPool,
        onHeapStorageMemoryPool,
        maxOnHeapStorageMemory)
      case MemoryMode.OFF_HEAP => (
        offHeapExecutionMemoryPool,
        offHeapStorageMemoryPool,
        maxOffHeapStorageMemory)
    }
    
    if (numBytes > maxMemory) {
      return false
    }
    
    if (numBytes > storagePool.memoryFree) {
      // å°è¯•ä»æ‰§è¡Œå†…å­˜æ± å€Ÿç”¨
      val memoryBorrowedFromExecution = math.min(
        executionPool.memoryFree, 
        numBytes - storagePool.memoryFree)
      
      executionPool.decrementPoolSize(memoryBorrowedFromExecution)
      storagePool.incrementPoolSize(memoryBorrowedFromExecution)
    }
    
    storagePool.acquireMemory(blockId, numBytes)
  }
}
```

### ShuffleåŸç†  â­â­â­

**Shuffle** æ˜¯Sparkä¸­æ•°æ®é‡æ–°åˆ†å¸ƒçš„è¿‡ç¨‹ï¼Œå‘ç”Ÿåœ¨éœ€è¦è·¨åˆ†åŒºè¿›è¡Œæ•°æ®äº¤æ¢çš„æ“ä½œä¸­ã€‚

**è§¦å‘Shuffleçš„æ“ä½œ**ï¼š

```scala
// 1. èšåˆæ“ä½œ
val grouped = rdd.groupByKey()        // è§¦å‘Shuffle
val reduced = rdd.reduceByKey(_ + _)  // è§¦å‘Shuffle

// 2. è¿æ¥æ“ä½œ
val joined = rdd1.join(rdd2)          // è§¦å‘Shuffle

// 3. é‡åˆ†åŒºæ“ä½œ
val repartitioned = rdd.repartition(10)  // è§¦å‘Shuffle
```

**Shuffleç±»å‹å¯¹æ¯” ğŸ”¥**


| Shuffleç±»å‹       | ç‰¹ç‚¹                                  | ä¼˜ç¼ºç‚¹               |
| ------------------- | --------------------------------------- | ---------------------- |
| **Hash Shuffle**  | æ¯ä¸ªMap Taskä¸ºæ¯ä¸ªReduce Taskåˆ›å»ºæ–‡ä»¶ | æ–‡ä»¶æ•°è¿‡å¤šï¼Œå½±å“æ€§èƒ½ |
| **Sort Shuffle**  | æ¯ä¸ªMap Taskåˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼ŒæŒ‰åˆ†åŒºæ’åº  | å‡å°‘æ–‡ä»¶æ•°ï¼Œæé«˜æ€§èƒ½ |
| **Tungsten Sort** | ä½¿ç”¨å †å¤–å†…å­˜ï¼Œä¼˜åŒ–æ’åºæ€§èƒ½            | å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ       |

#### Shuffleå®ç°æœºåˆ¶

**Hash Shuffle**

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[File 1-1]
        A --> E[File 1-2]
        A --> F[File 1-3]
        
        B[Map Task 2] --> G[File 2-1]
        B --> H[File 2-2] 
        B --> I[File 2-3]
        
        C[Map Task 3] --> J[File 3-1]
        C --> K[File 3-2]
        C --> L[File 3-3]
    end
    
    subgraph "Reduceé˜¶æ®µ"
        D --> M[Reduce Task 1]
        G --> M
        J --> M
        
        E --> N[Reduce Task 2]
        H --> N
        K --> N
        
        F --> O[Reduce Task 3]
        I --> O
        L --> O
    end
    
    style A fill:#ffebee
    style B fill:#ffebee
    style C fill:#ffebee
```

**Hash Shuffleé—®é¢˜**ï¼š

- **æ–‡ä»¶æ•°çˆ†ç‚¸**ï¼šMä¸ªMap Task Ã— Nä¸ªReduce Task = MÃ—Nä¸ªæ–‡ä»¶
- **éšæœºI/O**ï¼šå¤§é‡å°æ–‡ä»¶å¯¼è‡´éšæœºI/O
- **å†…å­˜å‹åŠ›**ï¼šéœ€è¦ä¸ºæ¯ä¸ªæ–‡ä»¶ç»´æŠ¤ç¼“å†²åŒº

**Sort Shuffle**

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[æ’åºç¼“å†²åŒº]
        D --> E[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        E --> F[ç´¢å¼•æ–‡ä»¶]
        
        B[Map Task 2] --> G[æ’åºç¼“å†²åŒº]
        G --> H[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        H --> I[ç´¢å¼•æ–‡ä»¶]
    end
    
    subgraph "Reduceé˜¶æ®µ"
        F --> J[Reduce Task 1]
        I --> J
        
        F --> K[Reduce Task 2]
        I --> K
    end
    
    style D fill:#fff3e0
    style G fill:#fff3e0
    style E fill:#e8f5e8
    style H fill:#e8f5e8
```

**Sort Shuffleä¼˜åŠ¿**ï¼š

- **æ–‡ä»¶æ•°å‡å°‘**ï¼šæ¯ä¸ªMap Taskåªäº§ç”Ÿä¸€ä¸ªæ•°æ®æ–‡ä»¶å’Œä¸€ä¸ªç´¢å¼•æ–‡ä»¶
- **é¡ºåºI/O**ï¼šæ•°æ®æŒ‰åˆ†åŒºIDæ’åºå†™å…¥ï¼Œæé«˜I/Oæ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨å¤–éƒ¨æ’åºï¼Œæ”¯æŒspillåˆ°ç£ç›˜

**Tungsten Sort Shuffle**

**Tungstenä¼˜åŒ–**ï¼š

- **å †å¤–å†…å­˜ç®¡ç†**ï¼šå‡å°‘GCå‹åŠ›
- **ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„**ï¼šæé«˜CPUç¼“å­˜å‘½ä¸­ç‡
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ç”Ÿæˆä¼˜åŒ–çš„å­—èŠ‚ç 

```scala
// Tungsten Sortå®ç°
class UnsafeShuffleWriter[K, V](
    blockManager: BlockManager,
    shuffleBlockResolver: IndexShuffleBlockResolver,
    taskMemoryManager: TaskMemoryManager,
    handle: SerializedShuffleHandle[K, V],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val partitioner = handle.dependency.partitioner
  private val numPartitions = partitioner.numPartitions
  private var sorter: UnsafeShuffleExternalSorter = _
  
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    // ä½¿ç”¨Tungstenå†…å­˜ç®¡ç†
    val taskContext = context.asInstanceOf[TaskContextImpl]
    sorter = UnsafeShuffleExternalSorter.create(
      taskContext.taskMemoryManager(),
      blockManager,
      context,
      numPartitions,
      shouldCompress = true)

    // åºåˆ—åŒ–å¹¶æ’å…¥è®°å½•
    while (records.hasNext) {
      insertRecordIntoSorter(records.next())
    }
    
    // å†™å‡ºæ’åºç»“æœ
    val outputFile = shuffleBlockResolver.getDataFile(handle.shuffleId, mapId)
    val partitionLengths = sorter.closeAndGetSpills.map(_.file)
      .foldLeft(Array.fill[Long](numPartitions)(0)) { (lengths, file) =>
        // åˆå¹¶spillæ–‡ä»¶
        mergeSpillsWithTransferTo(file, outputFile, lengths)
      }
    
    shuffleBlockResolver.writeIndexFileAndCommit(handle.shuffleId, mapId, partitionLengths, outputFile)
  }
}

```

**Hash Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant HashWriter
    participant FileSystem
    participant ReduceTask
    participant HashReader
    
    MapTask->>HashWriter: å†™å…¥è®°å½•
    HashWriter->>FileSystem: ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºæ–‡ä»¶
    Note over FileSystem: MÃ—Nä¸ªæ–‡ä»¶åˆ›å»º
    HashWriter->>FileSystem: å†™å…¥æ•°æ®åˆ°å¯¹åº”æ–‡ä»¶
    
    ReduceTask->>HashReader: å¼€å§‹è¯»å–
    HashReader->>FileSystem: è¯»å–ç›¸å…³åˆ†åŒºæ–‡ä»¶
    FileSystem-->>HashReader: è¿”å›æ•°æ®
    HashReader-->>ReduceTask: èšåˆåæ•°æ®
```

**Sort Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant SortWriter
    participant ExternalSorter
    participant FileSystem
    participant ReduceTask
    participant SortReader
    
    MapTask->>SortWriter: å†™å…¥è®°å½•
    SortWriter->>ExternalSorter: ç¼“å­˜å¹¶æ’åº
    ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
    ExternalSorter->>FileSystem: å†™å…¥å•ä¸ªæ•°æ®æ–‡ä»¶
    SortWriter->>FileSystem: å†™å…¥ç´¢å¼•æ–‡ä»¶
    
    ReduceTask->>SortReader: å¼€å§‹è¯»å–
    SortReader->>FileSystem: æ ¹æ®ç´¢å¼•è¯»å–æ•°æ®
    FileSystem-->>SortReader: è¿”å›åˆ†åŒºæ•°æ®
    SortReader-->>ReduceTask: èšåˆåæ•°æ®
```

#### Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. ShuffleManageræ¶æ„ç»„ä»¶**


| ç»„ä»¶                   | ç±»å                                       | ä¸»è¦èŒè´£           | é€‚ç”¨åœºæ™¯         |
| ------------------------ | -------------------------------------------- | -------------------- | ------------------ |
| **SortShuffleManager** | `SortShuffleManager`                       | Sort Shuffleç®¡ç†å™¨ | é»˜è®¤Shuffleæ–¹å¼  |
| **HashShuffleManager** | `HashShuffleManager`                       | Hash Shuffleç®¡ç†å™¨ | å·²åºŸå¼ƒ           |
| **ShuffleWriter**      | `SortShuffleWriter`, `UnsafeShuffleWriter` | Shuffleå†™å…¥å™¨      | Mapç«¯æ•°æ®å†™å…¥    |
| **ShuffleReader**      | `BlockStoreShuffleReader`                  | Shuffleè¯»å–å™¨      | Reduceç«¯æ•°æ®è¯»å– |

**2. ShuffleWriteræ ¸å¿ƒå®ç°**

```scala
// Sort Shuffleå®ç°æ ¸å¿ƒ
class SortShuffleWriter[K, V, C](
    shuffleBlockResolver: IndexShuffleBlockResolver,
    handle: BaseShuffleHandle[K, V, C],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val dep = handle.dependency
  private val blockManager = SparkEnv.get.blockManager
  private val sorter: ExternalSorter[K, V, _] = {
    if (dep.mapSideCombine) {
      new ExternalSorter[K, V, C](
        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    } else {
      new ExternalSorter[K, V, V](
        context, aggregator = None, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    }
  }

  // å†™å…¥æ•°æ®
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    sorter.insertAll(records)
    
    // è·å–è¾“å‡ºæ–‡ä»¶
    val outputFile = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
    
    // å†™å…¥æ’åºåçš„æ•°æ®
    val partitionLengths = sorter.writePartitionedFile(blockId, outputFile)
    
    // å†™å…¥ç´¢å¼•æ–‡ä»¶
    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, outputFile)
  }
}
```

**3. ExternalSorterå†…å­˜ç®¡ç†**

```scala
// ExternalSorteræ ¸å¿ƒå®ç°
class ExternalSorter[K, V, C](
    context: TaskContext,
    aggregator: Option[Aggregator[K, V, C]] = None,
    partitioner: Option[Partitioner] = None,
    ordering: Option[Ordering[K]] = None,
    serializer: Serializer = SparkEnv.get.serializer)
  extends Spillable[WritablePartitionedPairCollection[K, C]](context.taskMemoryManager())
  with Logging {

  // å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
  private var map = new PartitionedAppendOnlyMap[K, C]
  private val buffer = new PartitionedPairBuffer[K, C]

  // æ’å…¥æ•°æ®
  def insertAll(records: Iterator[Product2[K, V]]): Unit = {
    val shouldCombine = aggregator.isDefined
    
    if (shouldCombine) {
      // éœ€è¦èšåˆçš„æƒ…å†µ
      val mergeValue = aggregator.get.mergeValue
      val createCombiner = aggregator.get.createCombiner
      var kv: Product2[K, V] = null
      
      val update = (hadValue: Boolean, oldValue: C) => {
        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
      }
      
      while (records.hasNext) {
        addElementsRead()
        kv = records.next()
        map.changeValue((getPartition(kv._1), kv._1), update)
        maybeSpillCollection(usingMap = true)
      }
    } else {
      // ä¸éœ€è¦èšåˆçš„æƒ…å†µ
      while (records.hasNext) {
        addElementsRead()
        val kv = records.next()
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)
      }
    }
  }

  // Spillåˆ°ç£ç›˜
  override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): SpilledFile = {
    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)
    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)
    collection.clear()
    spillFile
  }
}
```

**4. ShuffleReaderæ•°æ®è¯»å–**

```scala
// ShuffleReaderæ ¸å¿ƒå®ç°
class BlockStoreShuffleReader[K, C](
    handle: BaseShuffleHandle[K, _, C],
    startPartition: Int,
    endPartition: Int,
    context: TaskContext,
    serializerManager: SerializerManager = SparkEnv.get.serializerManager,
    blockManager: BlockManager = SparkEnv.get.blockManager,
    mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker)
  extends ShuffleReader[K, C] with Logging {

  private val dep = handle.dependency

  override def read(): Iterator[Product2[K, C]] = {
    // 1. è·å–Shuffleæ•°æ®å—ä½ç½®
    val blocksByAddress = mapOutputTracker.getMapSizesByExecutorId(
      handle.shuffleId, startPartition, endPartition)
    
    // 2. è¯»å–æ•°æ®å—
    val blockFetcherItr = new ShuffleBlockFetcherIterator(
      context,
      blockManager.blockTransferService,
      blockManager,
      blocksByAddress,
      serializerManager.wrapStream(blockId, _),
      // æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨serializerManageræ¥è·å–å‹ç¼©å’ŒåŠ å¯†åŒ…è£…å™¨
      maxBytesInFlight = SparkEnv.get.conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024,
      maxReqsInFlight = SparkEnv.get.conf.getInt("spark.reducer.maxReqsInFlight", Int.MaxValue),
      maxBlocksInFlightPerAddress = SparkEnv.get.conf.getInt(
        "spark.reducer.maxBlocksInFlightPerAddress", Int.MaxValue),
      maxReqSizeShuffleToMem = SparkEnv.get.conf.getSizeAsBytes(
        "spark.reducer.maxReqSizeShuffleToMem", Long.MaxValue),
      detectCorrupt = SparkEnv.get.conf.getBoolean("spark.shuffle.detectCorrupt", true))

    // 3. ååºåˆ—åŒ–å¹¶èšåˆ
    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
      if (dep.mapSideCombine) {
        // Mapç«¯å·²ç»èšåˆï¼ŒReduceç«¯ç»§ç»­èšåˆ
        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
      } else {
        // Mapç«¯æœªèšåˆï¼ŒReduceç«¯è¿›è¡Œèšåˆ
        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, V)]]
        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
      }
    } else {
      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
    }

    // 4. æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
    dep.keyOrdering match {
      case Some(keyOrd: Ordering[K]) =>
        // åˆ›å»ºExternalSorterè¿›è¡Œæ’åº
        val sorter = new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
        sorter.insertAll(aggregatedIter)
        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
      case None =>
        aggregatedIter
    }
  }
}
```

**5. ShuffleBlockResolveræ–‡ä»¶ç®¡ç†**

```scala
// ShuffleBlockResolveræ ¸å¿ƒå®ç°
class IndexShuffleBlockResolver(conf: SparkConf, _blockManager: BlockManager = null)
  extends ShuffleBlockResolver with Logging {

  // è·å–æ•°æ®æ–‡ä»¶
  def getDataFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.data")
  }
  
  // è·å–ç´¢å¼•æ–‡ä»¶
  def getIndexFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.index")
  }
  
  // å†™å…¥ç´¢å¼•æ–‡ä»¶å¹¶æäº¤
  def writeIndexFileAndCommit(
      shuffleId: Int,
      mapId: Long,
      lengths: Array[Long],
      dataTmp: File): Unit = {
    
    val indexFile = getIndexFile(shuffleId, mapId)
    val indexTmp = new File(indexFile.getAbsolutePath + ".tmp")
    
    try {
      val out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexTmp)))
      Utils.tryWithSafeFinally {
        // å†™å…¥åç§»é‡
        var offset = 0L
        out.writeLong(offset)
        for (length <- lengths) {
          offset += length
          out.writeLong(offset)
        }
      } {
        out.close()
      }
      
      // åŸå­æ€§é‡å‘½å
      val dataFile = getDataFile(shuffleId, mapId)
      if (dataTmp.exists() && !dataTmp.renameTo(dataFile)) {
        throw new IOException("Failed to rename data file")
      }
      if (!indexTmp.renameTo(indexFile)) {
        throw new IOException("Failed to rename index file")
      }
    } catch {
      case e: Exception =>
        indexTmp.delete()
        throw e
    }
  }
}
```

**6. Shuffleæ•°æ®æµç»„ä»¶äº¤äº’**

```mermaid
sequenceDiagram
  participant MapTask
  participant ExternalSorter
  participant ShuffleWriter
  participant ShuffleBlockResolver
  participant Disk
  participant ReduceTask
  participant ShuffleReader
  participant BlockManager

  MapTask->>ExternalSorter: insertAll(records)
  ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
  ExternalSorter->>ShuffleWriter: writePartitionedFile()
  ShuffleWriter->>ShuffleBlockResolver: writeIndexFileAndCommit()
  ShuffleBlockResolver->>Disk: å†™å…¥data/indexæ–‡ä»¶
  
  ReduceTask->>ShuffleReader: read()
  ShuffleReader->>BlockManager: è·å–blockä½ç½®
  BlockManager->>Disk: è¯»å–Shuffleæ–‡ä»¶
  Disk-->>ShuffleReader: è¿”å›æ•°æ®
  ShuffleReader->>ReduceTask: èšåˆ/æ’åºç»“æœ
```

**7. Shuffleæ€§èƒ½ç›‘æ§ç»„ä»¶**

```scala
// Shuffleæ€§èƒ½æŒ‡æ ‡æ”¶é›†
class ShuffleWriteMetrics extends TaskMetrics {
  // å†™å…¥å­—èŠ‚æ•°
  private var _bytesWritten: Long = 0L
  // å†™å…¥è®°å½•æ•°
  private var _recordsWritten: Long = 0L
  // å†™å…¥æ—¶é—´
  private var _writeTime: Long = 0L
  
  def bytesWritten: Long = _bytesWritten
  def recordsWritten: Long = _recordsWritten
  def writeTime: Long = _writeTime
}

class ShuffleReadMetrics extends TaskMetrics {
  // è¯»å–å­—èŠ‚æ•°
  private var _bytesRead: Long = 0L
  // è¯»å–è®°å½•æ•°
  private var _recordsRead: Long = 0L
  // è¯»å–æ—¶é—´
  private var _readTime: Long = 0L
  // è¿œç¨‹è¯»å–å­—èŠ‚æ•°
  private var _remoteBytesRead: Long = 0L
  
  def bytesRead: Long = _bytesRead
  def recordsRead: Long = _recordsRead
  def readTime: Long = _readTime
  def remoteBytesRead: Long = _remoteBytesRead
}
```

#### Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜

**ä¸»è¦ä¼˜åŒ–ç­–ç•¥**ï¼š

- **å‹ç¼©**ï¼š`spark.shuffle.compress`ï¼Œå‡å°‘ç½‘ç»œä¼ è¾“é‡
- **åˆç†è®¾ç½®åˆ†åŒºæ•°**ï¼š`spark.sql.shuffle.partitions`ï¼Œé¿å…åˆ†åŒºè¿‡å¤šæˆ–è¿‡å°‘
- **ä½¿ç”¨æœ¬åœ°åŒ–Shuffle**ï¼šå‡å°‘ç½‘ç»œI/O
- **å¯ç”¨spillæœºåˆ¶**ï¼šå†…å­˜ä¸è¶³æ—¶æº¢å†™ç£ç›˜ï¼Œé˜²æ­¢OOM
- **èšåˆç¼“å†²åŒº**ï¼šMapç«¯æœ¬åœ°èšåˆï¼Œå‡å°‘ä¼ è¾“æ•°æ®é‡

**1. åˆ†åŒºä¼˜åŒ–ç­–ç•¥**

```properties
# æ¨èè®¾ç½®ï¼ˆæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
spark.sql.shuffle.partitions=200
spark.default.parallelism=200

# åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
```

**2. åŠ¨æ€èµ„æºåˆ†é…**

```properties
# å¯ç”¨åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.initialExecutors=2

# èµ„æºåˆ†é…ç­–ç•¥
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=120s
```

**3. å‹ç¼©ä¸åºåˆ—åŒ–ä¼˜åŒ–**


| é…ç½®é¡¹                            | æ¨èå€¼           | è¯´æ˜            |
| ----------------------------------- | ------------------ | ----------------- |
| `spark.shuffle.compress`          | `true`           | å¯ç”¨Shuffleå‹ç¼© |
| `spark.shuffle.compress.codec`    | `snappy`         | å‹ç¼©ç®—æ³•é€‰æ‹©    |
| `spark.serializer`                | `KryoSerializer` | åºåˆ—åŒ–å™¨é€‰æ‹©    |
| `spark.kryo.registrationRequired` | `false`          | æ˜¯å¦è¦æ±‚æ³¨å†Œç±»  |

**4. æœ¬åœ°åŒ–Shuffleä¼˜åŒ–**

```properties
# æœ¬åœ°åŒ–é…ç½®
spark.locality.wait=3s
spark.locality.wait.process=3s
spark.locality.wait.node=3s
spark.locality.wait.rack=3s
```

**5. é«˜çº§ä¼˜åŒ–æŠ€å·§**

**Mapç«¯èšåˆ**ï¼š

```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val result = rdd.reduceByKey(_ + _)  // æ¨è
// val result = rdd.groupByKey().mapValues(_.sum)  // ä¸æ¨è
```

**å¹¿æ’­å˜é‡ä¼˜åŒ–**ï¼š

```scala
// å°è¡¨å¹¿æ’­ï¼Œé¿å…Shuffle
val smallTable = spark.table("small_table").collect()
val broadcastVar = spark.sparkContext.broadcast(smallTable)
```

#### Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**1. æ•°æ®å€¾æ–œé—®é¢˜**

**ç°è±¡**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´Taskæ‰§è¡Œæ—¶é—´å·®å¼‚å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š

```scala
// æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†
val skewedRDD = rdd.map(x => {
  val key = x._1
  val value = x._2
  if (isSkewedKey(key)) {
    (key + "_" + Random.nextInt(10), value)
  } else {
    (key, value)
  }
})

// æ–¹æ¡ˆ2ï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨
class SkewPartitioner(numPartitions: Int) extends Partitioner {
  override def numPartitions: Int = numPartitions
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val rawKey = key.toString.split("_")(0)
    math.abs(rawKey.hashCode) % numPartitions
  }
}
```

**2. Shuffleæ–‡ä»¶è¿‡å¤šé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­äº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼Œå½±å“æ€§èƒ½

**è§£å†³æ–¹æ¡ˆ**ï¼š

```properties
# åˆå¹¶å°æ–‡ä»¶
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200
```

**3. å†…å­˜æº¢å‡ºé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­å‡ºç°OOM

**è§£å†³æ–¹æ¡ˆ**ï¼š

```properties
# å¯ç”¨Spillæœºåˆ¶
spark.shuffle.spill=true
spark.shuffle.spill.compress=true

# è°ƒæ•´å†…å­˜é…ç½®
spark.executor.memory=4g
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3
```

---

## Spark SQLä¸Catalyst â­â­

### Spark SQLæ¦‚è¿°

**Spark SQL** æ˜¯Sparkç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®çš„æ¨¡å—ï¼Œæä¾›äº†DataFrameå’ŒDataset APIã€‚

#### ä¸»è¦ç‰¹æ€§

- **ç»Ÿä¸€æ•°æ®è®¿é—®**ï¼šæ”¯æŒå¤šç§æ•°æ®æº
- **Hiveå…¼å®¹æ€§**ï¼šå®Œå…¨å…¼å®¹Hive SQL
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šCatalystä¼˜åŒ–å™¨
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ä»£ç ç”Ÿæˆ

#### ä½¿ç”¨æ–¹å¼

```scala
// åˆ›å»ºSparkSession
val spark = SparkSession.builder()
  .appName("SparkSQLExample")
  .config("spark.sql.adaptive.enabled", "true")
  .getOrCreate()

// è¯»å–æ•°æ®
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// SQLæŸ¥è¯¢
df.createOrReplaceTempView("people")
val result = spark.sql("""
  SELECT age, count(*) as count
  FROM people 
  WHERE age > 21
  GROUP BY age
  ORDER BY age
""")

// DataFrame API
val result2 = df
  .filter($"age" > 21)
  .groupBy("age")
  .count()
  .orderBy("age")
```

### Catalystä¼˜åŒ–å™¨ ğŸ”¥

**Catalyst** æ˜¯Spark SQLçš„æŸ¥è¯¢ä¼˜åŒ–æ¡†æ¶ï¼ŒåŸºäºScalaçš„å‡½æ•°å¼ç¼–ç¨‹æ„å»ºã€‚

#### ä¼˜åŒ–æµç¨‹

```mermaid
graph LR
    A[SQL/DataFrame] --> B[è§£æå™¨<br/>Parser]
    B --> C[é€»è¾‘è®¡åˆ’<br/>Logical Plan]
    C --> D[ä¼˜åŒ–å™¨<br/>Optimizer]
    D --> E[ç‰©ç†è®¡åˆ’<br/>Physical Plan]
    E --> F[ä»£ç ç”Ÿæˆ<br/>CodeGen]
    F --> G[æ‰§è¡Œ<br/>Execution]
    
    style B fill:#e8f5e8
    style D fill:#e1f5fe
    style F fill:#fff3e0
```

**ä¼˜åŒ–é˜¶æ®µ**ï¼š

1. **é€»è¾‘è®¡åˆ’ä¼˜åŒ–**ï¼šè°“è¯ä¸‹æ¨ã€æŠ•å½±ä¸‹æ¨ã€å¸¸é‡æŠ˜å 
2. **ç‰©ç†è®¡åˆ’ç”Ÿæˆ**ï¼šé€‰æ‹©æœ€ä¼˜çš„ç‰©ç†æ‰§è¡Œç­–ç•¥
3. **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆé«˜æ•ˆçš„Javaå­—èŠ‚ç 

#### ä¼˜åŒ–è§„åˆ™

**ä¸»è¦ä¼˜åŒ–è§„åˆ™**ï¼š

```scala
// è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT * FROM (SELECT * FROM table WHERE col1 > 10) WHERE col2 = 'value'
// ä¼˜åŒ–å  
SELECT * FROM table WHERE col1 > 10 AND col2 = 'value'

// æŠ•å½±ä¸‹æ¨ï¼ˆProjection Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT col1 FROM (SELECT col1, col2, col3 FROM table)
// ä¼˜åŒ–å
SELECT col1 FROM table

// å¸¸é‡æŠ˜å ï¼ˆConstant Foldingï¼‰
// ä¼˜åŒ–å‰
SELECT col1 + 1 + 2 FROM table
// ä¼˜åŒ–å
SELECT col1 + 3 FROM table
```

#### ä»£ç ç”Ÿæˆ

**Whole-Stage Code Generation**ï¼š

```scala
// ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
```

### SparkSQL å®ç”¨å‡½æ•°ä¸è¯­æ³•

#### æ—¥æœŸä¸æ—¶é—´å¤„ç†

```sql
-- è·å–å½“å‰æ—¶é—´æˆ³
SELECT current_timestamp() as current_time,
       unix_timestamp() as unix_timestamp;

-- æ—¶é—´æˆ³è½¬æ¢
SELECT from_unixtime(1640995200) as datetime,
       unix_timestamp('2022-01-01 00:00:00') as timestamp;

-- æ—¥æœŸåŠ å‡æ“ä½œ
SELECT date_add('2022-01-01', 7) as next_week,
       date_sub('2022-01-01', 7) as last_week,
       add_months('2022-01-01', 1) as next_month;

-- æ—¥æœŸå·®è®¡ç®—
SELECT datediff('2022-01-15', '2022-01-01') as days_diff,
       months_between('2022-03-01', '2022-01-01') as months_diff;

-- æ—¥æœŸæ ¼å¼åŒ–
SELECT date_format('2022-01-01', 'yyyy-MM-dd') as formatted_date,
       to_date('2022-01-01') as date_type;
```

#### å­—ç¬¦ä¸²å¤„ç†

```sql
-- å­—ç¬¦ä¸²è¿æ¥
SELECT concat('Hello', ' ', 'World') as greeting,
       concat_ws('-', '2022', '01', '01') as date_str;

-- å­—ç¬¦ä¸²æˆªå–
æ³¨æ„ç¬¬ä¸€ä½ç´¢å¼•æ˜¯1ï¼Œè€Œä¸æ˜¯0
SELECT substring('Hello World', 1, 5) as substring,
       substr('Hello World', 7) as substr_from_7;

-- å­—ç¬¦ä¸²æ›¿æ¢
SELECT replace('Hello World', 'World', 'Spark') as replaced,
       regexp_replace('Hello123World', '\\d+', '') as no_digits;

-- æ­£åˆ™è¡¨è¾¾å¼æå–
SELECT regexp_extract('Hello123World456', '(\\d+)', 1) as first_number,
       regexp_extract('email@example.com', '([^@]+)@([^@]+)', 1) as username;

-- å­—ç¬¦ä¸²åˆ†å‰²
SELECT split('a,b,c,d', ',') as split_array,
       size(split('a,b,c,d', ',')) as array_size;
```

#### æ•°ç»„ä¸é›†åˆæ“ä½œ

```sql
-- æ•°ç»„åˆ›å»º
SELECT array(1, 2, 3) as simple_array,
       array_contains(array(1, 2, 3), 2) as contains_2;

-- æ•°ç»„å±•å¼€ï¼ˆè¡Œè½¬åˆ—ï¼‰
SELECT explode(array(1, 2, 3)) as exploded_value;

-- æ•°ç»„èšåˆï¼ˆåˆ—è½¬è¡Œï¼‰
SELECT collect_list(column_name) as list_agg,
       collect_set(column_name) as set_agg;

-- æ•°ç»„äº¤é›†
SELECT arrays_overlap(array(1, 2, 3), array(2, 3, 4)) as has_overlap;

-- å¤æ‚æ•°ç»„æ“ä½œ
SELECT 
    id,
    explode(split(tags, ',')) as tag
FROM user_tags;
```

#### JSONå¤„ç†

```sql
-- JSONè§£æ
SELECT get_json_object('{"name": "John", "age": 30}', '$.name') as name,
       get_json_object('{"name": "John", "age": 30}', '$.age') as age;

-- JSONæ•°ç»„å¤„ç†
SELECT json_array_length('[1, 2, 3, 4]') as array_length;

-- è½¬æ¢ä¸ºJSON
SELECT to_json(struct('John' as name, 30 as age)) as json_string;

-- å¤æ‚JSONæ“ä½œ
SELECT 
    user_id,
    get_json_object(profile, '$.email') as email,
    get_json_object(profile, '$.address.city') as city
FROM user_profiles;
```

#### æ¡ä»¶ä¸åˆ¤æ–­

```sql
-- CASE WHENè¯­å¥
SELECT 
    name,
    age,
    CASE 
        WHEN age < 18 THEN 'æœªæˆå¹´'
        WHEN age < 30 THEN 'é’å¹´'
        WHEN age < 50 THEN 'ä¸­å¹´'
        ELSE 'è€å¹´'
    END as age_group
FROM users;

-- IFå‡½æ•°
SELECT 
    name,
    IF(age >= 18, 'æˆå¹´', 'æœªæˆå¹´') as adult_status,
    IFNULL(email, 'æ— é‚®ç®±') as email_info
FROM users;

-- COALESCEå‡½æ•°
SELECT 
    user_id,
    COALESCE(nickname, real_name, 'Unknown') as display_name
FROM user_info;
```

#### çª—å£å‡½æ•°

```sql
-- ROW_NUMBER() è¡Œå·
SELECT 
    name,
    salary,
    ROW_NUMBER() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- RANK() æ’åï¼ˆç›¸åŒå€¼ç›¸åŒæ’åï¼Œè·³è¿‡ï¼‰
SELECT 
    name,
    salary,
    RANK() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- DENSE_RANK() å¯†é›†æ’åï¼ˆç›¸åŒå€¼ç›¸åŒæ’åï¼Œä¸è·³è¿‡ï¼‰
SELECT 
    name,
    salary,
    DENSE_RANK() OVER (ORDER BY salary DESC) as rank
FROM employees;

-- LAG/LEAD å‰åå€¼
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) as prev_sales,
    LEAD(sales, 1) OVER (ORDER BY date) as next_sales
FROM daily_sales;

-- åˆ†åŒºçª—å£å‡½æ•°
SELECT 
    department,
    name,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank
FROM employees;
```

#### èšåˆå‡½æ•°

```sql
-- åŸºæœ¬èšåˆ
SELECT 
    department,
    COUNT(*) as employee_count,
    AVG(salary) as avg_salary,
    SUM(salary) as total_salary,
    MIN(salary) as min_salary,
    MAX(salary) as max_salary
FROM employees
GROUP BY department;

-- æ¡ä»¶èšåˆ
SELECT 
    department,
    COUNT(CASE WHEN gender = 'M' THEN 1 END) as male_count,
    COUNT(CASE WHEN gender = 'F' THEN 1 END) as female_count,
    AVG(CASE WHEN age > 30 THEN salary END) as senior_avg_salary
FROM employees
GROUP BY department;

-- å»é‡èšåˆ
SELECT 
    department,
    COUNT(DISTINCT employee_id) as unique_employees
FROM employees
GROUP BY department;
```

#### å®ç”¨æŸ¥è¯¢ç¤ºä¾‹

```sql
-- ç”¨æˆ·ç•™å­˜åˆ†æ
WITH user_activity AS (
    SELECT 
        user_id,
        date,
        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY date) as visit_order
    FROM user_visits
)
SELECT 
    first_visit.date as cohort_date,
    COUNT(DISTINCT first_visit.user_id) as cohort_size,
    COUNT(DISTINCT CASE WHEN second_visit.user_id IS NOT NULL THEN first_visit.user_id END) as retained_users,
    COUNT(DISTINCT CASE WHEN second_visit.user_id IS NOT NULL THEN first_visit.user_id END) / 
        COUNT(DISTINCT first_visit.user_id) as retention_rate
FROM user_activity first_visit
LEFT JOIN user_activity second_visit 
    ON first_visit.user_id = second_visit.user_id 
    AND second_visit.visit_order = 2
WHERE first_visit.visit_order = 1
GROUP BY first_visit.date;

-- æ¼æ–—åˆ†æ
SELECT 
    step_name,
    COUNT(DISTINCT user_id) as users,
    LAG(COUNT(DISTINCT user_id)) OVER (ORDER BY step_order) as prev_step_users,
    COUNT(DISTINCT user_id) / LAG(COUNT(DISTINCT user_id)) OVER (ORDER BY step_order) as conversion_rate
FROM funnel_events
GROUP BY step_name, step_order
ORDER BY step_order;

-- æ—¶é—´åºåˆ—åˆ†æ
SELECT 
    date,
    sales,
    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d,
    SUM(sales) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) as rolling_sum_30d
FROM daily_sales
ORDER BY date;

// åŸå§‹æŸ¥è¯¢ï¼šSELECT sum(x + y) FROM table WHERE z > 10
class GeneratedIterator extends Iterator[InternalRow] {
  private var sum: Long = 0L
  
  def processNext(): Unit = {
    while (input.hasNext) {
      val row = input.next()
      val z = row.getLong(2)
      if (z > 10) {  // è°“è¯è®¡ç®—
        val x = row.getLong(0)
        val y = row.getLong(1)
        sum += (x + y)  // èšåˆè®¡ç®—
      }
    }
    // è¿”å›æœ€ç»ˆç»“æœ
    result.setLong(0, sum)
  }
}
```

### æ•°æ®æºæ”¯æŒ

#### å†…ç½®æ•°æ®æº

**æ”¯æŒçš„æ•°æ®æ ¼å¼**ï¼š

- **Parquet**ï¼šåˆ—å¼å­˜å‚¨ï¼Œé«˜å‹ç¼©æ¯”
- **JSON**ï¼šåŠç»“æ„åŒ–æ•°æ®
- **CSV**ï¼šæ–‡æœ¬æ ¼å¼
- **ORC**ï¼šä¼˜åŒ–çš„è¡Œåˆ—å­˜å‚¨
- **Avro**ï¼šæ¨¡å¼æ¼”åŒ–æ”¯æŒ

```scala
// è¯»å–ä¸åŒæ ¼å¼æ•°æ®
val parquetDF = spark.read.parquet("path/to/data.parquet")
val jsonDF = spark.read.json("path/to/data.json")
val csvDF = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// å†™å…¥æ•°æ®
df.write
  .mode("overwrite")
  .option("compression", "snappy")
  .parquet("output/path")
```

#### å¤–éƒ¨æ•°æ®æº

**å¸¸ç”¨å¤–éƒ¨æ•°æ®æº**ï¼š

```scala
// JDBCæ•°æ®æº
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "username")
  .option("password", "password")
  .load()

// Kafkaæ•°æ®æº
val kafkaDF = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// Hiveè¡¨
val hiveDF = spark.sql("SELECT * FROM hive_table")
```

---

---

## æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ– â­â­â­

### æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–

**å­˜å‚¨æ ¼å¼ä¼˜åŒ–**

**æ¨èå­˜å‚¨æ ¼å¼å¯¹æ¯”**ï¼š


| æ ¼å¼        | å‹ç¼©æ¯” | æŸ¥è¯¢é€Ÿåº¦ | å†™å…¥é€Ÿåº¦ | é€‚ç”¨åœºæ™¯             |
| ------------- | -------- | ---------- | ---------- | ---------------------- |
| **Parquet** | é«˜     | å¿«       | ä¸­ç­‰     | åˆ†ææŸ¥è¯¢ï¼Œåˆ—å¼å­˜å‚¨   |
| **ORC**     | å¾ˆé«˜   | å¾ˆå¿«     | å¿«       | Hiveé›†æˆï¼Œé«˜å‹ç¼©æ¯”   |
| **Avro**    | ä¸­ç­‰   | ä¸­ç­‰     | å¿«       | è¡Œå¼å­˜å‚¨ï¼ŒSchemaæ¼”è¿› |
| **JSON**    | ä½     | æ…¢       | å¿«       | å¼€å‘è°ƒè¯•ï¼Œçµæ´»æ€§é«˜   |

**Parquetæ ¼å¼ä¼˜åŒ–**ï¼š

```scala
// æ¨èä½¿ç”¨Parquetæ ¼å¼
df.write.mode("overwrite").parquet("data.parquet")
val optimizedDF = spark.read.parquet("data.parquet")

// é…ç½®å‹ç¼©
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

// è®¾ç½®åˆ—å¼è¯»å–æ‰¹æ¬¡å¤§å°
spark.conf.set("spark.sql.parquet.columnarReaderBatchSize", "10000")

// å¯ç”¨å‘é‡åŒ–è¯»å–
spark.conf.set("spark.sql.parquet.enableVectorizedReader", "true")
```

**ORCæ ¼å¼ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨ORCæ ¼å¼
df.write.format("orc").mode("overwrite").save("data.orc")

// ORCä¼˜åŒ–é…ç½®
spark.conf.set("spark.sql.orc.compression.codec", "snappy")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.orc.enableVectorizedReader", "true")
```

**åˆ†åŒºç­–ç•¥ä¼˜åŒ–**

**æ—¶é—´åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æŒ‰æ—¶é—´åˆ†åŒºï¼ˆæ¨èï¼‰
df.write
  .partitionBy("year", "month", "day")
  .parquet("time_partitioned_data")

// é¿å…è¿‡åº¦åˆ†åŒº
val dailyData = df.withColumn("date", to_date($"timestamp"))
dailyData.write
  .partitionBy("date")
  .parquet("daily_partitioned_data")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "true")
```

**ä¸šåŠ¡åˆ†åŒºç­–ç•¥**ï¼š

```scala
// æŒ‰ä¸šåŠ¡å­—æ®µåˆ†åŒº
df.write
  .partitionBy("region", "category")
  .parquet("business_partitioned_data")

// åˆ†åŒºæ•°æ§åˆ¶
val numPartitions = spark.conf.get("spark.sql.shuffle.partitions", "200").toInt
val optimalPartitions = Math.max(100, Math.min(numPartitions, 1000))

df.repartition(optimalPartitions, $"partition_key")
  .write
  .partitionBy("partition_key")
  .parquet("optimized_data")
```

**åˆ†åŒºè£å‰ªä¼˜åŒ–**ï¼š

```scala
// å¯ç”¨åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

// æŸ¥è¯¢æ—¶ä½¿ç”¨åˆ†åŒºè¿‡æ»¤
val result = spark.read.parquet("partitioned_data")
  .filter($"year" === 2023 && $"month" >= 6)  // æœ‰æ•ˆåˆ†åŒºè£å‰ª
  .select("id", "name", "value")
```

**è°“è¯ä¸‹æ¨ä¼˜åŒ–**

**å¯ç”¨è°“è¯ä¸‹æ¨**ï¼š

```scala
// å¯ç”¨å„ç§æ•°æ®æºçš„è°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.json.filterPushdown", "true")

// JDBCè°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.pushDownPredicate", "true")
```

**ä¼˜åŒ–ç¤ºä¾‹**ï¼š

```scala
// åŸå§‹æŸ¥è¯¢ï¼ˆæœªä¼˜åŒ–ï¼‰
val df = spark.read.parquet("large_dataset.parquet")
val result = df.select("*").filter($"age" > 18 && $"city" === "Beijing")

// ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆè°“è¯ä¸‹æ¨ï¼‰
val result = spark.read.parquet("large_dataset.parquet")
  .filter($"age" > 18)  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .filter($"city" === "Beijing")  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .select("id", "name", "age")  // åˆ—è£å‰ª
```

**å¤æ‚è°“è¯ä¼˜åŒ–**ï¼š

```scala
// ç»„åˆæ¡ä»¶ä¼˜åŒ–
val complexFilter = ($"age".between(18, 65)) && 
                   ($"salary" > 50000) && 
                   ($"department".isin("IT", "Finance"))

val result = spark.read.parquet("employee_data.parquet")
  .filter(complexFilter)  // å¤æ‚è°“è¯ä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–å’Œä¸‹æ¨
  .select("id", "name", "salary")
```

### Joinä¼˜åŒ–

**Joinç­–ç•¥é€‰æ‹©**

**Joinç±»å‹å¯¹æ¯”**ï¼š


| Joinç±»å‹              | é€‚ç”¨åœºæ™¯     | ä¼˜åŠ¿                | åŠ£åŠ¿               | è§¦å‘æ¡ä»¶    |
| ----------------------- | -------------- | --------------------- | -------------------- | ------------- |
| **Broadcast Join**    | å°è¡¨Joinå¤§è¡¨ | æ— Shuffleï¼Œæ€§èƒ½æœ€å¥½ | å°è¡¨å¿…é¡»èƒ½æ”¾å…¥å†…å­˜ | å°è¡¨ < 10MB |
| **Sort Merge Join**   | å¤§è¡¨Joinå¤§è¡¨ | å†…å­˜å‹å¥½ï¼Œç¨³å®š      | éœ€è¦Shuffle        | é»˜è®¤Join    |
| **Shuffle Hash Join** | ä¸­ç­‰è¡¨Join   | å†…å­˜ä½¿ç”¨é€‚ä¸­        | éœ€è¦Shuffle        | ä¸­ç­‰æ•°æ®é‡  |
| **Cartesian Join**    | ç¬›å¡å°”ç§¯     | ç®€å•                | æ€§èƒ½æå·®           | æ— Joiné”®    |

**å¹¿æ’­Joinä¼˜åŒ–**

**è‡ªåŠ¨å¹¿æ’­é…ç½®**ï¼š

```scala
// è®¾ç½®è‡ªåŠ¨å¹¿æ’­é˜ˆå€¼
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

// å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
```

**æ‰‹åŠ¨å¹¿æ’­ä¼˜åŒ–**ï¼š

```scala
// æ‰‹åŠ¨å¹¿æ’­å°è¡¨
val smallTable = spark.table("small_table")
val broadcastDF = broadcast(smallTable)
val result = largeTable.join(broadcastDF, "id")

// å¼ºåˆ¶å¹¿æ’­ï¼ˆå³ä½¿è¶…è¿‡é˜ˆå€¼ï¼‰
val result = largeTable.join(broadcast(mediumTable), "id")

// å¹¿æ’­å˜é‡ä¼˜åŒ–
val lookupMap = smallTable.collect()
  .map(row => row.getAs[String]("key") -> row.getAs[String]("value"))
  .toMap
val broadcastMap = spark.sparkContext.broadcast(lookupMap)

val enrichedData = largeTable.map { row =>
  val key = row.getAs[String]("key")
  val enrichValue = broadcastMap.value.getOrElse(key, "unknown")
  (row, enrichValue)
}
```

**Sort Merge Joinä¼˜åŒ–**

**é¢„æ’åºä¼˜åŒ–**ï¼š

```scala
// é¢„å…ˆæ’åºå‡å°‘Shuffleæˆæœ¬
val sortedTable1 = table1.sort("join_key")
val sortedTable2 = table2.sort("join_key")
val result = sortedTable1.join(sortedTable2, "join_key")

// ä½¿ç”¨åˆ†æ¡¶è¡¨
table1.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table1")

table2.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table2")

// åˆ†æ¡¶è¡¨Joinï¼ˆæ— Shuffleï¼‰
val result = spark.table("bucketed_table1")
  .join(spark.table("bucketed_table2"), "join_key")
```

**æ•°æ®å€¾æ–œå¤„ç†**

**å€¾æ–œæ£€æµ‹**ï¼š

```scala
// æ£€æµ‹Joiné”®åˆ†å¸ƒ
val keyDistribution = largeTable
  .groupBy("join_key")
  .count()
  .orderBy($"count".desc)

keyDistribution.show(20)  // æŸ¥çœ‹top 20çš„é”®åˆ†å¸ƒ

// ç»Ÿè®¡åˆ†æ
val stats = keyDistribution.agg(
  avg("count").as("avg_count"),
  max("count").as("max_count"),
  min("count").as("min_count"),
  stddev("count").as("stddev_count")
)
stats.show()
```

**å€¾æ–œè§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†**ï¼š

```scala
// åŠ ç›Join
import scala.util.Random

// ç»™å°è¡¨åŠ ç›
val saltedSmallTable = smallTable.flatMap { row =>
  (0 until 10).map { salt =>
    Row.fromSeq(row.toSeq :+ salt)
  }
}

// ç»™å¤§è¡¨çš„å€¾æ–œé”®åŠ éšæœºç›
val saltedLargeTable = largeTable.map { row =>
  val key = row.getAs[String]("join_key")
  val salt = if (isSkewedKey(key)) Random.nextInt(10) else 0
  Row.fromSeq(row.toSeq :+ salt)
}

// æ‰§è¡ŒJoin
val result = saltedLargeTable.join(saltedSmallTable, 
  Seq("join_key", "salt_column"))
```

**æ–¹æ¡ˆ2ï¼šå€¾æ–œé”®å•ç‹¬å¤„ç†**ï¼š

```scala
// è¯†åˆ«å€¾æ–œé”®
val skewedKeys = Set("skewed_key_1", "skewed_key_2")

// åˆ†ç¦»å€¾æ–œæ•°æ®å’Œæ­£å¸¸æ•°æ®
val normalData = largeTable.filter(!$"join_key".isin(skewedKeys.toSeq:_*))
val skewedData = largeTable.filter($"join_key".isin(skewedKeys.toSeq:_*))

// æ­£å¸¸æ•°æ®æ­£å¸¸Join
val normalResult = normalData.join(smallTable, "join_key")

// å€¾æ–œæ•°æ®ä½¿ç”¨å¹¿æ’­Join
val skewedResult = skewedData.join(broadcast(smallTable), "join_key")

// åˆå¹¶ç»“æœ
val finalResult = normalResult.union(skewedResult)
```

**æ–¹æ¡ˆ3ï¼šä¸¤é˜¶æ®µèšåˆ**ï¼š

```scala
// é¢„èšåˆé˜¶æ®µ
val preAggregated = largeTable
  .withColumn("salt", (rand() * 10).cast("int"))
  .withColumn("salted_key", concat($"join_key", lit("_"), $"salt"))
  .groupBy("salted_key")
  .agg(sum("value").as("partial_sum"))

// æœ€ç»ˆèšåˆé˜¶æ®µ
val finalAggregated = preAggregated
  .withColumn("original_key", split($"salted_key", "_").getItem(0))
  .groupBy("original_key")
  .agg(sum("partial_sum").as("final_sum"))
```

### ç¼“å­˜ä¸æŒä¹…åŒ–

**å­˜å‚¨çº§åˆ«é€‰æ‹©**

**å­˜å‚¨çº§åˆ«å¯¹æ¯”**ï¼š


| å­˜å‚¨çº§åˆ«            | å†…å­˜ä½¿ç”¨ | ç£ç›˜ä½¿ç”¨ | åºåˆ—åŒ– | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯             |
| --------------------- | ---------- | ---------- | -------- | --------- | ---------------------- |
| **MEMORY_ONLY**     | é«˜       | æ—        | æ—      | ä½      | å°æ•°æ®é›†ï¼Œé¢‘ç¹è®¿é—®   |
| **MEMORY_AND_DISK** | ä¸­ç­‰     | æœ‰       | æ—      | ä½      | å¤§æ•°æ®é›†ï¼Œå†…å­˜ä¸è¶³æ—¶ |
| **MEMORY_ONLY_SER** | ä½       | æ—        | æœ‰     | é«˜      | å†…å­˜ç´§å¼ ï¼ŒCPUå……è¶³    |
| **DISK_ONLY**       | æ—        | é«˜       | æœ‰     | ä¸­ç­‰    | å¤§æ•°æ®é›†ï¼Œå†…å­˜ç´§å¼    |
| **OFF_HEAP**        | å †å¤–     | æ—        | æœ‰     | ä¸­ç­‰    | å‡å°‘GCå‹åŠ›           |

**ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**

**æ™ºèƒ½ç¼“å­˜ç­–ç•¥**ï¼š

```scala
// æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©å­˜å‚¨çº§åˆ«
def selectStorageLevel(dataSize: Long, memoryAvailable: Long): StorageLevel = {
  val ratio = dataSize.toDouble / memoryAvailable
  
  if (ratio < 0.3) {
    StorageLevel.MEMORY_ONLY  // å†…å­˜å……è¶³
  } else if (ratio < 0.8) {
    StorageLevel.MEMORY_ONLY_SER  // å†…å­˜ç´§å¼ ï¼Œåºåˆ—åŒ–èŠ‚çœç©ºé—´
  } else {
    StorageLevel.MEMORY_AND_DISK_SER  // å†…å­˜ä¸è¶³ï¼Œæº¢å‡ºåˆ°ç£ç›˜
  }
}

// åº”ç”¨ç¼“å­˜ç­–ç•¥
val dataSize = rdd.map(_.toString.length).sum()
val storageLevel = selectStorageLevel(dataSize, availableMemory)
rdd.persist(storageLevel)
```

**ç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

```scala
// ç¼“å­˜ç®¡ç†å™¨
class CacheManager {
  private val cachedRDDs = mutable.Map[String, RDD[_]]()
  
  def cache[T](name: String, rdd: RDD[T], level: StorageLevel = StorageLevel.MEMORY_AND_DISK): RDD[T] = {
    // æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
    if (!cachedRDDs.contains(name)) {
      rdd.persist(level)
      cachedRDDs(name) = rdd
      println(s"Cached RDD: $name")
    }
    rdd
  }
  
  def uncache(name: String): Unit = {
    cachedRDDs.get(name).foreach { rdd =>
      rdd.unpersist()
      cachedRDDs.remove(name)
      println(s"Uncached RDD: $name")
    }
  }
  
  def clearAll(): Unit = {
    cachedRDDs.values.foreach(_.unpersist())
    cachedRDDs.clear()
    println("Cleared all cached RDDs")
  }
}
```

**Checkpointä¼˜åŒ–**

**Checkpointç­–ç•¥**ï¼š

```scala
// è®¾ç½®checkpointç›®å½•
spark.sparkContext.setCheckpointDir("hdfs://namenode:8020/checkpoint")

// æ™ºèƒ½checkpoint
def smartCheckpoint[T](rdd: RDD[T], lineageDepth: Int = 10): RDD[T] = {
  // è®¡ç®—è¡€ç¼˜æ·±åº¦
  def getLineageDepth(rdd: RDD[_]): Int = {
    if (rdd.dependencies.isEmpty) {
      1
    } else {
      1 + rdd.dependencies.map(_.rdd).map(getLineageDepth).max
    }
  }
  
  val currentDepth = getLineageDepth(rdd)
  if (currentDepth > lineageDepth) {
    println(s"Checkpointing RDD with lineage depth: $currentDepth")
    rdd.checkpoint()
  }
  rdd
}

// ä½¿ç”¨ç¤ºä¾‹
val deepRDD = rdd1.map(transform1)
  .filter(filter1)
  .join(rdd2)
  .map(transform2)
  .filter(filter2)

val checkpointedRDD = smartCheckpoint(deepRDD)
```

### ä»£ç å±‚é¢ä¼˜åŒ–

**ç®—å­é€‰æ‹©ä¼˜åŒ–**

**é«˜æ•ˆç®—å­ä½¿ç”¨**ï¼š

```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val wordCounts = words.map(word => (word, 1))
  .reduceByKey(_ + _)  // æ¨èï¼šMapç«¯é¢„èšåˆ

// è€Œä¸æ˜¯
val wordCounts = words.map(word => (word, 1))
  .groupByKey()  // ä¸æ¨èï¼šæ‰€æœ‰æ•°æ®éƒ½è¦Shuffle
  .mapValues(_.sum)

// ä½¿ç”¨mapPartitionså‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
val result = rdd.mapPartitions { partition =>
  // åœ¨åˆ†åŒºçº§åˆ«åˆå§‹åŒ–èµ„æº
  val connection = createConnection()
  val buffer = new ArrayBuffer[ProcessedRecord]()
  
  try {
    partition.foreach { record =>
      buffer += processWithConnection(record, connection)
    }
    buffer.iterator
  } finally {
    connection.close()
  }
}

// ä½¿ç”¨aggregateByKeyè¿›è¡Œå¤æ‚èšåˆ
val result = rdd.aggregateByKey((0, 0))(
  // åˆ†åŒºå†…èšåˆ
  (acc, value) => (acc._1 + value, acc._2 + 1),
  // åˆ†åŒºé—´èšåˆ
  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
).mapValues { case (sum, count) => sum.toDouble / count }
```

**æ•°æ®ç»“æ„ä¼˜åŒ–**

**é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„**ï¼š

```scala
// ä½¿ç”¨åŸå§‹ç±»å‹æ•°ç»„æ›¿ä»£é›†åˆ
class OptimizedProcessor {
  // æ¨èï¼šåŸå§‹ç±»å‹æ•°ç»„
  private val intArray = new Array[Int](1000000)
  
  // ä¸æ¨èï¼šè£…ç®±ç±»å‹é›†åˆ
  private val intList = new ArrayBuffer[Integer]()
  
  // ä½¿ç”¨ä¸“ç”¨çš„æ•°æ®ç»“æ„
  def processLargeDataset(data: RDD[String]): RDD[String] = {
    data.mapPartitions { partition =>
      val bloom = new BloomFilter[String](1000000, 0.01)
      val deduped = new mutable.HashSet[String]()
    
      partition.filter { item =>
        if (bloom.mightContain(item)) {
          if (deduped.contains(item)) {
            false  // é‡å¤é¡¹
          } else {
            deduped += item
            true
          }
        } else {
          bloom.put(item)
          deduped += item
          true
        }
      }
    }
  }
}
```

**å†…å­˜ä½¿ç”¨ä¼˜åŒ–**

**å¯¹è±¡å¤ç”¨**ï¼š

```scala
// å¯¹è±¡æ± æ¨¡å¼
class ObjectPool[T](createFunc: () => T, resetFunc: T => Unit) {
  private val pool = new mutable.Queue[T]()
  
  def borrow(): T = {
    pool.synchronized {
      if (pool.nonEmpty) {
        pool.dequeue()
      } else {
        createFunc()
      }
    }
  }
  
  def return(obj: T): Unit = {
    resetFunc(obj)
    pool.synchronized {
      pool.enqueue(obj)
    }
  }
}

// ä½¿ç”¨å¯¹è±¡æ± 
val stringBuilderPool = new ObjectPool[StringBuilder](
  () => new StringBuilder(),
  _.clear()
)

val result = rdd.mapPartitions { partition =>
  partition.map { item =>
    val sb = stringBuilderPool.borrow()
    try {
      sb.append(item).append("_processed").toString
    } finally {
      stringBuilderPool.return(sb)
    }
  }
}
```

**å¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ä¼˜åŒ–**

**å¹¿æ’­å˜é‡æœ€ä½³å®è·µ**ï¼š

```scala
// å¤§æŸ¥æ‰¾è¡¨å¹¿æ’­
val lookupTableMap = smallTable.collect()
  .map(row => row.getString(0) -> row.getString(1))
  .toMap

val broadcastLookup = spark.sparkContext.broadcast(lookupTableMap)

val enrichedData = largeRDD.map { record =>
  val enrichValue = broadcastLookup.value.getOrElse(record.key, "unknown")
  EnrichedRecord(record, enrichValue)
}

// è®°ä½åŠæ—¶é‡Šæ”¾
broadcastLookup.destroy()
```

**ç´¯åŠ å™¨ä¼˜åŒ–**ï¼š

```scala
// è‡ªå®šä¹‰ç´¯åŠ å™¨
class HistogramAccumulator extends AccumulatorV2[Double, mutable.Map[String, Long]] {
  private val histogram = mutable.Map[String, Long]()
  
  override def isZero: Boolean = histogram.isEmpty
  
  override def copy(): HistogramAccumulator = {
    val newAcc = new HistogramAccumulator
    newAcc.histogram ++= this.histogram
    newAcc
  }
  
  override def reset(): Unit = histogram.clear()
  
  override def add(value: Double): Unit = {
    val bucket = getBucket(value)
    histogram(bucket) = histogram.getOrElse(bucket, 0L) + 1
  }
  
  override def merge(other: AccumulatorV2[Double, mutable.Map[String, Long]]): Unit = {
    other match {
      case o: HistogramAccumulator =>
        o.histogram.foreach { case (bucket, count) =>
          histogram(bucket) = histogram.getOrElse(bucket, 0L) + count
        }
    }
  }
  
  override def value: mutable.Map[String, Long] = histogram.toMap
  
  private def getBucket(value: Double): String = {
    if (value < 0) "negative"
    else if (value < 10) "0-10"
    else if (value < 100) "10-100"
    else "100+"
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰ç´¯åŠ å™¨
val histogramAcc = new HistogramAccumulator
spark.sparkContext.register(histogramAcc, "histogram")

rdd.foreach(value => histogramAcc.add(value))
println(s"Histogram: ${histogramAcc.value}")
```

### ç½‘ç»œä¸I/Oä¼˜åŒ–

**åºåˆ—åŒ–ä¼˜åŒ–**

**Kryoåºåˆ—åŒ–é…ç½®**ï¼š

```scala
// Kryoé…ç½®
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")
spark.conf.set("spark.kryo.unsafe", "true")

// æ³¨å†Œå¸¸ç”¨ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[scala.collection.mutable.WrappedArray.ofRef[_]],
  classOf[org.apache.spark.sql.types.StructType],
  classOf[Array[org.apache.spark.sql.types.StructField]]
))
```

**å‹ç¼©ä¼˜åŒ–**

**å‹ç¼©ç®—æ³•é€‰æ‹©**ï¼š

```scala
// æ ¹æ®åœºæ™¯é€‰æ‹©å‹ç¼©ç®—æ³•
spark.conf.set("spark.io.compression.codec", "snappy")  // å¹³è¡¡å‹ç¼©æ¯”å’Œé€Ÿåº¦
// spark.conf.set("spark.io.compression.codec", "lz4")     // æ›´å¿«çš„å‹ç¼©/è§£å‹
// spark.conf.set("spark.io.compression.codec", "gzip")    // æ›´é«˜çš„å‹ç¼©æ¯”

// å¯ç”¨å„ç§å‹ç¼©
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")
spark.conf.set("spark.shuffle.spill.compress", "true")
spark.conf.set("spark.rdd.compress", "true")
```

**ç½‘ç»œè°ƒä¼˜**

**ç½‘ç»œå‚æ•°ä¼˜åŒ–**ï¼š

```bash
# ç½‘ç»œè¶…æ—¶è®¾ç½®
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.rpc.askTimeout", "800s")
spark.conf.set("spark.rpc.lookupTimeout", "800s")

# ç½‘ç»œè¿æ¥ä¼˜åŒ–
spark.conf.set("spark.rpc.netty.dispatcher.numThreads", "8")
spark.conf.set("spark.shuffle.io.numConnectionsPerPeer", "3")
spark.conf.set("spark.shuffle.io.preferDirectBufs", "true")

# ä¼ è¾“ä¼˜åŒ–
spark.conf.set("spark.reducer.maxSizeInFlight", "96m")
spark.conf.set("spark.reducer.maxReqsInFlight", "Int.MaxValue")
```

### å¸¸è§æ€§èƒ½é—®é¢˜

**å†…å­˜æº¢å‡ºé—®é¢˜ ğŸ”¥**

**OOMé—®é¢˜è¯Šæ–­**ï¼š

```scala
// 1. Driver OOM
// åŸå› ï¼šcollect()æ“ä½œæ•°æ®é‡è¿‡å¤§
val result = largeRDD.collect()  // å±é™©æ“ä½œ

// è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨take()æˆ–åˆ†æ‰¹å¤„ç†
val sample = largeRDD.take(1000)
largeRDD.foreachPartition { partition =>
  // åˆ†åŒºå†…å¤„ç†ï¼Œé¿å…å°†æ‰€æœ‰æ•°æ®æ‹‰åˆ°Driver
}

// 2. Executor OOM  
// åŸå› ï¼šå•ä¸ªåˆ†åŒºæ•°æ®è¿‡å¤§
// è§£å†³æ–¹æ¡ˆï¼šå¢åŠ åˆ†åŒºæ•°
val repartitionedRDD = rdd.repartition(numPartitions * 2)
```

**æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ ğŸ”¥**

**å€¾æ–œæ£€æµ‹å’Œè§£å†³**ï¼š

```scala
// æ£€æµ‹å€¾æ–œ
def detectSkew[T](rdd: RDD[T]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val maxSize = partitionSizes.maxBy(_._2)
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  
  if (maxSize._2 > avgSize * 3) {
    println(s"æ•°æ®å€¾æ–œè­¦å‘Šï¼šåˆ†åŒº${maxSize._1}æœ‰${maxSize._2}æ¡è®°å½•ï¼Œå¹³å‡${avgSize}æ¡")
  }
}
```

### ç›‘æ§ä¸è¯Šæ–­

Spark UIç›‘æ§

**å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š

- **Jobsé¡µé¢**ï¼šä½œä¸šæ‰§è¡Œæ—¶é—´ã€Stageä¿¡æ¯
- **Stagesé¡µé¢**ï¼šStageæ‰§è¡Œè¯¦æƒ…ã€ä»»åŠ¡åˆ†å¸ƒ
- **Storageé¡µé¢**ï¼šRDDç¼“å­˜ä½¿ç”¨æƒ…å†µ
- **Executorsé¡µé¢**ï¼šExecutorèµ„æºä½¿ç”¨æƒ…å†µ
- **SQLé¡µé¢**ï¼šSQLæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’

æ€§èƒ½æŒ‡æ ‡

**æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡**ï¼š

```bash
# æŸ¥çœ‹åº”ç”¨ç¨‹åºæŒ‡æ ‡
curl http://driver-host:4040/api/v1/applications
curl http://driver-host:4040/api/v1/applications/[app-id]/jobs
curl http://driver-host:4040/api/v1/applications/[app-id]/stages
curl http://driver-host:4040/api/v1/applications/[app-id]/executors
```

---

---

## Spark Streaming â­

### æµå¤„ç†æ¦‚å¿µ

#### å¾®æ‰¹æ¬¡å¤„ç†

**Spark Streaming** åŸºäºå¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰å¤„ç†æ¨¡å‹ï¼Œå°†è¿ç»­çš„æ•°æ®æµåˆ’åˆ†ä¸ºå°çš„æ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

```mermaid
graph LR
    A[å®æ—¶æ•°æ®æµ] --> B[æ‰¹æ¬¡1<br/>1-2ç§’]
    A --> C[æ‰¹æ¬¡2<br/>2-3ç§’]
    A --> D[æ‰¹æ¬¡3<br/>3-4ç§’]
    
    B --> E[RDD1]
    C --> F[RDD2]
    D --> G[RDD3]
    
    E --> H[å¤„ç†ç»“æœ1]
    F --> I[å¤„ç†ç»“æœ2]
    G --> J[å¤„ç†ç»“æœ3]
    
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

#### DStreamæ¦‚å¿µ

**DStream (Discretized Stream)** æ˜¯Spark Streamingçš„åŸºæœ¬æŠ½è±¡ï¼Œä»£è¡¨è¿ç»­çš„æ•°æ®æµã€‚

```scala
// DStreamåŸºæœ¬ä½¿ç”¨
val conf = new SparkConf().setAppName("StreamingExample")
val ssc = new StreamingContext(conf, Seconds(2))

// åˆ›å»ºDStream
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)

// è¾“å‡ºç»“æœ
wordCounts.print()

// å¯åŠ¨æµå¤„ç†
ssc.start()
ssc.awaitTermination()
```

### Structured Streaming

#### æ ¸å¿ƒæ¦‚å¿µ

**Structured Streaming** æ˜¯Spark 2.0å¼•å…¥çš„æ–°æµå¤„ç†å¼•æ“ï¼ŒåŸºäºSpark SQLæ„å»ºã€‚

```scala
// Structured Streamingç¤ºä¾‹
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("StructuredStreamingExample")
  .getOrCreate()

// å®šä¹‰Schema
val schema = StructType(
  StructField("timestamp", TimestampType, true) ::
  StructField("value", StringType, true) :: Nil
)

// åˆ›å»ºæµæ•°æ®æº
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// å¤„ç†æ•°æ®
val words = df
  .selectExpr("CAST(value AS STRING) as message")
  .flatMap(_.split(" "))
  .groupBy("word")
  .count()

// è¾“å‡ºåˆ°æ§åˆ¶å°
val query = words.writeStream
  .outputMode("complete")
  .format("console")
  .trigger(Trigger.ProcessingTime("2 seconds"))
  .start()

query.awaitTermination()
```

#### è¾“å‡ºæ¨¡å¼


| è¾“å‡ºæ¨¡å¼     | æè¿°           | é€‚ç”¨åœºæ™¯           |
| -------------- | ---------------- | -------------------- |
| **Complete** | è¾“å‡ºå®Œæ•´ç»“æœè¡¨ | èšåˆæŸ¥è¯¢           |
| **Append**   | åªè¾“å‡ºæ–°å¢è¡Œ   | æ— èšåˆçš„æŸ¥è¯¢       |
| **Update**   | è¾“å‡ºæ›´æ–°çš„è¡Œ   | èšåˆæŸ¥è¯¢çš„å¢é‡æ›´æ–° |

#### çª—å£æ“ä½œ

```scala
// çª—å£èšåˆ
val windowedCounts = df
  .withWatermark("timestamp", "10 minutes")
  .groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  )
  .count()

// ä¼šè¯çª—å£
val sessionCounts = df
  .groupBy(
    session_window($"timestamp", "30 minutes"),
    $"userId"
  )
  .count()
```

### å®¹é”™æœºåˆ¶

#### Checkpointæœºåˆ¶

**Checkpoint** æä¾›å®¹é”™æ¢å¤èƒ½åŠ›ï¼Œä¿å­˜DStreamçš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚

```scala
// è®¾ç½®Checkpoint
ssc.checkpoint("hdfs://checkpoint/path")

// æœ‰çŠ¶æ€æ“ä½œéœ€è¦Checkpoint
val runningCounts = words.updateStateByKey { (values: Seq[Int], state: Option[Int]) =>
  val currentCount = values.sum
  val previousCount = state.getOrElse(0)
  Some(currentCount + previousCount)
}
```

#### WALæœºåˆ¶

**Write-Ahead Logs** ç¡®ä¿æ¥æ”¶åˆ°çš„æ•°æ®åœ¨å¤„ç†å‰å…ˆå†™å…¥å¯é å­˜å‚¨ã€‚

```scala
// å¯ç”¨WAL
spark.conf.set("spark.streaming.receiver.writeAheadLog.enable", "true")
spark.conf.set("spark.streaming.driver.writeAheadLog.closeFileAfterWrite", "true")
```

---

## å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ

### å†…å­˜ç›¸å…³é”™è¯¯

**OutOfMemoryError: Java heap space**

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)
    at java.lang.StringBuilder.append(StringBuilder.java:136)
```

**åŸå› åˆ†æ**ï¼š

- **å †å†…å­˜ä¸è¶³**ï¼šExecutoræˆ–Driverçš„å †å†…å­˜è®¾ç½®è¿‡å°
- **æ•°æ®å€¾æ–œ**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿‡å¤§ï¼Œå¯¼è‡´å•ä¸ªTaskå†…å­˜æº¢å‡º
- **ç¼“å­˜è¿‡å¤š**ï¼šRDDç¼“å­˜å ç”¨è¿‡å¤šå†…å­˜
- **å¯¹è±¡åˆ›å»ºè¿‡å¤š**ï¼šé¢‘ç¹åˆ›å»ºå¤§å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´å†…å­˜é…ç½®**ï¼š

```bash
# å¢åŠ Executorå†…å­˜
spark-submit \
  --conf spark.executor.memory=8g \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memory=4g \
  --conf spark.driver.memoryOverhead=1g \
  your-app.jar

# è°ƒæ•´å†…å­˜æ¯”ä¾‹
spark-submit \
  --conf spark.memory.fraction=0.8 \
  --conf spark.memory.storageFraction=0.3 \
  your-app.jar
```

**2. å¤„ç†æ•°æ®å€¾æ–œ**ï¼š

```scala
// æ–¹æ³•1ï¼šå¢åŠ åˆ†åŒºæ•°
val skewedRDD = originalRDD.repartition(200)

// æ–¹æ³•2ï¼šè‡ªå®šä¹‰åˆ†åŒºç­–ç•¥
val customPartitioner = new Partitioner {
  override def numPartitions: Int = 200
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val hash = key.hashCode()
    Math.abs(hash) % numPartitions
  }
}
val skewedRDD = originalRDD.partitionBy(customPartitioner)

// æ–¹æ³•3ï¼šä¸¤é˜¶æ®µèšåˆ
val stage1RDD = originalRDD
  .map(x => (x._1 + "_" + Random.nextInt(10), x._2))  // æ·»åŠ éšæœºå‰ç¼€
  .reduceByKey(_ + _)
  .map(x => (x._1.split("_")(0), x._2))  // å»æ‰éšæœºå‰ç¼€
  .reduceByKey(_ + _)
```

**3. ä¼˜åŒ–ç¼“å­˜ç­–ç•¥**ï¼š

```scala
// ä½¿ç”¨MEMORY_AND_DISK_SERå‡å°‘å†…å­˜å ç”¨
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

// åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpointå‡å°‘å†…å­˜å‹åŠ›
rdd.checkpoint()
```

**4. ä»£ç ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨mapPartitionså‡å°‘å¯¹è±¡åˆ›å»º
val optimizedRDD = rdd.mapPartitions(iter => {
  val result = new ArrayBuffer[String]()
  while (iter.hasNext) {
    val item = iter.next()
    // å¤„ç†é€»è¾‘
    result += processedItem
  }
  result.iterator
})

// å¤ç”¨å¯¹è±¡
case class User(id: Long, name: String)
val userRDD = rdd.mapPartitions(iter => {
  val user = User(0L, "")  // å¤ç”¨å¯¹è±¡
  iter.map { case (id, name) =>
    user.id = id
    user.name = name
    user.copy()  // è¿”å›å‰¯æœ¬
  }
})
```

**OutOfMemoryError: Direct buffer memory**

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Direct buffer memory
    at java.nio.Bits.reserveMemory(Bits.java:694)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
```

**åŸå› åˆ†æ**ï¼š

- **å †å¤–å†…å­˜ä¸è¶³**ï¼šDirectBufferå†…å­˜è®¾ç½®è¿‡å°
- **ç½‘ç»œä¼ è¾“è¿‡å¤š**ï¼šå¤§é‡æ•°æ®é€šè¿‡ç½‘ç»œä¼ è¾“
- **åºåˆ—åŒ–é—®é¢˜**ï¼šåºåˆ—åŒ–è¿‡ç¨‹ä¸­å ç”¨è¿‡å¤šå †å¤–å†…å­˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ å †å¤–å†…å­˜**ï¼š

```bash
spark-submit \
  --conf spark.executor.memoryOverhead=4g \
  --conf spark.driver.memoryOverhead=2g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  your-app.jar
```

**2. ä¼˜åŒ–ç½‘ç»œä¼ è¾“**ï¼š

```scala
// å¯ç”¨å‹ç¼©
spark.conf.set("spark.io.compression.codec", "snappy")
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")

// è°ƒæ•´ç½‘ç»œç¼“å†²åŒº
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.executor.heartbeatInterval", "60s")
```

**3. ä¼˜åŒ–åºåˆ—åŒ–**ï¼š

```scala
// ä½¿ç”¨Kryoåºåˆ—åŒ–
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")

// æ³¨å†Œè‡ªå®šä¹‰ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(classOf[MyClass]))
```

1. OutOfMemoryError: Metaspace

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.OutOfMemoryError: Metaspace
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»åŠ è½½è¿‡å¤š**ï¼šåŠ¨æ€ç”Ÿæˆå¤§é‡ç±»
- **Metaspaceè®¾ç½®è¿‡å°**ï¼šJVM Metaspaceç©ºé—´ä¸è¶³
- **UDFä½¿ç”¨è¿‡å¤š**ï¼šå¤§é‡UDFå¯¼è‡´ç±»åŠ è½½

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´JVMå‚æ•°**ï¼š

```bash
spark-submit \
  --conf spark.executor.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  --conf spark.driver.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  your-app.jar
```

**2. ä¼˜åŒ–UDFä½¿ç”¨**ï¼š

```scala
// é¿å…åœ¨UDFä¸­åˆ›å»ºè¿‡å¤šç±»
val optimizedUDF = udf((value: String) => {
  // ä½¿ç”¨ç®€å•é€»è¾‘ï¼Œé¿å…åŠ¨æ€ç±»ç”Ÿæˆ
  value.toUpperCase
})

// å¤ç”¨UDFå®ä¾‹
val myUDF = udf((x: Int) => x * 2)
df.select(myUDF(col("value")))
```

### ç½‘ç»œç›¸å…³é”™è¯¯

**FetchFailedException**

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hostname:7337
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
```

**åŸå› åˆ†æ**ï¼š

- **ç½‘ç»œè¶…æ—¶**ï¼šç½‘ç»œè¿æ¥è¶…æ—¶
- **Executorä¸¢å¤±**ï¼šExecutorè¿›ç¨‹å¼‚å¸¸é€€å‡º
- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³å¯¼è‡´è¿›ç¨‹é€€å‡º
- **ç½‘ç»œä¸ç¨³å®š**ï¼šç½‘ç»œè¿æ¥ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´ç½‘ç»œè¶…æ—¶å‚æ•°**ï¼š

```bash
spark-submit \
  --conf spark.network.timeout=800s \
  --conf spark.executor.heartbeatInterval=60s \
  --conf spark.rpc.askTimeout=800s \
  --conf spark.rpc.lookupTimeout=800s \
  your-app.jar
```

**2. å¢åŠ é‡è¯•æœºåˆ¶**ï¼š

```bash
spark-submit \
  --conf spark.task.maxFailures=8 \
  --conf spark.stage.maxAttempts=4 \
  your-app.jar
```

**3. ä¼˜åŒ–Shuffleé…ç½®**ï¼š

```bash
spark-submit \
  --conf spark.shuffle.io.maxRetries=3 \
  --conf spark.shuffle.io.retryWait=60s \
  --conf spark.shuffle.file.buffer=32k \
  your-app.jar
```

**4. ç›‘æ§ExecutorçŠ¶æ€**ï¼š

```scala
// æ·»åŠ ç›‘æ§ä»£ç 
spark.sparkContext.addSparkListener(new SparkListener {
  override def onExecutorLost(executorLost: SparkListenerExecutorLost): Unit = {
    println(s"Executor lost: ${executorLost.executorId}")
    // è®°å½•æ—¥å¿—æˆ–å‘é€å‘Šè­¦
  }
})
```

**ConnectionTimeoutException**

**é”™è¯¯ç°è±¡**ï¼š

```
java.net.ConnectTimeoutException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
```

**åŸå› åˆ†æ**ï¼š

- **ç½‘ç»œå»¶è¿Ÿ**ï¼šç½‘ç»œå»¶è¿Ÿè¿‡é«˜
- **é˜²ç«å¢™é™åˆ¶**ï¼šé˜²ç«å¢™é˜»æ­¢è¿æ¥
- **ç«¯å£å†²çª**ï¼šç«¯å£è¢«å ç”¨
- **DNSè§£æé—®é¢˜**ï¼šDNSè§£æå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´è¿æ¥è¶…æ—¶**ï¼š

```bash
spark-submit \
  --conf spark.network.timeout=1200s \
  --conf spark.rpc.askTimeout=1200s \
  your-app.jar
```

**2. æ£€æŸ¥ç½‘ç»œé…ç½®**ï¼š

```bash
# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping hostname
telnet hostname port

# æ£€æŸ¥é˜²ç«å¢™
iptables -L
```

**3. ä½¿ç”¨æœ¬åœ°åŒ–ç­–ç•¥**ï¼š

```scala
// å¯ç”¨æ•°æ®æœ¬åœ°åŒ–
spark.conf.set("spark.locality.wait", "30s")
spark.conf.set("spark.locality.wait.process", "30s")
spark.conf.set("spark.locality.wait.node", "30s")
spark.conf.set("spark.locality.wait.rack", "30s")
```

### åºåˆ—åŒ–ç›¸å…³é”™è¯¯

**NotSerializableException**

**é”™è¯¯ç°è±¡**ï¼š

```
java.io.NotSerializableException: com.example.MyClass
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»æœªå®ç°Serializable**ï¼šè‡ªå®šä¹‰ç±»æœªå®ç°Serializableæ¥å£
- **é—­åŒ…é—®é¢˜**ï¼šåœ¨é—­åŒ…ä¸­å¼•ç”¨äº†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
- **é™æ€å˜é‡**ï¼šå¼•ç”¨äº†é™æ€å˜é‡æˆ–å•ä¾‹å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å®ç°Serializableæ¥å£**ï¼š

```scala
// æ–¹æ³•1ï¼šå®ç°Serializable
case class MyClass(id: Int, name: String) extends Serializable

// æ–¹æ³•2ï¼šä½¿ç”¨@transientæ³¨è§£
class MyClass(val id: Int, val name: String) extends Serializable {
  @transient
  private val nonSerializableField = new NonSerializableClass()
}
```

**2. é¿å…é—­åŒ…é—®é¢˜**ï¼š

```scala
// é”™è¯¯ç¤ºä¾‹
val nonSerializableObject = new NonSerializableClass()
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => nonSerializableObject.process(x))  // ä¼šæŠ¥é”™

// æ­£ç¡®ç¤ºä¾‹
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.mapPartitions(iter => {
  val localObject = new NonSerializableClass()  // åœ¨åˆ†åŒºå†…åˆ›å»º
  iter.map(x => localObject.process(x))
})
```

**3. ä½¿ç”¨å¹¿æ’­å˜é‡**ï¼š

```scala
// å¯¹äºåªè¯»çš„å¤§å¯¹è±¡ï¼Œä½¿ç”¨å¹¿æ’­å˜é‡
val largeData = spark.sparkContext.parallelize(1 to 1000000).collect()
val broadcastData = spark.sparkContext.broadcast(largeData)

val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => {
  val data = broadcastData.value  // ä½¿ç”¨å¹¿æ’­å˜é‡
  processWithData(x, data)
})
```

**KryoSerializationException**

**é”™è¯¯ç°è±¡**ï¼š

```
com.esotericsoftware.kryo.KryoException: java.lang.ClassNotFoundException: com.example.MyClass
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)
```

**åŸå› åˆ†æ**ï¼š

- **ç±»æœªæ³¨å†Œ**ï¼šä½¿ç”¨Kryoåºåˆ—åŒ–æ—¶ç±»æœªæ³¨å†Œ
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **ç‰ˆæœ¬ä¸å…¼å®¹**ï¼šåºåˆ—åŒ–å’Œååºåˆ—åŒ–ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ³¨å†Œè‡ªå®šä¹‰ç±»**ï¼š

```scala
val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[MyOtherClass]
))

val spark = SparkSession.builder()
  .config(conf)
  .getOrCreate()
```

**2. ä½¿ç”¨Kryoæ³¨å†Œå™¨**ï¼š

```scala
class MyKryoRegistrator extends KryoRegistrator {
  override def registerClasses(kryo: Kryo): Unit = {
    kryo.register(classOf[MyClass])
    kryo.register(classOf[MyOtherClass])
  }
}

val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.set("spark.kryo.registrator", "com.example.MyKryoRegistrator")
```

**3. ç¦ç”¨ä¸¥æ ¼æ¨¡å¼**ï¼š

```scala
spark.conf.set("spark.kryo.registrationRequired", "false")
```

### èµ„æºç›¸å…³é”™è¯¯

**ExecutorLostFailure**

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.scheduler.ExecutorLostFailure: Executor 1 lost
    at org.apache.spark.scheduler.TaskSetManager$$anonfun$abortIfCompletelyBlacklisted$1.apply(TaskSetManager.scala:1023)
```

**åŸå› åˆ†æ**ï¼š

- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³è¢«æ€æ­»
- **CPUè¿‡è½½**ï¼šCPUä½¿ç”¨ç‡è¿‡é«˜å¯¼è‡´è¿›ç¨‹å¼‚å¸¸
- **ç£ç›˜ç©ºé—´ä¸è¶³**ï¼šç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´å†™å…¥å¤±è´¥
- **ç½‘ç»œé—®é¢˜**ï¼šç½‘ç»œè¿æ¥é—®é¢˜å¯¼è‡´å¿ƒè·³è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ èµ„æºé…é¢**ï¼š

```bash
spark-submit \
  --executor-memory 8g \
  --executor-cores 4 \
  --num-executors 10 \
  --conf spark.executor.memoryOverhead=2g \
  your-app.jar
```

**2. ç›‘æ§èµ„æºä½¿ç”¨**ï¼š

```scala
// æ·»åŠ èµ„æºç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    println(s"Task ${taskEnd.taskInfo.taskId} completed:")
    println(s"  Duration: ${taskEnd.taskInfo.duration}ms")
    println(s"  Memory: ${metrics.peakExecutionMemory} bytes")
    println(s"  Disk: ${metrics.diskBytesSpilled} bytes spilled")
  }
})
```

**3. ä¼˜åŒ–èµ„æºåˆ†é…**ï¼š

```bash
spark-submit \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=2 \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.dynamicAllocation.initialExecutors=5 \
  your-app.jar
```

**NoClassDefFoundError**

**é”™è¯¯ç°è±¡**ï¼š

```
java.lang.NoClassDefFoundError: com.example.MyClass
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š

- **ä¾èµ–ç¼ºå¤±**ï¼šç¼ºå°‘å¿…è¦çš„jaråŒ…
- **ç‰ˆæœ¬å†²çª**ï¼šä¾èµ–ç‰ˆæœ¬å†²çª
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **æ‰“åŒ…é—®é¢˜**ï¼šjaråŒ…æ‰“åŒ…ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ·»åŠ ä¾èµ–jaråŒ…**ï¼š

```bash
spark-submit \
  --jars /path/to/dependency1.jar,/path/to/dependency2.jar \
  --conf spark.executor.extraClassPath=/path/to/dependencies/* \
  your-app.jar
```

**2. ä½¿ç”¨fat jar**ï¼š

```xml
<!-- Mavené…ç½® -->
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.2.4</version>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
    </execution>
  </executions>
</plugin>
```

**3. æ£€æŸ¥ä¾èµ–å†²çª**ï¼š

```bash
# æŸ¥çœ‹ä¾èµ–æ ‘
mvn dependency:tree

# æ’é™¤å†²çªä¾èµ–
<dependency>
  <groupId>com.example</groupId>
  <artifactId>library</artifactId>
  <version>1.0.0</version>
  <exclusions>
    <exclusion>
      <groupId>conflicting.group</groupId>
      <artifactId>conflicting-artifact</artifactId>
    </exclusion>
  </exclusions>
</dependency>
```

### æ•°æ®ç›¸å…³é”™è¯¯

**FileNotFoundException**

**é”™è¯¯ç°è±¡**ï¼š

```
java.io.FileNotFoundException: File does not exist: hdfs://namenode:8020/path/to/file
    at org.apache.hadoop.hdfs.DFSClient.checkPath(DFSClient.java:1274)
    at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1527)
```

**åŸå› åˆ†æ**ï¼š

- **æ–‡ä»¶ä¸å­˜åœ¨**ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶è¢«åˆ é™¤
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è¯»å–æ–‡ä»¶çš„æƒé™
- **è·¯å¾„é”™è¯¯**ï¼šæ–‡ä»¶è·¯å¾„æ ¼å¼é”™è¯¯
- **HDFSé—®é¢˜**ï¼šHDFSæœåŠ¡å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„**ï¼š

```scala
// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
val hadoopConf = spark.sparkContext.hadoopConfiguration
val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
val path = new org.apache.hadoop.fs.Path("/path/to/file")

if (fs.exists(path)) {
  println("File exists")
} else {
  println("File does not exist")
}
```

**2. è®¾ç½®æ–‡ä»¶ç³»ç»Ÿé…ç½®**ï¼š

```scala
// è®¾ç½®HDFSé…ç½®
spark.conf.set("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020")
spark.conf.set("spark.hadoop.dfs.namenode.rpc-address", "namenode:8020")
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š

```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
hdfs dfs -ls /path/to/file

# ä¿®æ”¹æ–‡ä»¶æƒé™
hdfs dfs -chmod 644 /path/to/file

# ä¿®æ”¹æ–‡ä»¶æ‰€æœ‰è€…
hdfs dfs -chown username:group /path/to/file
```

**DataSourceException**

**é”™è¯¯ç°è±¡**ï¼š

```
org.apache.spark.sql.AnalysisException: Table or view not found: table_name
    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
```

**åŸå› åˆ†æ**ï¼š

- **è¡¨ä¸å­˜åœ¨**ï¼šæ•°æ®åº“è¡¨ä¸å­˜åœ¨
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è®¿é—®è¡¨çš„æƒé™
- **æ•°æ®åº“è¿æ¥é—®é¢˜**ï¼šæ•°æ®åº“è¿æ¥å¤±è´¥
- **è¡¨åé”™è¯¯**ï¼šè¡¨åæ‹¼å†™é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨**ï¼š

```scala
// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
val tables = spark.catalog.listTables()
tables.filter(_.name == "table_name").show()

// æˆ–è€…ä½¿ç”¨SQL
spark.sql("SHOW TABLES").show()
```

**2. è®¾ç½®æ•°æ®åº“è¿æ¥**ï¼š

```scala
// è®¾ç½®æ•°æ®åº“è¿æ¥
spark.conf.set("spark.sql.warehouse.dir", "/user/hive/warehouse")
spark.conf.set("hive.metastore.uris", "thrift://metastore:9083")

// ä½¿ç”¨Hive
spark.sql("USE database_name")
spark.sql("SHOW TABLES").show()
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š

```sql
-- æˆäºˆæƒé™
GRANT SELECT ON TABLE table_name TO USER username;

-- æ£€æŸ¥æƒé™
SHOW GRANT USER username ON TABLE table_name;
```

### è°ƒè¯•å’Œè¯Šæ–­å·¥å…·

1. Spark Web UI

**è®¿é—®æ–¹å¼**ï¼š

```
http://driver-host:4040  # åº”ç”¨è¿è¡Œæ—¶
http://driver-host:18080 # å†å²æœåŠ¡å™¨
```

**å…³é”®æŒ‡æ ‡**ï¼š

- **Stagesé¡µé¢**ï¼šæŸ¥çœ‹Stageæ‰§è¡Œæƒ…å†µå’Œå¤±è´¥åŸå› 
- **Executorsé¡µé¢**ï¼šæŸ¥çœ‹Executorèµ„æºä½¿ç”¨æƒ…å†µ
- **Storageé¡µé¢**ï¼šæŸ¥çœ‹RDDç¼“å­˜æƒ…å†µ
- **Environmenté¡µé¢**ï¼šæŸ¥çœ‹é…ç½®å‚æ•°

1. æ—¥å¿—åˆ†æ

**æŸ¥çœ‹æ—¥å¿—**ï¼š

```bash
# æŸ¥çœ‹Driveræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-driver-*.log

# æŸ¥çœ‹Executoræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-executor-*.log

# æŸ¥çœ‹YARNæ—¥å¿—
yarn logs -applicationId application_1234567890_0001
```

**å…³é”®æ—¥å¿—æ¨¡å¼**ï¼š

```bash
# æŸ¥æ‰¾é”™è¯¯ä¿¡æ¯
grep -i "error\|exception\|failed" /path/to/logs/*.log

# æŸ¥æ‰¾å†…å­˜ç›¸å…³é”™è¯¯
grep -i "outofmemory\|oom" /path/to/logs/*.log

# æŸ¥æ‰¾ç½‘ç»œç›¸å…³é”™è¯¯
grep -i "timeout\|connection" /path/to/logs/*.log
```

1. æ€§èƒ½åˆ†æå·¥å…·

**JVMåˆ†æ**ï¼š

```bash
# æŸ¥çœ‹JVMå †å†…å­˜ä½¿ç”¨
jstat -gc <pid> 1000

# æŸ¥çœ‹çº¿ç¨‹çŠ¶æ€
jstack <pid>

# æŸ¥çœ‹å†…å­˜dump
jmap -dump:format=b,file=heap.hprof <pid>
```

**ç³»ç»Ÿèµ„æºç›‘æ§**ï¼š

```bash
# æŸ¥çœ‹ç³»ç»Ÿèµ„æºä½¿ç”¨
top -p <pid>
iostat -x 1
netstat -i
```

1. è°ƒè¯•ä»£ç 

**æ·»åŠ è°ƒè¯•ä¿¡æ¯**ï¼š

```scala
// æ·»åŠ æ—¥å¿—
import org.apache.log4j.{Level, Logger}
Logger.getLogger("org.apache.spark").setLevel(Level.DEBUG)

// æ·»åŠ ç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {
    println(s"Task started: ${taskStart.taskInfo.taskId}")
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    println(s"Task ended: ${taskEnd.taskInfo.taskId}, " +
            s"status: ${taskEnd.taskInfo.status}")
  }
})
```

**ä½¿ç”¨Spark Shellè°ƒè¯•**ï¼š

```scala
// å¯åŠ¨Spark Shell
spark-shell --master local[*]

// é€æ­¥è°ƒè¯•
val rdd = sc.textFile("/path/to/file")
rdd.take(10).foreach(println)  // æŸ¥çœ‹æ•°æ®
rdd.count()  // æ£€æŸ¥æ•°æ®é‡
```

### é¢„é˜²æªæ–½

1. é…ç½®ä¼˜åŒ–

**åŸºç¡€é…ç½®**ï¼š

```bash
# å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2g
spark.driver.memory=4g
spark.driver.memoryOverhead=1g

# ç½‘ç»œé…ç½®
spark.network.timeout=800s
spark.executor.heartbeatInterval=60s

# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
```

**æ€§èƒ½é…ç½®**ï¼š

```bash
# Shuffleé…ç½®
spark.shuffle.file.buffer=32k
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
```

1. ä»£ç æœ€ä½³å®è·µ

**å†…å­˜ä¼˜åŒ–**ï¼š

```scala
// ä½¿ç”¨å¹¿æ’­å˜é‡
val broadcastVar = sc.broadcast(largeData)

// åŠæ—¶é‡Šæ”¾ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpoint
rdd.checkpoint()
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š

```scala
// åˆç†è®¾ç½®åˆ†åŒºæ•°
val optimalPartitions = Math.max(rdd.partitions.length, 200)
val repartitionedRDD = rdd.repartition(optimalPartitions)

// ä½¿ç”¨mapPartitions
val optimizedRDD = rdd.mapPartitions(iter => {
  // æ‰¹é‡å¤„ç†é€»è¾‘
  iter.map(processItem)
})
```

1. ç›‘æ§å‘Šè­¦

**è®¾ç½®ç›‘æ§**ï¼š

```scala
// æ·»åŠ ç›‘æ§æŒ‡æ ‡
val metrics = spark.sparkContext.getStatusTracker
val stageInfo = metrics.getStageInfo(stageId)
println(s"Stage $stageId: ${stageInfo.numTasks} tasks, " +
        s"${stageInfo.numCompletedTasks} completed")
```

**å‘Šè­¦é…ç½®**ï¼š

```bash
# è®¾ç½®å‘Šè­¦é˜ˆå€¼
spark.executor.failures.max=3
spark.task.maxFailures=8
spark.stage.maxAttempts=4
```

é€šè¿‡ä»¥ä¸Šè¯¦ç»†çš„é”™è¯¯åˆ†æå’Œè§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆå¤„ç†Sparkä»»åŠ¡ä¸­çš„å¸¸è§é—®é¢˜ï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

---

---

---

## å…³é”®å‚æ•°ä¸é…ç½®æ¨¡æ¿ âš™ï¸

### èµ„æºç›¸å…³å‚æ•°

**å†…å­˜é…ç½®**ï¼š

```properties
# åŸºç¡€å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2048m
spark.driver.memory=4g
spark.driver.memoryOverhead=1024m

# å †å¤–å†…å­˜
spark.memory.offHeap.enabled=true
spark.memory.offHeap.size=4g

# å†…å­˜åˆ†é…æ¯”ä¾‹
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3

# åŠ¨æ€å†…å­˜åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=5
spark.dynamicAllocation.maxExecutors=100
spark.dynamicAllocation.initialExecutors=10
```

**CPUé…ç½®**ï¼š

```properties
# CPUèµ„æºé…ç½®
spark.executor.cores=4
spark.driver.cores=2
spark.default.parallelism=400
spark.sql.shuffle.partitions=400

# ä»»åŠ¡è°ƒåº¦
spark.task.cpus=1
spark.task.maxFailures=8
spark.stage.maxConsecutiveAttempts=4
```

### JVMç›¸å…³å‚æ•°

**åƒåœ¾å›æ”¶é…ç½®**ï¼š

```bash
# G1GCé…ç½®ï¼ˆæ¨èï¼‰
--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC 
-XX:G1HeapRegionSize=16m 
-XX:MaxGCPauseMillis=200 
-XX:+G1PrintRegionRememberedSetInfo 
-XX:+UseCompressedOops 
-XX:+UseCompressedClassPointers
-XX:+PrintHeapAtGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps
-Xloggc:/var/log/spark/gc-executor.log"

--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC 
-XX:MaxGCPauseMillis=200 
-XX:+PrintHeapAtGC
-Xloggc:/var/log/spark/gc-driver.log"
```

**å¹¶å‘GCé…ç½®**ï¼š

```bash
# CMS GCé…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC 
-XX:+CMSParallelRemarkEnabled 
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:CMSInitiatingOccupancyFraction=70
-XX:+PrintGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps"
```

**å†…å­˜è°ƒè¯•å‚æ•°**ï¼š

```bash
# å†…å­˜è°ƒè¯•é…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/log/spark/heapdump
-XX:+TraceClassLoading
-XX:+PrintStringDeduplication"
```

### æ€§èƒ½ä¼˜åŒ–å‚æ•°

**SQLæ‰§è¡Œä¼˜åŒ–**ï¼š

```properties
# è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# æ•°æ®æ ¼å¼ä¼˜åŒ–
spark.sql.parquet.columnarReaderBatchSize=10000
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.binaryAsString=false
spark.sql.execution.arrow.pyspark.enabled=true

# Joinä¼˜åŒ–
spark.sql.autoBroadcastJoinThreshold=100MB
spark.sql.broadcastTimeout=300s
```

**Shuffleä¼˜åŒ–**ï¼š

```properties
# ShuffleåŸºç¡€é…ç½®
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.file.buffer=64k
spark.shuffle.unsafe.file.output.buffer=64k

# Shuffleé«˜çº§é…ç½®
spark.shuffle.sort.bypassMergeThreshold=200
spark.shuffle.consolidateFiles=true
spark.shuffle.memoryFraction=0.2
spark.shuffle.safetyFraction=0.8

# Shuffleç½‘ç»œé…ç½®
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s
spark.shuffle.service.enabled=true
```

**å…¶ä»–å¸¸ç”¨å‚æ•°**ï¼š

```properties
# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryo.unsafe=true

# å‹ç¼©é…ç½®
spark.io.compression.codec=snappy
spark.broadcast.compress=true
spark.rdd.compress=true

# æ–‡ä»¶ç³»ç»Ÿé…ç½®
spark.files.maxPartitionBytes=128m
spark.files.openCostInBytes=4m

# Hadoopå…¼å®¹é…ç½®
spark.hadoop.parquet.enable.summary-metadata=false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
```

### é…ç½®æ¨¡æ¿

**å¼€å‘ç¯å¢ƒé…ç½®**ï¼š

```bash
#!/bin/bash
# å¼€å‘ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master local[*] \
  --deploy-mode client \
  --executor-memory 2g \
  --driver-memory 1g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  your-app.jar
```

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š

```bash
#!/bin/bash
# ç”Ÿäº§ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 50 \
  --executor-memory 8g \
  --executor-cores 4 \
  --driver-memory 4g \
  --driver-cores 2 \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memoryOverhead=1g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=10 \
  --conf spark.dynamicAllocation.maxExecutors=100 \
  --conf spark.shuffle.service.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.execution.arrow.pyspark.enabled=true \
  your-app.jar
```

---
## Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: è¯·è¯¦ç»†è§£é‡ŠRDDã€DataFrameå’ŒDatasetçš„åŒºåˆ«åŠå„è‡ªçš„åº”ç”¨åœºæ™¯ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæä¾›äº†ä¸‰ç§æ ¸å¿ƒæ•°æ®æŠ½è±¡ï¼šRDDã€DataFrameå’ŒDatasetï¼Œå®ƒä»¬å„è‡ªé€‚ç”¨äºä¸åŒçš„åœºæ™¯ï¼Œå…·æœ‰ä¸åŒçš„ç‰¹æ€§å’Œä¼˜åŠ¿ã€‚

**1. åŸºæœ¬æ¦‚å¿µå¯¹æ¯”**

- **RDD (Resilient Distributed Dataset)**ï¼šSparkæœ€åˆçš„æ•°æ®æŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªä¸å¯å˜çš„ã€åˆ†å¸ƒå¼çš„å¯¹è±¡é›†åˆï¼Œæ”¯æŒå‡½æ•°å¼ç¼–ç¨‹æ“ä½œã€‚
- **DataFrame**ï¼šåœ¨RDDåŸºç¡€ä¸Šå¼•å…¥äº†Schemaæ¦‚å¿µï¼Œç±»ä¼¼å…³ç³»å‹æ•°æ®åº“ä¸­çš„è¡¨ç»“æ„ï¼Œæ”¯æŒSQLæŸ¥è¯¢ã€‚
- **Dataset**ï¼šDataFrameçš„æ‰©å±•ï¼Œæä¾›ç±»å‹å®‰å…¨çš„ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹æ¥å£ï¼Œç»“åˆäº†RDDçš„ç±»å‹å®‰å…¨å’ŒDataFrameçš„ä¼˜åŒ–æ€§èƒ½ã€‚

**2. æ ¸å¿ƒç‰¹æ€§å¯¹æ¯”**

| ç‰¹æ€§           | RDD                  | DataFrame            | Dataset              |
| -------------- | -------------------- | -------------------- | -------------------- |
| **ç±»å‹å®‰å…¨**   | ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥       | è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥       | ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥       |
| **æ€§èƒ½ä¼˜åŒ–**   | æ— å†…ç½®ä¼˜åŒ–           | Catalystä¼˜åŒ–å™¨       | Catalystä¼˜åŒ–å™¨       |
| **å†…å­˜ç®¡ç†**   | Javaå¯¹è±¡/Kryoåºåˆ—åŒ–  | TungstenäºŒè¿›åˆ¶æ ¼å¼   | TungstenäºŒè¿›åˆ¶æ ¼å¼   |
| **APIé£æ ¼**    | å‡½æ•°å¼API            | SQL + å‡½æ•°å¼API      | ç±»å‹å®‰å…¨çš„å‡½æ•°å¼API  |
| **Schemaæ„ŸçŸ¥** | æ— Schema             | æœ‰Schema             | æœ‰Schema             |
| **ä½¿ç”¨éš¾åº¦**   | è¾ƒå¤æ‚               | ç®€å•                 | ä¸­ç­‰                 |
| **é€‚ç”¨åœºæ™¯**   | éç»“æ„åŒ–æ•°æ®å¤„ç†     | ç»“æ„åŒ–æ•°æ®åˆ†æ       | ç»“æ„åŒ–æ•°æ®å¤æ‚å¤„ç†   |

**3. ä»£ç ç¤ºä¾‹å¯¹æ¯”**

```scala
// RDDç¤ºä¾‹
val rdd = sc.textFile("data.txt")
  .map(line => line.split(","))
  .map(fields => Person(fields(0), fields(1).toInt))
  .filter(person => person.age > 30)

// DataFrameç¤ºä¾‹
val df = spark.read.json("people.json")
df.filter($"age" > 30)
  .select($"name", $"age")
  .groupBy($"age")
  .count()

// Datasetç¤ºä¾‹
case class Person(name: String, age: Int)
val ds = spark.read.json("people.json").as[Person]
ds.filter(p => p.age > 30)
  .map(p => (p.name, p.age))
  .groupByKey(_._2)
  .count()
```

**4. æ€§èƒ½ä¸ä¼˜åŒ–å¯¹æ¯”**

- **RDD**ï¼šä¾èµ–äºJVMçš„åƒåœ¾å›æ”¶å’ŒJavaåºåˆ—åŒ–ï¼Œæ€§èƒ½å—é™
- **DataFrame**ï¼š
  - ä½¿ç”¨Catalystä¼˜åŒ–å™¨è¿›è¡Œé€»è¾‘å’Œç‰©ç†æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–
  - ä½¿ç”¨Tungstené«˜æ•ˆå†…å­˜ç®¡ç†ï¼Œå‡å°‘GCå¼€é”€
  - æ”¯æŒåˆ—å¼å­˜å‚¨å’Œå‹ç¼©
- **Dataset**ï¼š
  - ç»“åˆäº†DataFrameçš„æ‰€æœ‰ä¼˜åŒ–
  - å¢åŠ äº†ç¼–ç å™¨(Encoder)ï¼Œåœ¨å¯¹è±¡å’Œå†…éƒ¨Tungstenè¡¨ç¤ºä¹‹é—´é«˜æ•ˆè½¬æ¢

**5. é€‰æ‹©å»ºè®®**

- **é€‰æ‹©RDD**ï¼šå½“éœ€è¦ç»†ç²’åº¦æ§åˆ¶æˆ–å¤„ç†éç»“æ„åŒ–æ•°æ®æ—¶
- **é€‰æ‹©DataFrame**ï¼šå¤„ç†ç»“æ„åŒ–æ•°æ®ï¼Œéœ€è¦é«˜æ€§èƒ½ä¼˜åŒ–ï¼Œæˆ–éœ€è¦ä½¿ç”¨SQLæŸ¥è¯¢
- **é€‰æ‹©Dataset**ï¼šéœ€è¦ç±»å‹å®‰å…¨å’Œå‡½æ•°å¼ç¼–ç¨‹ï¼ŒåŒæ—¶åˆéœ€è¦Catalystä¼˜åŒ–å™¨çš„æ€§èƒ½æå‡

éšç€Sparkçš„å‘å±•ï¼ŒDataFrameå’ŒDataset APIå·²ç»æˆä¸ºæ¨èçš„æ•°æ®å¤„ç†æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨Spark 2.0ä¹‹åï¼ŒDataFrameå®é™…ä¸Šæ˜¯Dataset[Row]çš„ç±»å‹åˆ«åã€‚

**Q2: è¯·è¯¦ç»†æè¿°Sparkä»»åŠ¡çš„æ‰§è¡Œæµç¨‹ï¼Œä»æäº¤åº”ç”¨åˆ°ä»»åŠ¡å®Œæˆçš„å…¨è¿‡ç¨‹ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkä»»åŠ¡æ‰§è¡Œæ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ†å¸ƒå¼è®¡ç®—è¿‡ç¨‹ï¼Œæ¶‰åŠå¤šä¸ªç»„ä»¶ååŒå·¥ä½œã€‚ç†è§£è¿™ä¸ªæµç¨‹å¯¹äºä¼˜åŒ–Sparkåº”ç”¨å’Œæ’æŸ¥é—®é¢˜è‡³å…³é‡è¦ã€‚

**1. æ•´ä½“æ‰§è¡Œæ¶æ„**

Sparkåº”ç”¨ç¨‹åºçš„æ‰§è¡Œæ¶‰åŠä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
- **Driver Program**ï¼šåŒ…å«åº”ç”¨ç¨‹åºçš„mainå‡½æ•°ï¼Œè´Ÿè´£åˆ›å»ºSparkContext
- **Cluster Manager**ï¼šè´Ÿè´£èµ„æºåˆ†é…ï¼ˆå¦‚YARNã€Kubernetesã€Mesosæˆ–Standaloneï¼‰
- **Worker Node**ï¼šæ‰§è¡Œè®¡ç®—ä»»åŠ¡çš„èŠ‚ç‚¹
- **Executor**ï¼šåœ¨WorkerèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„è®¡ç®—ä»»åŠ¡

**2. è¯¦ç»†æ‰§è¡Œæµç¨‹**

```mermaid
graph TD
    A[ç”¨æˆ·ç¨‹åº] --> B[SparkContext]
    B --> C[DAGScheduler]
    C --> D[TaskScheduler]
    D --> E[WorkerèŠ‚ç‚¹]
    E --> F[Executor]
    F --> G[Taskæ‰§è¡Œ]
    G --> H[ç»“æœæ”¶é›†]
    H --> I[ä½œä¸šå®Œæˆ]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
    style E fill:#e1d5e7,stroke:#9673a6
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#f8cecc,stroke:#b85450
    style H fill:#d4f1f9,stroke:#05a4d1
    style I fill:#f9f9f9,stroke:#333
```

**3. æ‰§è¡Œæ­¥éª¤è¯¦è§£**

1. **åº”ç”¨ç¨‹åºå¯åŠ¨**ï¼š
   - ç”¨æˆ·æäº¤åº”ç”¨ç¨‹åº
   - åˆ›å»ºSparkContextï¼ˆSparkçš„å…¥å£ç‚¹ï¼‰
   - SparkContextè¿æ¥åˆ°é›†ç¾¤ç®¡ç†å™¨

2. **èµ„æºç”³è¯·**ï¼š
   - SparkContexté€šè¿‡é›†ç¾¤ç®¡ç†å™¨ç”³è¯·èµ„æº
   - é›†ç¾¤ç®¡ç†å™¨åœ¨WorkerèŠ‚ç‚¹ä¸Šå¯åŠ¨Executorè¿›ç¨‹

3. **DAGæ„å»º**ï¼š
   - ç”¨æˆ·ä»£ç é€šè¿‡RDDè½¬æ¢æ“ä½œæ„å»ºDAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰
   - å»¶è¿Ÿè®¡ç®—ï¼šè½¬æ¢æ“ä½œåªæ„å»ºDAGï¼Œä¸æ‰§è¡Œè®¡ç®—

4. **ä½œä¸šæäº¤**ï¼š
   - å½“é‡åˆ°Actionæ“ä½œæ—¶ï¼Œè§¦å‘ä½œä¸šæäº¤
   - SparkContextå°†ä½œä¸šæäº¤ç»™DAGScheduler

5. **Stageåˆ’åˆ†**ï¼š
   - DAGSchedulerå°†DAGåˆ’åˆ†ä¸ºå¤šä¸ªStage
   - åˆ’åˆ†ä¾æ®ï¼šShuffleæ“ä½œï¼ˆå¦‚reduceByKeyã€joinç­‰ï¼‰
   - æ¯ä¸ªStageåŒ…å«å¯ä»¥æµæ°´çº¿æ‰§è¡Œçš„ä¸€ç»„Task

6. **Taskç”Ÿæˆä¸è°ƒåº¦**ï¼š
   - ä¸ºæ¯ä¸ªStageç”ŸæˆTaskSet
   - TaskSchedulerå°†TaskSetæäº¤ç»™TaskSetManager
   - TaskSetManagerè´Ÿè´£å…·ä½“çš„ä»»åŠ¡è°ƒåº¦å’Œå¤±è´¥é‡è¯•

7. **Taskæ‰§è¡Œ**ï¼š
   - Executoræ¥æ”¶å¹¶æ‰§è¡ŒTask
   - æ‰§è¡Œè®¡ç®—å¹¶å°†ç»“æœä¿å­˜åœ¨å†…å­˜æˆ–ç£ç›˜
   - å¯¹äºShuffleæ“ä½œï¼Œå°†ä¸­é—´ç»“æœå†™å…¥æœ¬åœ°ç£ç›˜

8. **ç»“æœæ”¶é›†**ï¼š
   - å¯¹äºéœ€è¦è¿”å›ç»“æœçš„Actionæ“ä½œï¼ŒDriveræ”¶é›†ç»“æœ
   - å¯¹äºå†™å…¥å¤–éƒ¨å­˜å‚¨çš„æ“ä½œï¼Œç›´æ¥å†™å…¥ç›®æ ‡ä½ç½®

9. **ä½œä¸šå®Œæˆ**ï¼š
   - æ‰€æœ‰Taskæ‰§è¡Œå®Œæˆåï¼Œä½œä¸šç»“æŸ
   - é‡Šæ”¾èµ„æºæˆ–ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªä½œä¸š

**4. å…³é”®æ¦‚å¿µè§£æ**

- **Job**ï¼šç”±Actionæ“ä½œè§¦å‘çš„ä¸€ç»„è®¡ç®—ä»»åŠ¡
- **Stage**ï¼šJobçš„å­é›†ï¼Œç”±ä¸€ç»„å¯ä»¥æµæ°´çº¿æ‰§è¡Œçš„Taskç»„æˆ
- **Task**ï¼šåœ¨å•ä¸ªExecutorä¸Šæ‰§è¡Œçš„æœ€å°å·¥ä½œå•å…ƒï¼Œå¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®
- **Shuffle**ï¼šæ•°æ®é‡åˆ†å¸ƒè¿‡ç¨‹ï¼Œæ˜¯Stageåˆ’åˆ†çš„è¾¹ç•Œ

**5. ç¤ºä¾‹è¯´æ˜**

```scala
// è¿™ä¸ªç®€å•çš„Sparkç¨‹åºæ¼”ç¤ºäº†æ‰§è¡Œæµç¨‹
val sc = new SparkContext(conf)  // åˆ›å»ºSparkContext
val lines = sc.textFile("data.txt")  // æ„å»ºRDDï¼Œä½†ä¸æ‰§è¡Œ
val words = lines.flatMap(_.split(" "))  // ç»§ç»­æ„å»ºDAG
val wordCounts = words.map((_, 1)).reduceByKey(_ + _)  // reduceByKeyä¼šå¯¼è‡´Shuffleï¼Œåˆ’åˆ†Stage
wordCounts.collect()  // Actionæ“ä½œï¼Œè§¦å‘å®é™…è®¡ç®—
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š
- ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªStageçš„Job
- ç¬¬ä¸€ä¸ªStageæ‰§è¡ŒtextFileã€flatMapå’Œmapæ“ä½œ
- ç¬¬äºŒä¸ªStageæ‰§è¡ŒreduceByKeyæ“ä½œ
- collect()è§¦å‘æ•´ä¸ªJobçš„æ‰§è¡Œ

ç†è§£Sparkä»»åŠ¡æ‰§è¡Œæµç¨‹æœ‰åŠ©äºç¼–å†™é«˜æ•ˆçš„Sparkåº”ç”¨ç¨‹åºï¼Œå¹¶èƒ½æ›´å¥½åœ°è¿›è¡Œæ€§èƒ½è°ƒä¼˜å’Œæ•…éšœæ’æŸ¥ã€‚

**Q3: è¯·è§£é‡ŠSparkçš„å†…å­˜ç®¡ç†æœºåˆ¶ï¼ŒåŒ…æ‹¬å†…å­˜åˆ†é…ç­–ç•¥å’Œä¼˜åŒ–æ–¹æ³•ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkå†…å­˜ç®¡ç†æ˜¯å½±å“Sparkåº”ç”¨æ€§èƒ½çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚Sparké€šè¿‡ç²¾ç»†çš„å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œåœ¨æœ‰é™çš„å†…å­˜èµ„æºä¸‹å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è®¡ç®—ã€‚

**1. Sparkå†…å­˜æ¶æ„**

Sparkçš„JVMå †å†…å­˜ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

```mermaid
graph TD
    A[JVMå †å†…å­˜] --> B[Reserved Memory<br>300MB]
    A --> C[User Memory<br>ç”¨æˆ·ä»£ç ä½¿ç”¨]
    A --> D[Spark Memory<br>æ‰§è¡Œå’Œå­˜å‚¨]
    
    D --> E[Storage Memory<br>ç¼“å­˜æ•°æ®]
    D --> F[Execution Memory<br>è®¡ç®—è¿‡ç¨‹]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#f8cecc,stroke:#b85450
    style C fill:#d5e8d4,stroke:#82b366
    style D fill:#d4f1f9,stroke:#05a4d1
    style E fill:#ffe6cc,stroke:#d79b00
    style F fill:#e1d5e7,stroke:#9673a6
```

**2. å†…å­˜ç®¡ç†æ¨¡å¼**

Sparkæä¾›ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼ï¼š

- **é™æ€å†…å­˜ç®¡ç†(Static Memory Management)**ï¼š
  - Spark 1.6ä¹‹å‰çš„é»˜è®¤æ¨¡å¼
  - ä¸ºå­˜å‚¨å’Œæ‰§è¡Œå†…å­˜åˆ†é…å›ºå®šæ¯”ä¾‹ï¼Œä¸èƒ½åŠ¨æ€è°ƒæ•´
  - é…ç½®å‚æ•°ï¼š`spark.storage.memoryFraction`å’Œ`spark.shuffle.memoryFraction`
  
- **ç»Ÿä¸€å†…å­˜ç®¡ç†(Unified Memory Management)**ï¼š
  - Spark 1.6åŠä¹‹åçš„é»˜è®¤æ¨¡å¼
  - å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜å…±äº«ä¸€ä¸ªåŒºåŸŸï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´
  - é…ç½®å‚æ•°ï¼š`spark.memory.fraction`å’Œ`spark.memory.storageFraction`

**3. ç»Ÿä¸€å†…å­˜ç®¡ç†è¯¦è§£**

åœ¨ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼ä¸‹ï¼š

- **Spark Memory**ï¼šå JVMå †å†…å­˜çš„æ¯”ä¾‹ç”±`spark.memory.fraction`æ§åˆ¶ï¼Œé»˜è®¤ä¸º0.6
- **Storage Memory**ï¼šåˆå§‹å¤§å°ç”±`spark.memory.storageFraction`æ§åˆ¶ï¼Œé»˜è®¤ä¸º0.5
- **Execution Memory**ï¼šåˆå§‹å¤§å°ä¸ºSpark Memoryå‡å»Storage Memory

**å†…å­˜åŠ¨æ€è°ƒæ•´æœºåˆ¶**ï¼š

1. **å­˜å‚¨å†…å­˜ä¸è¶³æ—¶**ï¼š
   - å¦‚æœæ‰§è¡Œå†…å­˜æœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨æ‰§è¡Œå†…å­˜
   - å¦‚æœæ‰§è¡Œå†…å­˜æ²¡æœ‰ç©ºé—²ï¼Œåˆ™æŒ‰LRUç­–ç•¥æ·˜æ±°å·²ç¼“å­˜çš„RDDåˆ†åŒº

2. **æ‰§è¡Œå†…å­˜ä¸è¶³æ—¶**ï¼š
   - å¦‚æœå­˜å‚¨å†…å­˜æœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨å­˜å‚¨å†…å­˜
   - å¦‚æœå­˜å‚¨å†…å­˜æ²¡æœ‰ç©ºé—²ï¼Œä½†å­˜å‚¨å†…å­˜ä¸­æœ‰éƒ¨åˆ†æ˜¯è¢«æ‰§è¡Œå†…å­˜å€Ÿç”¨çš„ï¼Œåˆ™å¯ä»¥æŠ¢å è¿™éƒ¨åˆ†å†…å­˜
   - æ‰§è¡Œå†…å­˜ä¸ä¼šæ·˜æ±°å­˜å‚¨å†…å­˜ä¸­çš„æ•°æ®

**4. å†…å­˜ç®¡ç†ç›¸å…³é…ç½®å‚æ•°**

```scala
// ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼å…³é”®å‚æ•°
spark.memory.fraction = 0.6  // Spark Memoryå JVMå †å†…å­˜çš„æ¯”ä¾‹
spark.memory.storageFraction = 0.5  // Storage Memoryåˆå§‹å æ¯”

// å…¶ä»–é‡è¦å†…å­˜å‚æ•°
spark.executor.memory = "4g"  // Executorçš„JVMå †å†…å­˜å¤§å°
spark.memory.offHeap.enabled = false  // æ˜¯å¦å¯ç”¨å †å¤–å†…å­˜
spark.memory.offHeap.size = "2g"  // å †å¤–å†…å­˜å¤§å°
```

**5. å †å¤–å†…å­˜(Off-Heap Memory)**

ä»Spark 2.0å¼€å§‹ï¼ŒSparkæ”¯æŒä½¿ç”¨å †å¤–å†…å­˜ï¼š

- é€šè¿‡`spark.memory.offHeap.enabled`å¼€å¯
- ä½¿ç”¨`spark.memory.offHeap.size`è®¾ç½®å¤§å°
- ä¼˜åŠ¿ï¼šå‡å°‘GCå¼€é”€ï¼Œæé«˜æ€§èƒ½
- ç¼ºç‚¹ï¼šéœ€è¦æ‰‹åŠ¨ç®¡ç†å†…å­˜ï¼Œé…ç½®å¤æ‚

**6. å†…å­˜ç®¡ç†æœ€ä½³å®è·µ**

- **åˆç†è®¾ç½®Executorå†…å­˜**ï¼šæ ¹æ®é›†ç¾¤èŠ‚ç‚¹å†…å­˜å’Œå¹¶å‘ä»»åŠ¡æ•°
- **ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ**ï¼šé€šè¿‡Spark UIæŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
- **è°ƒæ•´å†…å­˜åˆ†é…æ¯”ä¾‹**ï¼šæ ¹æ®åº”ç”¨ç‰¹ç‚¹è°ƒæ•´å­˜å‚¨å’Œæ‰§è¡Œå†…å­˜æ¯”ä¾‹
- **ä½¿ç”¨å †å¤–å†…å­˜**ï¼šå¯¹äºå¤§æ•°æ®é‡å¤„ç†ï¼Œè€ƒè™‘å¯ç”¨å †å¤–å†…å­˜
- **é¿å…å†…å­˜æ³„æ¼**ï¼šæ³¨æ„é‡Šæ”¾ä¸å†ä½¿ç”¨çš„RDDï¼Œä½¿ç”¨`unpersist()`æ–¹æ³•

**7. å†…å­˜ä¸è¶³é—®é¢˜æ’æŸ¥**

å½“é‡åˆ°`OutOfMemoryError`æˆ–æ€§èƒ½ä¸‹é™æ—¶ï¼š

- æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„æ•°æ®ç¼“å­˜
- è€ƒè™‘å¢åŠ åˆ†åŒºæ•°ï¼Œå‡å°‘æ¯ä¸ªä»»åŠ¡çš„å†…å­˜ä½¿ç”¨
- è°ƒæ•´GCç­–ç•¥ï¼Œå¦‚ä½¿ç”¨G1GC
- ä½¿ç”¨Kryoåºåˆ—åŒ–å‡å°‘å†…å­˜å ç”¨
- è€ƒè™‘å¢åŠ Executorå†…å­˜æˆ–å‡å°‘æ¯ä¸ªExecutorçš„æ ¸å¿ƒæ•°

æ·±å…¥ç†è§£Sparkå†…å­˜ç®¡ç†æœºåˆ¶ï¼Œå¯¹äºä¼˜åŒ–Sparkåº”ç”¨æ€§èƒ½å’Œè§£å†³å†…å­˜ç›¸å…³é—®é¢˜è‡³å…³é‡è¦ã€‚

### æ¶æ„åŸç†é¢˜

**Q4: è¯·è¯¦ç»†ä»‹ç»Sparkçš„æ¶æ„ç»„ä»¶åŠå…¶èŒè´£ï¼Œå„ç»„ä»¶ä¹‹é—´å¦‚ä½•ååŒå·¥ä½œï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå…¶æ¶æ„ç”±å¤šä¸ªç»„ä»¶ååŒå·¥ä½œï¼Œå…±åŒæ”¯æ’‘åˆ†å¸ƒå¼æ•°æ®å¤„ç†èƒ½åŠ›ã€‚æ·±å…¥ç†è§£Sparkæ¶æ„ç»„ä»¶åŠå…¶äº¤äº’æ–¹å¼ï¼Œå¯¹äºæœ‰æ•ˆä½¿ç”¨Sparkå’Œæ’æŸ¥é—®é¢˜è‡³å…³é‡è¦ã€‚

**1. Sparkæ¶æ„æ€»è§ˆ**

```mermaid
graph TB
    A[ç”¨æˆ·åº”ç”¨] --> B[SparkContext]
    B --> C[é›†ç¾¤ç®¡ç†å™¨<br>YARN/Kubernetes/Mesos/Standalone]
    C --> D[WorkerèŠ‚ç‚¹]
    D --> E[Executor]
    B --> F[DAGScheduler]
    F --> G[TaskScheduler]
    G --> E
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
    style E fill:#e1d5e7,stroke:#9673a6
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#f8cecc,stroke:#b85450
```

**2. æ ¸å¿ƒç»„ä»¶è¯¦è§£**

| ç»„ä»¶ | ä½ç½® | ä¸»è¦èŒè´£ | å…³é”®ç‰¹æ€§ |
|------|------|---------|---------|
| **Driver Program** | å®¢æˆ·ç«¯æˆ–é›†ç¾¤ä¸­ | è¿è¡Œåº”ç”¨ç¨‹åºçš„mainå‡½æ•°<br>åˆ›å»ºSparkContext<br>æäº¤ä½œä¸š<br>æ”¶é›†ç»“æœ | åº”ç”¨ç¨‹åºçš„æ§åˆ¶ä¸­å¿ƒ<br>åŒ…å«DAGSchedulerå’ŒTaskScheduler |
| **SparkContext** | Driverä¸­ | Sparkç¨‹åºçš„å…¥å£ç‚¹<br>è¿æ¥é›†ç¾¤ç®¡ç†å™¨<br>è·å–Executor<br>æ„å»ºRDD | æ¯ä¸ªåº”ç”¨åªæœ‰ä¸€ä¸ª<br>è´Ÿè´£ä½œä¸šæäº¤å’Œèµ„æºç”³è¯· |
| **SparkSession** | Driverä¸­ | Spark 2.0åçš„å…¥å£ç‚¹<br>æ•´åˆSQLã€DataFrameã€Dataset API | æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£<br>åŒ…å«SparkContext |
| **Cluster Manager** | ç‹¬ç«‹è¿›ç¨‹ | èµ„æºåˆ†é…<br>å¯åŠ¨Executor | æ”¯æŒå¤šç§å®ç°ï¼š<br>YARNã€Kubernetesã€Mesosã€Standalone |
| **Worker Node** | é›†ç¾¤èŠ‚ç‚¹ | æä¾›è®¡ç®—èµ„æº<br>å¯åŠ¨Executorè¿›ç¨‹ | ç‰©ç†èŠ‚ç‚¹æˆ–è™šæ‹Ÿæœº<br>å¯ä»¥è¿è¡Œå¤šä¸ªExecutor |
| **Executor** | WorkerèŠ‚ç‚¹ä¸Š | æ‰§è¡ŒTask<br>ç¼“å­˜RDD<br>è¿”å›ç»“æœ | æ¯ä¸ªåº”ç”¨æœ‰å¤šä¸ª<br>ç”Ÿå‘½å‘¨æœŸä¸åº”ç”¨ç›¸åŒ |
| **DAGScheduler** | Driverä¸­ | æ„å»ºDAG<br>åˆ’åˆ†Stage<br>ç”ŸæˆTaskSet | åŸºäºShuffleä¾èµ–åˆ’åˆ†Stage<br>ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ |
| **TaskScheduler** | Driverä¸­ | å°†Taskåˆ†å‘ç»™Executor<br>ç›‘æ§Taskæ‰§è¡Œ<br>é‡è¯•å¤±è´¥çš„Task | è´Ÿè´£å…·ä½“çš„ä»»åŠ¡è°ƒåº¦<br>å¤„ç†ä»»åŠ¡å¤±è´¥å’Œé‡è¯• |
| **BlockManager** | Driverå’ŒExecutorä¸­ | ç®¡ç†å†…å­˜å’Œç£ç›˜å­˜å‚¨<br>å¤„ç†æ•°æ®å—ä¼ è¾“ | è´Ÿè´£RDDç¼“å­˜<br>ç®¡ç†Shuffleæ•°æ® |

**3. ç»„ä»¶äº¤äº’æµç¨‹**

1. **åº”ç”¨ç¨‹åºåˆå§‹åŒ–**ï¼š
 ```scala
 val spark = SparkSession.builder().appName("MyApp").getOrCreate()
 val sc = spark.sparkContext
 ```

2. **èµ„æºç”³è¯·ä¸Executorå¯åŠ¨**ï¼š
   - SparkContextè¿æ¥é›†ç¾¤ç®¡ç†å™¨
   - é›†ç¾¤ç®¡ç†å™¨åœ¨WorkerèŠ‚ç‚¹ä¸Šå¯åŠ¨Executorè¿›ç¨‹
   - Executorå‘Driveræ³¨å†Œ

3. **ä½œä¸šæäº¤ä¸æ‰§è¡Œ**ï¼š
   - DAGSchedulerå°†RDD DAGåˆ’åˆ†ä¸ºStage
   - TaskSchedulerå°†TaskSetæäº¤ç»™Executor
   - Executoræ‰§è¡ŒTaskå¹¶è¿”å›ç»“æœ

**4. ä¸åŒéƒ¨ç½²æ¨¡å¼å¯¹ç»„ä»¶çš„å½±å“**

| éƒ¨ç½²æ¨¡å¼ | Driverä½ç½® | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|----------|------|----------|
| **Clientæ¨¡å¼** | å®¢æˆ·ç«¯æœºå™¨ | Driverä¸å®¢æˆ·ç«¯åœ¨åŒä¸€è¿›ç¨‹<br>ä¾¿äºè°ƒè¯•å’ŒæŸ¥çœ‹è¾“å‡º | å¼€å‘æµ‹è¯•<br>äº¤äº’å¼åº”ç”¨ |
| **Clusteræ¨¡å¼** | é›†ç¾¤ä¸­çš„WorkerèŠ‚ç‚¹ | Driveråœ¨é›†ç¾¤ä¸­è¿è¡Œ<br>å®¢æˆ·ç«¯å¯ä»¥æ–­å¼€è¿æ¥ | ç”Ÿäº§ç¯å¢ƒ<br>é•¿æ—¶é—´è¿è¡Œçš„ä½œä¸š |

**5. å„ç»„ä»¶çš„é«˜å¯ç”¨æ€§è€ƒè™‘**

- **Driver**ï¼šåœ¨YARNæˆ–Kubernetesä¸Šå¯ä»¥å¯ç”¨AM (ApplicationMaster) é‡å¯
- **Worker**ï¼šèŠ‚ç‚¹å¤±è´¥æ—¶ï¼Œå…¶ä¸Šçš„Executorä¼šåœ¨å…¶ä»–èŠ‚ç‚¹é‡å¯
- **Executor**ï¼šå¤±è´¥æ—¶ä¼šé‡å¯ï¼Œæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ä¼šé‡è¯•
- **Task**ï¼šå¤±è´¥åä¼šè‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤šé‡è¯•æ¬¡æ•°å¯é…ç½®

**6. å®é™…åº”ç”¨ä¸­çš„æ¶æ„é€‰æ‹©**

- **å°è§„æ¨¡åº”ç”¨**ï¼šStandaloneæ¨¡å¼ç®€å•æ˜“ç”¨
- **ä¼ä¸šç”Ÿäº§ç¯å¢ƒ**ï¼šYARNæˆ–Kubernetesæä¾›æ›´å¥½çš„èµ„æºéš”ç¦»å’Œç®¡ç†
- **æ··åˆè´Ÿè½½ç¯å¢ƒ**ï¼šKubernetesé€‚åˆä¸å…¶ä»–å·¥ä½œè´Ÿè½½å…±å­˜
- **ä¼ ç»Ÿå¤§æ•°æ®ç¯å¢ƒ**ï¼šYARNä¸Hadoopç”Ÿæ€ç³»ç»Ÿé›†æˆæ›´å¥½

æ·±å…¥ç†è§£Sparkæ¶æ„ç»„ä»¶åŠå…¶äº¤äº’æ–¹å¼ï¼Œæœ‰åŠ©äºä¼˜åŒ–åº”ç”¨æ€§èƒ½ã€æ’æŸ¥é—®é¢˜ï¼Œä»¥åŠè®¾è®¡é€‚åˆç‰¹å®šåœºæ™¯çš„Sparkåº”ç”¨æ¶æ„ã€‚

**Q5: è¯·è¯¦ç»†è§£é‡ŠSparkçš„Shuffleæœºåˆ¶åŸç†åŠå…¶æ¼”è¿›å†å²ï¼Œå¦‚ä½•ä¼˜åŒ–Shuffleæ“ä½œï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Shuffleæ˜¯Sparkä¸­æœ€å…³é”®ä¹Ÿæœ€å¤æ‚çš„æœºåˆ¶ä¹‹ä¸€ï¼Œå®ƒæ¶‰åŠåˆ°æ•°æ®çš„é‡æ–°åˆ†åŒºå’Œè·¨èŠ‚ç‚¹ä¼ è¾“ï¼Œå¯¹Sparkåº”ç”¨çš„æ€§èƒ½æœ‰ç€é‡å¤§å½±å“ã€‚æ·±å…¥ç†è§£Shuffleæœºåˆ¶å¯¹äºä¼˜åŒ–Sparkåº”ç”¨è‡³å…³é‡è¦ã€‚

**1. Shuffleçš„åŸºæœ¬æ¦‚å¿µ**

Shuffleæ˜¯æŒ‡å°†åˆ†å¸ƒåœ¨å„ä¸ªåˆ†åŒºçš„æ•°æ®æŒ‰ç…§æŸç§è§„åˆ™é‡æ–°ç»„ç»‡ï¼Œä½¿å¾—å…·æœ‰ç›¸åŒç‰¹å¾ï¼ˆå¦‚ç›¸åŒçš„keyï¼‰çš„æ•°æ®èšé›†åœ¨ä¸€èµ·è¿›è¡Œè®¡ç®—çš„è¿‡ç¨‹ã€‚åœ¨Sparkä¸­ï¼ŒShuffleæ“ä½œæ˜¯Stageåˆ’åˆ†çš„è¾¹ç•Œã€‚

**è§¦å‘Shuffleçš„æ“ä½œåŒ…æ‹¬**ï¼š
- **é‡åˆ†åŒºæ“ä½œ**ï¼š`repartition`ã€`coalesce`
- **ByKeyç±»æ“ä½œ**ï¼š`groupByKey`ã€`reduceByKey`ã€`aggregateByKey`
- **Joinç±»æ“ä½œ**ï¼š`join`ã€`cogroup`
- **æ’åºæ“ä½œ**ï¼š`sortBy`ã€`sortByKey`

**2. Shuffleçš„æ¼”è¿›å†å²**

Spark Shuffleæœºåˆ¶ç»å†äº†å¤šæ¬¡é‡å¤§æ”¹è¿›ï¼š

| Shuffleç‰ˆæœ¬ | Sparkç‰ˆæœ¬ | ç‰¹ç‚¹ | ä¸»è¦é—®é¢˜ |
|------------|----------|------|---------|
| **Hash Shuffle V1** | 0.8åŠä¹‹å‰ | æ¯ä¸ªmap taskè¾“å‡ºMÃ—Rä¸ªæ–‡ä»¶<br>(M=mapä»»åŠ¡æ•°ï¼ŒR=reduceä»»åŠ¡æ•°) | æ–‡ä»¶æ•°è¿‡å¤šï¼Œå ç”¨æ–‡ä»¶å¥æŸ„ |
| **Hash Shuffle V2** | 0.8.1 - 1.1 | æ¯ä¸ªexecutorè¾“å‡ºCÃ—Rä¸ªæ–‡ä»¶<br>(C=coreæ•°ï¼ŒR=reduceä»»åŠ¡æ•°) | æ–‡ä»¶æ•°ä»ç„¶è¾ƒå¤š |
| **Sort Shuffle V1** | 1.1 - 1.5 | æ¯ä¸ªmap taskè¾“å‡º1ä¸ªæ–‡ä»¶ï¼ŒæŒ‰keyæ’åº | æ‰€æœ‰æ•°æ®éƒ½æ’åºï¼Œå¼€é”€å¤§ |
| **Sort Shuffle V2<br>(Tungsten)** | 1.5 - 2.0 | äºŒè¿›åˆ¶åºåˆ—åŒ–ï¼Œç›´æ¥æ“ä½œå†…å­˜ | ç‰¹å®šåœºæ™¯ä¼˜åŒ– |
| **Sort Shuffle V3** | 2.0+ | ç»Ÿä¸€çš„Sort-based Shuffle<br>å°æ•°æ®é‡å¯ç»•è¿‡æ’åº | å½“å‰é»˜è®¤å®ç° |

**3. Sort-based Shuffleè¯¦ç»†å·¥ä½œæµç¨‹**

```mermaid
graph TD
    A[Mapä»»åŠ¡] --> B[å†…å­˜ä¸­æŒ‰Partitioneråˆ†åŒº]
    B --> C{æ˜¯å¦éœ€è¦æ’åº?}
    C -->|æ˜¯| D[å¯¹æ¯ä¸ªåˆ†åŒºå†…æ•°æ®æ’åº]
    C -->|å¦| E[è·³è¿‡æ’åº]
    D --> F[æº¢å†™åˆ°ç£ç›˜]
    E --> F
    F --> G[åˆå¹¶æº¢å†™æ–‡ä»¶]
    G --> H[ç”Ÿæˆæ•°æ®æ–‡ä»¶å’Œç´¢å¼•æ–‡ä»¶]
    H --> I[Reduceä»»åŠ¡]
    I --> J[é€šè¿‡ç½‘ç»œæ‹‰å–æ•°æ®]
    J --> K[åˆå¹¶æ•°æ®]
    K --> L[è¿›è¡ŒReduceè®¡ç®—]
    
    style A fill:#d4f1f9,stroke:#05a4d1
    style I fill:#e1d5e7,stroke:#9673a6
    style F fill:#ffe6cc,stroke:#d79b00
    style G fill:#ffe6cc,stroke:#d79b00
    style H fill:#ffe6cc,stroke:#d79b00
    style J fill:#d5e8d4,stroke:#82b366
    style K fill:#d5e8d4,stroke:#82b366
    style L fill:#d5e8d4,stroke:#82b366
```

**4. Mapç«¯è¯¦è§£**

1. **åˆ†åŒºè®¡ç®—**ï¼šæ ¹æ®Partitionerç¡®å®šæ¯æ¡æ•°æ®çš„ç›®æ ‡åˆ†åŒº
2. **å†…å­˜ç¼“å†²**ï¼šæ•°æ®å…ˆå†™å…¥å†…å­˜ç¼“å†²åŒº
3. **æ’åºä¸èšåˆ**ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œæ’åºå’Œèšåˆ
4. **æº¢å†™æœºåˆ¶**ï¼š
   - å½“ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼ï¼ˆ`spark.shuffle.spill.numElementsForceSpillThreshold`ï¼‰æ—¶è§¦å‘æº¢å†™
   - æº¢å†™è¿‡ç¨‹ä¸­å¯èƒ½è¿›è¡Œæ’åºå’Œèšåˆ
5. **æ–‡ä»¶åˆå¹¶**ï¼šå¤šä¸ªæº¢å†™æ–‡ä»¶æœ€ç»ˆåˆå¹¶ä¸ºä¸€ä¸ªæ•°æ®æ–‡ä»¶å’Œä¸€ä¸ªç´¢å¼•æ–‡ä»¶

**5. Reduceç«¯è¯¦è§£**

1. **ä»»åŠ¡åˆå§‹åŒ–**ï¼šReduceä»»åŠ¡å¯åŠ¨æ—¶ï¼Œå‘DAGSchedulerè·å–ä¸Šæ¸¸Shuffleæ•°æ®çš„ä½ç½®ä¿¡æ¯
2. **æ•°æ®æ‹‰å–**ï¼šé€šè¿‡BlockManagerä»å„ä¸ªMapä»»åŠ¡æ‰€åœ¨èŠ‚ç‚¹æ‹‰å–æ•°æ®
3. **æ‹‰å–ç­–ç•¥**ï¼š
   - æŒ‰æ‰¹æ¬¡æ‹‰å–ï¼Œé¿å…ä¸€æ¬¡æ€§æ‹‰å–è¿‡å¤šæ•°æ®
   - æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œå¤„ç†ä¸´æ—¶ç½‘ç»œæ•…éšœ
4. **æ•°æ®èšåˆ**ï¼šå°†æ‹‰å–çš„æ•°æ®è¿›è¡Œåˆå¹¶å’Œèšåˆå¤„ç†
5. **ç»“æœè®¡ç®—**ï¼šå¯¹èšåˆåçš„æ•°æ®æ‰§è¡ŒReduceæ“ä½œ

**6. å…³é”®é…ç½®å‚æ•°**

```scala
// Shuffleè¡Œä¸ºæ§åˆ¶
spark.shuffle.manager = "sort"  // Shuffleå®ç°æ–¹å¼ï¼Œé»˜è®¤sort
spark.shuffle.sort.bypassMergeThreshold = 200  // å°åˆ†åŒºæ•°é‡ç»•è¿‡æ’åºçš„é˜ˆå€¼

// å†…å­˜ä½¿ç”¨æ§åˆ¶
spark.shuffle.file.buffer = "32k"  // æ¯ä¸ªè¾“å‡ºæµçš„ç¼“å†²å¤§å°
spark.shuffle.spill.compress = true  // æ˜¯å¦å‹ç¼©æº¢å†™æ–‡ä»¶

// ç½‘ç»œä¼ è¾“æ§åˆ¶
spark.reducer.maxSizeInFlight = "48m"  // æ¯ä¸ªreduceä»»åŠ¡åŒæ—¶æ‹‰å–çš„æœ€å¤§æ•°æ®é‡
spark.shuffle.io.retryWait = "5s"  // é‡è¯•ç­‰å¾…æ—¶é—´
spark.shuffle.io.maxRetries = 3  // æœ€å¤§é‡è¯•æ¬¡æ•°
```

**7. Shuffleä¼˜åŒ–ç­–ç•¥**

1. **å‡å°‘Shuffleæ“ä½œ**ï¼š
   - ä½¿ç”¨`mapPartitions`æ›¿ä»£`map`åæ¥`reduceByKey`
   - ä½¿ç”¨å¹¿æ’­å˜é‡æ›¿ä»£`join`

2. **è°ƒæ•´åˆ†åŒºæ•°é‡**ï¼š
   - è¿‡å°‘ï¼šæ•°æ®å€¾æ–œï¼Œä»»åŠ¡å¹¶è¡Œåº¦ä½
   - è¿‡å¤šï¼šå°æ–‡ä»¶è¿‡å¤šï¼Œè°ƒåº¦å¼€é”€å¤§
   - å»ºè®®ï¼šæ¯ä¸ªåˆ†åŒºå¤§å°åœ¨128MBå·¦å³

3. **å¯ç”¨èšåˆ**ï¼š
   - ä½¿ç”¨`reduceByKey`æ›¿ä»£`groupByKey`
   - ä½¿ç”¨`aggregateByKey`è¿›è¡Œæœ¬åœ°é¢„èšåˆ

4. **å†…å­˜è°ƒä¼˜**ï¼š
   - å¢åŠ Shuffleç¼“å†²åŒºå¤§å°å‡å°‘ç£ç›˜I/O
   - è°ƒæ•´æ‰§è¡Œå†…å­˜æ¯”ä¾‹é€‚åº”Shuffleéœ€æ±‚

5. **åºåˆ—åŒ–ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨Kryoåºåˆ—åŒ–å‡å°‘æ•°æ®å¤§å°
   - æ³¨å†Œè‡ªå®šä¹‰ç±»æé«˜åºåˆ—åŒ–æ€§èƒ½

æ·±å…¥ç†è§£Shuffleæœºåˆ¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…ç¼–å†™æ›´é«˜æ•ˆçš„Sparkåº”ç”¨ï¼Œé¿å…å¸¸è§çš„æ€§èƒ½é™·é˜±ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ã€‚

### æ€§èƒ½è°ƒä¼˜é¢˜

**Q6: è¯·è¯¦è¿°Sparkåº”ç”¨çš„æ€§èƒ½è°ƒä¼˜ç­–ç•¥ï¼Œä»å“ªäº›æ–¹é¢å¯ä»¥æå‡Sparkä½œä¸šçš„æ‰§è¡Œæ•ˆç‡ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæ€§èƒ½è°ƒä¼˜æ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§å·¥ä½œï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œç»¼åˆä¼˜åŒ–ã€‚ä¸€ä¸ªé«˜æ•ˆçš„Sparkåº”ç”¨éœ€è¦åˆç†çš„èµ„æºé…ç½®ã€ä¼˜åŒ–çš„ä»£ç ç»“æ„ã€é€‚å½“çš„æ•°æ®å¤„ç†ç­–ç•¥ä»¥åŠç²¾ç»†çš„å‚æ•°è°ƒæ•´ã€‚

**1. æ€§èƒ½è°ƒä¼˜çš„æ•´ä½“æ–¹æ³•è®º**

æ€§èƒ½è°ƒä¼˜åº”éµå¾ªä»¥ä¸‹æ–¹æ³•è®ºï¼š
- **è‡ªä¸Šè€Œä¸‹**ï¼šä»åº”ç”¨æ¶æ„åˆ°å…·ä½“å‚æ•°
- **æ•°æ®é©±åŠ¨**ï¼šåŸºäºç›‘æ§æŒ‡æ ‡å’Œæ€§èƒ½æµ‹è¯•
- **æ¸è¿›å¼**ï¼šä»æœ€å¤§ç“¶é¢ˆå¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–
- **æƒè¡¡å–èˆ**ï¼šåœ¨èµ„æºæ¶ˆè€—ã€æ‰§è¡Œé€Ÿåº¦ã€ç¨³å®šæ€§ä¹‹é—´å¯»æ‰¾å¹³è¡¡

**2. èµ„æºé…ç½®ä¼˜åŒ–**

```mermaid
graph TD
    A[èµ„æºé…ç½®ä¼˜åŒ–] --> B[Executoré…ç½®]
    A --> C[Driveré…ç½®]
    A --> D[é›†ç¾¤èµ„æº]
    
    B --> B1[å†…å­˜å¤§å°]
    B --> B2[æ ¸å¿ƒæ•°é‡]
    B --> B3[å®ä¾‹æ•°é‡]
    
    C --> C1[å†…å­˜å¤§å°]
    C --> C2[å¹¶è¡Œåº¦]
    
    D --> D1[èŠ‚ç‚¹è§„æ ¼]
    D --> D2[èµ„æºéš”ç¦»]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
```

**Executoré…ç½®æœ€ä½³å®è·µ**ï¼š
- **å†…å­˜å¤§å°**ï¼šæ¯ä¸ªExecutor 4-8GBå†…å­˜ï¼ˆè¿‡å¤§å¯¼è‡´GCå»¶è¿Ÿï¼‰
- **æ ¸å¿ƒæ•°é‡**ï¼šæ¯ä¸ªExecutor 4-5ä¸ªæ ¸å¿ƒï¼ˆè¿‡å¤šå¯¼è‡´çº¿ç¨‹ç«äº‰ï¼‰
- **å®ä¾‹æ•°é‡**ï¼š`(é›†ç¾¤æ€»æ ¸å¿ƒæ•° / æ¯ä¸ªExecutoræ ¸å¿ƒæ•°)`ï¼Œé¢„ç•™10%èµ„æº

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// 10èŠ‚ç‚¹é›†ç¾¤ï¼Œæ¯èŠ‚ç‚¹16æ ¸64GBå†…å­˜
spark.executor.instances = 30       // (10 * 16) / 5 = 32ï¼Œé¢„ç•™éƒ¨åˆ†
spark.executor.cores = 5            // æ¯ä¸ªExecutor 5ä¸ªæ ¸å¿ƒ
spark.executor.memory = "20g"       // æ¯ä¸ªExecutor 20GBå†…å­˜
spark.driver.memory = "10g"         // Driver 10GBå†…å­˜
```

**3. å¹¶è¡Œåº¦ä¼˜åŒ–**

å¹¶è¡Œåº¦æ˜¯æŒ‡ä»»åŠ¡åˆ’åˆ†çš„åˆ†åŒºæ•°ï¼Œå½±å“ä»»åŠ¡çš„å¹¶è¡Œæ‰§è¡Œæ•ˆç‡ã€‚

**å¹¶è¡Œåº¦è®¾ç½®åŸåˆ™**ï¼š
- **åŸºå‡†å€¼**ï¼šé›†ç¾¤æ€»æ ¸å¿ƒæ•°çš„2-3å€
- **æ•°æ®é‡**ï¼šæ¯ä¸ªåˆ†åŒºæ•°æ®é‡åœ¨128MBå·¦å³
- **åŠ¨æ€è°ƒæ•´**ï¼š`spark.sql.adaptive.enabled=true`

**å¹¶è¡Œåº¦ç›¸å…³é…ç½®**ï¼š
```scala
// é™æ€é…ç½®
spark.default.parallelism = 600     // é»˜è®¤å¹¶è¡Œåº¦
spark.sql.shuffle.partitions = 600  // SQLæ“ä½œçš„å¹¶è¡Œåº¦

// åŠ¨æ€é…ç½®
spark.sql.adaptive.enabled = true   // å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.coalescePartitions.enabled = true  // åˆå¹¶å°åˆ†åŒº
```

**4. æ•°æ®å€¾æ–œä¼˜åŒ–**

æ•°æ®å€¾æ–œæ˜¯æŒ‡æŸäº›åˆ†åŒºçš„æ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ—¶é—´ä¸å‡è¡¡ã€‚

**è¯†åˆ«æ•°æ®å€¾æ–œ**ï¼š
- Spark UIä¸­è§‚å¯ŸStageé¡µé¢çš„ä»»åŠ¡æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
- æŸ¥çœ‹Shuffleè¯»å†™æ•°æ®é‡çš„åˆ†å¸ƒæƒ…å†µ

**è§£å†³æ–¹æ¡ˆ**ï¼š

| å€¾æ–œç±»å‹ | è§£å†³æ–¹æ¡ˆ | å®ç°æ–¹å¼ |
|---------|---------|---------|
| **Joinå€¾æ–œ** | å¹¿æ’­Join | `broadcast(smallDF).join(largeDF)` |
| **Joinå€¾æ–œ** | æ‹†åˆ†çƒ­ç‚¹é”® | å¯¹çƒ­ç‚¹é”®æ·»åŠ éšæœºå‰ç¼€ï¼Œæ‰©å¤§Join |
| **èšåˆå€¾æ–œ** | ä¸¤é˜¶æ®µèšåˆ | å±€éƒ¨èšåˆ+å…¨å±€èšåˆ |
| **èšåˆå€¾æ–œ** | è‡ªå®šä¹‰åˆ†åŒº | å®ç°è‡ªå®šä¹‰Partitioner |
| **æ•°æ®æºå€¾æ–œ** | é¢„å¤„ç† | ETLé˜¶æ®µé‡æ–°åˆ†åŒº |

**ä»£ç ç¤ºä¾‹**ï¼š
```scala
// ä¸¤é˜¶æ®µèšåˆç¤ºä¾‹
val result = rdd
  .map(x => (x._1 + "_" + Random.nextInt(10), x._2))  // åŠ ç›
  .reduceByKey(_ + _)  // å±€éƒ¨èšåˆ
  .map(x => (x._1.split("_")(0), x._2))  // å»ç›
  .reduceByKey(_ + _)  // å…¨å±€èšåˆ
```

**5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**

åˆç†çš„ç¼“å­˜ç­–ç•¥å¯ä»¥é¿å…é‡å¤è®¡ç®—ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡ã€‚

**ç¼“å­˜çº§åˆ«é€‰æ‹©**ï¼š

| å­˜å‚¨çº§åˆ« | å†…å­˜ä½¿ç”¨ | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| MEMORY_ONLY | é«˜ | ä½ | é»˜è®¤é€‰æ‹©ï¼Œå†…å­˜å……è¶³ |
| MEMORY_AND_DISK | ä¸­ | ä½ | æ•°æ®é‡å¤§äºå†…å­˜ |
| MEMORY_ONLY_SER | ä½ | é«˜ | å†…å­˜å—é™ï¼Œå¯æ¥å—åºåˆ—åŒ–å¼€é”€ |
| OFF_HEAP | ä½ | ä¸­ | éœ€è¦è·¨åº”ç”¨å…±äº«æ•°æ® |

**ç¼“å­˜ä½¿ç”¨åŸåˆ™**ï¼š
- åªç¼“å­˜é‡å¤ä½¿ç”¨çš„RDD/DataFrame
- åœ¨Shuffleæ“ä½œä¹‹åã€Actionæ“ä½œä¹‹å‰ç¼“å­˜
- åŠæ—¶ä½¿ç”¨`unpersist()`é‡Šæ”¾ä¸å†ä½¿ç”¨çš„ç¼“å­˜

**6. Shuffleä¼˜åŒ–**

Shuffleæ˜¯Sparkä¸­æœ€æ˜‚è´µçš„æ“ä½œï¼Œä¼˜åŒ–Shuffleå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚

**Shuffleä¼˜åŒ–ç­–ç•¥**ï¼š
- **å‡å°‘Shuffle**ï¼šä½¿ç”¨`mapPartitions`æ›¿ä»£`map`+`reduceByKey`
- **æœ¬åœ°èšåˆ**ï¼šä½¿ç”¨`reduceByKey`æ›¿ä»£`groupByKey`
- **å¹¿æ’­å˜é‡**ï¼šå°è¡¨å¹¿æ’­é¿å…Shuffle
- **å‚æ•°è°ƒæ•´**ï¼šè°ƒæ•´ç¼“å†²åŒºå¤§å°ã€å‹ç¼©ç®—æ³•ç­‰

**å…³é”®å‚æ•°**ï¼š
```scala
spark.shuffle.file.buffer = "64k"  // å¢åŠ ç¼“å†²åŒºå‡å°‘ç£ç›˜I/O
spark.shuffle.compress = true      // å¯ç”¨å‹ç¼©
spark.shuffle.io.maxRetries = 6    // å¢åŠ é‡è¯•æ¬¡æ•°
```

**7. SQLä¼˜åŒ–**

å¯¹äºSpark SQLåº”ç”¨ï¼Œå¯ä»¥åº”ç”¨ä»¥ä¸‹ä¼˜åŒ–æŠ€æœ¯ï¼š

**æŸ¥è¯¢ä¼˜åŒ–**ï¼š
- **è°“è¯ä¸‹æ¨**ï¼šå°½æ—©è¿‡æ»¤æ•°æ®
- **åˆ—è£å‰ª**ï¼šåªè¯»å–éœ€è¦çš„åˆ—
- **åˆ†åŒºè£å‰ª**ï¼šåªè¯»å–éœ€è¦çš„åˆ†åŒº
- **è‡ªåŠ¨ä¼˜åŒ–**ï¼šå¯ç”¨AQEã€åŠ¨æ€åˆ†åŒºè£å‰ªç­‰

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.enabled = true
// å¯ç”¨åŠ¨æ€åˆ†åŒºè£å‰ª
spark.sql.optimizer.dynamicPartitionPruning.enabled = true
// å¯ç”¨Joiné‡æ’åº
spark.sql.adaptive.optimizeSkewedJoin = true
```

**8. åºåˆ—åŒ–ä¼˜åŒ–**

åºåˆ—åŒ–å½±å“æ•°æ®ä¼ è¾“å’Œå­˜å‚¨æ•ˆç‡ã€‚

**åºåˆ—åŒ–é€‰æ‹©**ï¼š
- **Kryoåºåˆ—åŒ–**ï¼šæ¯”Javaåºåˆ—åŒ–æ›´é«˜æ•ˆ
- **åˆ—å¼æ ¼å¼**ï¼šParquetã€ORCç­‰æ ¼å¼æ›´é«˜æ•ˆ

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// å¯ç”¨Kryoåºåˆ—åŒ–
spark.serializer = "org.apache.spark.serializer.KryoSerializer"
// æ³¨å†Œè‡ªå®šä¹‰ç±»
spark.kryo.registrator = "com.example.MyRegistrator"
```

**9. ç»¼åˆæ€§èƒ½è°ƒä¼˜æ¡ˆä¾‹**

**å¤§è§„æ¨¡æ•°æ®Joinä¼˜åŒ–**ï¼š
```scala
// ä¼˜åŒ–å‰
val result = largeDF.join(smallDF, Seq("key"))

// ä¼˜åŒ–å
// 1. å¹¿æ’­å°è¡¨
val broadcastDF = broadcast(smallDF)
val result = largeDF.join(broadcastDF, Seq("key"))

// 2. å¯ç”¨AQEå’ŒJoinä¼˜åŒ–
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

**æ•°æ®å€¾æ–œå¤„ç†**ï¼š
```scala
// ä¼˜åŒ–å‰
val result = rdd.reduceByKey(_ + _)

// ä¼˜åŒ–å
// 1. ä¸¤é˜¶æ®µèšåˆ
val result = rdd
  .map(x => ((x._1, Random.nextInt(10)), x._2))  // åŠ ç›
  .reduceByKey(_ + _)  // å±€éƒ¨èšåˆ
  .map(x => (x._1._1, x._2))  // å»ç›
  .reduceByKey(_ + _)  // å…¨å±€èšåˆ
```

æ€§èƒ½è°ƒä¼˜æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ç»“åˆå…·ä½“åº”ç”¨åœºæ™¯ã€æ•°æ®ç‰¹ç‚¹å’Œèµ„æºæƒ…å†µï¼Œé‡‡ç”¨é€‚å½“çš„ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒä¼˜ï¼Œå¯ä»¥æ˜¾è‘—æå‡Sparkåº”ç”¨çš„æ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

**Q7: å¦‚ä½•è¯†åˆ«å’Œè§£å†³Sparkä¸­çš„æ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿè¯·ç»™å‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆå’Œä»£ç ç¤ºä¾‹ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

æ•°æ®å€¾æ–œæ˜¯Sparkåº”ç”¨ä¸­å¸¸è§çš„æ€§èƒ½ç“¶é¢ˆï¼Œè¡¨ç°ä¸ºæŸäº›åˆ†åŒºçš„æ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ—¶é—´ä¸å‡è¡¡ï¼Œæ•´ä½“ä½œä¸šå»¶è¿Ÿã€‚æœ‰æ•ˆè§£å†³æ•°æ®å€¾æ–œé—®é¢˜æ˜¯Sparkæ€§èƒ½ä¼˜åŒ–çš„å…³é”®ç¯èŠ‚ã€‚

**1. æ•°æ®å€¾æ–œçš„è¯†åˆ«**

åœ¨è§£å†³æ•°æ®å€¾æ–œå‰ï¼Œé¦–å…ˆéœ€è¦å‡†ç¡®è¯†åˆ«é—®é¢˜ï¼š

**è¯†åˆ«æ–¹æ³•**ï¼š
- **Spark UI**ï¼šè§‚å¯ŸStageé¡µé¢ä¸­ä»»åŠ¡æ‰§è¡Œæ—¶é—´åˆ†å¸ƒï¼Œå¦‚æœ‰æ˜æ˜¾"é•¿å°¾"ç°è±¡åˆ™å¯èƒ½å­˜åœ¨å€¾æ–œ
- **Shuffleç»Ÿè®¡**ï¼šæ£€æŸ¥Shuffleè¯»å†™æ•°æ®é‡åˆ†å¸ƒæ˜¯å¦å‡è¡¡
- **æ•°æ®é‡‡æ ·**ï¼šå¯¹å¯èƒ½çš„å€¾æ–œé”®è¿›è¡Œé‡‡æ ·åˆ†æï¼Œç¡®å®šçƒ­ç‚¹æ•°æ®

**å€¾æ–œç‰¹å¾**ï¼š
```
Taskæ‰§è¡Œæ—¶é—´åˆ†å¸ƒï¼š
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 12s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 13s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 11s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 120s  <- æ˜æ˜¾çš„å€¾æ–œä»»åŠ¡
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 14s
```

**2. æ•°æ®å€¾æ–œçš„æ ¹æœ¬åŸå› **

æ•°æ®å€¾æ–œé€šå¸¸ç”±ä»¥ä¸‹åŸå› å¯¼è‡´ï¼š

```mermaid
graph TD
    A[æ•°æ®å€¾æ–œæ ¹å› ] --> B[æ•°æ®æœ¬èº«åˆ†å¸ƒä¸å‡]
    A --> C[ä¸šåŠ¡é€»è¾‘å¯¼è‡´]
    A --> D[æŠ€æœ¯å®ç°é—®é¢˜]
    
    B --> B1[çƒ­ç‚¹é”®/å€¼]
    B --> B2[å¼‚å¸¸æ•°æ®]
    
    C --> C1[æ—¶é—´ç»´åº¦èšåˆ]
    C --> C2[åœ°åŸŸç»´åº¦èšåˆ]
    
    D --> D1[é»˜è®¤åˆ†åŒºå™¨é—®é¢˜]
    D --> D2[å¹¶è¡Œåº¦è®¾ç½®ä¸å½“]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
```

**3. è§£å†³æ–¹æ¡ˆåˆ†ç±»**

æ ¹æ®å€¾æ–œåœºæ™¯å’ŒåŸå› ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼š

| å€¾æ–œåœºæ™¯ | è§£å†³æ–¹æ¡ˆ | é€‚ç”¨æ¡ä»¶ | ä¼˜ç¼ºç‚¹ |
|---------|---------|---------|--------|
| **Joinå€¾æ–œ** | å¹¿æ’­Join | ä¸€ä¾§æ•°æ®é›†è¾ƒå°(<10GB) | ç®€å•é«˜æ•ˆï¼Œä½†å—å†…å­˜é™åˆ¶ |
| **Joinå€¾æ–œ** | æ‹†åˆ†çƒ­ç‚¹é”® | èƒ½è¯†åˆ«å‡ºçƒ­ç‚¹é”® | é’ˆå¯¹æ€§å¼ºï¼Œä½†å®ç°å¤æ‚ |
| **Joinå€¾æ–œ** | éšæœºå‰ç¼€+æ‰©å®¹ | çƒ­ç‚¹é”®è¾ƒå¤š | é€šç”¨æ€§å¥½ï¼Œä½†å¢åŠ è®¡ç®—é‡ |
| **èšåˆå€¾æ–œ** | ä¸¤é˜¶æ®µèšåˆ | èšåˆæ“ä½œ(å¦‚reduceByKey) | æ•ˆæœå¥½ï¼Œé€‚ç”¨é¢å¹¿ |
| **èšåˆå€¾æ–œ** | è‡ªå®šä¹‰åˆ†åŒº | æ•°æ®åˆ†å¸ƒå·²çŸ¥ | ç²¾ç¡®æ§åˆ¶ï¼Œä½†éœ€å®šåˆ¶å¼€å‘ |
| **æ•°æ®æºå€¾æ–œ** | é¢„å¤„ç†é‡åˆ†åŒº | ETLé˜¶æ®µå¯æ§ | æ²»æœ¬æ–¹æ³•ï¼Œä½†å¢åŠ å‰ç½®å¤„ç† |

**4. è¯¦ç»†è§£å†³æ–¹æ¡ˆ**

**4.1 Joinæ“ä½œå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šå¹¿æ’­Join**
```scala
// ä¼˜åŒ–å‰
val result = largeDF.join(smallDF, "key")

// ä¼˜åŒ–å
import org.apache.spark.sql.functions.broadcast
val result = largeDF.join(broadcast(smallDF), "key")
```

**æ–¹æ¡ˆäºŒï¼šæ‹†åˆ†çƒ­ç‚¹é”®**
```scala
// å‡è®¾å‘ç°"000"æ˜¯çƒ­ç‚¹é”®
// 1. å°†å¤§è¡¨ä¸­çƒ­ç‚¹é”®å¯¹åº”çš„æ•°æ®æ‹†åˆ†å‡ºæ¥
val largeDF_normal = largeDF.filter($"key" =!= "000")
val largeDF_skew = largeDF.filter($"key" === "000")
  .withColumn("key_random", concat($"key", lit("_"), rand()*10))

// 2. å°†å°è¡¨å¯¹åº”çƒ­ç‚¹é”®æ•°æ®æ‰©å®¹
val smallDF_normal = smallDF.filter($"key" =!= "000")
val smallDF_skew = smallDF.filter($"key" === "000")
  .withColumn("key_random", 
    explode(array((0 until 10).map(i => concat($"key", lit("_"), lit(i))): _*)))

// 3. åˆ†åˆ«Joinååˆå¹¶ç»“æœ
val join1 = largeDF_normal.join(smallDF_normal, "key")
val join2 = largeDF_skew.join(smallDF_skew, 
  largeDF_skew("key_random") === smallDF_skew("key_random"))
  .drop("key_random")

val result = join1.union(join2)
```

**æ–¹æ¡ˆä¸‰ï¼šéšæœºå‰ç¼€+æ‰©å®¹Join**
```scala
// 1. å¤§è¡¨æ·»åŠ éšæœºå‰ç¼€
val largeDF_rand = largeDF.withColumn("prefix", (rand()*10).cast("int"))
  .withColumn("key_prefixed", concat(col("prefix").cast("string"), lit("_"), col("key")))

// 2. å°è¡¨æ‰©å®¹10å€
val smallDF_expanded = smallDF.withColumn("prefix", 
  explode(array((0 until 10).map(lit(_)): _*)))
  .withColumn("key_prefixed", concat(col("prefix").cast("string"), lit("_"), col("key")))

// 3. åœ¨prefixed keyä¸ŠJoin
val joinResult = largeDF_rand.join(smallDF_expanded, "key_prefixed")
  .drop("prefix", "key_prefixed")
```

**4.2 èšåˆæ“ä½œå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šä¸¤é˜¶æ®µèšåˆ**
```scala
// ä¼˜åŒ–å‰
val result = rdd.reduceByKey(_ + _)

// ä¼˜åŒ–å
val result = rdd
  // ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨èšåˆï¼ŒåŠ éšæœºå‰ç¼€
  .map(x => ((x._1, Random.nextInt(100)), x._2))
  .reduceByKey(_ + _)
  // ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆï¼Œå»é™¤éšæœºå‰ç¼€
  .map(x => (x._1._1, x._2))
  .reduceByKey(_ + _)
```

**æ–¹æ¡ˆäºŒï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨**
```scala
// å®šä¹‰è‡ªå®šä¹‰åˆ†åŒºå™¨
class BalancedPartitioner(partitions: Int) extends Partitioner {
  def numPartitions: Int = partitions
  
  def getPartition(key: Any): Int = {
    val k = key.toString
    // å¯¹çƒ­ç‚¹é”®ç‰¹æ®Šå¤„ç†
    if (k == "hot_key_1") {
      Math.abs(Random.nextInt() % partitions)
    } else {
      Math.abs(k.hashCode % partitions)
    }
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
val result = rdd
  .partitionBy(new BalancedPartitioner(100))
  .reduceByKey(_ + _)
```

**4.3 æ•°æ®æºå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šé¢„å¤„ç†è¿‡æ»¤å¼‚å¸¸æ•°æ®**
```scala
// è¿‡æ»¤æ‰å¯èƒ½å¯¼è‡´å€¾æ–œçš„å¼‚å¸¸å€¼
val cleanedDF = rawDF.filter($"key".isNotNull && $"key" =!= "")
```

**æ–¹æ¡ˆäºŒï¼šé¢„èšåˆå¤„ç†**
```scala
// åœ¨ETLé˜¶æ®µè¿›è¡Œé¢„èšåˆ
val preAggregatedDF = rawDF
  .repartition(200, $"date", $"region")  // å…ˆæŒ‰éå€¾æ–œç»´åº¦é‡åˆ†åŒº
  .groupBy($"date", $"region", $"user_id")  // ä½ç²’åº¦é¢„èšåˆ
  .agg(sum($"value").as("value"))
```

**5. å®é™…æ¡ˆä¾‹åˆ†æ**

**æ¡ˆä¾‹ï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†æä¸­çš„æ•°æ®å€¾æ–œ**

**é—®é¢˜æè¿°**ï¼š
åœ¨ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æä¸­ï¼Œéœ€è¦ç»Ÿè®¡æ¯ä¸ªå•†å“çš„ç‚¹å‡»æ¬¡æ•°ï¼Œä½†æŸäº›çƒ­é—¨å•†å“çš„ç‚¹å‡»é‡è¿œé«˜äºå…¶ä»–å•†å“ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```scala
// åŸå§‹ä»£ç 
val clickCounts = userClicks
  .groupBy("product_id")
  .count()

// ä¼˜åŒ–åä»£ç 
// 1. æ•°æ®é‡‡æ ·ï¼Œè¯†åˆ«çƒ­ç‚¹å•†å“
val sampleDF = userClicks.sample(0.1)
val hotProducts = sampleDF
  .groupBy("product_id")
  .count()
  .orderBy($"count".desc)
  .limit(10)
  .collect()
  .map(_.getAs[String]("product_id"))
  .toSet
val bcHotProducts = spark.sparkContext.broadcast(hotProducts)

// 2. å¯¹çƒ­ç‚¹å•†å“ç‰¹æ®Šå¤„ç†
val processedClicks = userClicks.mapPartitions(iter => {
  val hotProds = bcHotProducts.value
  iter.map(row => {
    val productId = row.getAs[String]("product_id")
    if (hotProds.contains(productId)) {
      // ä¸ºçƒ­ç‚¹å•†å“æ·»åŠ éšæœºåç¼€
      Row.fromSeq(row.toSeq :+ (productId + "_" + Random.nextInt(100)))
    } else {
      // éçƒ­ç‚¹å•†å“ä¿æŒä¸å˜
      Row.fromSeq(row.toSeq :+ productId)
    }
  })
}, true)

// 3. ä½¿ç”¨å¤„ç†åçš„é”®è¿›è¡Œèšåˆ
val schema = userClicks.schema.add("balanced_key", StringType)
val balancedDF = spark.createDataFrame(processedClicks, schema)

val result = balancedDF
  .groupBy("balanced_key")
  .count()
  // å»é™¤éšæœºåç¼€ï¼Œæ¢å¤åŸå§‹å•†å“ID
  .withColumn("product_id", 
    when($"balanced_key".contains("_"), 
      split($"balanced_key", "_").getItem(0))
    .otherwise($"balanced_key"))
  .groupBy("product_id")
  .sum("count")
  .drop("balanced_key")
```

**6. é¢„é˜²æ•°æ®å€¾æ–œçš„æœ€ä½³å®è·µ**

1. **åˆç†è®¾è®¡é”®**ï¼šé¿å…ä½¿ç”¨å¯èƒ½äº§ç”Ÿçƒ­ç‚¹çš„é”®ï¼ˆå¦‚æ—¶é—´æˆ³ç²¾ç¡®åˆ°ç§’ï¼‰
2. **æå‰é¢„ä¼°**ï¼šåœ¨å¼€å‘å‰è¯„ä¼°æ•°æ®åˆ†å¸ƒæƒ…å†µ
3. **ç›‘æ§æœºåˆ¶**ï¼šå»ºç«‹ä»»åŠ¡ç›‘æ§ï¼ŒåŠæ—¶å‘ç°å€¾æ–œé—®é¢˜
4. **æ•°æ®è´¨é‡**ï¼šåœ¨æ•°æ®æ¥å…¥é˜¶æ®µå¤„ç†å¼‚å¸¸å€¼å’Œç©ºå€¼
5. **å¹¶è¡Œåº¦**ï¼šè®¾ç½®åˆç†çš„å¹¶è¡Œåº¦ï¼Œé¿å…åˆ†åŒºè¿‡å°‘

æœ‰æ•ˆè§£å†³æ•°æ®å€¾æ–œé—®é¢˜éœ€è¦ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯å’Œæ•°æ®ç‰¹ç‚¹ï¼Œçµæ´»è¿ç”¨å„ç§æŠ€æœ¯æ‰‹æ®µï¼Œä»æ ¹æœ¬ä¸Šä¼˜åŒ–æ•°æ®åˆ†å¸ƒï¼Œæé«˜Sparkä½œä¸šçš„æ‰§è¡Œæ•ˆç‡ã€‚

### å®æˆ˜åº”ç”¨é¢˜

**Q8: è¯·ä»‹ç»Spark SQLçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ä½•æé«˜SQLæŸ¥è¯¢æ€§èƒ½ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Spark SQLæ˜¯Sparkç”Ÿæ€ç³»ç»Ÿä¸­çš„é‡è¦ç»„ä»¶ï¼Œå®ƒæä¾›äº†ç»“æ„åŒ–æ•°æ®å¤„ç†èƒ½åŠ›å’ŒSQLæŸ¥è¯¢æ¥å£ã€‚é€šè¿‡ä¸€ç³»åˆ—ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡SQLæŸ¥è¯¢æ€§èƒ½ã€‚

**1. Spark SQLä¼˜åŒ–æŠ€æœ¯æ¦‚è¿°**

Spark SQLä¼˜åŒ–ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
- **Catalystä¼˜åŒ–å™¨**ï¼šåŸºäºè§„åˆ™å’Œæˆæœ¬çš„æŸ¥è¯¢ä¼˜åŒ–
- **Tungstenæ‰§è¡Œå¼•æ“**ï¼šå†…å­˜ç®¡ç†å’Œä»£ç ç”Ÿæˆä¼˜åŒ–
- **å‚æ•°é…ç½®**ï¼šé’ˆå¯¹ç‰¹å®šåœºæ™¯çš„å‚æ•°è°ƒæ•´

**2. å…³é”®ä¼˜åŒ–é…ç½®**

```scala
// å¼€å¯è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ(AQE)
spark.conf.set("spark.sql.adaptive.enabled", "true")
// å¯ç”¨å°åˆ†åŒºåˆå¹¶
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
// è®¾ç½®åˆå¹¶åçš„ç›®æ ‡åˆ†åŒºå¤§å°
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128m")

// å¹¿æ’­Joinä¼˜åŒ–
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")
// å¯ç”¨AQEä¼˜åŒ–çš„Joinç­–ç•¥
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
// å¯ç”¨Joiné‡æ’åº
spark.conf.set("spark.sql.optimizer.joinReorder.enabled", "true")
```

**3. æŸ¥è¯¢ä¼˜åŒ–å®ä¾‹**

```scala
// ä¼˜åŒ–å‰
val result = spark.sql("""
  SELECT c.customer_name, sum(o.order_amount) as total_amount
  FROM orders o
  JOIN customers c ON o.customer_id = c.customer_id
  WHERE o.order_date > '2023-01-01'
  GROUP BY c.customer_name
""")

// ä¼˜åŒ–å
// 1. ä½¿ç”¨å¹¿æ’­Join
val customers = spark.table("customers")
val orders = spark.table("orders").filter($"order_date" > "2023-01-01")
import org.apache.spark.sql.functions.broadcast
val result = orders.join(broadcast(customers), "customer_id")
  .groupBy($"customer_name")
  .agg(sum($"order_amount").as("total_amount"))

// 2. å¯ç”¨AQEå’Œå…¶ä»–ä¼˜åŒ–
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
val result = spark.sql("""
  SELECT /*+ BROADCAST(c) */ 
    c.customer_name, sum(o.order_amount) as total_amount
  FROM orders o
  JOIN customers c ON o.customer_id = c.customer_id
  WHERE o.order_date > '2023-01-01'
  GROUP BY c.customer_name
""")
```

**4. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ**

- **æ•°æ®æ ¼å¼é€‰æ‹©**ï¼šä½¿ç”¨åˆ—å¼å­˜å‚¨æ ¼å¼ï¼ˆParquetã€ORCï¼‰
- **åˆ†åŒºç­–ç•¥**ï¼šæ ¹æ®æŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆé€‚çš„åˆ†åŒºé”®
- **é¢„èšåˆ**ï¼šå¯¹å¸¸ç”¨æŸ¥è¯¢åˆ›å»ºç‰©åŒ–è§†å›¾
- **ç¼“å­˜ç®¡ç†**ï¼šç¼“å­˜é¢‘ç¹ä½¿ç”¨çš„è¡¨æˆ–æŸ¥è¯¢ç»“æœ
- **SQL Hint**ï¼šä½¿ç”¨æŸ¥è¯¢æç¤ºæŒ‡å¯¼ä¼˜åŒ–å™¨

é€šè¿‡ç»¼åˆåº”ç”¨è¿™äº›ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡Spark SQLæŸ¥è¯¢æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚

**Q9: å½“Sparkåº”ç”¨å‡ºç°æ•…éšœæˆ–æ€§èƒ½é—®é¢˜æ—¶ï¼Œå¦‚ä½•è¿›è¡Œæ’æŸ¥å’Œè§£å†³ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkåº”ç”¨æ•…éšœæ’æŸ¥æ˜¯ä¸€é¡¹ç³»ç»Ÿæ€§å·¥ä½œï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦æ”¶é›†ä¿¡æ¯ï¼Œåˆ†ææ ¹å› ï¼Œå¹¶é‡‡å–ç›¸åº”çš„è§£å†³æªæ–½ã€‚

**1. æ•…éšœæ’æŸ¥æ–¹æ³•è®º**

æœ‰æ•ˆçš„æ•…éšœæ’æŸ¥éœ€è¦éµå¾ªä»¥ä¸‹æ–¹æ³•è®ºï¼š
- **ç³»ç»Ÿæ€§åˆ†æ**ï¼šä»åº”ç”¨ã€é›†ç¾¤åˆ°èµ„æºå…¨é¢è€ƒè™‘
- **æ•°æ®é©±åŠ¨**ï¼šåŸºäºæ—¥å¿—å’Œç›‘æ§æ•°æ®è¿›è¡Œåˆ†æ
- **é€å±‚æ’é™¤**ï¼šä»å¤–åˆ°å†…æˆ–ä»å†…åˆ°å¤–é€å±‚æ’æŸ¥
- **å¤ç°éªŒè¯**ï¼šå°è¯•å¤ç°é—®é¢˜å¹¶éªŒè¯è§£å†³æ–¹æ¡ˆ

**2. æ’æŸ¥æ­¥éª¤è¯¦è§£**

1. **æŸ¥çœ‹Spark UI**
   - åˆ†æå¤±è´¥çš„Stageå’ŒTask
   - æ£€æŸ¥Jobæ‰§è¡Œæ—¶é—´å’Œèµ„æºä½¿ç”¨æƒ…å†µ
   - è¯†åˆ«å¼‚å¸¸çš„æ‰§è¡Œæ¨¡å¼ï¼ˆå¦‚æ•°æ®å€¾æ–œï¼‰

2. **æ£€æŸ¥æ—¥å¿—ä¿¡æ¯**
   - Driveræ—¥å¿—ï¼šåº”ç”¨çº§åˆ«é”™è¯¯å’Œå¼‚å¸¸
   - Executoræ—¥å¿—ï¼šä»»åŠ¡æ‰§è¡Œé”™è¯¯
   - Worker/Masteræ—¥å¿—ï¼šé›†ç¾¤çº§åˆ«é—®é¢˜
   - ç³»ç»Ÿæ—¥å¿—ï¼šèµ„æºå’Œç¯å¢ƒé—®é¢˜

3. **èµ„æºç›‘æ§åˆ†æ**
   - CPUä½¿ç”¨ç‡ï¼šæ˜¯å¦å­˜åœ¨è®¡ç®—ç“¶é¢ˆ
   - å†…å­˜ä½¿ç”¨ï¼šæ˜¯å¦å­˜åœ¨OOMæˆ–GCé—®é¢˜
   - ç£ç›˜I/Oï¼šæ˜¯å¦å­˜åœ¨å­˜å‚¨ç“¶é¢ˆ
   - ç½‘ç»œä¼ è¾“ï¼šæ˜¯å¦å­˜åœ¨ç½‘ç»œç“¶é¢ˆ

4. **å¸¸è§é—®é¢˜è¯Šæ–­**

| é—®é¢˜ç±»å‹ | ç—‡çŠ¶ | è¯Šæ–­æ–¹æ³• | å¯èƒ½è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|------------|
| **OOMé”™è¯¯** | `java.lang.OutOfMemoryError` | æ£€æŸ¥GCæ—¥å¿—ï¼Œå†…å­˜ä½¿ç”¨è¶‹åŠ¿ | å¢åŠ å†…å­˜ï¼Œè°ƒæ•´åˆ†åŒºï¼Œä¼˜åŒ–ä»£ç  |
| **æ•°æ®å€¾æ–œ** | å°‘æ•°ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿œé•¿äºå…¶ä»–ä»»åŠ¡ | æŸ¥çœ‹Stageè¯¦æƒ…ï¼Œåˆ†ææ•°æ®åˆ†å¸ƒ | åŠ ç›å¤„ç†ï¼Œé¢„èšåˆï¼Œè°ƒæ•´åˆ†åŒº |
| **åºåˆ—åŒ–é”™è¯¯** | `java.io.NotSerializableException` | æ£€æŸ¥ç±»çš„åºåˆ—åŒ–å®ç° | å®ç°Serializableæ¥å£ï¼Œä½¿ç”¨@transientæ³¨è§£ |
| **Shuffleå¤±è´¥** | `FetchFailedException` | æ£€æŸ¥Shuffleå†™å…¥å’Œè¯»å–æ—¥å¿— | å¢åŠ å†…å­˜ï¼Œè°ƒæ•´Shuffleå‚æ•° |
| **èµ„æºä¸è¶³** | ä»»åŠ¡æ’é˜Ÿï¼Œæ‰§è¡Œç¼“æ…¢ | æŸ¥çœ‹é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ | å¢åŠ èµ„æºï¼Œä¼˜åŒ–èµ„æºåˆ†é… |

**3. æ€§èƒ½é—®é¢˜æ’æŸ¥å·¥å…·**

```bash
# æŸ¥çœ‹Sparkåº”ç”¨æ—¥å¿—
yarn logs -applicationId application_1234567890_0001

# ä½¿ç”¨jstackæŸ¥çœ‹JVMçº¿ç¨‹çŠ¶æ€
jstack <pid> > thread_dump.txt

# ä½¿ç”¨jmapæŸ¥çœ‹å†…å­˜ä½¿ç”¨
jmap -heap <pid>

# ä½¿ç”¨Spark History ServeræŸ¥çœ‹å†å²åº”ç”¨
http://history-server:18080
```

**4. å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ**

```scala
// è§£å†³OOMé—®é¢˜
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

// è§£å†³Shuffleé—®é¢˜
spark.conf.set("spark.shuffle.file.buffer", "64k")
spark.conf.set("spark.reducer.maxSizeInFlight", "96m")
spark.conf.set("spark.shuffle.io.maxRetries", "10")

// è§£å†³æ•°æ®å€¾æ–œ
// å‚è§æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ
```

**5. é¢„é˜²æªæ–½**

- **ç›‘æ§ç³»ç»Ÿ**ï¼šå»ºç«‹åº”ç”¨å’Œé›†ç¾¤ç›‘æ§
- **æ€§èƒ½æµ‹è¯•**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒå‰è¿›è¡Œå‹åŠ›æµ‹è¯•
- **æ¸è¿›å¼éƒ¨ç½²**ï¼šå…ˆå°è§„æ¨¡æµ‹è¯•ï¼Œå†æ‰©å¤§è§„æ¨¡
- **å®¹é‡è§„åˆ’**ï¼šæ ¹æ®æ•°æ®å¢é•¿é¢„ä¼°èµ„æºéœ€æ±‚

é€šè¿‡ç³»ç»Ÿæ€§çš„æ•…éšœæ’æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–ï¼Œå¯ä»¥æé«˜Sparkåº”ç”¨çš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œå‡å°‘ç”Ÿäº§ç¯å¢ƒä¸­çš„é—®é¢˜å‘ç”Ÿã€‚

### æ·±åº¦æŠ€æœ¯åŸç†é¢˜

**Q10: è¯·è¯¦ç»†è§£é‡ŠSparkçš„Catalystä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†åŠå…¶ä¼˜åŒ–è§„åˆ™ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Catalystä¼˜åŒ–å™¨æ˜¯Spark SQLçš„æ ¸å¿ƒä¼˜åŒ–å¼•æ“ï¼Œå®ƒåŸºäºScalaçš„æ¨¡å¼åŒ¹é…å’Œå‡½æ•°å¼ç¼–ç¨‹ç‰¹æ€§æ„å»ºï¼Œä¸ºSpark SQLæä¾›äº†å¼ºå¤§çš„æŸ¥è¯¢ä¼˜åŒ–èƒ½åŠ›ã€‚ç†è§£Catalystçš„å·¥ä½œåŸç†å¯¹äºç¼–å†™é«˜æ•ˆçš„Spark SQLåº”ç”¨è‡³å…³é‡è¦ã€‚

**1. Catalystä¼˜åŒ–å™¨æ¶æ„**

Catalystä¼˜åŒ–å™¨çš„æ ¸å¿ƒæ¶æ„åŒ…æ‹¬ä»¥ä¸‹ç»„ä»¶ï¼š
- **æ ‘èŠ‚ç‚¹è½¬æ¢æ¡†æ¶**ï¼šåŸºäºScalaæ¨¡å¼åŒ¹é…çš„æ ‘è½¬æ¢æœºåˆ¶
- **è§„åˆ™æ‰§è¡Œå¼•æ“**ï¼šåº”ç”¨ä¼˜åŒ–è§„åˆ™çš„æ‰§è¡Œå™¨
- **æˆæœ¬æ¨¡å‹**ï¼šè¯„ä¼°ä¸åŒæ‰§è¡Œè®¡åˆ’æ€§èƒ½çš„æ¨¡å‹
- **ä»£ç ç”Ÿæˆå¼•æ“**ï¼šå°†ç‰©ç†è®¡åˆ’è½¬æ¢ä¸ºé«˜æ•ˆæ‰§è¡Œä»£ç 

**2. ä¼˜åŒ–æµç¨‹è¯¦è§£**

```mermaid
graph TD
    A[SQL/DataFrame API] --> B["æŠ½è±¡è¯­æ³•æ ‘(AST)"]
    B --> C[æœªè§£æé€»è¾‘è®¡åˆ’]
    C --> D[è§£æé€»è¾‘è®¡åˆ’]
    D --> E[ä¼˜åŒ–é€»è¾‘è®¡åˆ’]
    E --> F[ç‰©ç†è®¡åˆ’ç”Ÿæˆ]
    F --> G[ç‰©ç†è®¡åˆ’ä¼˜åŒ–]
    G --> H[é€‰æ‹©æœ€ä¼˜è®¡åˆ’]
    H --> I[ä»£ç ç”Ÿæˆ]
    I --> J[æ‰§è¡Œ]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#d4f1f9,stroke:#05a4d1
    style D fill:#ffe6cc,stroke:#d79b00
    style E fill:#ffe6cc,stroke:#d79b00
    style F fill:#d5e8d4,stroke:#82b366
    style G fill:#d5e8d4,stroke:#82b366
    style H fill:#d5e8d4,stroke:#82b366
    style I fill:#e1d5e7,stroke:#9673a6
    style J fill:#f8cecc,stroke:#b85450
```

**3. ä¼˜åŒ–é˜¶æ®µè¯¦ç»†è¯´æ˜**

1. **è¯­æ³•åˆ†æ**
   - å°†SQLè¯­å¥è§£æä¸ºæŠ½è±¡è¯­æ³•æ ‘(AST)
   - ä½¿ç”¨ANTLRè¯­æ³•è§£æå™¨å¤„ç†SQLè¯­æ³•
   - è½¬æ¢DataFrame/Dataset APIè°ƒç”¨ä¸ºå†…éƒ¨è¡¨ç¤º

2. **é€»è¾‘è®¡åˆ’ç”Ÿæˆä¸è§£æ**
   - å°†ASTè½¬æ¢ä¸ºæœªè§£æé€»è¾‘è®¡åˆ’
   - é€šè¿‡Catalogè§£æè¡¨åã€åˆ—åå’Œå‡½æ•°å
   - è¿›è¡Œç±»å‹æ¨æ–­å’Œç±»å‹æ£€æŸ¥

3. **é€»è¾‘è®¡åˆ’ä¼˜åŒ–**
   - åº”ç”¨åŸºäºè§„åˆ™çš„ä¼˜åŒ–ç­–ç•¥
   - ä¼˜åŒ–è½¬æ¢æ˜¯å£°æ˜å¼çš„ï¼ŒåŸºäºæ¨¡å¼åŒ¹é…
   - å¤šè½®åº”ç”¨è§„åˆ™ç›´è‡³è®¡åˆ’ç¨³å®š

4. **ç‰©ç†è®¡åˆ’ç”Ÿæˆ**
   - å°†é€»è¾‘ç®—å­è½¬æ¢ä¸ºç‰©ç†ç®—å­
   - ä¸ºåŒä¸€é€»è¾‘æ“ä½œç”Ÿæˆå¤šç§ç‰©ç†å®ç°
   - å¦‚Sortå¯å®ç°ä¸ºSortExecæˆ–ExternalSortExec

5. **ç‰©ç†è®¡åˆ’ä¼˜åŒ–ä¸é€‰æ‹©**
   - ä½¿ç”¨åŸºäºæˆæœ¬çš„ä¼˜åŒ–å™¨è¯„ä¼°è®¡åˆ’
   - è€ƒè™‘æ•°æ®å¤§å°ã€æ“ä½œå¤æ‚åº¦ç­‰å› ç´ 
   - é€‰æ‹©æˆæœ¬æœ€ä½çš„æ‰§è¡Œè®¡åˆ’

6. **ä»£ç ç”Ÿæˆ**
   - ä½¿ç”¨Janinoç¼–è¯‘å™¨ç”ŸæˆJavaå­—èŠ‚ç 
   - å°†å¤šä¸ªæ“ä½œèåˆä¸ºå•ä¸ªå‡½æ•°
   - å‡å°‘è™šå‡½æ•°è°ƒç”¨å’Œè§£é‡Šå¼€é”€

**4. æ ¸å¿ƒä¼˜åŒ–è§„åˆ™è¯¦è§£**

| ä¼˜åŒ–è§„åˆ™ | æè¿° | ç¤ºä¾‹ |
|---------|------|------|
| **è°“è¯ä¸‹æ¨** | å°†è¿‡æ»¤æ¡ä»¶å°½æ—©åº”ç”¨ï¼Œå‡å°‘æ•°æ®é‡ | `SELECT * FROM t1 JOIN t2 WHERE t1.id > 100` â†’ å…ˆè¿‡æ»¤t1.id > 100å†Join |
| **åˆ—è£å‰ª** | åªè¯»å–å’Œå¤„ç†æŸ¥è¯¢æ‰€éœ€çš„åˆ— | `SELECT name FROM (SELECT id, name, age FROM users)` â†’ åªè¯»å–nameåˆ— |
| **å¸¸é‡æŠ˜å ** | ç¼–è¯‘æ—¶è®¡ç®—å¸¸é‡è¡¨è¾¾å¼ | `SELECT id + 5 * 10 FROM t` â†’ `SELECT id + 50 FROM t` |
| **Joiné‡æ’åº** | ä¼˜åŒ–å¤šè¡¨Joinçš„é¡ºåº | å°è¡¨å…ˆJoinï¼Œå‡å°‘ä¸­é—´ç»“æœ |
| **Joiné€‰æ‹©** | æ ¹æ®è¡¨å¤§å°é€‰æ‹©Joinç­–ç•¥ | å°è¡¨ä½¿ç”¨BroadcastHashJoinï¼Œå¤§è¡¨ä½¿ç”¨SortMergeJoin |
| **åˆ†åŒºè£å‰ª** | åªè¯»å–åŒ…å«æ‰€éœ€æ•°æ®çš„åˆ†åŒº | `WHERE date='2023-01-01'` â†’ åªè¯»å–è¯¥æ—¥æœŸçš„åˆ†åŒº |
| **èšåˆä¼˜åŒ–** | éƒ¨åˆ†èšåˆ+æœ€ç»ˆèšåˆ | å…ˆåœ¨æ¯ä¸ªåˆ†åŒºå†…èšåˆï¼Œå†å…¨å±€èšåˆ |

**5. ä»£ç ç¤ºä¾‹ï¼šCatalystè½¬æ¢è¿‡ç¨‹**

```scala
// ç¤ºä¾‹æŸ¥è¯¢
val query = spark.sql("""
  SELECT c.name, sum(o.amount) as total
  FROM orders o
  JOIN customers c ON o.customer_id = c.id
  WHERE o.date > '2023-01-01'
  GROUP BY c.name
  HAVING sum(o.amount) > 1000
""")

// æŸ¥çœ‹é€»è¾‘è®¡åˆ’
println("Logical Plan:")
query.queryExecution.logical.explain(true)

// æŸ¥çœ‹ä¼˜åŒ–åçš„é€»è¾‘è®¡åˆ’
println("Optimized Logical Plan:")
query.queryExecution.optimizedPlan.explain(true)

// æŸ¥çœ‹ç‰©ç†è®¡åˆ’
println("Physical Plan:")
query.queryExecution.sparkPlan.explain(true)

// æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
println("Executed Plan:")
query.queryExecution.executedPlan.explain(true)
```

**6. è‡ªå®šä¹‰ä¼˜åŒ–è§„åˆ™**

Catalystå…è®¸å¼€å‘è€…æ‰©å±•ä¼˜åŒ–è§„åˆ™ï¼Œå®ç°è‡ªå®šä¹‰ä¼˜åŒ–ï¼š

```scala
// è‡ªå®šä¹‰ä¼˜åŒ–è§„åˆ™ç¤ºä¾‹
object MyOptimizationRule extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    case Filter(condition, child) if canOptimize(condition) =>
      // è‡ªå®šä¹‰ä¼˜åŒ–é€»è¾‘
      OptimizedFilter(optimizeCondition(condition), child)
  }
}

// æ³¨å†Œè‡ªå®šä¹‰è§„åˆ™
spark.experimental.extraOptimizations = Seq(MyOptimizationRule)
```

Catalystä¼˜åŒ–å™¨æ˜¯Spark SQLæ€§èƒ½ä¼˜è¶Šçš„å…³é”®å› ç´ ï¼Œé€šè¿‡ç†è§£å…¶å·¥ä½œåŸç†å’Œä¼˜åŒ–è§„åˆ™ï¼Œå¯ä»¥ç¼–å†™æ›´é«˜æ•ˆçš„Spark SQLåº”ç”¨ï¼Œå¹¶åœ¨å¿…è¦æ—¶é€šè¿‡è‡ªå®šä¹‰è§„åˆ™è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**Q11: è¯·è¯¦ç»†ä»‹ç»Sparkå†…å­˜ç®¡ç†çš„æ¼”è¿›å†å²ï¼Œæ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkå†…å­˜ç®¡ç†æœºåˆ¶ç»å†äº†é‡è¦çš„æ¼”è¿›è¿‡ç¨‹ï¼Œä»æ—©æœŸçš„é™æ€å†…å­˜ç®¡ç†åˆ°ç°ä»£çš„ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œæ˜¾è‘—æå‡äº†å†…å­˜åˆ©ç”¨ç‡å’Œåº”ç”¨æ€§èƒ½ã€‚

**1. å†…å­˜ç®¡ç†æ¼”è¿›çš„èƒŒæ™¯**

Sparkä½œä¸ºå†…å­˜è®¡ç®—æ¡†æ¶ï¼Œå…¶æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå†…å­˜ç®¡ç†æ•ˆç‡ã€‚æ—©æœŸç‰ˆæœ¬çš„å†…å­˜ç®¡ç†æœºåˆ¶å­˜åœ¨è¯¸å¤šé—®é¢˜ï¼Œå¦‚å†…å­˜åˆ’åˆ†å›ºå®šã€é…ç½®å¤æ‚ã€èµ„æºåˆ©ç”¨ç‡ä½ç­‰ï¼Œè¿™ä¿ƒä½¿Sparkå›¢é˜Ÿä¸æ–­ä¼˜åŒ–å†…å­˜ç®¡ç†æœºåˆ¶ã€‚

**2. é™æ€å†…å­˜ç®¡ç†ï¼ˆStatic Memory Managementï¼‰**

é™æ€å†…å­˜ç®¡ç†æ˜¯Spark 1.6ä¹‹å‰çš„é»˜è®¤æ¨¡å¼ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼š

- **å›ºå®šå†…å­˜åˆ’åˆ†**ï¼šé¢„å…ˆä¸ºä¸åŒç”¨é€”åˆ’åˆ†å›ºå®šæ¯”ä¾‹çš„å†…å­˜
- **ä¸¥æ ¼è¾¹ç•Œ**ï¼šå„å†…å­˜åŒºåŸŸä¹‹é—´ä¸èƒ½åŠ¨æ€è°ƒæ•´
- **æ‰‹åŠ¨é…ç½®**ï¼šéœ€è¦ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå‚æ•°

**é™æ€å†…å­˜åˆ’åˆ†**ï¼š
```
+---------------------------+---------------------------+---------------+
|     Storage Memory        |     Execution Memory      |  Other Memory |
| spark.storage.memoryFraction  | spark.shuffle.memoryFraction |  Remainder    |
|        (é»˜è®¤0.6)          |        (é»˜è®¤0.2)          |    (0.2)      |
+---------------------------+---------------------------+---------------+
```

**å…³é”®å‚æ•°**ï¼š
```scala
// é™æ€å†…å­˜ç®¡ç†å…³é”®å‚æ•°
spark.storage.memoryFraction = 0.6  // ç¼“å­˜RDDæ•°æ®çš„å†…å­˜æ¯”ä¾‹
spark.storage.unrollFraction = 0.2  // ç”¨äºå±•å¼€RDDçš„å†…å­˜æ¯”ä¾‹
spark.shuffle.memoryFraction = 0.2  // Shuffleæ“ä½œçš„å†…å­˜æ¯”ä¾‹
```

**3. ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼ˆUnified Memory Managementï¼‰**

ç»Ÿä¸€å†…å­˜ç®¡ç†æ˜¯Spark 1.6åŠä¹‹åçš„é»˜è®¤æ¨¡å¼ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼š

- **åŠ¨æ€å†…å­˜å…±äº«**ï¼šStorageå’ŒExecutionå†…å­˜å¯ä»¥ç›¸äº’å€Ÿç”¨
- **ç®€åŒ–é…ç½®**ï¼šå‡å°‘é…ç½®å‚æ•°ï¼Œæ›´æ˜“ä½¿ç”¨
- **è‡ªé€‚åº”è°ƒæ•´**ï¼šæ ¹æ®è¿è¡Œæ—¶éœ€æ±‚åŠ¨æ€åˆ†é…å†…å­˜

**ç»Ÿä¸€å†…å­˜åˆ’åˆ†**ï¼š
```
+--------------------+----------------------+---------------+
|      Reserved      |      User Memory     |  Spark Memory |
|      (300MB)       | (1-spark.memory.fraction) | spark.memory.fraction |
+--------------------+----------------------+---------------+
                                            |
                     +----------------------+---------------+
                     |    Storage Memory    | Execution Memory |
                     | spark.memory.storageFraction | Remainder |
                     +----------------------+---------------+
```

**å…³é”®å‚æ•°**ï¼š
```scala
// ç»Ÿä¸€å†…å­˜ç®¡ç†å…³é”®å‚æ•°
spark.memory.fraction = 0.75  // Spark Memoryå JVMå †å†…å­˜çš„æ¯”ä¾‹
spark.memory.storageFraction = 0.5  // Storage Memoryåˆå§‹å æ¯”
```

**4. ä¸¤ç§æ¨¡å¼çš„æ ¸å¿ƒåŒºåˆ«**

| ç‰¹æ€§ | é™æ€å†…å­˜ç®¡ç† | ç»Ÿä¸€å†…å­˜ç®¡ç† |
|------|------------|------------|
| **å†…å­˜åˆ’åˆ†** | å›ºå®šæ¯”ä¾‹ï¼Œä¸å¯è°ƒæ•´ | åŠ¨æ€å…±äº«ï¼Œå¯ç›¸äº’å€Ÿç”¨ |
| **é…ç½®å¤æ‚åº¦** | å¤šå‚æ•°ï¼Œè°ƒä¼˜å¤æ‚ | å°‘é‡å‚æ•°ï¼Œç®€åŒ–é…ç½® |
| **å†…å­˜åˆ©ç”¨ç‡** | è¾ƒä½ï¼Œå¸¸æœ‰æµªè´¹ | è¾ƒé«˜ï¼ŒæŒ‰éœ€åˆ†é… |
| **é€‚ç”¨åœºæ™¯** | è´Ÿè½½ç¨³å®šï¼Œå¯é¢„æµ‹ | å¤šæ ·åŒ–è´Ÿè½½ï¼Œèµ„æºç«äº‰ |
| **æº¢å‡ºå¤„ç†** | ç›´æ¥æº¢å‡ºåˆ°ç£ç›˜ | å…ˆå°è¯•å€Ÿç”¨ï¼Œå†æº¢å‡º |
| **ç‰ˆæœ¬æ”¯æŒ** | 1.6ä¹‹å‰é»˜è®¤ | 1.6åŠä¹‹åé»˜è®¤ |

**5. å†…å­˜å€Ÿç”¨æœºåˆ¶è¯¦è§£**

åœ¨ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼ä¸‹ï¼š

1. **Storage Memoryä¸è¶³æ—¶**ï¼š
   - å¦‚æœExecution Memoryæœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨
   - å¦‚æœæ²¡æœ‰ç©ºé—²ï¼Œåˆ™æº¢å‡ºåˆ°ç£ç›˜

2. **Execution Memoryä¸è¶³æ—¶**ï¼š
   - å¦‚æœStorage Memoryæœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨
   - å¦‚æœStorage Memoryä¸­æœ‰éƒ¨åˆ†æ˜¯è¢«Execution Memoryå€Ÿç”¨çš„ï¼Œå¯ä»¥å¼ºåˆ¶æ”¶å›
   - Execution Memoryä¸ä¼šæ·˜æ±°Storage Memoryä¸­çš„æ•°æ®

**6. ä»£ç ç¤ºä¾‹ï¼šé…ç½®å¯¹æ¯”**

```scala
// é™æ€å†…å­˜ç®¡ç†é…ç½®ç¤ºä¾‹
spark.storage.memoryFraction = 0.6
spark.storage.unrollFraction = 0.2
spark.shuffle.memoryFraction = 0.2

// ç»Ÿä¸€å†…å­˜ç®¡ç†é…ç½®ç¤ºä¾‹
spark.memory.fraction = 0.75
spark.memory.storageFraction = 0.5
```

**7. å®é™…åº”ç”¨å»ºè®®**

- **å†…å­˜å¯†é›†å‹è®¡ç®—**ï¼šå¢åŠ `spark.memory.fraction`ï¼Œåˆ†é…æ›´å¤šå†…å­˜ç»™Spark
- **ç¼“å­˜ä¼˜å…ˆåœºæ™¯**ï¼šå¢åŠ `spark.memory.storageFraction`ï¼Œæé«˜ç¼“å­˜å®¹é‡
- **Shuffleå¯†é›†åœºæ™¯**ï¼šé™ä½`spark.memory.storageFraction`ï¼Œæä¾›æ›´å¤šæ‰§è¡Œå†…å­˜
- **ç›‘æ§å†…å­˜ä½¿ç”¨**ï¼šé€šè¿‡Spark UIç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µï¼ŒåŠæ—¶è°ƒæ•´å‚æ•°

Sparkå†…å­˜ç®¡ç†çš„æ¼”è¿›ä½“ç°äº†åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶å¯¹èµ„æºåˆ©ç”¨æ•ˆç‡çš„ä¸æ–­è¿½æ±‚ã€‚ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶æ˜¾è‘—æå‡äº†Sparkçš„å†…å­˜åˆ©ç”¨ç‡ï¼Œå‡å°‘äº†OOMé”™è¯¯ï¼Œç®€åŒ–äº†é…ç½®ï¼Œæ˜¯Sparkæ€§èƒ½ä¼˜åŒ–çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚

**Q12: è¯·è¯¦ç»†å¯¹æ¯”Spark Streamingå’ŒStructured Streamingçš„åŒºåˆ«ï¼Œå¹¶è¯´æ˜å„è‡ªçš„é€‚ç”¨åœºæ™¯ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæä¾›äº†ä¸¤ç§æµå¤„ç†å¼•æ“ï¼šSpark Streamingå’ŒStructured Streamingï¼Œå®ƒä»¬åœ¨è®¾è®¡ç†å¿µã€APIã€å¤„ç†æ¨¡å‹ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ·±å…¥ç†è§£è¿™äº›å·®å¼‚å¯¹äºé€‰æ‹©åˆé€‚çš„æµå¤„ç†æŠ€æœ¯è‡³å…³é‡è¦ã€‚

**1. åŸºæœ¬æ¦‚å¿µå¯¹æ¯”**

- **Spark Streaming**ï¼š
  Spark 0.7ç‰ˆæœ¬å¼•å…¥çš„ç¬¬ä¸€ä»£æµå¤„ç†å¼•æ“ï¼ŒåŸºäºRDDæ¨¡å‹ï¼Œé‡‡ç”¨å¾®æ‰¹å¤„ç†æ¶æ„ï¼Œå°†æµæ•°æ®åˆ†å‰²æˆå°æ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

- **Structured Streaming**ï¼š
  Spark 2.0ç‰ˆæœ¬å¼•å…¥çš„ç¬¬äºŒä»£æµå¤„ç†å¼•æ“ï¼ŒåŸºäºSpark SQLå¼•æ“ï¼Œå°†æµæ•°æ®è§†ä¸ºæ— ç•Œè¡¨ï¼Œæä¾›æ›´é«˜çº§çš„APIå’Œä¼˜åŒ–ã€‚

**2. æ ¸å¿ƒç‰¹æ€§å¯¹æ¯”**

| ç‰¹æ€§ | Spark Streaming | Structured Streaming | å·®å¼‚å½±å“ |
|------|----------------|---------------------|---------|
| **å¤„ç†æ¨¡å‹** | å¾®æ‰¹å¤„ç†ï¼ˆDStreamï¼‰ | è¿ç»­å¤„ç†ï¼ˆæ— ç•Œè¡¨ï¼‰ | Structured Streamingå¯å®ç°æ›´ä½å»¶è¿Ÿ |
| **APIæŠ½è±¡** | DStream API | DataFrame/Dataset API | Structured Streamingä½¿ç”¨æ›´ç»Ÿä¸€çš„API |
| **ç¼–ç¨‹æ¨¡å‹** | å‡½æ•°å¼è½¬æ¢ | å£°æ˜å¼æŸ¥è¯¢ | Structured Streamingä»£ç æ›´ç®€æ´ |
| **å®¹é”™æœºåˆ¶** | WAL + Checkpoint | çŠ¶æ€å­˜å‚¨ + Checkpoint | ä¸¤è€…éƒ½æ”¯æŒå®¹é”™ï¼Œä½†å®ç°æ–¹å¼ä¸åŒ |
| **å»¶è¿Ÿ** | ç§’çº§ | æ¯«ç§’çº§ï¼ˆè¿ç»­å¤„ç†æ¨¡å¼ï¼‰ | Structured Streamingå¯å®ç°æ›´ä½å»¶è¿Ÿ |
| **çŠ¶æ€ç®¡ç†** | updateStateByKey/mapWithState | å†…ç½®çŠ¶æ€ç®¡ç† | Structured StreamingçŠ¶æ€ç®¡ç†æ›´å¼ºå¤§ |
| **äº‹ä»¶æ—¶é—´** | æœ‰é™æ”¯æŒ | åŸç”Ÿæ”¯æŒ | Structured Streamingæ›´é€‚åˆäº‹ä»¶æ—¶é—´å¤„ç† |
| **æ°´å°æœºåˆ¶** | ä¸æ”¯æŒ | æ”¯æŒ | Structured Streamingèƒ½æ›´å¥½å¤„ç†ä¹±åºæ•°æ® |
| **è¾“å‡ºæ¨¡å¼** | å›ºå®šæ¨¡å¼ | Complete/Append/Update | Structured Streamingæä¾›æ›´çµæ´»çš„è¾“å‡ºé€‰é¡¹ |
| **ä¼˜åŒ–å™¨** | æ—  | Catalystä¼˜åŒ–å™¨ | Structured StreamingæŸ¥è¯¢æ€§èƒ½æ›´é«˜ |
| **ç«¯åˆ°ç«¯ä¸€è‡´æ€§** | è‡³å°‘ä¸€æ¬¡ | ç²¾ç¡®ä¸€æ¬¡ | Structured Streamingæä¾›æ›´å¼ºçš„ä¸€è‡´æ€§ä¿è¯ |

**3. ä»£ç ç¤ºä¾‹å¯¹æ¯”**

**Spark Streamingç¤ºä¾‹**ï¼š
```scala
// åˆ›å»ºStreamingContext
val ssc = new StreamingContext(sparkContext, Seconds(1))

// ä»Kafkaè¯»å–æ•°æ®
val kafkaStream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// å¤„ç†æ•°æ®
val wordCounts = kafkaStream
  .map(record => record.value)
  .flatMap(_.split(" "))
  .map(word => (word, 1))
  .reduceByKey(_ + _)

// è¾“å‡ºç»“æœ
wordCounts.print()

// å¯åŠ¨æµå¤„ç†
ssc.start()
ssc.awaitTermination()
```

**Structured Streamingç¤ºä¾‹**ï¼š
```scala
// ä»Kafkaè¯»å–æ•°æ®
val kafkaStream = spark.readStream
          .format("kafka")
  .option("kafka.bootstrap.servers", "host:port")
  .option("subscribe", "topic")
          .load()

// å¤„ç†æ•°æ®
val wordCounts = kafkaStream
  .selectExpr("CAST(value AS STRING)")
  .as[String]
  .flatMap(_.split(" "))
  .groupBy("value")
  .count()

// è¾“å‡ºç»“æœ
val query = wordCounts.writeStream
  .outputMode("complete")
          .format("console")
          .start()
      
query.awaitTermination()
```

**4. é«˜çº§ç‰¹æ€§å¯¹æ¯”**

**äº‹ä»¶æ—¶é—´å¤„ç†**ï¼š
- **Spark Streaming**ï¼šéœ€è¦æ‰‹åŠ¨å®ç°ï¼Œè¾ƒä¸ºå¤æ‚
- **Structured Streaming**ï¼šå†…ç½®æ”¯æŒ
  ```scala
  // ä½¿ç”¨äº‹ä»¶æ—¶é—´çª—å£
  val windowedCounts = kafkaStream
    .selectExpr("CAST(value AS STRING)", "CAST(timestamp AS TIMESTAMP)")
    .as[(String, Timestamp)]
    .flatMap(record => record._1.split(" ").map((_, record._2)))
    .groupBy(
      window($"_2", "10 minutes", "5 minutes"),
      $"_1"
    )
    .count()
  ```

**æ°´å°æœºåˆ¶**ï¼š
- **Spark Streaming**ï¼šä¸æ”¯æŒ
- **Structured Streaming**ï¼šå†…ç½®æ”¯æŒ
```scala
  // æ·»åŠ æ°´å°
  val windowedCounts = kafkaStream
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
      window($"timestamp", "10 minutes", "5 minutes"),
      $"value"
    )
    .count()
  ```

**5. æ€§èƒ½ä¸æ‰©å±•æ€§å¯¹æ¯”**

| æ–¹é¢ | Spark Streaming | Structured Streaming | 
|------|----------------|---------------------|
| **æŸ¥è¯¢ä¼˜åŒ–** | æ— å†…ç½®ä¼˜åŒ–å™¨ | ä½¿ç”¨Catalystä¼˜åŒ–å™¨ | 
| **å†…å­˜ä½¿ç”¨** | åŸºäºRDD | åŸºäºTungsten | 
| **ååé‡** | é«˜ | æ›´é«˜ï¼ˆå¾—ç›Šäºä¼˜åŒ–ï¼‰ | 
| **å»¶è¿Ÿ** | å–å†³äºæ‰¹æ¬¡é—´éš” | å¯é…ç½®ï¼ˆå¾®æ‰¹æˆ–è¿ç»­ï¼‰ | 
| **æ‰©å±•æ€§** | æ°´å¹³æ‰©å±• | æ°´å¹³æ‰©å±• | 

**6. é€‚ç”¨åœºæ™¯å»ºè®®**

**é€‰æ‹©Spark Streamingçš„åœºæ™¯**ï¼š
- ä¸ç°æœ‰RDDä»£ç é›†æˆ
- ç®€å•çš„æµå¤„ç†éœ€æ±‚
- å¯¹APIç¨³å®šæ€§è¦æ±‚é«˜ï¼ˆAPIè¾ƒä¸ºç¨³å®šï¼‰
- éœ€è¦è‡ªå®šä¹‰å¤æ‚çš„è½¬æ¢æ“ä½œ
- ä¸é—ç•™ç³»ç»Ÿé›†æˆ

**é€‰æ‹©Structured Streamingçš„åœºæ™¯**ï¼š
- æ–°é¡¹ç›®å¼€å‘
- éœ€è¦ä½å»¶è¿Ÿå¤„ç†
- éœ€è¦äº‹ä»¶æ—¶é—´å¤„ç†å’Œæ°´å°æ”¯æŒ
- éœ€è¦å¼ºä¸€è‡´æ€§ä¿è¯
- å¤æ‚çš„æµå¤„ç†éœ€æ±‚ï¼ˆå¦‚çª—å£èšåˆã€ä¼šè¯åˆ†æï¼‰
- ä¸Spark SQLç”Ÿæ€ç³»ç»Ÿé›†æˆ

**7. æœªæ¥å‘å±•è¶‹åŠ¿**

Structured Streamingæ˜¯Sparkæµå¤„ç†çš„æœªæ¥å‘å±•æ–¹å‘ï¼ŒSparkå›¢é˜Ÿå°†ä¸»è¦ç²¾åŠ›æŠ•å…¥åˆ°Structured Streamingçš„æ”¹è¿›å’Œä¼˜åŒ–ä¸­ã€‚å¯¹äºæ–°é¡¹ç›®ï¼Œå»ºè®®ä¼˜å…ˆè€ƒè™‘Structured Streamingï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚åªèƒ½ä½¿ç”¨Spark Streamingã€‚

æ€»ä½“è€Œè¨€ï¼ŒStructured Streamingç›¸æ¯”Spark Streamingæä¾›äº†æ›´é«˜çº§çš„APIã€æ›´å¼ºå¤§çš„åŠŸèƒ½å’Œæ›´å¥½çš„æ€§èƒ½ï¼Œæ˜¯Sparkæµå¤„ç†æŠ€æœ¯çš„é‡è¦è¿›æ­¥ã€‚

**Q13: è¯·è¯¦ç»†è§£é‡ŠSparkå¦‚ä½•å®ç°Exactly-Onceè¯­ä¹‰ï¼ŒåŒ…æ‹¬åŸç†å’Œå®ç°æ–¹å¼ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

åœ¨åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿä¸­ï¼Œæ•°æ®å¤„ç†è¯­ä¹‰æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹èŠ‚ç‚¹æ•…éšœå’Œç½‘ç»œåˆ†åŒºç­‰æƒ…å†µæ—¶ã€‚Exactly-Onceè¯­ä¹‰æ˜¯æŒ‡æ¯æ¡æ•°æ®è¢«ç²¾ç¡®å¤„ç†ä¸€æ¬¡ï¼Œä¸å¤šä¸å°‘ï¼Œè¿™æ˜¯æœ€å¼ºçš„ä¸€è‡´æ€§ä¿è¯ã€‚Sparké€šè¿‡å¤šå±‚æœºåˆ¶å®ç°äº†Exactly-Onceè¯­ä¹‰ã€‚

**1. æ•°æ®å¤„ç†è¯­ä¹‰çº§åˆ«**

é¦–å…ˆç†è§£ä¸‰ç§ä¸»è¦çš„æ•°æ®å¤„ç†è¯­ä¹‰ï¼š

| è¯­ä¹‰çº§åˆ« | æè¿° | å®ç°éš¾åº¦ | æ€§èƒ½å½±å“ |
|---------|------|---------|---------|
| **At-most-once** | æ•°æ®æœ€å¤šå¤„ç†ä¸€æ¬¡ï¼Œå¯èƒ½ä¸¢å¤± | ä½ | å‡ ä¹æ— å½±å“ |
| **At-least-once** | æ•°æ®è‡³å°‘å¤„ç†ä¸€æ¬¡ï¼Œå¯èƒ½é‡å¤ | ä¸­ | è½»å¾®å½±å“ |
| **Exactly-once** | æ•°æ®ç²¾ç¡®å¤„ç†ä¸€æ¬¡ï¼Œä¸ä¸¢å¤±ä¸é‡å¤ | é«˜ | å¯èƒ½æ˜¾è‘—å½±å“ |

**2. Sparkå®ç°Exactly-Onceçš„æ ¸å¿ƒæœºåˆ¶**

Sparké€šè¿‡ä»¥ä¸‹å¤šå±‚æœºåˆ¶å…±åŒä¿è¯Exactly-Onceè¯­ä¹‰ï¼š

**2.1 æ•°æ®æºç«¯ä¿è¯**

- **å¯é‡æ”¾çš„æ•°æ®æº**ï¼šä½¿ç”¨æ”¯æŒåç§»é‡è·Ÿè¸ªçš„æ•°æ®æºï¼ˆå¦‚Kafkaï¼‰
- **åç§»é‡ç®¡ç†**ï¼šç²¾ç¡®è®°å½•å’Œç®¡ç†å·²å¤„ç†æ•°æ®çš„åç§»é‡
- **æ–­ç‚¹ç»­ä¼ **ï¼šä»ä¸Šæ¬¡å¤„ç†çš„ä½ç½®ç»§ç»­å¤„ç†

**2.2 å¤„ç†è¿‡ç¨‹ä¿è¯**

- **ç¡®å®šæ€§æ“ä½œ**ï¼šç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º
- **Checkpointæœºåˆ¶**ï¼šå®šæœŸä¿å­˜è®¡ç®—çŠ¶æ€å’Œè¿›åº¦
- **WALï¼ˆé¢„å†™æ—¥å¿—ï¼‰**ï¼šè®°å½•æ‰€æœ‰çŠ¶æ€å˜æ›´æ“ä½œ
- **å¹‚ç­‰æ€§è½¬æ¢**ï¼šç¡®ä¿é‡å¤æ‰§è¡Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨

**2.3 æ•°æ®è¾“å‡ºä¿è¯**

- **å¹‚ç­‰æ€§å†™å…¥**ï¼šç¡®ä¿é‡å¤å†™å…¥ä¸ä¼šå¯¼è‡´æ•°æ®é‡å¤
- **äº‹åŠ¡æ€§å†™å…¥**ï¼šä½¿ç”¨æ”¯æŒäº‹åŠ¡çš„å­˜å‚¨ç³»ç»Ÿ
- **ä¸¤é˜¶æ®µæäº¤**ï¼šç¡®ä¿æ•°æ®å¤„ç†å’Œç»“æœå†™å…¥çš„åŸå­æ€§
- **è¾“å‡ºæäº¤åè®®**ï¼šå°†è¾“å‡ºæ“ä½œä¸åç§»é‡æäº¤ç»‘å®š

**3. Structured Streamingä¸­çš„Exactly-Onceå®ç°**

Structured Streamingé€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°Exactly-Onceï¼š

```mermaid
graph TD
    A[æ•°æ®æº] --> B[åç§»é‡è·Ÿè¸ª]
    B --> C[å¾®æ‰¹å¤„ç†]
    C --> D[çŠ¶æ€ç®¡ç†]
    D --> E[è¾“å‡ºæ¨¡å¼]
    E --> F[äº‹åŠ¡æ€§Sink]
    
    B -.-> G[WAL/Checkpoint]
    D -.-> G
    F -.-> G
    
    style A fill:#d4f1f9,stroke:#05a4d1
    style B fill:#ffe6cc,stroke:#d79b00
    style C fill:#d5e8d4,stroke:#82b366
    style D fill:#e1d5e7,stroke:#9673a6
    style E fill:#f8cecc,stroke:#b85450
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#fff2cc,stroke:#d6b656
```

**3.1 è¯¦ç»†å®ç°æœºåˆ¶**

1. **åç§»é‡è·Ÿè¸ª**ï¼š
   - è®°å½•æ¯ä¸ªæ•°æ®æºçš„è¯»å–ä½ç½®
   - å°†åç§»é‡ä¸å¤„ç†ç»“æœå…³è”
   - åœ¨Checkpointä¸­ä¿å­˜åç§»é‡ä¿¡æ¯

2. **çŠ¶æ€ç®¡ç†**ï¼š
   - ä½¿ç”¨çŠ¶æ€å­˜å‚¨ä¿å­˜ä¸­é—´çŠ¶æ€
   - å®šæœŸCheckpointçŠ¶æ€åˆ°å¯é å­˜å‚¨
   - æ•…éšœæ¢å¤æ—¶é‡å»ºçŠ¶æ€

3. **è¾“å‡ºæäº¤åè®®**ï¼š
   - å°†ç»“æœå†™å…¥ä¸åç§»é‡æäº¤ç»‘å®š
   - ç¡®ä¿åŸå­æ€§ï¼šè¦ä¹ˆéƒ½æˆåŠŸï¼Œè¦ä¹ˆéƒ½å¤±è´¥
   - ä½¿ç”¨äº‹åŠ¡æ€§Sinkæˆ–å¹‚ç­‰æ€§å†™å…¥

**4. ä»£ç ç¤ºä¾‹ï¼šå®ç°Exactly-Onceè¯­ä¹‰**

**4.1 ä½¿ç”¨äº‹åŠ¡æ€§è¾“å‡º**ï¼š

```scala
// ä½¿ç”¨foreachBatchå®ç°äº‹åŠ¡æ€§è¾“å‡º
val query = inputStream
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    // å¼€å¯äº‹åŠ¡
    val connection = getConnection()  // è·å–æ•°æ®åº“è¿æ¥
    connection.beginTransaction()
    try {
      // ä½¿ç”¨batchIdç¡®ä¿å¹‚ç­‰æ€§
      val outputPath = s"output/batch_$batchId"
      
      // åˆ é™¤å¯èƒ½å­˜åœ¨çš„æ—§æ•°æ®ï¼ˆå¹‚ç­‰æ€§ä¿è¯ï¼‰
      connection.executeUpdate(s"DELETE FROM results WHERE batch_id = $batchId")
      
      // å†™å…¥æ–°æ•°æ®
      batchDF.write
        .format("jdbc")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("url", jdbcUrl)
        .option("dbtable", "results")
        .option("user", user)
        .option("password", password)
        .mode("append")
        .save()
        
      // æäº¤äº‹åŠ¡
      connection.commit()
    } catch {
      case e: Exception =>
        // å›æ»šäº‹åŠ¡
        connection.rollback()
        throw e
    } finally {
      connection.close()
    }
  }
  .option("checkpointLocation", "/checkpoint")
  .start()
```

**4.2 ä½¿ç”¨å¹‚ç­‰æ€§å†™å…¥**ï¼š

```scala
// ä½¿ç”¨å¹‚ç­‰æ€§å†™å…¥
val query = inputStream
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    // ä½¿ç”¨æ‰¹æ¬¡IDä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œç¡®ä¿å¹‚ç­‰æ€§
    val outputPath = s"output/batch_$batchId"
    
    // è¦†ç›–å†™å…¥ï¼Œç¡®ä¿å¹‚ç­‰æ€§
    batchDF.write
      .mode("overwrite")  // è¦†ç›–æ¨¡å¼ç¡®ä¿å¹‚ç­‰æ€§
      .parquet(outputPath)
      
    // å¯é€‰ï¼šæ›´æ–°å…ƒæ•°æ®è¡¨è®°å½•å·²å¤„ç†çš„æ‰¹æ¬¡
    spark.sql(s"""
      MERGE INTO batch_metadata
      USING (SELECT $batchId as id) AS source
      ON batch_metadata.id = source.id
      WHEN MATCHED THEN UPDATE SET processed_time = current_timestamp()
      WHEN NOT MATCHED THEN INSERT (id, processed_time) VALUES ($batchId, current_timestamp())
    """)
  }
  .option("checkpointLocation", "/checkpoint")
  .start()
```

**5. ä¸åŒåœºæ™¯ä¸‹çš„Exactly-Onceå®ç°**

| åœºæ™¯ | å®ç°æ–¹å¼ | å…³é”®ç‚¹ |
|------|---------|-------|
| **Kafka â†’ HDFS** | WAL + å¹‚ç­‰æ€§å†™å…¥ | ä½¿ç”¨æ‰¹æ¬¡IDä½œä¸ºæ–‡ä»¶åæˆ–è·¯å¾„ |
| **Kafka â†’ æ•°æ®åº“** | ä¸¤é˜¶æ®µæäº¤ | å°†åç§»é‡ä¸æ•°æ®åº“äº‹åŠ¡ç»‘å®š |
| **Kafka â†’ Kafka** | äº‹åŠ¡æ€§API | ä½¿ç”¨Kafkaäº‹åŠ¡API |
| **æœ‰çŠ¶æ€æ“ä½œ** | Checkpoint + çŠ¶æ€å­˜å‚¨ | å®šæœŸCheckpointçŠ¶æ€ |

**6. å®ç°Exactly-Onceçš„æœ€ä½³å®è·µ**

1. **é€‰æ‹©åˆé€‚çš„æ•°æ®æº**ï¼šä½¿ç”¨æ”¯æŒé‡æ”¾å’Œåç§»é‡ç®¡ç†çš„æ•°æ®æº
2. **å¯ç”¨Checkpoint**ï¼šé…ç½®å¯é çš„Checkpointå­˜å‚¨
3. **ä½¿ç”¨å¹‚ç­‰æ€§æ“ä½œ**ï¼šç¡®ä¿é‡å¤æ‰§è¡Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨
4. **äº‹åŠ¡æ€§è¾“å‡º**ï¼šä½¿ç”¨æ”¯æŒäº‹åŠ¡çš„è¾“å‡ºç³»ç»Ÿ
5. **é”™è¯¯å¤„ç†**ï¼šå¦¥å–„å¤„ç†å¼‚å¸¸ï¼Œé¿å…éƒ¨åˆ†æäº¤
6. **ç›‘æ§ä¸éªŒè¯**ï¼šå»ºç«‹ç›‘æ§æœºåˆ¶éªŒè¯Exactly-Onceè¯­ä¹‰

**7. é™åˆ¶ä¸æŒ‘æˆ˜**

- **æ€§èƒ½å¼€é”€**ï¼šå®ç°Exactly-Onceé€šå¸¸ä¼šå¸¦æ¥ä¸€å®šæ€§èƒ½å¼€é”€
- **å¤–éƒ¨ç³»ç»Ÿé™åˆ¶**ï¼šä¾èµ–å¤–éƒ¨ç³»ç»Ÿå¯¹äº‹åŠ¡çš„æ”¯æŒ
- **å¤æ‚æ€§å¢åŠ **ï¼šå®ç°å’Œç»´æŠ¤æ›´ä¸ºå¤æ‚
- **è°ƒè¯•éš¾åº¦**ï¼šé—®é¢˜æ’æŸ¥æ›´å…·æŒ‘æˆ˜æ€§

Sparkçš„Exactly-Onceè¯­ä¹‰å®ç°æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡çš„ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆå¤šç§æœºåˆ¶å…±åŒä¿è¯æ•°æ®å¤„ç†çš„ä¸€è‡´æ€§ã€‚ç†è§£è¿™äº›æœºåˆ¶æœ‰åŠ©äºæ„å»ºå¯é çš„æµå¤„ç†åº”ç”¨ï¼Œå¹¶åœ¨é¢å¯¹æ•…éšœæ—¶ä¿æŒæ•°æ®ä¸€è‡´æ€§ã€‚

### æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜

**Q14: å¦‚ä½•è¯Šæ–­å’Œè§£å†³Sparkåº”ç”¨çš„æ€§èƒ½é—®é¢˜ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
è¯Šæ–­æ–¹æ³•ï¼š
- åˆ†æSpark UIä¸Šçš„ä½œä¸šæ‰§è¡Œæƒ…å†µ
- æ£€æŸ¥Stageæ‰§è¡Œæ—¶é—´å’Œä»»åŠ¡åˆ†å¸ƒæƒ…å†µ
- ç›‘æ§Shuffleè¯»å†™æ•°æ®é‡
- æŸ¥çœ‹GCæ—¥å¿—åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ

å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼š
- æ•°æ®å€¾æ–œï¼šä¼˜åŒ–åˆ†åŒºç­–ç•¥ï¼ŒåŠ ç›å¤„ç†
- èµ„æºä¸è¶³ï¼šè°ƒæ•´executoræ•°é‡å’Œå†…å­˜é…ç½®
- Shuffleä¼˜åŒ–ï¼šè°ƒæ•´åˆ†åŒºæ•°ï¼Œå¯ç”¨å‹ç¼©
- åºåˆ—åŒ–é—®é¢˜ï¼šä½¿ç”¨Kryoæ›¿ä»£é»˜è®¤åºåˆ—åŒ–
- å¹¿æ’­å¤§è¡¨ï¼šå¯¹å°è¡¨ä½¿ç”¨broadcast join

**Q15: Sparkåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•è¿›è¡Œç›‘æ§ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
ç›‘æ§æŒ‡æ ‡ï¼š
- èµ„æºä½¿ç”¨ç‡ï¼šCPUã€å†…å­˜ã€ç£ç›˜I/Oã€ç½‘ç»œ
- ä½œä¸šæ‰§è¡Œï¼šå»¶è¿Ÿã€ååé‡ã€å¤±è´¥ç‡
- GCæ´»åŠ¨ï¼šé¢‘ç‡ã€åœé¡¿æ—¶é—´
- Shuffleç»Ÿè®¡ï¼šè¯»å†™æ•°æ®é‡ã€æº¢å‡ºé‡

ç›‘æ§å·¥å…·ï¼š
- Spark UIå’ŒHistory Server
- Gangliaã€Prometheusã€Grafanaç­‰ç¬¬ä¸‰æ–¹å·¥å…·
- æ—¥å¿—ç›‘æ§å’Œåˆ†æç³»ç»Ÿ

å‘Šè­¦ç­–ç•¥ï¼š
- ä»»åŠ¡å¤±è´¥ç‡é˜ˆå€¼å‘Šè­¦
- é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡å‘Šè­¦
- èµ„æºä½¿ç”¨ç‡è¶…é˜ˆå€¼å‘Šè­¦
- GCé¢‘ç¹æˆ–æ—¶é—´é•¿å‘Šè­¦

**Q16: Sparkä¸Hadoopç”Ÿæ€ç³»ç»Ÿçš„é›†æˆæœ€ä½³å®è·µï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
HDFSé›†æˆï¼š
- ä¼˜åŒ–å¹¶è¡Œåº¦åŒ¹é…HDFSå—å¤§å°
- åˆ©ç”¨æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–è¯»å–æ€§èƒ½
- é…ç½®é€‚å½“çš„å‰¯æœ¬å› å­å’Œå—å¤§å°

Hiveé›†æˆï¼š
- é…ç½®spark.sql.hive.metastore.version
- å…±äº«Hiveå…ƒæ•°æ®ä»“åº“
- ä½¿ç”¨ORC/Parquetç­‰åˆ—å¼å­˜å‚¨æ ¼å¼

YARNèµ„æºç®¡ç†ï¼š
- client/clusteræ¨¡å¼é€‰æ‹©
- åŠ¨æ€èµ„æºåˆ†é…é…ç½®
- é˜Ÿåˆ—å’Œè°ƒåº¦ç­–ç•¥è®¾ç½®

Kafkaé›†æˆï¼š
- è°ƒæ•´å¹¶è¡Œåº¦åŒ¹é…Kafkaåˆ†åŒº
- ä¼˜åŒ–æ‰¹æ¬¡å¤§å°å’Œå¤„ç†é—´éš”
- å®ç°ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰çš„æœ€ä½³å®è·µ

**Q17: å¦‚ä½•å¤„ç†Sparkåº”ç”¨ä¸­çš„OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰é—®é¢˜ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
ä¸åŒç±»å‹OOMï¼š
- Driver OOMï¼šå‡å°‘collect()è¿”å›æ•°æ®é‡
- Executor OOMï¼šå¢åŠ å†…å­˜é…ç½®æˆ–åˆ†åŒºæ•°
- Shuffle OOMï¼šä¼˜åŒ–shuffleæ“ä½œï¼Œå¢åŠ spark.shuffle.memoryFraction

é˜²èŒƒæªæ–½ï¼š
- ä½¿ç”¨æŒä¹…åŒ–æ§åˆ¶ç¼“å­˜æ•°æ®é‡
- æ­£ç¡®è®¾ç½®å†…å­˜ç›¸å…³å‚æ•°
- å‡å°‘æ¯ä¸ªtaskå¤„ç†çš„æ•°æ®é‡
- ä¼˜åŒ–æ•°æ®ç»“æ„å’Œç®—æ³•

è°ƒè¯•å·¥å…·ï¼š
- Spark UIå†…å­˜ç»Ÿè®¡
- JVMå‚æ•°-XX:+HeapDumpOnOutOfMemoryErrorç”Ÿæˆå †è½¬å‚¨
- GCæ—¥å¿—åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼

**Q18: å¦‚ä½•ä¼˜åŒ–é•¿æ—¶é—´è¿è¡Œçš„Spark Streamingä½œä¸šï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
ç¨³å®šæ€§ä¼˜åŒ–ï¼š
- å®æ–½ç›‘æ§å’Œè‡ªåŠ¨é‡å¯æœºåˆ¶
- å®šæœŸCheckpointä¿å­˜çŠ¶æ€
- ä½¿ç”¨updateStateByKeyä¿ç•™å†å²çŠ¶æ€

æ€§èƒ½ä¼˜åŒ–ï¼š
- è°ƒæ•´æ‰¹å¤„ç†é—´éš”å¹³è¡¡å»¶è¿Ÿå’Œååé‡
- é€‚å½“è®¾ç½®å¹¶è¡Œåº¦åŒ¹é…æ•°æ®é€Ÿç‡
- ä½¿ç”¨çª—å£æ“ä½œä¼˜åŒ–èšåˆ

èµ„æºç®¡ç†ï¼š
- å¯ç”¨åŠ¨æ€èµ„æºåˆ†é…åº”å¯¹è´Ÿè½½å˜åŒ–
- åˆç†è®¾ç½®é˜Ÿåˆ—å®¹é‡å’Œå›æ”¶ç­–ç•¥
- å®æ–½èƒŒå‹æœºåˆ¶(backpressure)é˜²æ­¢è¿‡è½½

è¿ç»´ç­–ç•¥ï¼š
- ä¼˜é›…åœæ­¢å’Œå¯åŠ¨æœºåˆ¶
- æ•°æ®é‡æ”¾å’Œæ¢å¤æœºåˆ¶
- å®šæœŸç»´æŠ¤è®¡åˆ’ï¼ˆå¦‚æ‰‹åŠ¨Checkpointæ¸…ç†ï¼‰

### é«˜çº§åº”ç”¨é¢˜

**Q19: å¦‚ä½•åœ¨Sparkä¸­å®ç°æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
æ•°æ®å‡†å¤‡ï¼š
- ä½¿ç”¨DataFrame APIè¿›è¡Œç‰¹å¾å·¥ç¨‹
- ä½¿ç”¨VectorAssembleråˆå¹¶ç‰¹å¾
- åº”ç”¨StandardScalerç­‰ç‰¹å¾è½¬æ¢å™¨

æ¨¡å‹æ„å»ºï¼š
- ä½¿ç”¨Pipeline APIå°†è½¬æ¢å™¨å’Œä¼°è®¡å™¨è¿æ¥
- ä½¿ç”¨CrossValidatorè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
- æ”¯æŒçš„ç®—æ³•åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ç­‰

æ¨¡å‹éƒ¨ç½²ï¼š
- ä½¿ç”¨save/loadæ–¹æ³•åºåˆ—åŒ–æ¨¡å‹
- æ‰¹å¤„ç†é¢„æµ‹ï¼štransformæ•´ä¸ªDataFrame
- æµå¼é¢„æµ‹ï¼šå°†æ¨¡å‹é›†æˆåˆ°Structured Streaming

æœ€ä½³å®è·µï¼š
- ç¼“å­˜é¢„å¤„ç†åçš„æ•°æ®åŠ é€Ÿè®­ç»ƒ
- å¹¶è¡ŒåŒ–å‚æ•°æœç´¢æé«˜æ•ˆç‡
- ä½¿ç”¨MLflowç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ

**Q20: Sparkåœ¨å®æ—¶æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
æ¶æ„è®¾è®¡ï¼š
- ç¦»çº¿è®­ç»ƒï¼šä½¿ç”¨Spark MLè®­ç»ƒæ¨èæ¨¡å‹
- è¿‘å®æ—¶æ›´æ–°ï¼šå®šæœŸä½¿ç”¨Spark Streamingæ›´æ–°æ¨¡å‹
- å®æ—¶æ¨ç†ï¼šä½å»¶è¿ŸæœåŠ¡å±‚æä¾›æ¨è

æ ¸å¿ƒæŠ€æœ¯ï¼š
- ååŒè¿‡æ»¤ï¼šALSç®—æ³•å¤„ç†ç”¨æˆ·-ç‰©å“äº¤äº’
- ç‰¹å¾å·¥ç¨‹ï¼šç”¨æˆ·å’Œç‰©å“ç‰¹å¾æå–
- æ¨¡å‹èåˆï¼šç»„åˆå¤šç§æ¨¡å‹ç»“æœ

æ€§èƒ½ä¼˜åŒ–ï¼š
- æ¨¡å‹é¢„è®¡ç®—ï¼šæå‰è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
- é«˜æ•ˆç´¢å¼•ï¼šä½¿ç”¨LSHç­‰æŠ€æœ¯åŠ é€Ÿæœ€è¿‘é‚»æœç´¢
- ç¼“å­˜ç­–ç•¥ï¼šçƒ­é—¨æ¨èç»“æœç¼“å­˜

**Q21: å¦‚ä½•ä½¿ç”¨Sparkå¤„ç†å’Œåˆ†æå¤§è§„æ¨¡å›¾æ•°æ®ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
GraphX/GraphFrames APIï¼š
- é¡¶ç‚¹å’Œè¾¹çš„åˆ†å¸ƒå¼è¡¨ç¤º
- æ”¯æŒPregelæ¨¡å‹çš„è¿­ä»£è®¡ç®—
- ä¸DataFrame APIé›†æˆ

å¸¸è§ç®—æ³•å®ç°ï¼š
- PageRankï¼šç½‘é¡µé‡è¦æ€§è¯„ä¼°
- Connected Componentsï¼šç¤¾åŒºå‘ç°
- Triangle Countingï¼šç»“æ„åˆ†æ
- æœ€çŸ­è·¯å¾„ï¼šè·¯å¾„è§„åˆ’

æ€§èƒ½ä¼˜åŒ–ï¼š
- é¡¶ç‚¹åˆ‡åˆ†ç­–ç•¥é€‰æ‹©
- è¾¹ç¼“å­˜ä¼˜åŒ–
- è¿­ä»£è®¡ç®—ä¸­çš„çŠ¶æ€ä¿æŒ
- ä½¿ç”¨Checkpointå‡å°ä¾èµ–é“¾

**Q22: åœ¨Sparkä¸­å¦‚ä½•å¤„ç†æ—¶é—´åºåˆ—æ•°æ®åˆ†æï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
æ•°æ®è¡¨ç¤ºï¼š
- ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„DataFrame
- æ”¯æŒæ—¥æœŸæ—¶é—´å‡½æ•°å’Œçª—å£æ“ä½œ

åˆ†ææ–¹æ³•ï¼š
- æ—¶é—´çª—å£èšåˆï¼šä½¿ç”¨windowå‡½æ•°
- æ»šåŠ¨ç»Ÿè®¡ï¼šä½¿ç”¨overçª—å£å‡½æ•°
- å¼‚å¸¸æ£€æµ‹ï¼šåŸºäºç»Ÿè®¡æˆ–æœºå™¨å­¦ä¹ æ¨¡å‹

å¸¸è§ç®—æ³•ï¼š
- ARIMAæ¨¡å‹ï¼šä½¿ç”¨spark-tsåº“
- æŒ‡æ•°å¹³æ»‘ï¼šè‡ªå®šä¹‰UDFå®ç°
- å­£èŠ‚æ€§åˆ†è§£ï¼šä½¿ç”¨çª—å£å‡½æ•°å®ç°

ä¼˜åŒ–æŠ€å·§ï¼š
- æŒ‰æ—¶é—´åˆ†åŒºæé«˜æŸ¥è¯¢æ•ˆç‡
- ä½¿ç”¨ç¼“å­˜åŠ é€Ÿé‡å¤æŸ¥è¯¢
- å¹¶è¡ŒåŒ–é•¿æ—¶é—´æ®µå¤„ç†

**Q23: å¦‚ä½•ä½¿ç”¨Sparkå®ç°å¤§è§„æ¨¡åœ°ç†ç©ºé—´æ•°æ®å¤„ç†ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**
æ•°æ®è¡¨ç¤ºï¼š
- ä½¿ç”¨GeoSparkæˆ–Sedonaæ‰©å±•Spark
- æ”¯æŒç‚¹ã€çº¿ã€é¢ç­‰å‡ ä½•å¯¹è±¡
- ç©ºé—´ç´¢å¼•ï¼šR-æ ‘ã€å››å‰æ ‘ç­‰

ç©ºé—´æ“ä½œï¼š
- ç©ºé—´è¿æ¥ï¼šæŸ¥æ‰¾é‚»è¿‘å¯¹è±¡
- ç©ºé—´èšåˆï¼šèšç±»å’Œçƒ­ç‚¹åˆ†æ
- è½¨è¿¹åˆ†æï¼šç§»åŠ¨å¯¹è±¡è½¨è¿¹å¤„ç†

æ€§èƒ½ä¼˜åŒ–ï¼š
- ç©ºé—´åˆ†åŒºç­–ç•¥ï¼šç½‘æ ¼ã€å››å‰æ ‘ç­‰
- ç´¢å¼•ä¼˜åŒ–ï¼šå‡å°‘æ¯”è¾ƒæ¬¡æ•°
- è‡ªå®šä¹‰UDFå®ç°å¤æ‚ç©ºé—´è®¡ç®—
- æ•°æ®å‡è¡¡ï¼šé¿å…ç©ºé—´æ•°æ®å€¾æ–œ