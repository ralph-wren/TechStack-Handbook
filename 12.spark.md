# 12. Spark

## ç›®å½•
- [12. Spark](#12-spark)
  - [ç›®å½•](#ç›®å½•)
  - [Spark æ¦‚è¿°ä¸ç¯å¢ƒ](#spark-æ¦‚è¿°ä¸ç¯å¢ƒ)
    - [Sparkç®€ä»‹](#sparkç®€ä»‹)
      - [Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿](#sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿)
      - [Spark vs Hadoop MapReduce](#spark-vs-hadoop-mapreduce)
      - [Sparkåº”ç”¨åœºæ™¯](#sparkåº”ç”¨åœºæ™¯)
    - [Sparkç”Ÿæ€ç³»ç»Ÿ](#sparkç”Ÿæ€ç³»ç»Ÿ)
      - [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
      - [ç”Ÿæ€ç»„ä»¶](#ç”Ÿæ€ç»„ä»¶)
    - [Sparkç¯å¢ƒæ­å»º](#sparkç¯å¢ƒæ­å»º)
      - [æœ¬åœ°æ¨¡å¼](#æœ¬åœ°æ¨¡å¼)
      - [é›†ç¾¤æ¨¡å¼](#é›†ç¾¤æ¨¡å¼)
      - [å¸¸ç”¨é…ç½®](#å¸¸ç”¨é…ç½®)
  - [Spark æ ¸å¿ƒæ¦‚å¿µ â­](#spark-æ ¸å¿ƒæ¦‚å¿µ-)
    - [RDDæ ¸å¿ƒæ¦‚å¿µ](#rddæ ¸å¿ƒæ¦‚å¿µ)
      - [RDDç‰¹æ€§](#rddç‰¹æ€§)
      - [RDDæ“ä½œåˆ†ç±»](#rddæ“ä½œåˆ†ç±»)
      - [RDDä¾èµ–å…³ç³»](#rddä¾èµ–å…³ç³»)
    - [DataFrameä¸Dataset](#dataframeä¸dataset)
      - [DataFrameæ¦‚å¿µ](#dataframeæ¦‚å¿µ)
      - [Datasetæ¦‚å¿µ](#datasetæ¦‚å¿µ)
      - [ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥](#ä¸‰è€…å¯¹æ¯”åˆ†æ-)
    - [åˆ†åŒºæœºåˆ¶](#åˆ†åŒºæœºåˆ¶)
      - [åˆ†åŒºç­–ç•¥](#åˆ†åŒºç­–ç•¥)
      - [åˆ†åŒºè°ƒä¼˜](#åˆ†åŒºè°ƒä¼˜)
  - [Spark æ¶æ„ä¸åŸç† â­â­](#spark-æ¶æ„ä¸åŸç†-)
    - [Sparkæ•´ä½“æ¶æ„](#sparkæ•´ä½“æ¶æ„)
      - [é›†ç¾¤æ¶æ„ç»„ä»¶](#é›†ç¾¤æ¶æ„ç»„ä»¶)
      - [åº”ç”¨ç¨‹åºæ¶æ„](#åº”ç”¨ç¨‹åºæ¶æ„)
    - [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [SparkContext](#sparkcontext)
      - [Driver Program](#driver-program)
      - [Cluster Manager](#cluster-manager)
      - [Executor](#executor)
    - [ä»»åŠ¡è°ƒåº¦åŸç†](#ä»»åŠ¡è°ƒåº¦åŸç†)
      - [DAGSchedulerè°ƒåº¦](#dagschedulerè°ƒåº¦)
      - [TaskSchedulerè°ƒåº¦](#taskschedulerè°ƒåº¦)
      - [Stageåˆ’åˆ†æœºåˆ¶ ğŸ”¥](#stageåˆ’åˆ†æœºåˆ¶-)
    - [BlockManageræ ¸å¿ƒç»„ä»¶ â­](#blockmanageræ ¸å¿ƒç»„ä»¶-)
      - [æ“ä½œæ—¶åºå›¾](#æ“ä½œæ—¶åºå›¾)
      - [BlockManageræ ¸å¿ƒç»„ä»¶è¯¦è§£](#blockmanageræ ¸å¿ƒç»„ä»¶è¯¦è§£)
    - [å†…å­˜ç®¡ç†æœºåˆ¶ â­](#å†…å­˜ç®¡ç†æœºåˆ¶-)
      - [å†…å­˜æ¨¡å‹](#å†…å­˜æ¨¡å‹)
      - [å†…å­˜ç®¡ç†æ ¸å¿ƒç»„ä»¶è¯¦è§£](#å†…å­˜ç®¡ç†æ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [å†…å­˜åˆ†é…ç­–ç•¥](#å†…å­˜åˆ†é…ç­–ç•¥)
      - [åƒåœ¾å›æ”¶ä¼˜åŒ–](#åƒåœ¾å›æ”¶ä¼˜åŒ–)
  - [Spark SQLä¸Catalyst â­â­](#spark-sqlä¸catalyst-)
    - [Spark SQLæ¦‚è¿°](#spark-sqlæ¦‚è¿°)
      - [ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
      - [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
    - [Catalystä¼˜åŒ–å™¨ ğŸ”¥](#catalystä¼˜åŒ–å™¨-)
      - [ä¼˜åŒ–æµç¨‹](#ä¼˜åŒ–æµç¨‹)
      - [ä¼˜åŒ–è§„åˆ™](#ä¼˜åŒ–è§„åˆ™)
      - [ä»£ç ç”Ÿæˆ](#ä»£ç ç”Ÿæˆ)
    - [æ•°æ®æºæ”¯æŒ](#æ•°æ®æºæ”¯æŒ)
      - [å†…ç½®æ•°æ®æº](#å†…ç½®æ•°æ®æº)
      - [å¤–éƒ¨æ•°æ®æº](#å¤–éƒ¨æ•°æ®æº)
  - [Shuffleæœºåˆ¶æ·±åº¦è§£æ â­â­â­](#shuffleæœºåˆ¶æ·±åº¦è§£æ-)
    - [ShuffleåŸç†](#shuffleåŸç†)
      - [Shuffleæ¦‚å¿µ](#shuffleæ¦‚å¿µ)
      - [Shuffleè§¦å‘æ¡ä»¶](#shuffleè§¦å‘æ¡ä»¶)
      - [Shuffleç±»å‹å¯¹æ¯” ğŸ”¥](#shuffleç±»å‹å¯¹æ¯”-)
    - [Shuffleå®ç°æœºåˆ¶](#shuffleå®ç°æœºåˆ¶)
      - [Hash Shuffle](#hash-shuffle)
      - [Sort Shuffle](#sort-shuffle)
      - [Shuffle ç±»å‹ä¸æ—¶åºå›¾](#shuffle-ç±»å‹ä¸æ—¶åºå›¾)
      - [Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£](#shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£)
      - [Tungsten Sort Shuffle](#tungsten-sort-shuffle)
      - [Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜](#shuffle-ä¼˜åŒ–ä¸è°ƒä¼˜)
      - [Spark Shuffle è°ƒä¼˜](#spark-shuffle-è°ƒä¼˜)
      - [Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
  - [æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§ â­â­â­](#æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§-)
    - [æ€§èƒ½è°ƒä¼˜ç­–ç•¥](#æ€§èƒ½è°ƒä¼˜ç­–ç•¥)
      - [èµ„æºé…ç½®è°ƒä¼˜](#èµ„æºé…ç½®è°ƒä¼˜)
      - [ä»£ç ä¼˜åŒ–æŠ€å·§](#ä»£ç ä¼˜åŒ–æŠ€å·§)
      - [ç¼“å­˜ç­–ç•¥ä¼˜åŒ–](#ç¼“å­˜ç­–ç•¥ä¼˜åŒ–)
    - [å¸¸è§æ€§èƒ½é—®é¢˜](#å¸¸è§æ€§èƒ½é—®é¢˜)
      - [å†…å­˜æº¢å‡ºé—®é¢˜ ğŸ”¥](#å†…å­˜æº¢å‡ºé—®é¢˜-)
      - [æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ ğŸ”¥](#æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ-)
    - [ç›‘æ§ä¸è¯Šæ–­](#ç›‘æ§ä¸è¯Šæ–­)
      - [Spark UIç›‘æ§](#spark-uiç›‘æ§)
      - [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
  - [Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥](#sparké«˜é¢‘é¢è¯•é¢˜-)
    - [åŸºç¡€æ¦‚å¿µé¢˜](#åŸºç¡€æ¦‚å¿µé¢˜)
      - [RDDã€DataFrameã€DatasetåŒºåˆ«](#rdddataframedatasetåŒºåˆ«)
      - [Sparkä»»åŠ¡æ‰§è¡Œæµç¨‹](#sparkä»»åŠ¡æ‰§è¡Œæµç¨‹)
      - [Sparkå†…å­˜ç®¡ç†](#sparkå†…å­˜ç®¡ç†)
    - [æ¶æ„åŸç†é¢˜](#æ¶æ„åŸç†é¢˜)
      - [Sparkæ¶æ„ç»„ä»¶](#sparkæ¶æ„ç»„ä»¶)
      - [Shuffleæœºåˆ¶åŸç†](#shuffleæœºåˆ¶åŸç†)
    - [æ€§èƒ½è°ƒä¼˜é¢˜](#æ€§èƒ½è°ƒä¼˜é¢˜)
      - [æ€§èƒ½è°ƒä¼˜ç­–ç•¥](#æ€§èƒ½è°ƒä¼˜ç­–ç•¥-1)
      - [æ•°æ®å€¾æ–œè§£å†³](#æ•°æ®å€¾æ–œè§£å†³)
    - [å®æˆ˜åº”ç”¨é¢˜](#å®æˆ˜åº”ç”¨é¢˜)
      - [Spark SQLåº”ç”¨](#spark-sqlåº”ç”¨)
      - [æ•…éšœæ’æŸ¥æ–¹æ³•](#æ•…éšœæ’æŸ¥æ–¹æ³•)
    - [æ·±åº¦æŠ€æœ¯åŸç†é¢˜](#æ·±åº¦æŠ€æœ¯åŸç†é¢˜)
      - [Sparkçš„Catalystä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ](#sparkçš„catalystä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆ)
      - [Sparkçš„å†…å­˜ç®¡ç†æ¼”è¿›å†å²ï¼Œæ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ](#sparkçš„å†…å­˜ç®¡ç†æ¼”è¿›å†å²æ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬æœ‰ä»€ä¹ˆåŒºåˆ«)
      - [Spark Streamingå’ŒStructured Streamingçš„åŒºåˆ«ï¼Ÿ](#spark-streamingå’Œstructured-streamingçš„åŒºåˆ«)
      - [Sparkå¦‚ä½•å®ç°Exactly-Onceè¯­ä¹‰ï¼Ÿ](#sparkå¦‚ä½•å®ç°exactly-onceè¯­ä¹‰)
    - [æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜](#æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜)
      - [å¦‚ä½•è¯Šæ–­å’Œè§£å†³Sparkåº”ç”¨çš„æ€§èƒ½é—®é¢˜ï¼Ÿ](#å¦‚ä½•è¯Šæ–­å’Œè§£å†³sparkåº”ç”¨çš„æ€§èƒ½é—®é¢˜)
      - [Sparkåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•è¿›è¡Œç›‘æ§ï¼Ÿ](#sparkåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•è¿›è¡Œç›‘æ§)
      - [Sparkä¸Hadoopç”Ÿæ€ç³»ç»Ÿçš„é›†æˆæœ€ä½³å®è·µï¼Ÿ](#sparkä¸hadoopç”Ÿæ€ç³»ç»Ÿçš„é›†æˆæœ€ä½³å®è·µ)
    - [é«˜çº§åº”ç”¨é¢˜](#é«˜çº§åº”ç”¨é¢˜)
      - [å¦‚ä½•åœ¨Sparkä¸­å®ç°æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Ÿ](#å¦‚ä½•åœ¨sparkä¸­å®ç°æœºå™¨å­¦ä¹ æµæ°´çº¿)
      - [Sparkåœ¨å®æ—¶æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Ÿ](#sparkåœ¨å®æ—¶æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨)
  - [Spark Streaming â­](#spark-streaming-)
    - [æµå¤„ç†æ¦‚å¿µ](#æµå¤„ç†æ¦‚å¿µ)
      - [å¾®æ‰¹æ¬¡å¤„ç†](#å¾®æ‰¹æ¬¡å¤„ç†)
      - [DStreamæ¦‚å¿µ](#dstreamæ¦‚å¿µ)
    - [Structured Streaming](#structured-streaming)
      - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
      - [è¾“å‡ºæ¨¡å¼](#è¾“å‡ºæ¨¡å¼)
      - [çª—å£æ“ä½œ](#çª—å£æ“ä½œ)
    - [å®¹é”™æœºåˆ¶](#å®¹é”™æœºåˆ¶)
      - [Checkpointæœºåˆ¶](#checkpointæœºåˆ¶)
      - [WALæœºåˆ¶](#walæœºåˆ¶)
  - [é‡ç‚¹å†…å®¹æºç è®²è§£](#é‡ç‚¹å†…å®¹æºç è®²è§£)
    - [æ ¸å¿ƒæ¨¡å—æºç ](#æ ¸å¿ƒæ¨¡å—æºç )
    - [è°ƒåº¦å™¨æºç ](#è°ƒåº¦å™¨æºç )
    - [å­˜å‚¨ç³»ç»Ÿæºç ](#å­˜å‚¨ç³»ç»Ÿæºç )
    - [ç½‘ç»œé€šä¿¡æºç ](#ç½‘ç»œé€šä¿¡æºç )
    - [ç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹æºç è¯¦è§£](#ç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹æºç è¯¦è§£)
      - [GroupByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹](#groupbykeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹)
      - [ReduceByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹](#reducebykeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹)
      - [Joinç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹](#joinç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹)
      - [å†…å­˜å­˜å‚¨çŠ¶æ€ç›‘æ§](#å†…å­˜å­˜å‚¨çŠ¶æ€ç›‘æ§)
      - [å†…å­˜å­˜å‚¨ä¼˜åŒ–ç­–ç•¥](#å†…å­˜å­˜å‚¨ä¼˜åŒ–ç­–ç•¥)
    - [ä»»åŠ¡æäº¤æµç¨‹æºç è§£æ](#ä»»åŠ¡æäº¤æµç¨‹æºç è§£æ)
      - [DAGçš„ç”Ÿæˆä¸ä¾èµ–åˆ†æ](#dagçš„ç”Ÿæˆä¸ä¾èµ–åˆ†æ)
      - [ä»»åŠ¡åˆ†å‘ä¸è°ƒåº¦æµç¨‹](#ä»»åŠ¡åˆ†å‘ä¸è°ƒåº¦æµç¨‹)
      - [å¤±è´¥é‡è¯•ä¸å®¹é”™æœºåˆ¶](#å¤±è´¥é‡è¯•ä¸å®¹é”™æœºåˆ¶)
      - [Executorå·¥ä½œæœºåˆ¶ä¸Taskæ‰§è¡Œ](#executorå·¥ä½œæœºåˆ¶ä¸taskæ‰§è¡Œ)
      - [æ•°æ®è¯»å–ã€å¤„ç†ä¸RDDä¾èµ–](#æ•°æ®è¯»å–å¤„ç†ä¸rddä¾èµ–)
      - [Taskç±»å‹ä¸æ‰§è¡Œå·®å¼‚](#taskç±»å‹ä¸æ‰§è¡Œå·®å¼‚)
  - [å®æˆ˜åº”ç”¨æ¡ˆä¾‹](#å®æˆ˜åº”ç”¨æ¡ˆä¾‹)
    - [æ‰¹å¤„ç†åº”ç”¨](#æ‰¹å¤„ç†åº”ç”¨)
      - [ETLæ•°æ®å¤„ç†](#etlæ•°æ®å¤„ç†)
      - [æ•°æ®åˆ†ææ¡ˆä¾‹](#æ•°æ®åˆ†ææ¡ˆä¾‹)
    - [æµå¤„ç†åº”ç”¨](#æµå¤„ç†åº”ç”¨)
      - [å®æ—¶æ•°æ®å¤„ç†](#å®æ—¶æ•°æ®å¤„ç†)
      - [æœºå™¨å­¦ä¹ æµæ°´çº¿](#æœºå™¨å­¦ä¹ æµæ°´çº¿)
    - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
      - [å¼€å‘è§„èŒƒ](#å¼€å‘è§„èŒƒ)
      - [éƒ¨ç½²ç­–ç•¥](#éƒ¨ç½²ç­–ç•¥)
      - [è¿ç»´ç®¡ç†](#è¿ç»´ç®¡ç†)
  - [Sparkå¸¸è§ä»»åŠ¡æŠ¥é”™åŠè§£å†³åŠæ³•](#sparkå¸¸è§ä»»åŠ¡æŠ¥é”™åŠè§£å†³åŠæ³•)
    - [å†…å­˜ç›¸å…³é”™è¯¯](#å†…å­˜ç›¸å…³é”™è¯¯)
      - [1. OutOfMemoryError: Java heap space](#1-outofmemoryerror-java-heap-space)
      - [2. OutOfMemoryError: Direct buffer memory](#2-outofmemoryerror-direct-buffer-memory)
      - [3. OutOfMemoryError: Metaspace](#3-outofmemoryerror-metaspace)
    - [ç½‘ç»œç›¸å…³é”™è¯¯](#ç½‘ç»œç›¸å…³é”™è¯¯)
      - [1. FetchFailedException](#1-fetchfailedexception)
      - [2. ConnectionTimeoutException](#2-connectiontimeoutexception)
    - [åºåˆ—åŒ–ç›¸å…³é”™è¯¯](#åºåˆ—åŒ–ç›¸å…³é”™è¯¯)
      - [1. NotSerializableException](#1-notserializableexception)
      - [2. KryoSerializationException](#2-kryoserializationexception)
    - [èµ„æºç›¸å…³é”™è¯¯](#èµ„æºç›¸å…³é”™è¯¯)
      - [1. ExecutorLostFailure](#1-executorlostfailure)
      - [2. NoClassDefFoundError](#2-noclassdeffounderror)
    - [æ•°æ®ç›¸å…³é”™è¯¯](#æ•°æ®ç›¸å…³é”™è¯¯)
      - [1. FileNotFoundException](#1-filenotfoundexception)
      - [2. DataSourceException](#2-datasourceexception)
    - [è°ƒè¯•å’Œè¯Šæ–­å·¥å…·](#è°ƒè¯•å’Œè¯Šæ–­å·¥å…·)
      - [1. Spark Web UI](#1-spark-web-ui)
      - [2. æ—¥å¿—åˆ†æ](#2-æ—¥å¿—åˆ†æ)
      - [3. æ€§èƒ½åˆ†æå·¥å…·](#3-æ€§èƒ½åˆ†æå·¥å…·)
      - [4. è°ƒè¯•ä»£ç ](#4-è°ƒè¯•ä»£ç )
    - [é¢„é˜²æªæ–½](#é¢„é˜²æªæ–½)
      - [1. é…ç½®ä¼˜åŒ–](#1-é…ç½®ä¼˜åŒ–)
      - [2. ä»£ç æœ€ä½³å®è·µ](#2-ä»£ç æœ€ä½³å®è·µ)
      - [3. ç›‘æ§å‘Šè­¦](#3-ç›‘æ§å‘Šè­¦)
  - [Sparkæ€§èƒ½ä¼˜åŒ–è¯¦è§£](#sparkæ€§èƒ½ä¼˜åŒ–è¯¦è§£)
    - [æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–](#æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–)
      - [1. å­˜å‚¨æ ¼å¼ä¼˜åŒ–](#1-å­˜å‚¨æ ¼å¼ä¼˜åŒ–)
      - [2. åˆ†åŒºç­–ç•¥ä¼˜åŒ–](#2-åˆ†åŒºç­–ç•¥ä¼˜åŒ–)
      - [3. è°“è¯ä¸‹æ¨ä¼˜åŒ–](#3-è°“è¯ä¸‹æ¨ä¼˜åŒ–)
    - [Joinä¼˜åŒ–](#joinä¼˜åŒ–)
      - [1. Joinç­–ç•¥é€‰æ‹©](#1-joinç­–ç•¥é€‰æ‹©)
      - [2. å¹¿æ’­Joinä¼˜åŒ–](#2-å¹¿æ’­joinä¼˜åŒ–)
      - [3. Sort Merge Joinä¼˜åŒ–](#3-sort-merge-joinä¼˜åŒ–)
      - [4. æ•°æ®å€¾æ–œå¤„ç†](#4-æ•°æ®å€¾æ–œå¤„ç†)
    - [ç¼“å­˜ä¸æŒä¹…åŒ–](#ç¼“å­˜ä¸æŒä¹…åŒ–)
      - [1. å­˜å‚¨çº§åˆ«é€‰æ‹©](#1-å­˜å‚¨çº§åˆ«é€‰æ‹©)
      - [2. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–](#2-ç¼“å­˜ç­–ç•¥ä¼˜åŒ–)
      - [3. Checkpointä¼˜åŒ–](#3-checkpointä¼˜åŒ–)
    - [ä»£ç å±‚é¢ä¼˜åŒ–](#ä»£ç å±‚é¢ä¼˜åŒ–)
      - [1. ç®—å­é€‰æ‹©ä¼˜åŒ–](#1-ç®—å­é€‰æ‹©ä¼˜åŒ–)
      - [2. æ•°æ®ç»“æ„ä¼˜åŒ–](#2-æ•°æ®ç»“æ„ä¼˜åŒ–)
      - [3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–](#3-å†…å­˜ä½¿ç”¨ä¼˜åŒ–)
      - [4. å¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ä¼˜åŒ–](#4-å¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ä¼˜åŒ–)
    - [ç½‘ç»œä¸I/Oä¼˜åŒ–](#ç½‘ç»œä¸ioä¼˜åŒ–)
      - [1. åºåˆ—åŒ–ä¼˜åŒ–](#1-åºåˆ—åŒ–ä¼˜åŒ–)
      - [2. å‹ç¼©ä¼˜åŒ–](#2-å‹ç¼©ä¼˜åŒ–)
      - [3. ç½‘ç»œè°ƒä¼˜](#3-ç½‘ç»œè°ƒä¼˜)
  - [Sparké€šä¿¡ä¸ç½‘ç»œ ğŸŒ](#sparké€šä¿¡ä¸ç½‘ç»œ-)
    - [NettyåŸºç¡€ä¸åº”ç”¨](#nettyåŸºç¡€ä¸åº”ç”¨)
      - [Nettyæ¶æ„æ¨¡å‹](#nettyæ¶æ„æ¨¡å‹)
      - [å…³é”®å‚æ•°é…ç½®](#å…³é”®å‚æ•°é…ç½®)
      - [å¼‚å¸¸æ’æŸ¥](#å¼‚å¸¸æ’æŸ¥)
  - [Sparkå…³é”®å‚æ•°ä¸é…ç½® âš™ï¸](#sparkå…³é”®å‚æ•°ä¸é…ç½®-ï¸)
    - [èµ„æºç›¸å…³å‚æ•°](#èµ„æºç›¸å…³å‚æ•°)
    - [JVMç›¸å…³å‚æ•°](#jvmç›¸å…³å‚æ•°)
    - [æ€§èƒ½ä¼˜åŒ–å‚æ•°](#æ€§èƒ½ä¼˜åŒ–å‚æ•°)
    - [é…ç½®æ¨¡æ¿](#é…ç½®æ¨¡æ¿)
  - [Sparké«˜çº§ç‰¹æ€§ä¸æ‰©å±• ğŸš€](#sparké«˜çº§ç‰¹æ€§ä¸æ‰©å±•-)
    - [è‡ªå®šä¹‰æ•°æ®æº](#è‡ªå®šä¹‰æ•°æ®æº)
    - [æ’ä»¶æœºåˆ¶](#æ’ä»¶æœºåˆ¶)
    - [æ‰©å±•ç‚¹å¼€å‘](#æ‰©å±•ç‚¹å¼€å‘)
    - [ç¬¬ä¸‰æ–¹é›†æˆ](#ç¬¬ä¸‰æ–¹é›†æˆ)
  - [Sparkå®‰å…¨ä¸æƒé™ç®¡ç† ğŸ”](#sparkå®‰å…¨ä¸æƒé™ç®¡ç†-)
    - [è®¤è¯æœºåˆ¶](#è®¤è¯æœºåˆ¶)
    - [æˆæƒæ§åˆ¶](#æˆæƒæ§åˆ¶)
    - [æ•°æ®åŠ å¯†](#æ•°æ®åŠ å¯†)
    - [å®¡è®¡æ—¥å¿—](#å®¡è®¡æ—¥å¿—)
  - [Sparkç›‘æ§ä¸è¿ç»´ ğŸ“Š](#sparkç›‘æ§ä¸è¿ç»´-)
    - [ç›‘æ§ä½“ç³»](#ç›‘æ§ä½“ç³»)
    - [å‘Šè­¦æœºåˆ¶](#å‘Šè­¦æœºåˆ¶)
    - [æ—¥å¿—ç®¡ç†](#æ—¥å¿—ç®¡ç†)
    - [è¿ç»´è‡ªåŠ¨åŒ–](#è¿ç»´è‡ªåŠ¨åŒ–)
  - [Sparkä¼ä¸šçº§å®è·µ ğŸ¢](#sparkä¼ä¸šçº§å®è·µ-)
    - [å¤§æ•°æ®å¹³å°æ¶æ„](#å¤§æ•°æ®å¹³å°æ¶æ„)
    - [æ•°æ®æ¹–å»ºè®¾](#æ•°æ®æ¹–å»ºè®¾)
    - [å®æ—¶è®¡ç®—å¹³å°](#å®æ—¶è®¡ç®—å¹³å°)
    - [æœºå™¨å­¦ä¹ å¹³å°](#æœºå™¨å­¦ä¹ å¹³å°)
  - [Sparkæ•…éšœè¯Šæ–­ä¸è°ƒä¼˜å®æˆ˜ ğŸ› ï¸](#sparkæ•…éšœè¯Šæ–­ä¸è°ƒä¼˜å®æˆ˜-ï¸)
    - [æ€§èƒ½ç“¶é¢ˆåˆ†æ](#æ€§èƒ½ç“¶é¢ˆåˆ†æ)
    - [ç½‘ç»œè°ƒä¼˜å®æˆ˜](#ç½‘ç»œè°ƒä¼˜å®æˆ˜)
    - [å­˜å‚¨è°ƒä¼˜å®æˆ˜](#å­˜å‚¨è°ƒä¼˜å®æˆ˜)

---

## Spark æ¦‚è¿°ä¸ç¯å¢ƒ

### Sparkç®€ä»‹

**Apache Spark** æ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡ã€‚å®ƒæä¾›äº†é«˜çº§APIï¼ˆJavaã€Scalaã€Pythonã€Rï¼‰ï¼Œå¹¶æ”¯æŒç”¨äºSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¤„ç†çš„ä¼˜åŒ–å¼•æ“ã€‚

#### Sparkç‰¹ç‚¹ä¸ä¼˜åŠ¿

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- **é€Ÿåº¦å¿«**ï¼šå†…å­˜è®¡ç®—æ¯”Hadoop MapReduceå¿«100å€ï¼Œç£ç›˜è®¡ç®—å¿«10å€
- **æ˜“ç”¨æ€§**ï¼šæä¾›å¤šç§è¯­è¨€APIï¼Œæ”¯æŒ80å¤šç§é«˜çº§ç®—å­
- **é€šç”¨æ€§**ï¼šæ”¯æŒSQLæŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
- **å…¼å®¹æ€§**ï¼šå¯è¿è¡Œåœ¨Hadoopã€Mesosã€Kubernetesã€standaloneç­‰é›†ç¾¤ä¸Š

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§         | Spark               | Hadoop MapReduce |
| ------------ | ------------------- | ---------------- |
| **è®¡ç®—æ¨¡å¼** | å†…å­˜è®¡ç®— + ç£ç›˜å­˜å‚¨ | ç£ç›˜è®¡ç®—         |
| **æ•°æ®å…±äº«** | RDDå†…å­˜å…±äº«         | ç£ç›˜æ–‡ä»¶ç³»ç»Ÿ     |
| **è¿­ä»£è®¡ç®—** | æ”¯æŒé«˜æ•ˆè¿­ä»£        | æ•ˆç‡ä½           |
| **å®æ—¶å¤„ç†** | æ”¯æŒæµå¤„ç†          | ä»…æ‰¹å¤„ç†         |
| **å®¹é”™æœºåˆ¶** | RDDè¡€ç»Ÿæ¢å¤         | æ•°æ®å‰¯æœ¬         |
| **å¼€å‘æ•ˆç‡** | ä»£ç ç®€æ´            | ä»£ç å¤æ‚         |

#### Spark vs Hadoop MapReduce

```mermaid
graph TD
    A[æ•°æ®è¾“å…¥] --> B{è®¡ç®—å¼•æ“}
    
    B -->|MapReduce| C[Mapé˜¶æ®µ]
    C --> D[Shuffleå†™ç£ç›˜]
    D --> E[Reduceé˜¶æ®µ]
    E --> F[ç»“æœå†™HDFS]
    
    B -->|Spark| G[RDDè½¬æ¢]
    G --> H[å†…å­˜è®¡ç®—]
    H --> I[ç»“æœè¾“å‡º]
    
    style C fill:#ffcccb
    style D fill:#ffcccb
    style E fill:#ffcccb
    style F fill:#ffcccb
    style G fill:#90EE90
    style H fill:#90EE90
    style I fill:#90EE90
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- **å†…å­˜è®¡ç®—**ï¼šSparkåœ¨å†…å­˜ä¸­ç¼“å­˜æ•°æ®ï¼Œé¿å…é‡å¤I/O
- **DAGæ‰§è¡Œ**ï¼šSparkå°†ä½œä¸šæ„å»ºä¸ºDAGï¼Œä¼˜åŒ–æ‰§è¡Œè®¡åˆ’
- **Pipelining**ï¼šSparkæ”¯æŒç®—å­æµæ°´çº¿ï¼Œå‡å°‘ä¸­é—´æ•°æ®å­˜å‚¨
- **ä»£ç ç”Ÿæˆ**ï¼šCatalystä¼˜åŒ–å™¨ç”Ÿæˆé«˜æ•ˆçš„Javaä»£ç 

#### Sparkåº”ç”¨åœºæ™¯

**å…¸å‹åº”ç”¨é¢†åŸŸ**ï¼š

| åœºæ™¯           | æè¿°                       | ä¼˜åŠ¿                       |
| -------------- | -------------------------- | -------------------------- |
| **æ•°æ®ETL**    | å¤§è§„æ¨¡æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åŠ è½½ | å¤„ç†é€Ÿåº¦å¿«ï¼Œæ”¯æŒå¤šç§æ•°æ®æº |
| **å®æ—¶æµå¤„ç†** | å®æ—¶æ•°æ®åˆ†æã€ç›‘æ§å‘Šè­¦     | ä½å»¶è¿Ÿï¼Œé«˜ååé‡           |
| **æœºå™¨å­¦ä¹ **   | å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ     | MLlibç”Ÿæ€ï¼Œè¿­ä»£è®¡ç®—ä¼˜åŠ¿    |
| **äº¤äº’å¼æŸ¥è¯¢** | å³å¸­æŸ¥è¯¢ã€æ•°æ®æ¢ç´¢         | SQLæ”¯æŒï¼Œå“åº”é€Ÿåº¦å¿«        |
| **å›¾è®¡ç®—**     | ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿ     | GraphXå›¾å¤„ç†èƒ½åŠ›           |

### Sparkç”Ÿæ€ç³»ç»Ÿ

#### æ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    A[Spark Core] --> B[Spark SQL]
    A --> C[Spark Streaming]
    A --> D[MLlib]
    A --> E[GraphX]
    
    B --> F[DataFrames & Datasets]
    B --> G[Catalyst Optimizer]
    
    C --> H[DStreams]
    C --> I[Structured Streaming]
    
    D --> J[Classification]
    D --> K[Clustering] 
    D --> L[Collaborative Filtering]
    
    E --> M[Graph Processing]
    E --> N[Graph Algorithms]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#e8f5e8
    style E fill:#fce4ec
```

**ç»„ä»¶è¯¦è§£**ï¼š

1. **Spark Core**ï¼š
   - åŸºç¡€è¿è¡Œæ—¶å¼•æ“
   - RDDæŠ½è±¡
   - ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†ã€å®¹é”™æ¢å¤

2. **Spark SQL**ï¼š
   - ç»“æ„åŒ–æ•°æ®å¤„ç†
   - DataFrame/Dataset API
   - JDBC/ODBCè¿æ¥å™¨

3. **Spark Streaming**ï¼š
   - æµæ•°æ®å¤„ç†
   - å¾®æ‰¹æ¬¡å¤„ç†æ¨¡å‹
   - ä¸æ‰¹å¤„ç†ä»£ç ç»Ÿä¸€

4. **MLlib**ï¼š
   - æœºå™¨å­¦ä¹ åº“
   - åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤
   - ç®¡é“API

5. **GraphX**ï¼š
   - å›¾è®¡ç®—æ¡†æ¶
   - å›¾ç®—æ³•åº“
   - å›¾å¹¶è¡Œè®¡ç®—

#### ç”Ÿæ€ç»„ä»¶

**å¤–éƒ¨é›†æˆ**ï¼š

| ç»„ä»¶ç±»å‹     | ç»„ä»¶åç§°              | ç”¨é€”         |
| ------------ | --------------------- | ------------ |
| **èµ„æºç®¡ç†** | YARNã€Mesosã€K8s      | é›†ç¾¤èµ„æºç®¡ç† |
| **å­˜å‚¨ç³»ç»Ÿ** | HDFSã€S3ã€HBase       | æ•°æ®å­˜å‚¨     |
| **æ•°æ®æ ¼å¼** | Parquetã€Avroã€JSON   | æ•°æ®åºåˆ—åŒ–   |
| **æµæ•°æ®**   | Kafkaã€Flumeã€Kinesis | æ•°æ®é‡‡é›†     |
| **ç›‘æ§å·¥å…·** | Gangliaã€Nagios       | é›†ç¾¤ç›‘æ§     |

### Sparkç¯å¢ƒæ­å»º

#### æœ¬åœ°æ¨¡å¼

**ä¸‹è½½å®‰è£…**ï¼š
```bash
# ä¸‹è½½Spark
wget https://downloads.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

# è§£å‹
tar -xzf spark-3.4.0-bin-hadoop3.tgz
cd spark-3.4.0-bin-hadoop3

# è®¾ç½®ç¯å¢ƒå˜é‡
export SPARK_HOME=/path/to/spark-3.4.0-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
```

**å¯åŠ¨æœ¬åœ°æ¨¡å¼**ï¼š
```bash
# å¯åŠ¨Spark Shell (Scala)
spark-shell --master local[2]

# å¯åŠ¨PySpark Shell (Python)
pyspark --master local[2]

# æäº¤åº”ç”¨ç¨‹åº
spark-submit \
  --master local[2] \
  --class org.apache.spark.examples.SparkPi \
  examples/jars/spark-examples_2.12-3.4.0.jar \
  10
```

#### é›†ç¾¤æ¨¡å¼

**Standaloneæ¨¡å¼éƒ¨ç½²**ï¼š
```bash
# 1. é…ç½®slavesæ–‡ä»¶
echo "worker1" >> conf/slaves
echo "worker2" >> conf/slaves

# 2. å¯åŠ¨Master
./sbin/start-master.sh

# 3. å¯åŠ¨Workers
./sbin/start-slaves.sh

# 4. æäº¤åº”ç”¨åˆ°é›†ç¾¤
spark-submit \
  --master spark://master:7077 \
  --deploy-mode cluster \
  --class MainClass \
  --conf spark.sql.adaptive.enabled=true \
  app.jar
```

**YARNæ¨¡å¼éƒ¨ç½²**ï¼š
```bash
# é…ç½®Hadoopç¯å¢ƒ
export HADOOP_HOME=/path/to/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# æäº¤åˆ°YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class MainClass \
  app.jar
```

#### å¸¸ç”¨é…ç½®

**æ ¸å¿ƒé…ç½®å‚æ•°**ï¼š
```properties
# spark-defaults.conf

# åº”ç”¨ç¨‹åºé…ç½®
spark.app.name                MySparkApp
spark.master                  yarn
spark.submit.deployMode       cluster

# èµ„æºé…ç½®
spark.driver.memory           2g
spark.driver.cores            1
spark.executor.memory         4g
spark.executor.cores          2
spark.executor.instances      10

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled           true
spark.dynamicAllocation.minExecutors      2
spark.dynamicAllocation.maxExecutors      20
spark.dynamicAllocation.initialExecutors  5

# Shuffleé…ç½®
spark.sql.adaptive.enabled                true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled       true

# åºåˆ—åŒ–
spark.serializer              org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired  false

# å‹ç¼©
spark.sql.parquet.compression.codec  snappy
spark.sql.orc.compression.codec      snappy
```

**æ—¥å¿—é…ç½®**ï¼š
```properties
# log4j.properties
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# è®¾ç½®Sparkæ—¥å¿—çº§åˆ«
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.springframework.core.env.ConfigUtils=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.util.Utils=WARN
```

---

## Spark æ ¸å¿ƒæ¦‚å¿µ â­

### RDDæ ¸å¿ƒæ¦‚å¿µ

**RDD (Resilient Distributed Dataset)** æ˜¯Sparkçš„æ ¸å¿ƒæŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€å¯å¹¶è¡Œè®¡ç®—çš„æ•°æ®é›†åˆã€‚

#### RDDç‰¹æ€§

```mermaid
graph TD
    A[RDDæ ¸å¿ƒç‰¹æ€§] --> B[ä¸å¯å˜æ€§<br/>Immutable]
    A --> C[åˆ†å¸ƒå¼<br/>Distributed]
    A --> D[å¼¹æ€§å®¹é”™<br/>Resilient]
    A --> E[æƒ°æ€§æ±‚å€¼<br/>Lazy Evaluation]
    A --> F[åˆ†åŒºè®¡ç®—<br/>Partitioned]
    
    B --> B1[æ•°æ®ä¸€æ—¦åˆ›å»ºä¸å¯ä¿®æ”¹]
    C --> C1[æ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤å¤šä¸ªèŠ‚ç‚¹]
    D --> D1[é€šè¿‡è¡€ç»Ÿä¿¡æ¯è‡ªåŠ¨å®¹é”™]
    E --> E1[Transformæ“ä½œå»¶è¿Ÿæ‰§è¡Œ]
    F --> F1[æ”¯æŒå¹¶è¡Œè®¡ç®—]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#ffebee
    style E fill:#f3e5f5
    style F fill:#fce4ec
```

**RDDçš„äº”å¤§ç‰¹æ€§**ï¼š

| ç‰¹æ€§         | æè¿°                    | æ„ä¹‰             |
| ------------ | ----------------------- | ---------------- |
| **åˆ†åŒºåˆ—è¡¨** | RDDç”±å¤šä¸ªåˆ†åŒºç»„æˆ       | æ”¯æŒå¹¶è¡Œè®¡ç®—     |
| **è®¡ç®—å‡½æ•°** | æ¯ä¸ªåˆ†åŒºéƒ½æœ‰è®¡ç®—å‡½æ•°    | å®šä¹‰æ•°æ®å¤„ç†é€»è¾‘ |
| **ä¾èµ–å…³ç³»** | RDDä¹‹é—´çš„ä¾èµ–å…³ç³»       | æ”¯æŒå®¹é”™æ¢å¤     |
| **åˆ†åŒºå™¨**   | Key-Value RDDçš„åˆ†åŒºç­–ç•¥ | ä¼˜åŒ–æ•°æ®åˆ†å¸ƒ     |
| **ä½ç½®åå¥½** | è®¡ç®—åˆ†åŒºçš„æœ€ä½³ä½ç½®      | æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–   |

#### RDDæ“ä½œåˆ†ç±»

**Transformation vs Action**ï¼š

```mermaid
graph LR
    A[RDDæ“ä½œ] --> B[Transformation<br/>è½¬æ¢æ“ä½œ]
    A --> C[Action<br/>è¡ŒåŠ¨æ“ä½œ]
    
    B --> D[æƒ°æ€§æ‰§è¡Œ<br/>ä¸ç«‹å³è®¡ç®—]
    B --> E[è¿”å›æ–°RDD]
    B --> F[æ„å»ºè®¡ç®—å›¾]
    
    C --> G[ç«‹å³æ‰§è¡Œ<br/>è§¦å‘è®¡ç®—]
    C --> H[è¿”å›ç»“æœå€¼]
    C --> I[æäº¤ä½œä¸š]
    
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**å¸¸ç”¨Transformationæ“ä½œ**ï¼š
```scala
// åˆ›å»ºRDD
val rdd = sc.parallelize(1 to 100, 4)

// mapï¼šä¸€å¯¹ä¸€è½¬æ¢
val mapRDD = rdd.map(x => x * 2)

// filterï¼šè¿‡æ»¤æ•°æ®
val filterRDD = rdd.filter(x => x % 2 == 0)

// flatMapï¼šä¸€å¯¹å¤šè½¬æ¢
val flatMapRDD = rdd.flatMap(x => 1 to x)

// groupByKeyï¼šæŒ‰é”®åˆ†ç»„
val kvRDD = rdd.map(x => (x % 10, x))
val groupedRDD = kvRDD.groupByKey()

// reduceByKeyï¼šæŒ‰é”®èšåˆ
val reducedRDD = kvRDD.reduceByKey(_ + _)

// joinï¼šè¿æ¥æ“ä½œ
val rdd2 = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c")))
val joinedRDD = kvRDD.join(rdd2)
```

**å¸¸ç”¨Actionæ“ä½œ**ï¼š
```scala
// collectï¼šæ”¶é›†æ‰€æœ‰å…ƒç´ åˆ°Driver
val result = rdd.collect()

// countï¼šè®¡ç®—å…ƒç´ æ•°é‡
val cnt = rdd.count()

// firstï¼šè·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
val firstElement = rdd.first()

// takeï¼šè·å–å‰nä¸ªå…ƒç´ 
val firstN = rdd.take(10)

// reduceï¼šèšåˆæ‰€æœ‰å…ƒç´ 
val sum = rdd.reduce(_ + _)

// foreachï¼šéå†æ¯ä¸ªå…ƒç´ 
rdd.foreach(println)

// saveAsTextFileï¼šä¿å­˜åˆ°æ–‡ä»¶
rdd.saveAsTextFile("hdfs://output/path")
```

#### RDDä¾èµ–å…³ç³»

**ä¾èµ–ç±»å‹**ï¼š

```mermaid
graph TD
    A[RDDä¾èµ–å…³ç³»] --> B[çª„ä¾èµ–<br/>Narrow Dependency]
    A --> C[å®½ä¾èµ–<br/>Wide Dependency]
    
    B --> D[ä¸€å¯¹ä¸€æ˜ å°„<br/>1:1 Mapping]
    B --> E[åŒä¸€Stageå†…<br/>Pipelineæ‰§è¡Œ]
    B --> F[å±€éƒ¨å¤±è´¥æ¢å¤]
    
    C --> G[ä¸€å¯¹å¤šæ˜ å°„<br/>1:N Mapping]
    C --> H[éœ€è¦Shuffle<br/>è·¨Stageæ‰§è¡Œ]
    C --> I[å…¨é‡é‡æ–°è®¡ç®—]
    
    style B fill:#e8f5e8
    style C fill:#ffebee
```

**çª„ä¾èµ–ç¤ºä¾‹**ï¼š
```scala
// map, filter, unionç­‰æ“ä½œäº§ç”Ÿçª„ä¾èµ–
val rdd1 = sc.parallelize(1 to 10, 2)
val rdd2 = rdd1.map(_ * 2)        // çª„ä¾èµ–
val rdd3 = rdd2.filter(_ > 10)    // çª„ä¾èµ–
```

**å®½ä¾èµ–ç¤ºä¾‹**ï¼š
```scala
// groupByKey, reduceByKey, joinç­‰æ“ä½œäº§ç”Ÿå®½ä¾èµ–
val rdd1 = sc.parallelize(Seq((1, "a"), (2, "b"), (1, "c")), 2)
val rdd2 = rdd1.groupByKey()      // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
val rdd3 = rdd1.reduceByKey(_ + _) // å®½ä¾èµ–ï¼Œéœ€è¦Shuffle
```

### DataFrameä¸Dataset

#### DataFrameæ¦‚å¿µ

**DataFrame** æ˜¯Spark SQLçš„æ ¸å¿ƒæŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªä»¥å‘½ååˆ—æ–¹å¼ç»„ç»‡çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºå…³ç³»æ•°æ®åº“ä¸­çš„è¡¨ã€‚

**DataFrameç‰¹ç‚¹**ï¼š
- **ç»“æ„åŒ–æ•°æ®**ï¼šå…·æœ‰æ˜ç¡®çš„Schemaå®šä¹‰
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šä½¿ç”¨Catalystä¼˜åŒ–å™¨
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šScalaã€Javaã€Pythonã€R
- **ä¸°å¯ŒAPI**ï¼šSQLé£æ ¼å’Œå‡½æ•°å¼API

**DataFrameåˆ›å»º**ï¼š
```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("DataFrameExample")
  .getOrCreate()

import spark.implicits._

// æ–¹å¼1ï¼šä»RDDåˆ›å»º
val rdd = sc.parallelize(Seq(("Alice", 25), ("Bob", 30), ("Charlie", 35)))
val df1 = rdd.toDF("name", "age")

// æ–¹å¼2ï¼šä»åºåˆ—åˆ›å»º
val df2 = Seq(("Alice", 25), ("Bob", 30)).toDF("name", "age")

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val df3 = spark.read.json("path/to/file.json")
val df4 = spark.read.parquet("path/to/file.parquet")

// æ–¹å¼4ï¼šé€šè¿‡Schemaåˆ›å»º
val schema = StructType(Seq(
  StructField("name", StringType, nullable = true),
  StructField("age", IntegerType, nullable = true)
))
val df5 = spark.createDataFrame(rdd, schema)
```

#### Datasetæ¦‚å¿µ

**Dataset** æ˜¯DataFrameçš„æ‰©å±•ï¼Œæä¾›äº†ç±»å‹å®‰å…¨çš„é¢å‘å¯¹è±¡ç¼–ç¨‹æ¥å£ã€‚

**Datasetç‰¹ç‚¹**ï¼š
- **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šäº«å—Catalystä¼˜åŒ–å™¨
- **å‡½æ•°å¼API**ï¼šæ”¯æŒlambdaè¡¨è¾¾å¼
- **ç¼–ç å™¨æ”¯æŒ**ï¼šè‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–

**Datasetåˆ›å»º**ï¼š
```scala
// å®šä¹‰æ ·ä¾‹ç±»
case class Person(name: String, age: Int, city: String)

// æ–¹å¼1ï¼šä»åºåˆ—åˆ›å»º
val ds1 = Seq(
  Person("Alice", 25, "Beijing"),
  Person("Bob", 30, "Shanghai")
).toDS()

// æ–¹å¼2ï¼šä»DataFrameè½¬æ¢
val ds2 = df.as[Person]

// æ–¹å¼3ï¼šä»å¤–éƒ¨æ•°æ®æºåˆ›å»º
val ds3 = spark.read.json("path/to/file.json").as[Person]
```

#### ä¸‰è€…å¯¹æ¯”åˆ†æ ğŸ”¥

**RDD vs DataFrame vs Dataset å…¨é¢å¯¹æ¯”**ï¼š

| ç‰¹æ€§           | RDD                    | DataFrame           | Dataset            |
| -------------- | ---------------------- | ------------------- | ------------------ |
| **æ•°æ®æŠ½è±¡**   | åˆ†å¸ƒå¼å¯¹è±¡é›†åˆ         | ç»“æ„åŒ–æ•°æ®è¡¨        | ç±»å‹å®‰å…¨çš„æ•°æ®è¡¨   |
| **ç¼–è¯‘æ—¶æ£€æŸ¥** | âŒ è¿è¡Œæ—¶é”™è¯¯           | âŒ è¿è¡Œæ—¶é”™è¯¯        | âœ… ç¼–è¯‘æ—¶é”™è¯¯       |
| **æ‰§è¡Œä¼˜åŒ–**   | âŒ æ— ä¼˜åŒ–               | âœ… Catalystä¼˜åŒ–      | âœ… Catalystä¼˜åŒ–     |
| **ä»£ç ç”Ÿæˆ**   | âŒ æ—                    | âœ… æœ‰                | âœ… æœ‰               |
| **åºåˆ—åŒ–**     | Java/Kryoåºåˆ—åŒ–        | TungstenäºŒè¿›åˆ¶æ ¼å¼  | TungstenäºŒè¿›åˆ¶æ ¼å¼ |
| **APIé£æ ¼**    | å‡½æ•°å¼                 | SQL + å‡½æ•°å¼        | ç±»å‹å®‰å…¨å‡½æ•°å¼     |
| **æ€§èƒ½**       | ä½                     | é«˜                  | é«˜                 |
| **æ˜“ç”¨æ€§**     | å¤æ‚                   | ç®€å•                | ä¸­ç­‰               |
| **é€‚ç”¨åœºæ™¯**   | ä½çº§æ“ä½œã€éç»“æ„åŒ–æ•°æ® | SQLæŸ¥è¯¢ã€ç»“æ„åŒ–æ•°æ® | ç±»å‹å®‰å…¨è¦æ±‚é«˜     |

**æ€§èƒ½å¯¹æ¯”**ï¼š
```scala
// æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
import org.apache.spark.sql.functions._

// RDDæ–¹å¼ - æ€§èƒ½è¾ƒä½
val rddResult = rdd.filter(_.age > 25)
  .map(p => (p.city, 1))
  .reduceByKey(_ + _)
  .collect()

// DataFrameæ–¹å¼ - æ€§èƒ½ä¼˜åŒ–
val dfResult = df.filter($"age" > 25)
  .groupBy("city")
  .count()
  .collect()

// Datasetæ–¹å¼ - ç±»å‹å®‰å…¨ + æ€§èƒ½ä¼˜åŒ–
val dsResult = ds.filter(_.age > 25)
  .groupByKey(_.city)
  .count()
  .collect()
```

**é€‰æ‹©å»ºè®®**ï¼š

```mermaid
graph TD
    A[é€‰æ‹©æ•°æ®æŠ½è±¡] --> B{æ•°æ®ç±»å‹}
    B -->|éç»“æ„åŒ–| C[ä½¿ç”¨RDD]
    B -->|ç»“æ„åŒ–| D{ç±»å‹å®‰å…¨è¦æ±‚}
    D -->|ä¸éœ€è¦| E[ä½¿ç”¨DataFrame]
    D -->|éœ€è¦| F[ä½¿ç”¨Dataset]
    
    C --> G[å¤æ‚æ•°æ®å¤„ç†<br/>åº•å±‚æ§åˆ¶]
    E --> H[SQLæŸ¥è¯¢<br/>é«˜æ€§èƒ½è¦æ±‚]
    F --> I[ç±»å‹å®‰å…¨<br/>ç¼–è¯‘æ—¶æ£€æŸ¥]
    
    style C fill:#ffebee
    style E fill:#e8f5e8
    style F fill:#e1f5fe
```

### åˆ†åŒºæœºåˆ¶

#### åˆ†åŒºç­–ç•¥

**åˆ†åŒºçš„é‡è¦æ€§**ï¼š
- **å¹¶è¡Œåº¦æ§åˆ¶**ï¼šåˆ†åŒºæ•°å†³å®šä»»åŠ¡å¹¶è¡Œåº¦
- **æ•°æ®æœ¬åœ°æ€§**ï¼šå‡å°‘ç½‘ç»œä¼ è¾“
- **è´Ÿè½½å‡è¡¡**ï¼šé¿å…æ•°æ®å€¾æ–œ
- **èµ„æºåˆ©ç”¨**ï¼šå……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æº

**åˆ†åŒºå™¨ç±»å‹**ï¼š

| åˆ†åŒºå™¨               | é€‚ç”¨æ•°æ®ç±»å‹          | åˆ†åŒºç­–ç•¥                  | ä½¿ç”¨åœºæ™¯     |
| -------------------- | --------------------- | ------------------------- | ------------ |
| **HashPartitioner**  | Key-Value RDD         | Hash(key) % numPartitions | å‡åŒ€åˆ†å¸ƒçš„é”® |
| **RangePartitioner** | å¯æ’åºçš„Key-Value RDD | æŒ‰é”®å€¼èŒƒå›´åˆ†åŒº            | æœ‰åºæ•°æ®æŸ¥è¯¢ |
| **è‡ªå®šä¹‰åˆ†åŒºå™¨**     | ä»»æ„ç±»å‹              | ç”¨æˆ·å®šä¹‰é€»è¾‘              | ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚ |

**åˆ†åŒºæ“ä½œç¤ºä¾‹**ï¼š
```scala
// åˆ›å»ºå¸¦åˆ†åŒºçš„RDD
val rdd = sc.parallelize(1 to 100, 4)  // 4ä¸ªåˆ†åŒº

// æŸ¥çœ‹åˆ†åŒºä¿¡æ¯
println(s"åˆ†åŒºæ•°: ${rdd.getNumPartitions}")
println(s"åˆ†åŒºå†…å®¹: ${rdd.glom().collect().map(_.toList).toList}")

// é‡æ–°åˆ†åŒº
val repartitionedRDD = rdd.repartition(8)  // å¢åŠ åˆ†åŒºæ•°
val coalescedRDD = rdd.coalesce(2)         // å‡å°‘åˆ†åŒºæ•°

// Key-Value RDDåˆ†åŒº
val kvRDD = sc.parallelize(Seq((1, "a"), (2, "b"), (3, "c"), (4, "d")), 2)

// ä½¿ç”¨HashPartitioner
val hashPartitioned = kvRDD.partitionBy(new HashPartitioner(3))

// ä½¿ç”¨RangePartitioner
val rangePartitioned = kvRDD.partitionBy(new RangePartitioner(3, kvRDD))
```

**è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼š
```scala
import org.apache.spark.Partitioner

// è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼šæŒ‰ç”¨æˆ·IDçš„åœ°åŒºåˆ†åŒº
class RegionPartitioner(regions: Array[String]) extends Partitioner {
  
  override def numPartitions: Int = regions.length
  
  override def getPartition(key: Any): Int = {
    val userId = key.asInstanceOf[String]
    val region = getUserRegion(userId)
    math.abs(regions.indexOf(region)) % numPartitions
  }
  
  private def getUserRegion(userId: String): String = {
    // æ ¹æ®ç”¨æˆ·IDç¡®å®šåœ°åŒºçš„ä¸šåŠ¡é€»è¾‘
    userId.substring(0, 2) match {
      case "01" | "02" => "North"
      case "03" | "04" => "South"
      case "05" | "06" => "East"
      case _ => "West"
    }
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
val regions = Array("North", "South", "East", "West")
val customPartitioner = new RegionPartitioner(regions)
val customPartitioned = kvRDD.partitionBy(customPartitioner)
```

#### åˆ†åŒºè°ƒä¼˜

**åˆ†åŒºæ•°ä¼˜åŒ–**ï¼š

```scala
// åˆ†åŒºæ•°è®¾ç½®åŸåˆ™
val totalCores = 16  // é›†ç¾¤æ€»æ ¸å¿ƒæ•°
val optimalPartitions = totalCores * 2  // æ¨èåˆ†åŒºæ•°ä¸ºæ ¸å¿ƒæ•°çš„2-3å€

// åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
def getOptimalPartitions(dataSize: Long): Int = {
  val targetPartitionSize = 128 * 1024 * 1024  // 128MB per partition
  math.max(1, (dataSize / targetPartitionSize).toInt)
}

// åˆ†åŒºå€¾æ–œæ£€æµ‹
def detectPartitionSkew(rdd: RDD[_]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  val maxSize = partitionSizes.map(_._2).max
  val skewRatio = maxSize.toDouble / avgSize
  
  if (skewRatio > 2.0) {
    println(s"è­¦å‘Šï¼šæ£€æµ‹åˆ°åˆ†åŒºå€¾æ–œï¼Œå€¾æ–œæ¯”ä¾‹: $skewRatio")
    partitionSizes.foreach { case (index, size) =>
      println(s"åˆ†åŒº $index: $size æ¡è®°å½•")
    }
  }
}
```

**åˆ†åŒºä¼˜åŒ–ç­–ç•¥**ï¼š

1. **é¢„åˆ†åŒºç­–ç•¥**ï¼š
```scala
// æ ¹æ®æ•°æ®ç‰¹å¾é¢„åˆ†åŒº
val userRDD = sc.textFile("hdfs://users/*")
  .map(parseUser)
  .partitionBy(new HashPartitioner(numPartitions))
  .cache()  // ç¼“å­˜é¢„åˆ†åŒºçš„æ•°æ®
```

2. **Coalesce vs Repartition**ï¼š
```scala
// Coalesceï¼šå‡å°‘åˆ†åŒºï¼Œé¿å…å…¨é‡Shuffle
val reducedRDD = largeRDD.coalesce(10)

// Repartitionï¼šé‡æ–°åˆ†åŒºï¼Œä¼šè¿›è¡Œå…¨é‡Shuffle
val reshuffledRDD = largeRDD.repartition(20)

// æ¡ä»¶åˆ†åŒºè°ƒæ•´
def smartRepartition[T](rdd: RDD[T], targetPartitions: Int): RDD[T] = {
  val currentPartitions = rdd.getNumPartitions
  if (targetPartitions < currentPartitions) {
    rdd.coalesce(targetPartitions)
  } else {
    rdd.repartition(targetPartitions)
  }
}
```

3. **åˆ†åŒºä¿æŒç­–ç•¥**ï¼š
```scala
// ä½¿ç”¨mapPartitionsä¿æŒåˆ†åŒºç»“æ„
val optimizedRDD = rdd.mapPartitions { iter =>
  // åˆ†åŒºå†…å¤„ç†é€»è¾‘
  iter.map(processRecord)
}

// é¿å…ç ´ååˆ†åŒºçš„æ“ä½œ
val goodRDD = partitionedRDD.mapValues(_ * 2)  // ä¿æŒåˆ†åŒº
val badRDD = partitionedRDD.map(x => (x._1, x._2 * 2))  // å¯èƒ½ç ´ååˆ†åŒº
```

---

## Spark æ¶æ„ä¸åŸç† â­â­

### Sparkæ•´ä½“æ¶æ„

#### é›†ç¾¤æ¶æ„ç»„ä»¶

```mermaid
graph TB
    subgraph "Driver Program"
        A[SparkContext]
        A1[DAGScheduler]
        A2[TaskScheduler]
        A3[BackendScheduler]
    end
    
    subgraph "Cluster Manager"
        B[Resource Manager]
        B1[Application Master]
    end
    
    subgraph "Worker Node 1"
        C[Executor 1]
        C1[Task]
        C2[BlockManager]
        C3[Cache]
    end
    
    subgraph "Worker Node 2"
        D[Executor 2]
        D1[Task]
        D2[BlockManager] 
        D3[Cache]
    end
    
    A --> B
    B --> C
    B --> D
    A1 --> A2
    A2 --> A3
    A3 --> C1
    A3 --> D1
    C2 <--> D2
    
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#e8f5e8
    style D fill:#e8f5e8
```

**æ¶æ„ç»„ä»¶è¯¦è§£**ï¼š

| ç»„ä»¶                | èŒè´£                       | è¿è¡Œä½ç½®         |
| ------------------- | -------------------------- | ---------------- |
| **Driver Program**  | åº”ç”¨ç¨‹åºå…¥å£ï¼ŒåŒ…å«mainå‡½æ•° | å®¢æˆ·ç«¯æˆ–é›†ç¾¤èŠ‚ç‚¹ |
| **SparkContext**    | Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹      | Driver           |
| **Cluster Manager** | é›†ç¾¤èµ„æºç®¡ç†å™¨             | ç‹¬ç«‹èŠ‚ç‚¹         |
| **Worker Node**     | å·¥ä½œèŠ‚ç‚¹ï¼Œè¿è¡ŒExecutor     | é›†ç¾¤èŠ‚ç‚¹         |
| **Executor**        | ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œè¿è¡ŒTask       | Worker Node      |

#### åº”ç”¨ç¨‹åºæ¶æ„

**Sparkåº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸ**ï¼š

```mermaid
sequenceDiagram
    participant Client
    participant Driver
    participant ClusterManager as Cluster Manager
    participant Worker
    participant Executor
    
    Client->>Driver: 1. å¯åŠ¨åº”ç”¨ç¨‹åº
    Driver->>ClusterManager: 2. ç”³è¯·èµ„æº
    ClusterManager->>Worker: 3. å¯åŠ¨Executor
    Worker->>Executor: 4. åˆ›å»ºExecutorè¿›ç¨‹
    Executor->>Driver: 5. æ³¨å†Œåˆ°Driver
    Driver->>Driver: 6. æ„å»ºDAG
    Driver->>Executor: 7. åˆ†å‘Task
    Executor->>Executor: 8. æ‰§è¡ŒTask
    Executor->>Driver: 9. è¿”å›ç»“æœ
    Driver->>Client: 10. åº”ç”¨ç¨‹åºå®Œæˆ
```

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### SparkContext

**SparkContext** æ˜¯Sparkåº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ä¸é›†ç¾¤å»ºç«‹è¿æ¥ã€‚

```scala
// SparkContextæ ¸å¿ƒåŠŸèƒ½
class SparkContext(config: SparkConf) extends Logging {
  
  // 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
  private val env = SparkEnv.createDriverEnv(conf, isLocal, listenerBus, numCores, mockOutputCommitCoordinator)
  private val statusTracker = new SparkStatusTracker(this, sparkUI)
  private val taskScheduler = createTaskScheduler(this, master, deployMode)
  private val dagScheduler = new DAGScheduler(this)
  
  // 2. åˆ›å»ºRDD
  def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = {
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
  
  def textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = {
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions)
      .map(pair => pair._2.toString)
  }
  
  // 3. æäº¤ä½œä¸š
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
  }
  
  // 4. èµ„æºç®¡ç†
  def stop(): Unit = {
    dagScheduler.stop()
    taskScheduler.stop()
    env.stop()
  }
}
```

#### Driver Program

**Driver** æ˜¯è¿è¡Œåº”ç”¨ç¨‹åºmainå‡½æ•°çš„è¿›ç¨‹ï¼Œè´Ÿè´£ï¼š
- **åˆ›å»ºSparkContext**ï¼šåˆå§‹åŒ–Sparkåº”ç”¨ç¨‹åº
- **æ„å»ºé€»è¾‘è®¡åˆ’**ï¼šå°†ç”¨æˆ·ç¨‹åºè½¬æ¢ä¸ºDAG
- **ä»»åŠ¡è°ƒåº¦**ï¼šå°†DAGåˆ†è§£ä¸ºStageå’ŒTask
- **ç»“æœæ”¶é›†**ï¼šæ”¶é›†Executorè¿”å›çš„ç»“æœ

```scala
// Driverç¨‹åºç¤ºä¾‹
object WordCount {
  def main(args: Array[String]): Unit = {
    // 1. åˆ›å»ºSparkContext
    val conf = new SparkConf().setAppName("WordCount")
    val sc = new SparkContext(conf)
    
    try {
      // 2. åˆ›å»ºRDDå¹¶å®šä¹‰è½¬æ¢æ“ä½œ
      val lines = sc.textFile(args(0))
      val words = lines.flatMap(_.split("\\s+"))
      val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
      
      // 3. è§¦å‘Actionï¼Œæäº¤ä½œä¸š
      wordCounts.saveAsTextFile(args(1))
      
    } finally {
      // 4. åœæ­¢SparkContext
      sc.stop()
    }
  }
}
```

#### Cluster Manager

**é›†ç¾¤ç®¡ç†å™¨ç±»å‹**ï¼š

| ç±»å‹           | ç‰¹ç‚¹                | é€‚ç”¨åœºæ™¯             |
| -------------- | ------------------- | -------------------- |
| **Standalone** | Sparkå†…ç½®ï¼Œç®€å•æ˜“ç”¨ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡é›†ç¾¤ |
| **YARN**       | Hadoopç”Ÿæ€é›†æˆ      | ä¼ä¸šçº§Hadoopç¯å¢ƒ     |
| **Mesos**      | é€šç”¨èµ„æºç®¡ç†å™¨      | å¤šæ¡†æ¶å…±äº«é›†ç¾¤       |
| **Kubernetes** | å®¹å™¨åŒ–éƒ¨ç½²          | äº‘åŸç”Ÿç¯å¢ƒ           |

**YARNæ¨¡å¼è¯¦è§£**ï¼š
```scala
// YARN Clientæ¨¡å¼
spark-submit \
  --master yarn \
  --deploy-mode client \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar

// YARN Clusteræ¨¡å¼  
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --class com.example.MyApp \
  myapp.jar
```

#### Executor

**Executor** æ˜¯è¿è¡Œåœ¨WorkerèŠ‚ç‚¹ä¸Šçš„JVMè¿›ç¨‹ï¼Œè´Ÿè´£æ‰§è¡ŒTaskã€‚

```scala
// Executoræ ¸å¿ƒç»„ä»¶
class Executor(
    executorId: String,
    executorHostname: String,
    env: SparkEnv,
    userClassPath: Seq[URL] = Nil,
    isLocal: Boolean = false)
  extends Logging {

  // 1. çº¿ç¨‹æ± ç®¡ç†
  private val threadPool = ThreadUtils.newDaemonCachedThreadPool(
    "Executor task launch worker", sparkConf.get(EXECUTOR_CORES), 60)
  
  // 2. å†…å­˜ç®¡ç†
  private val memoryManager = env.memoryManager
  
  // 3. å­˜å‚¨ç®¡ç†
  private val blockManager = env.blockManager
  
  // 4. ä»»åŠ¡æ‰§è¡Œ
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
    val tr = new TaskRunner(context, taskDescription)
    runningTasks.put(taskDescription.taskId, tr)
    threadPool.execute(tr)
  }
  
  // 5. ä»»åŠ¡è¿è¡Œå™¨
  class TaskRunner(
      execBackend: ExecutorBackend,
      private val taskDescription: TaskDescription)
    extends Runnable {
    
    override def run(): Unit = {
      try {
        // ååºåˆ—åŒ–ä»»åŠ¡
        val task = ser.deserialize[Task[Any]](taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)
        
        // æ‰§è¡Œä»»åŠ¡
        val value = task.run(
          taskAttemptId = taskDescription.taskId,
          attemptNumber = taskDescription.attemptNumber,
          metricsSystem = env.metricsSystem)
        
        // è¿”å›ç»“æœ
        execBackend.statusUpdate(taskDescription.taskId, TaskState.FINISHED, ser.serialize(value))
        
      } catch {
        case e: Exception =>
          execBackend.statusUpdate(taskDescription.taskId, TaskState.FAILED, ser.serialize(TaskFailedReason))
      }
    }
  }
}
```

### ä»»åŠ¡è°ƒåº¦åŸç†

#### DAGSchedulerè°ƒåº¦

**DAGScheduler** è´Ÿè´£å°†RDDçš„DAGåˆ†è§£ä¸ºStageï¼Œå¹¶æäº¤Stageç»™TaskSchedulerã€‚

```mermaid
graph TD
    A[RDD DAG] --> B[DAGScheduler]
    B --> C[Stageåˆ’åˆ†]
    C --> D[Stage 0<br/>ShuffleMapStage]
    C --> E[Stage 1<br/>ShuffleMapStage]  
    C --> F[Stage 2<br/>ResultStage]
    
    D --> G[Task 0-1]
    D --> H[Task 0-2]
    E --> I[Task 1-1]
    E --> J[Task 1-2]
    F --> K[Task 2-1]
    F --> L[Task 2-2]
    
    style B fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#fff3e0
    style F fill:#e8f5e8
```

**DAGScheduleræ¶æ„ç»„ä»¶**ï¼š

| ç»„ä»¶             | ç±»å                                      | ä¸»è¦èŒè´£            | å…³é”®æ–¹æ³•                       |
| ---------------- | ----------------------------------------- | ------------------- | ------------------------------ |
| **DAGScheduler** | `DAGScheduler`                            | ä½œä¸šè°ƒåº¦å’ŒStageåˆ’åˆ† | `submitJob`, `submitStage`     |
| **EventLoop**    | `DAGSchedulerEventProcessLoop`            | äº‹ä»¶å¤„ç†å¾ªç¯        | `post`, `onReceive`            |
| **Stage**        | `Stage`, `ResultStage`, `ShuffleMapStage` | StageæŠ½è±¡           | `findMissingPartitions`        |
| **Job**          | `ActiveJob`                               | ä½œä¸šæŠ½è±¡            | `numFinished`, `numPartitions` |

**DAGScheduleräº‹ä»¶å¤„ç†**ï¼š
```scala
// DAGScheduleräº‹ä»¶ç±»å‹
sealed trait DAGSchedulerEvent

case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) extends DAGSchedulerEvent

case class StageCompleted(stage: Stage) extends DAGSchedulerEvent
case class TaskCompleted(task: Task[_], reason: TaskEndReason) extends DAGSchedulerEvent

// äº‹ä»¶å¤„ç†å¾ªç¯
class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    event match {
      case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
        dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
      case StageCompleted(stage) =>
        dagScheduler.handleStageCompletion(stage)
      case TaskCompleted(task, reason) =>
        dagScheduler.handleTaskCompletion(task, reason)
    }
  }
}
```

**Stageåˆ’åˆ†ä¸ä¾èµ–ç®¡ç†**ï¼š
```scala
// Stageåˆ’åˆ†æ ¸å¿ƒé€»è¾‘
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}
```

#### TaskSchedulerè°ƒåº¦

**TaskScheduler** è´Ÿè´£å°†Taskåˆ†å‘åˆ°Executoræ‰§è¡Œï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ã€‚

**TaskScheduleræ¶æ„ç»„ä»¶**ï¼š

| ç»„ä»¶                 | ç±»å                            | ä¸»è¦èŒè´£         | å…³é”®ç‰¹æ€§               |
| -------------------- | ------------------------------- | ---------------- | ---------------------- |
| **TaskScheduler**    | `TaskSchedulerImpl`             | ä»»åŠ¡è°ƒåº¦å’Œåˆ†å‘   | æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥       |
| **SchedulerBackend** | `CoarseGrainedSchedulerBackend` | ä¸é›†ç¾¤ç®¡ç†å™¨é€šä¿¡ | èµ„æºåˆ†é…å’ŒExecutorç®¡ç† |
| **TaskSetManager**   | `TaskSetManager`                | ç®¡ç†TaskSetæ‰§è¡Œ  | ä»»åŠ¡é‡è¯•ã€æ¨æµ‹æ‰§è¡Œ     |
| **Pool**             | `Pool`                          | è°ƒåº¦æ± ç®¡ç†       | å…¬å¹³è°ƒåº¦ã€FIFOè°ƒåº¦     |

**TaskSchedulerä»»åŠ¡åˆ†å‘**ï¼š
```scala
class TaskSchedulerImpl(
    val sc: SparkContext,
    val maxTaskFailures: Int,
    isLocal: Boolean = false)
  extends TaskScheduler with Logging {

  def submitTasks(taskSet: TaskSet): Unit = {
    val tasks = taskSet.tasks
    logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
    
    // 1. åˆ›å»ºTaskSetManager
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    
    // 2. æ·»åŠ åˆ°è°ƒåº¦é˜Ÿåˆ—
    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
    
    // 3. è§¦å‘èµ„æºåˆ†é…
    backend.reviveOffers()
  }
  
  def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = {
    // 1. éšæœºæ‰“ä¹±offersï¼Œé¿å…çƒ­ç‚¹
    val shuffledOffers = Random.shuffle(offers)
    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores))
    val availableCpus = shuffledOffers.map(o => o.cores).toArray
    
    // 2. æœ¬åœ°æ€§çº§åˆ«è°ƒåº¦
    for (taskSet <- rootPool.getSortedTaskSetQueue) {
      for (currentMaxLocality <- taskSet.myLocalityLevels) {
        do {
          launchedAnyTask = resourceOfferSingleTaskSet(
            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)
        } while (launchedAnyTask)
      }
    }
    
    tasks
  }
}
```

#### Stageåˆ’åˆ†æœºåˆ¶ ğŸ”¥

**Stageåˆ’åˆ†åŸåˆ™**ï¼š
- **å®½ä¾èµ–è¾¹ç•Œ**ï¼šé‡åˆ°å®½ä¾èµ–ï¼ˆShuffleï¼‰åˆ’åˆ†æ–°Stage
- **çª„ä¾èµ–åˆå¹¶**ï¼šçª„ä¾èµ–çš„RDDåœ¨åŒä¸€Stageå†…Pipelineæ‰§è¡Œ
- **Stageç±»å‹**ï¼šShuffleMapStageå’ŒResultStage

```mermaid
graph TD
    A[textFile] --> B[flatMap]
    B --> C[map]
    C --> D[reduceByKey]
    D --> E[map]
    E --> F[collect]
    
    subgraph "Stage 0 (ShuffleMapStage)"
        A
        B
        C
    end
    
    subgraph "Stage 1 (ResultStage)"
        E
        F
    end
    
    C -.->|Shuffle| E
    
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#e8f5e8
    style E fill:#e1f5fe
    style F fill:#e1f5fe
```

### BlockManageræ ¸å¿ƒç»„ä»¶ â­

**BlockManager** æ˜¯Sparkä¸­è´Ÿè´£æ•°æ®å­˜å‚¨å’Œç®¡ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œç»Ÿä¸€ç®¡ç†å†…å­˜å’Œç£ç›˜ä¸Šçš„æ•°æ®å—ã€‚

#### æ“ä½œæ—¶åºå›¾

```mermaid
sequenceDiagram
    participant Driver
    participant BlockManagerMaster
    participant Executor
    participant BlockManager
    participant MemoryStore
    participant DiskStore
    
    Driver->>BlockManagerMaster: 1. æ³¨å†ŒBlockManager
    Executor->>BlockManager: 2. åˆå§‹åŒ–BlockManager
    BlockManager->>BlockManagerMaster: 3. æ³¨å†Œåˆ°Master
    BlockManager->>MemoryStore: 4. åˆå§‹åŒ–å†…å­˜å­˜å‚¨
    BlockManager->>DiskStore: 5. åˆå§‹åŒ–ç£ç›˜å­˜å‚¨
    
    Note over BlockManager,DiskStore: æ•°æ®å­˜å‚¨æµç¨‹
    BlockManager->>MemoryStore: 6. å°è¯•å†…å­˜å­˜å‚¨
    alt å†…å­˜è¶³å¤Ÿ
        MemoryStore-->>BlockManager: 7a. å­˜å‚¨æˆåŠŸ
    else å†…å­˜ä¸è¶³
        BlockManager->>DiskStore: 7b. ç£ç›˜å­˜å‚¨
        DiskStore-->>BlockManager: 7c. å­˜å‚¨æˆåŠŸ
    end
    
    BlockManager->>BlockManagerMaster: 8. æŠ¥å‘Šå­˜å‚¨çŠ¶æ€
```

#### BlockManageræ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. BlockManageræ¶æ„ç»„ä»¶**

| ç»„ä»¶                   | ç±»å                 | ä¸»è¦èŒè´£           | å­˜å‚¨ä»‹è´¨   |
| ---------------------- | -------------------- | ------------------ | ---------- |
| **BlockManager**       | `BlockManager`       | æ•°æ®å—ç®¡ç†æ€»æ§åˆ¶å™¨ | å†…å­˜+ç£ç›˜  |
| **MemoryStore**        | `MemoryStore`        | å†…å­˜æ•°æ®å­˜å‚¨       | JVMå †å†…å­˜  |
| **DiskStore**          | `DiskStore`          | ç£ç›˜æ•°æ®å­˜å‚¨       | æœ¬åœ°ç£ç›˜   |
| **BlockManagerMaster** | `BlockManagerMaster` | å…ƒæ•°æ®ç®¡ç†         | Driverå†…å­˜ |
| **BlockInfoManager**   | `BlockInfoManager`   | Blockä¿¡æ¯ç®¡ç†      | å†…å­˜ç´¢å¼•   |

**2. BlockManageråˆ›å»ºä¸åˆå§‹åŒ–**

```scala
class BlockManager(
    executorId: String,
    rpcEnv: RpcEnv,
    master: BlockManagerMaster,
    serializerManager: SerializerManager,
    conf: SparkConf,
    memoryManager: MemoryManager,
    mapOutputTracker: MapOutputTracker)
  extends BlockDataManager with BlockEvictionHandler with Logging {

  // æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
  private[spark] val diskBlockManager = new DiskBlockManager(conf, deleteFilesOnStop = true)
  private[spark] val blockInfoManager = new BlockInfoManager
  
  // åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
  private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager)
  private[spark] val diskStore = new DiskStore(conf, diskBlockManager)
  
  // æ³¨å†Œåˆ°Master
  master.registerBlockManager(blockManagerId, maxMemory, slaveEndpoint)
}
```

**3. æ•°æ®å—å­˜å‚¨æµç¨‹**

```scala
// æ•°æ®å—å­˜å‚¨çš„æ ¸å¿ƒæ–¹æ³•
def putBlockData(
    blockId: BlockId,
    data: BlockData,
    level: StorageLevel,
    tellMaster: Boolean = true): Boolean = {
  
  // 1. æ£€æŸ¥å­˜å‚¨çº§åˆ«
  if (level.useMemory) {
    // 2. å°è¯•å­˜å‚¨åˆ°å†…å­˜
    val putSucceeded = memoryStore.putBytes(blockId, data, level)
    if (putSucceeded) {
      // 3. é€šçŸ¥Master
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, 0))
      }
      return true
    }
  }
  
  // 4. å†…å­˜ä¸è¶³ï¼Œå­˜å‚¨åˆ°ç£ç›˜
  if (level.useDisk) {
    val putSucceeded = diskStore.putBytes(blockId, data)
    if (putSucceeded) {
      if (tellMaster) {
        reportBlockStatus(blockId, BlockStatus(level, 0, data.size))
      }
      return true
    }
  }
  
  false
}
```

**4. æ•°æ®å—è·å–æµç¨‹**

```scala
// æ•°æ®å—è·å–çš„æ ¸å¿ƒæ–¹æ³•
def get[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. æ£€æŸ¥æœ¬åœ°å†…å­˜
  memoryStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 2. æ£€æŸ¥æœ¬åœ°ç£ç›˜
  diskStore.get(blockId) match {
    case Some(blockResult) => return Some(blockResult)
    case None => // ç»§ç»­æŸ¥æ‰¾
  }
  
  // 3. ä»è¿œç¨‹è·å–
  getRemote(blockId)
}

def getRemote[T](blockId: BlockId): Option[BlockResult[T]] = {
  // 1. ä»Masterè·å–blockä½ç½®
  val locations = master.getLocations(blockId)
  
  // 2. ä»è¿œç¨‹èŠ‚ç‚¹è·å–
  for (location <- locations) {
    val blockResult = blockTransferService.fetchBlockSync(
      location.host, location.port, location.executorId, blockId.toString)
    if (blockResult.isDefined) {
      return blockResult
    }
  }
  
  None
}
```

### å†…å­˜ç®¡ç†æœºåˆ¶ â­

#### å†…å­˜æ¨¡å‹

**Sparkå†…å­˜åˆ†åŒºæ¶æ„**ï¼š

```mermaid
graph TD
    A[JVMå†…å­˜] --> B[å †å†…å†…å­˜<br/>On-Heap]
    A --> C[å †å¤–å†…å­˜<br/>Off-Heap]
    
    B --> D[å­˜å‚¨å†…å­˜<br/>Storage Memory]
    B --> E[æ‰§è¡Œå†…å­˜<br/>Execution Memory]
    B --> F[å…¶ä»–å†…å­˜<br/>Other Memory]
    
    D --> G[RDDç¼“å­˜<br/>å¹¿æ’­å˜é‡]
    E --> H[Shuffle<br/>Joinèšåˆ]
    F --> I[ç”¨æˆ·æ•°æ®ç»“æ„<br/>Sparkå†…éƒ¨å¯¹è±¡]
    
    C --> J[å †å¤–å­˜å‚¨<br/>Off-Heap Storage]
    C --> K[å †å¤–æ‰§è¡Œ<br/>Off-Heap Execution]
    
    style B fill:#e8f5e8
    style C fill:#e1f5fe
    style D fill:#fff3e0
    style E fill:#ffebee
```

#### å†…å­˜ç®¡ç†æ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. å†…å­˜ç®¡ç†æ¶æ„ç»„ä»¶**

| ç»„ä»¶                    | ç±»å                   | ä¸»è¦èŒè´£       | ç®¡ç†èŒƒå›´      |
| ----------------------- | ---------------------- | -------------- | ------------- |
| **MemoryManager**       | `UnifiedMemoryManager` | ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨ | å †å†…+å †å¤–å†…å­˜ |
| **StorageMemoryPool**   | `StorageMemoryPool`    | å­˜å‚¨å†…å­˜æ±      | ç¼“å­˜æ•°æ®å†…å­˜  |
| **ExecutionMemoryPool** | `ExecutionMemoryPool`  | æ‰§è¡Œå†…å­˜æ±      | ä»»åŠ¡æ‰§è¡Œå†…å­˜  |
| **MemoryStore**         | `MemoryStore`          | å†…å­˜å­˜å‚¨ç®¡ç†   | ç¼“å­˜æ•°æ®å­˜å‚¨  |
| **TaskMemoryManager**   | `TaskMemoryManager`    | ä»»åŠ¡å†…å­˜ç®¡ç†   | å•ä¸ªä»»åŠ¡å†…å­˜  |

**2. MemoryStoreç¼“å­˜ç®¡ç†**

```scala
// MemoryStoreæ ¸å¿ƒå®ç°
class MemoryStore(
    conf: SparkConf,
    blockInfoManager: BlockInfoManager)
  extends BlockStore(BlockStore.MEMORY) with BlockEvictionHandler with Logging {

  // å†…å­˜æ˜ å°„è¡¨
  private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]](32, 0.75f, true)
  
  // å½“å‰å†…å­˜ä½¿ç”¨é‡
  private var _currentMemory = 0L
  
  def putBytes[T](
      blockId: BlockId,
      size: Long,
      memoryMode: MemoryMode,
      _bytes: () => ChunkedByteBuffer): Boolean = {
    
    // 1. æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
    if (!memoryManager.acquireStorageMemory(blockId, size, memoryMode)) {
      return false
    }
    
    // 2. åˆ†é…å†…å­˜å¹¶å­˜å‚¨æ•°æ®
    val bytes = _bytes()
    val entry = new SerializedMemoryEntry[T](bytes, memoryMode, implicitly[ClassTag[T]])
    entries.synchronized {
      entries.put(blockId, entry)
      _currentMemory += size
    }
    
    true
  }
  
  def get[T](blockId: BlockId): Option[BlockResult[T]] = {
    entries.synchronized {
      entries.get(blockId) match {
        case entry: SerializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case entry: DeserializedMemoryEntry[T] =>
          Some(BlockResult(entry.value.asInstanceOf[T], DataReadMethod.Memory, entry.size))
        case _ => None
      }
    }
  }
}
```

**3. TaskMemoryManagerä»»åŠ¡å†…å­˜ç®¡ç†**

```scala
// TaskMemoryManageræ ¸å¿ƒå®ç°
class TaskMemoryManager(
    memoryManager: MemoryManager,
    taskAttemptId: Long)
  extends MemoryManager with Logging {

  // ä»»åŠ¡å†…å­˜æ˜ å°„è¡¨
  private val memoryForTask = new mutable.HashMap[Long, Long]()
  
  // å†…å­˜åˆ†é…æ–¹æ³•
  def acquireExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Long = {
    
    // 1. å°è¯•ä»æ‰§è¡Œå†…å­˜æ± åˆ†é…
    val acquired = memoryManager.acquireExecutionMemory(numBytes, taskAttemptId, memoryMode)
    
    // 2. è®°å½•åˆ†é…çš„å†…å­˜
    if (acquired > 0) {
      memoryForTask.synchronized {
        memoryForTask(taskAttemptId) = memoryForTask.getOrElse(taskAttemptId, 0L) + acquired
      }
    }
    
    acquired
  }
  
  // é‡Šæ”¾å†…å­˜
  def releaseExecutionMemory(
      numBytes: Long,
      taskAttemptId: Long,
      memoryMode: MemoryMode): Unit = {
    
    memoryManager.releaseExecutionMemory(numBytes, taskAttemptId, memoryMode)
    
    memoryForTask.synchronized {
      val current = memoryForTask.getOrElse(taskAttemptId, 0L)
      val newTotal = math.max(0L, current - numBytes)
      if (newTotal == 0) {
        memoryForTask.remove(taskAttemptId)
      } else {
        memoryForTask(taskAttemptId) = newTotal
      }
    }
  }
}
```

#### å†…å­˜åˆ†é…ç­–ç•¥

**ç»Ÿä¸€å†…å­˜ç®¡ç†**ï¼š
```scala
class UnifiedMemoryManager(
    conf: SparkConf,
    val maxHeapMemory: Long,
    onHeapStorageRegionSize: Long,
    numCores: Int)
  extends MemoryManager(conf, numCores, onHeapStorageRegionSize, maxHeapMemory) {

  // å†…å­˜æ± é…ç½®
  private val maxPoolSize = maxHeapMemory - reservedMemory
  private val poolSize = maxPoolSize * memoryFraction
  
  // åŠ¨æ€å†…å­˜åˆ†é…
  override def acquireStorageMemory(
      blockId: BlockId,
      numBytes: Long,
      memoryMode: MemoryMode): Boolean = synchronized {
    
    val (executionPool, storagePool, maxMemory) = memoryMode match {
      case MemoryMode.ON_HEAP => (
        onHeapExecutionMemoryPool,
        onHeapStorageMemoryPool,
        maxOnHeapStorageMemory)
      case MemoryMode.OFF_HEAP => (
        offHeapExecutionMemoryPool,
        offHeapStorageMemoryPool,
        maxOffHeapStorageMemory)
    }
    
    if (numBytes > maxMemory) {
      return false
    }
    
    if (numBytes > storagePool.memoryFree) {
      // å°è¯•ä»æ‰§è¡Œå†…å­˜æ± å€Ÿç”¨
      val memoryBorrowedFromExecution = math.min(
        executionPool.memoryFree, 
        numBytes - storagePool.memoryFree)
      
      executionPool.decrementPoolSize(memoryBorrowedFromExecution)
      storagePool.incrementPoolSize(memoryBorrowedFromExecution)
    }
    
    storagePool.acquireMemory(blockId, numBytes)
  }
}
```

#### åƒåœ¾å›æ”¶ä¼˜åŒ–

**GCè°ƒä¼˜ç­–ç•¥**ï¼š
```bash
# G1GCé…ç½®ï¼ˆæ¨èï¼‰
--conf spark.executor.extraJavaOptions="
  -XX:+UseG1GC
  -XX:G1HeapRegionSize=16m
  -XX:MaxGCPauseMillis=200
  -XX:+G1PrintRegionRememberedSetInfo
  -XX:+UseCompressedOops
  -XX:+UseCompressedClassPointers
"

# å¹¶å‘GCé…ç½®
--conf spark.executor.extraJavaOptions="
  -XX:+UseConcMarkSweepGC
  -XX:+CMSParallelRemarkEnabled
  -XX:+UseCMSInitiatingOccupancyOnly
  -XX:CMSInitiatingOccupancyFraction=70
"

# å†…å­˜ç›‘æ§é…ç½®
--conf spark.executor.extraJavaOptions="
  -XX:+PrintGC
  -XX:+PrintGCDetails
  -XX:+PrintGCTimeStamps
  -XX:+PrintGCApplicationStoppedTime
"
```

---

## Spark SQLä¸Catalyst â­â­

### Spark SQLæ¦‚è¿°

**Spark SQL** æ˜¯Sparkç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®çš„æ¨¡å—ï¼Œæä¾›äº†DataFrameå’ŒDataset APIã€‚

#### ä¸»è¦ç‰¹æ€§

- **ç»Ÿä¸€æ•°æ®è®¿é—®**ï¼šæ”¯æŒå¤šç§æ•°æ®æº
- **Hiveå…¼å®¹æ€§**ï¼šå®Œå…¨å…¼å®¹Hive SQL
- **ä¼˜åŒ–æ‰§è¡Œ**ï¼šCatalystä¼˜åŒ–å™¨
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ä»£ç ç”Ÿæˆ

#### ä½¿ç”¨æ–¹å¼

```scala
// åˆ›å»ºSparkSession
val spark = SparkSession.builder()
  .appName("SparkSQLExample")
  .config("spark.sql.adaptive.enabled", "true")
  .getOrCreate()

// è¯»å–æ•°æ®
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// SQLæŸ¥è¯¢
df.createOrReplaceTempView("people")
val result = spark.sql("""
  SELECT age, count(*) as count
  FROM people 
  WHERE age > 21
  GROUP BY age
  ORDER BY age
""")

// DataFrame API
val result2 = df
  .filter($"age" > 21)
  .groupBy("age")
  .count()
  .orderBy("age")
```

### Catalystä¼˜åŒ–å™¨ ğŸ”¥

**Catalyst** æ˜¯Spark SQLçš„æŸ¥è¯¢ä¼˜åŒ–æ¡†æ¶ï¼ŒåŸºäºScalaçš„å‡½æ•°å¼ç¼–ç¨‹æ„å»ºã€‚

#### ä¼˜åŒ–æµç¨‹

```mermaid
graph LR
    A[SQL/DataFrame] --> B[è§£æå™¨<br/>Parser]
    B --> C[é€»è¾‘è®¡åˆ’<br/>Logical Plan]
    C --> D[ä¼˜åŒ–å™¨<br/>Optimizer]
    D --> E[ç‰©ç†è®¡åˆ’<br/>Physical Plan]
    E --> F[ä»£ç ç”Ÿæˆ<br/>CodeGen]
    F --> G[æ‰§è¡Œ<br/>Execution]
    
    style B fill:#e8f5e8
    style D fill:#e1f5fe
    style F fill:#fff3e0
```

**ä¼˜åŒ–é˜¶æ®µ**ï¼š
1. **é€»è¾‘è®¡åˆ’ä¼˜åŒ–**ï¼šè°“è¯ä¸‹æ¨ã€æŠ•å½±ä¸‹æ¨ã€å¸¸é‡æŠ˜å 
2. **ç‰©ç†è®¡åˆ’ç”Ÿæˆ**ï¼šé€‰æ‹©æœ€ä¼˜çš„ç‰©ç†æ‰§è¡Œç­–ç•¥
3. **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆé«˜æ•ˆçš„Javaå­—èŠ‚ç 

#### ä¼˜åŒ–è§„åˆ™

**ä¸»è¦ä¼˜åŒ–è§„åˆ™**ï¼š
```scala
// è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT * FROM (SELECT * FROM table WHERE col1 > 10) WHERE col2 = 'value'
// ä¼˜åŒ–å  
SELECT * FROM table WHERE col1 > 10 AND col2 = 'value'

// æŠ•å½±ä¸‹æ¨ï¼ˆProjection Pushdownï¼‰
// ä¼˜åŒ–å‰
SELECT col1 FROM (SELECT col1, col2, col3 FROM table)
// ä¼˜åŒ–å
SELECT col1 FROM table

// å¸¸é‡æŠ˜å ï¼ˆConstant Foldingï¼‰
// ä¼˜åŒ–å‰
SELECT col1 + 1 + 2 FROM table
// ä¼˜åŒ–å
SELECT col1 + 3 FROM table
```

#### ä»£ç ç”Ÿæˆ

**Whole-Stage Code Generation**ï¼š
```scala
// ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
// åŸå§‹æŸ¥è¯¢ï¼šSELECT sum(x + y) FROM table WHERE z > 10
class GeneratedIterator extends Iterator[InternalRow] {
  private var sum: Long = 0L
  
  def processNext(): Unit = {
    while (input.hasNext) {
      val row = input.next()
      val z = row.getLong(2)
      if (z > 10) {  // è°“è¯è®¡ç®—
        val x = row.getLong(0)
        val y = row.getLong(1)
        sum += (x + y)  // èšåˆè®¡ç®—
      }
    }
    // è¿”å›æœ€ç»ˆç»“æœ
    result.setLong(0, sum)
  }
}
```

### æ•°æ®æºæ”¯æŒ

#### å†…ç½®æ•°æ®æº

**æ”¯æŒçš„æ•°æ®æ ¼å¼**ï¼š
- **Parquet**ï¼šåˆ—å¼å­˜å‚¨ï¼Œé«˜å‹ç¼©æ¯”
- **JSON**ï¼šåŠç»“æ„åŒ–æ•°æ®
- **CSV**ï¼šæ–‡æœ¬æ ¼å¼
- **ORC**ï¼šä¼˜åŒ–çš„è¡Œåˆ—å­˜å‚¨
- **Avro**ï¼šæ¨¡å¼æ¼”åŒ–æ”¯æŒ

```scala
// è¯»å–ä¸åŒæ ¼å¼æ•°æ®
val parquetDF = spark.read.parquet("path/to/data.parquet")
val jsonDF = spark.read.json("path/to/data.json")
val csvDF = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/data.csv")

// å†™å…¥æ•°æ®
df.write
  .mode("overwrite")
  .option("compression", "snappy")
  .parquet("output/path")
```

#### å¤–éƒ¨æ•°æ®æº

**å¸¸ç”¨å¤–éƒ¨æ•°æ®æº**ï¼š
```scala
// JDBCæ•°æ®æº
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "username")
  .option("password", "password")
  .load()

// Kafkaæ•°æ®æº
val kafkaDF = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// Hiveè¡¨
val hiveDF = spark.sql("SELECT * FROM hive_table")
```

---

## Shuffleæœºåˆ¶æ·±åº¦è§£æ â­â­â­

### ShuffleåŸç†

#### Shuffleæ¦‚å¿µ

**Shuffle** æ˜¯Sparkä¸­æ•°æ®é‡æ–°åˆ†å¸ƒçš„è¿‡ç¨‹ï¼Œå‘ç”Ÿåœ¨éœ€è¦è·¨åˆ†åŒºè¿›è¡Œæ•°æ®äº¤æ¢çš„æ“ä½œä¸­ã€‚

#### Shuffleè§¦å‘æ¡ä»¶

**è§¦å‘Shuffleçš„æ“ä½œ**ï¼š
```scala
// 1. èšåˆæ“ä½œ
val grouped = rdd.groupByKey()        // è§¦å‘Shuffle
val reduced = rdd.reduceByKey(_ + _)  // è§¦å‘Shuffle

// 2. è¿æ¥æ“ä½œ
val joined = rdd1.join(rdd2)          // è§¦å‘Shuffle

// 3. é‡åˆ†åŒºæ“ä½œ
val repartitioned = rdd.repartition(10)  // è§¦å‘Shuffle
```

#### Shuffleç±»å‹å¯¹æ¯” ğŸ”¥

| Shuffleç±»å‹       | ç‰¹ç‚¹                                  | ä¼˜ç¼ºç‚¹               |
| ----------------- | ------------------------------------- | -------------------- |
| **Hash Shuffle**  | æ¯ä¸ªMap Taskä¸ºæ¯ä¸ªReduce Taskåˆ›å»ºæ–‡ä»¶ | æ–‡ä»¶æ•°è¿‡å¤šï¼Œå½±å“æ€§èƒ½ |
| **Sort Shuffle**  | æ¯ä¸ªMap Taskåˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼ŒæŒ‰åˆ†åŒºæ’åº  | å‡å°‘æ–‡ä»¶æ•°ï¼Œæé«˜æ€§èƒ½ |
| **Tungsten Sort** | ä½¿ç”¨å †å¤–å†…å­˜ï¼Œä¼˜åŒ–æ’åºæ€§èƒ½            | å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ       |

### Shuffleå®ç°æœºåˆ¶

#### Hash Shuffle

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[File 1-1]
        A --> E[File 1-2]
        A --> F[File 1-3]
        
        B[Map Task 2] --> G[File 2-1]
        B --> H[File 2-2] 
        B --> I[File 2-3]
        
        C[Map Task 3] --> J[File 3-1]
        C --> K[File 3-2]
        C --> L[File 3-3]
    end
    
    subgraph "Reduceé˜¶æ®µ"
        D --> M[Reduce Task 1]
        G --> M
        J --> M
        
        E --> N[Reduce Task 2]
        H --> N
        K --> N
        
        F --> O[Reduce Task 3]
        I --> O
        L --> O
    end
    
    style A fill:#ffebee
    style B fill:#ffebee
    style C fill:#ffebee
```

**Hash Shuffleé—®é¢˜**ï¼š
- **æ–‡ä»¶æ•°çˆ†ç‚¸**ï¼šMä¸ªMap Task Ã— Nä¸ªReduce Task = MÃ—Nä¸ªæ–‡ä»¶
- **éšæœºI/O**ï¼šå¤§é‡å°æ–‡ä»¶å¯¼è‡´éšæœºI/O
- **å†…å­˜å‹åŠ›**ï¼šéœ€è¦ä¸ºæ¯ä¸ªæ–‡ä»¶ç»´æŠ¤ç¼“å†²åŒº

#### Sort Shuffle

```mermaid
graph TD
    subgraph "Mapé˜¶æ®µ"
        A[Map Task 1] --> D[æ’åºç¼“å†²åŒº]
        D --> E[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        E --> F[ç´¢å¼•æ–‡ä»¶]
        
        B[Map Task 2] --> G[æ’åºç¼“å†²åŒº]
        G --> H[å•ä¸ªè¾“å‡ºæ–‡ä»¶]
        H --> I[ç´¢å¼•æ–‡ä»¶]
    end
    
    subgraph "Reduceé˜¶æ®µ"
        F --> J[Reduce Task 1]
        I --> J
        
        F --> K[Reduce Task 2]
        I --> K
    end
    
    style D fill:#fff3e0
    style G fill:#fff3e0
    style E fill:#e8f5e8
    style H fill:#e8f5e8
```

**Sort Shuffleä¼˜åŠ¿**ï¼š
- **æ–‡ä»¶æ•°å‡å°‘**ï¼šæ¯ä¸ªMap Taskåªäº§ç”Ÿä¸€ä¸ªæ•°æ®æ–‡ä»¶å’Œä¸€ä¸ªç´¢å¼•æ–‡ä»¶
- **é¡ºåºI/O**ï¼šæ•°æ®æŒ‰åˆ†åŒºIDæ’åºå†™å…¥ï¼Œæé«˜I/Oæ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨å¤–éƒ¨æ’åºï¼Œæ”¯æŒspillåˆ°ç£ç›˜

#### Shuffle ç±»å‹ä¸æ—¶åºå›¾

**Hash Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant HashWriter
    participant FileSystem
    participant ReduceTask
    participant HashReader
    
    MapTask->>HashWriter: å†™å…¥è®°å½•
    HashWriter->>FileSystem: ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºæ–‡ä»¶
    Note over FileSystem: MÃ—Nä¸ªæ–‡ä»¶åˆ›å»º
    HashWriter->>FileSystem: å†™å…¥æ•°æ®åˆ°å¯¹åº”æ–‡ä»¶
    
    ReduceTask->>HashReader: å¼€å§‹è¯»å–
    HashReader->>FileSystem: è¯»å–ç›¸å…³åˆ†åŒºæ–‡ä»¶
    FileSystem-->>HashReader: è¿”å›æ•°æ®
    HashReader-->>ReduceTask: èšåˆåæ•°æ®
```

**Sort Shuffle æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
    participant MapTask
    participant SortWriter
    participant ExternalSorter
    participant FileSystem
    participant ReduceTask
    participant SortReader
    
    MapTask->>SortWriter: å†™å…¥è®°å½•
    SortWriter->>ExternalSorter: ç¼“å­˜å¹¶æ’åº
    ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
    ExternalSorter->>FileSystem: å†™å…¥å•ä¸ªæ•°æ®æ–‡ä»¶
    SortWriter->>FileSystem: å†™å…¥ç´¢å¼•æ–‡ä»¶
    
    ReduceTask->>SortReader: å¼€å§‹è¯»å–
    SortReader->>FileSystem: æ ¹æ®ç´¢å¼•è¯»å–æ•°æ®
    FileSystem-->>SortReader: è¿”å›åˆ†åŒºæ•°æ®
    SortReader-->>ReduceTask: èšåˆåæ•°æ®
```

#### Shuffleæ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. ShuffleManageræ¶æ„ç»„ä»¶**

| ç»„ä»¶                   | ç±»å                                       | ä¸»è¦èŒè´£           | é€‚ç”¨åœºæ™¯         |
| ---------------------- | ------------------------------------------ | ------------------ | ---------------- |
| **SortShuffleManager** | `SortShuffleManager`                       | Sort Shuffleç®¡ç†å™¨ | é»˜è®¤Shuffleæ–¹å¼  |
| **HashShuffleManager** | `HashShuffleManager`                       | Hash Shuffleç®¡ç†å™¨ | å·²åºŸå¼ƒ           |
| **ShuffleWriter**      | `SortShuffleWriter`, `UnsafeShuffleWriter` | Shuffleå†™å…¥å™¨      | Mapç«¯æ•°æ®å†™å…¥    |
| **ShuffleReader**      | `BlockStoreShuffleReader`                  | Shuffleè¯»å–å™¨      | Reduceç«¯æ•°æ®è¯»å– |

**2. ShuffleWriteræ ¸å¿ƒå®ç°**

```scala
// Sort Shuffleå®ç°æ ¸å¿ƒ
class SortShuffleWriter[K, V, C](
    shuffleBlockResolver: IndexShuffleBlockResolver,
    handle: BaseShuffleHandle[K, V, C],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val dep = handle.dependency
  private val blockManager = SparkEnv.get.blockManager
  private val sorter: ExternalSorter[K, V, _] = {
    if (dep.mapSideCombine) {
      new ExternalSorter[K, V, C](
        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    } else {
      new ExternalSorter[K, V, V](
        context, aggregator = None, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    }
  }

  // å†™å…¥æ•°æ®
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    sorter.insertAll(records)
    
    // è·å–è¾“å‡ºæ–‡ä»¶
    val outputFile = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
    
    // å†™å…¥æ’åºåçš„æ•°æ®
    val partitionLengths = sorter.writePartitionedFile(blockId, outputFile)
    
    // å†™å…¥ç´¢å¼•æ–‡ä»¶
    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, outputFile)
  }
}
```

**3. ExternalSorterå†…å­˜ç®¡ç†**

```scala
// ExternalSorteræ ¸å¿ƒå®ç°
class ExternalSorter[K, V, C](
    context: TaskContext,
    aggregator: Option[Aggregator[K, V, C]] = None,
    partitioner: Option[Partitioner] = None,
    ordering: Option[Ordering[K]] = None,
    serializer: Serializer = SparkEnv.get.serializer)
  extends Spillable[WritablePartitionedPairCollection[K, C]](context.taskMemoryManager())
  with Logging {

  // å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
  private var map = new PartitionedAppendOnlyMap[K, C]
  private val buffer = new PartitionedPairBuffer[K, C]

  // æ’å…¥æ•°æ®
  def insertAll(records: Iterator[Product2[K, V]]): Unit = {
    val shouldCombine = aggregator.isDefined
    
    if (shouldCombine) {
      // éœ€è¦èšåˆçš„æƒ…å†µ
      val mergeValue = aggregator.get.mergeValue
      val createCombiner = aggregator.get.createCombiner
      var kv: Product2[K, V] = null
      
      val update = (hadValue: Boolean, oldValue: C) => {
        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
      }
      
      while (records.hasNext) {
        addElementsRead()
        kv = records.next()
        map.changeValue((getPartition(kv._1), kv._1), update)
        maybeSpillCollection(usingMap = true)
      }
    } else {
      // ä¸éœ€è¦èšåˆçš„æƒ…å†µ
      while (records.hasNext) {
        addElementsRead()
        val kv = records.next()
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)
      }
    }
  }

  // Spillåˆ°ç£ç›˜
  override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): SpilledFile = {
    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)
    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)
    collection.clear()
    spillFile
  }
}
```

**4. ShuffleReaderæ•°æ®è¯»å–**

```scala
// ShuffleReaderæ ¸å¿ƒå®ç°
class BlockStoreShuffleReader[K, C](
    handle: BaseShuffleHandle[K, _, C],
    startPartition: Int,
    endPartition: Int,
    context: TaskContext,
    serializerManager: SerializerManager = SparkEnv.get.serializerManager,
    blockManager: BlockManager = SparkEnv.get.blockManager,
    mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker)
  extends ShuffleReader[K, C] with Logging {

  private val dep = handle.dependency

  override def read(): Iterator[Product2[K, C]] = {
    // 1. è·å–Shuffleæ•°æ®å—ä½ç½®
    val blocksByAddress = mapOutputTracker.getMapSizesByExecutorId(
      handle.shuffleId, startPartition, endPartition)
    
    // 2. è¯»å–æ•°æ®å—
    val blockFetcherItr = new ShuffleBlockFetcherIterator(
      context,
      blockManager.blockTransferService,
      blockManager,
      blocksByAddress,
      serializerManager.wrapStream(blockId, _),
      // æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨serializerManageræ¥è·å–å‹ç¼©å’ŒåŠ å¯†åŒ…è£…å™¨
      maxBytesInFlight = SparkEnv.get.conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024,
      maxReqsInFlight = SparkEnv.get.conf.getInt("spark.reducer.maxReqsInFlight", Int.MaxValue),
      maxBlocksInFlightPerAddress = SparkEnv.get.conf.getInt(
        "spark.reducer.maxBlocksInFlightPerAddress", Int.MaxValue),
      maxReqSizeShuffleToMem = SparkEnv.get.conf.getSizeAsBytes(
        "spark.reducer.maxReqSizeShuffleToMem", Long.MaxValue),
      detectCorrupt = SparkEnv.get.conf.getBoolean("spark.shuffle.detectCorrupt", true))

    // 3. ååºåˆ—åŒ–å¹¶èšåˆ
    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
      if (dep.mapSideCombine) {
        // Mapç«¯å·²ç»èšåˆï¼ŒReduceç«¯ç»§ç»­èšåˆ
        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
      } else {
        // Mapç«¯æœªèšåˆï¼ŒReduceç«¯è¿›è¡Œèšåˆ
        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, V)]]
        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
      }
    } else {
      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
    }

    // 4. æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
    dep.keyOrdering match {
      case Some(keyOrd: Ordering[K]) =>
        // åˆ›å»ºExternalSorterè¿›è¡Œæ’åº
        val sorter = new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
        sorter.insertAll(aggregatedIter)
        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
      case None =>
        aggregatedIter
    }
  }
}
```

**5. ShuffleBlockResolveræ–‡ä»¶ç®¡ç†**

```scala
// ShuffleBlockResolveræ ¸å¿ƒå®ç°
class IndexShuffleBlockResolver(conf: SparkConf, _blockManager: BlockManager = null)
  extends ShuffleBlockResolver with Logging {

  // è·å–æ•°æ®æ–‡ä»¶
  def getDataFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.data")
  }
  
  // è·å–ç´¢å¼•æ–‡ä»¶
  def getIndexFile(shuffleId: Int, mapId: Long): File = {
    new File(getShuffleDataDir(shuffleId), s"shuffle_${shuffleId}_${mapId}_0.index")
  }
  
  // å†™å…¥ç´¢å¼•æ–‡ä»¶å¹¶æäº¤
  def writeIndexFileAndCommit(
      shuffleId: Int,
      mapId: Long,
      lengths: Array[Long],
      dataTmp: File): Unit = {
    
    val indexFile = getIndexFile(shuffleId, mapId)
    val indexTmp = new File(indexFile.getAbsolutePath + ".tmp")
    
    try {
      val out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexTmp)))
      Utils.tryWithSafeFinally {
        // å†™å…¥åç§»é‡
        var offset = 0L
        out.writeLong(offset)
        for (length <- lengths) {
          offset += length
          out.writeLong(offset)
        }
      } {
        out.close()
      }
      
      // åŸå­æ€§é‡å‘½å
      val dataFile = getDataFile(shuffleId, mapId)
      if (dataTmp.exists() && !dataTmp.renameTo(dataFile)) {
        throw new IOException("Failed to rename data file")
      }
      if (!indexTmp.renameTo(indexFile)) {
        throw new IOException("Failed to rename index file")
      }
    } catch {
      case e: Exception =>
        indexTmp.delete()
        throw e
    }
  }
}
```

**6. Shuffleæ•°æ®æµç»„ä»¶äº¤äº’**

```mermaid
sequenceDiagram
  participant MapTask
  participant ExternalSorter
  participant ShuffleWriter
  participant ShuffleBlockResolver
  participant Disk
  participant ReduceTask
  participant ShuffleReader
  participant BlockManager

  MapTask->>ExternalSorter: insertAll(records)
  ExternalSorter->>ExternalSorter: å†…å­˜æ’åº/Spill
  ExternalSorter->>ShuffleWriter: writePartitionedFile()
  ShuffleWriter->>ShuffleBlockResolver: writeIndexFileAndCommit()
  ShuffleBlockResolver->>Disk: å†™å…¥data/indexæ–‡ä»¶
  
  ReduceTask->>ShuffleReader: read()
  ShuffleReader->>BlockManager: è·å–blockä½ç½®
  BlockManager->>Disk: è¯»å–Shuffleæ–‡ä»¶
  Disk-->>ShuffleReader: è¿”å›æ•°æ®
  ShuffleReader->>ReduceTask: èšåˆ/æ’åºç»“æœ
```

**7. Shuffleæ€§èƒ½ç›‘æ§ç»„ä»¶**

```scala
// Shuffleæ€§èƒ½æŒ‡æ ‡æ”¶é›†
class ShuffleWriteMetrics extends TaskMetrics {
  // å†™å…¥å­—èŠ‚æ•°
  private var _bytesWritten: Long = 0L
  // å†™å…¥è®°å½•æ•°
  private var _recordsWritten: Long = 0L
  // å†™å…¥æ—¶é—´
  private var _writeTime: Long = 0L
  
  def bytesWritten: Long = _bytesWritten
  def recordsWritten: Long = _recordsWritten
  def writeTime: Long = _writeTime
}

class ShuffleReadMetrics extends TaskMetrics {
  // è¯»å–å­—èŠ‚æ•°
  private var _bytesRead: Long = 0L
  // è¯»å–è®°å½•æ•°
  private var _recordsRead: Long = 0L
  // è¯»å–æ—¶é—´
  private var _readTime: Long = 0L
  // è¿œç¨‹è¯»å–å­—èŠ‚æ•°
  private var _remoteBytesRead: Long = 0L
  
  def bytesRead: Long = _bytesRead
  def recordsRead: Long = _recordsRead
  def readTime: Long = _readTime
  def remoteBytesRead: Long = _remoteBytesRead
}
```

#### Tungsten Sort Shuffle

**Tungstenä¼˜åŒ–**ï¼š
- **å †å¤–å†…å­˜ç®¡ç†**ï¼šå‡å°‘GCå‹åŠ›
- **ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„**ï¼šæé«˜CPUç¼“å­˜å‘½ä¸­ç‡
- **ä»£ç ç”Ÿæˆ**ï¼šè¿è¡Œæ—¶ç”Ÿæˆä¼˜åŒ–çš„å­—èŠ‚ç 

```scala
// Tungsten Sortå®ç°
class UnsafeShuffleWriter[K, V](
    blockManager: BlockManager,
    shuffleBlockResolver: IndexShuffleBlockResolver,
    taskMemoryManager: TaskMemoryManager,
    handle: SerializedShuffleHandle[K, V],
    mapId: Int,
    context: TaskContext)
  extends ShuffleWriter[K, V] with Logging {

  private val partitioner = handle.dependency.partitioner
  private val numPartitions = partitioner.numPartitions
  private var sorter: UnsafeShuffleExternalSorter = _
  
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    // ä½¿ç”¨Tungstenå†…å­˜ç®¡ç†
    val taskContext = context.asInstanceOf[TaskContextImpl]
    sorter = UnsafeShuffleExternalSorter.create(
      taskContext.taskMemoryManager(),
      blockManager,
      context,
      numPartitions,
      shouldCompress = true)

    // åºåˆ—åŒ–å¹¶æ’å…¥è®°å½•
    while (records.hasNext) {
      insertRecordIntoSorter(records.next())
    }
    
    // å†™å‡ºæ’åºç»“æœ
    val outputFile = shuffleBlockResolver.getDataFile(handle.shuffleId, mapId)
    val partitionLengths = sorter.closeAndGetSpills.map(_.file)
      .foldLeft(Array.fill[Long](numPartitions)(0)) { (lengths, file) =>
        // åˆå¹¶spillæ–‡ä»¶
        mergeSpillsWithTransferTo(file, outputFile, lengths)
      }
    
    shuffleBlockResolver.writeIndexFileAndCommit(handle.shuffleId, mapId, partitionLengths, outputFile)
  }
}
```

#### Shuffle ä¼˜åŒ–ä¸è°ƒä¼˜

**ä¸»è¦ä¼˜åŒ–ç­–ç•¥**ï¼š
- **å‹ç¼©**ï¼š`spark.shuffle.compress`ï¼Œå‡å°‘ç½‘ç»œä¼ è¾“é‡
- **åˆç†è®¾ç½®åˆ†åŒºæ•°**ï¼š`spark.sql.shuffle.partitions`ï¼Œé¿å…åˆ†åŒºè¿‡å¤šæˆ–è¿‡å°‘
- **ä½¿ç”¨æœ¬åœ°åŒ–Shuffle**ï¼šå‡å°‘ç½‘ç»œI/O
- **å¯ç”¨spillæœºåˆ¶**ï¼šå†…å­˜ä¸è¶³æ—¶æº¢å†™ç£ç›˜ï¼Œé˜²æ­¢OOM
- **èšåˆç¼“å†²åŒº**ï¼šMapç«¯æœ¬åœ°èšåˆï¼Œå‡å°‘ä¼ è¾“æ•°æ®é‡

#### Spark Shuffle è°ƒä¼˜

**1. åˆ†åŒºä¼˜åŒ–ç­–ç•¥**

```properties
# æ¨èè®¾ç½®ï¼ˆæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
spark.sql.shuffle.partitions=200
spark.default.parallelism=200

# åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
```

**2. åŠ¨æ€èµ„æºåˆ†é…**

```properties
# å¯ç”¨åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.initialExecutors=2

# èµ„æºåˆ†é…ç­–ç•¥
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=120s
```

**3. å‹ç¼©ä¸åºåˆ—åŒ–ä¼˜åŒ–**

| é…ç½®é¡¹                            | æ¨èå€¼           | è¯´æ˜            |
| --------------------------------- | ---------------- | --------------- |
| `spark.shuffle.compress`          | `true`           | å¯ç”¨Shuffleå‹ç¼© |
| `spark.shuffle.compress.codec`    | `snappy`         | å‹ç¼©ç®—æ³•é€‰æ‹©    |
| `spark.serializer`                | `KryoSerializer` | åºåˆ—åŒ–å™¨é€‰æ‹©    |
| `spark.kryo.registrationRequired` | `false`          | æ˜¯å¦è¦æ±‚æ³¨å†Œç±»  |

**4. æœ¬åœ°åŒ–Shuffleä¼˜åŒ–**

```properties
# æœ¬åœ°åŒ–é…ç½®
spark.locality.wait=3s
spark.locality.wait.process=3s
spark.locality.wait.node=3s
spark.locality.wait.rack=3s
```

**5. é«˜çº§ä¼˜åŒ–æŠ€å·§**

**Mapç«¯èšåˆ**ï¼š
```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val result = rdd.reduceByKey(_ + _)  // æ¨è
// val result = rdd.groupByKey().mapValues(_.sum)  // ä¸æ¨è
```

**å¹¿æ’­å˜é‡ä¼˜åŒ–**ï¼š
```scala
// å°è¡¨å¹¿æ’­ï¼Œé¿å…Shuffle
val smallTable = spark.table("small_table").collect()
val broadcastVar = spark.sparkContext.broadcast(smallTable)
```

#### Shuffleå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**1. æ•°æ®å€¾æ–œé—®é¢˜**

**ç°è±¡**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´Taskæ‰§è¡Œæ—¶é—´å·®å¼‚å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š
```scala
// æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†
val skewedRDD = rdd.map(x => {
  val key = x._1
  val value = x._2
  if (isSkewedKey(key)) {
    (key + "_" + Random.nextInt(10), value)
  } else {
    (key, value)
  }
})

// æ–¹æ¡ˆ2ï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨
class SkewPartitioner(numPartitions: Int) extends Partitioner {
  override def numPartitions: Int = numPartitions
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val rawKey = key.toString.split("_")(0)
    math.abs(rawKey.hashCode) % numPartitions
  }
}
```

**2. Shuffleæ–‡ä»¶è¿‡å¤šé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­äº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼Œå½±å“æ€§èƒ½

**è§£å†³æ–¹æ¡ˆ**ï¼š
```properties
# åˆå¹¶å°æ–‡ä»¶
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200
```

**3. å†…å­˜æº¢å‡ºé—®é¢˜**

**ç°è±¡**ï¼šShuffleè¿‡ç¨‹ä¸­å‡ºç°OOM

**è§£å†³æ–¹æ¡ˆ**ï¼š
```properties
# å¯ç”¨Spillæœºåˆ¶
spark.shuffle.spill=true
spark.shuffle.spill.compress=true

# è°ƒæ•´å†…å­˜é…ç½®
spark.executor.memory=4g
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3
```

---

## æ€§èƒ½è°ƒä¼˜ä¸ç›‘æ§ â­â­â­

### æ€§èƒ½è°ƒä¼˜ç­–ç•¥

#### èµ„æºé…ç½®è°ƒä¼˜

**æ ¸å¿ƒèµ„æºå‚æ•°**ï¼š
```bash
# Executoré…ç½®
--conf spark.executor.memory=4g           # Executorå†…å­˜
--conf spark.executor.cores=4             # Executoræ ¸å¿ƒæ•°
--conf spark.executor.instances=10        # Executoræ•°é‡

# Driveré…ç½®
--conf spark.driver.memory=2g             # Driverå†…å­˜
--conf spark.driver.cores=2               # Driveræ ¸å¿ƒæ•°

# åŠ¨æ€åˆ†é…
--conf spark.dynamicAllocation.enabled=true
--conf spark.dynamicAllocation.minExecutors=2
--conf spark.dynamicAllocation.maxExecutors=20
```

#### ä»£ç ä¼˜åŒ–æŠ€å·§

**ä»£ç ä¼˜åŒ–å®ä¾‹**ï¼š
```scala
// 1. é¿å…é‡å¤è®¡ç®—
val processedRDD = rawRDD.map(process).cache()  // ç¼“å­˜ä¸­é—´ç»“æœ

// 2. ä½¿ç”¨mapPartitionså‡å°‘å¯¹è±¡åˆ›å»º
val result = rdd.mapPartitions { iter =>
  val connection = createConnection()  // æ¯ä¸ªåˆ†åŒºåˆ›å»ºä¸€æ¬¡è¿æ¥
  iter.map(processWithConnection(_, connection))
}

// 3. ä½¿ç”¨å¹¿æ’­å˜é‡
val broadcastVar = sc.broadcast(largeMap)
val result = rdd.map { record =>
  val value = broadcastVar.value.get(record.key)
  process(record, value)
}

// 4. åˆç†ä½¿ç”¨persist
val intermediateRDD = rdd.filter(condition).persist(StorageLevel.MEMORY_AND_DISK_SER)
```

#### ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

**Storage Levelé€‰æ‹©**ï¼š
```scala
import org.apache.spark.storage.StorageLevel

// å†…å­˜ä¼˜å…ˆï¼Œåºåˆ—åŒ–å­˜å‚¨
rdd.persist(StorageLevel.MEMORY_ONLY_SER)

// å†…å­˜+ç£ç›˜ï¼Œé€‚åˆå¤§æ•°æ®é›†
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

// å¤šå‰¯æœ¬å­˜å‚¨ï¼Œæé«˜å®¹é”™æ€§
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)
```

### å¸¸è§æ€§èƒ½é—®é¢˜

#### å†…å­˜æº¢å‡ºé—®é¢˜ ğŸ”¥

**OOMé—®é¢˜è¯Šæ–­**ï¼š
```scala
// 1. Driver OOM
// åŸå› ï¼šcollect()æ“ä½œæ•°æ®é‡è¿‡å¤§
val result = largeRDD.collect()  // å±é™©æ“ä½œ

// è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨take()æˆ–åˆ†æ‰¹å¤„ç†
val sample = largeRDD.take(1000)
largeRDD.foreachPartition { partition =>
  // åˆ†åŒºå†…å¤„ç†ï¼Œé¿å…å°†æ‰€æœ‰æ•°æ®æ‹‰åˆ°Driver
}

// 2. Executor OOM  
// åŸå› ï¼šå•ä¸ªåˆ†åŒºæ•°æ®è¿‡å¤§
// è§£å†³æ–¹æ¡ˆï¼šå¢åŠ åˆ†åŒºæ•°
val repartitionedRDD = rdd.repartition(numPartitions * 2)
```

#### æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ ğŸ”¥

**å€¾æ–œæ£€æµ‹å’Œè§£å†³**ï¼š
```scala
// æ£€æµ‹å€¾æ–œ
def detectSkew[T](rdd: RDD[T]): Unit = {
  val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
    Iterator((index, iter.size))
  }.collect()
  
  val maxSize = partitionSizes.maxBy(_._2)
  val avgSize = partitionSizes.map(_._2).sum / partitionSizes.length
  
  if (maxSize._2 > avgSize * 3) {
    println(s"æ•°æ®å€¾æ–œè­¦å‘Šï¼šåˆ†åŒº${maxSize._1}æœ‰${maxSize._2}æ¡è®°å½•ï¼Œå¹³å‡${avgSize}æ¡")
  }
}
```

### ç›‘æ§ä¸è¯Šæ–­

#### Spark UIç›‘æ§

**å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š
- **Jobsé¡µé¢**ï¼šä½œä¸šæ‰§è¡Œæ—¶é—´ã€Stageä¿¡æ¯
- **Stagesé¡µé¢**ï¼šStageæ‰§è¡Œè¯¦æƒ…ã€ä»»åŠ¡åˆ†å¸ƒ
- **Storageé¡µé¢**ï¼šRDDç¼“å­˜ä½¿ç”¨æƒ…å†µ
- **Executorsé¡µé¢**ï¼šExecutorèµ„æºä½¿ç”¨æƒ…å†µ
- **SQLé¡µé¢**ï¼šSQLæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’

#### æ€§èƒ½æŒ‡æ ‡

**æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡**ï¼š
```bash
# æŸ¥çœ‹åº”ç”¨ç¨‹åºæŒ‡æ ‡
curl http://driver-host:4040/api/v1/applications
curl http://driver-host:4040/api/v1/applications/[app-id]/jobs
curl http://driver-host:4040/api/v1/applications/[app-id]/stages
curl http://driver-host:4040/api/v1/applications/[app-id]/executors
```

---

## Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥

### åŸºç¡€æ¦‚å¿µé¢˜

#### RDDã€DataFrameã€DatasetåŒºåˆ«

**é¢è¯•è¦ç‚¹**ï¼š

| ç‰¹æ€§         | RDD       | DataFrame    | Dataset        |
| ------------ | --------- | ------------ | -------------- |
| **ç±»å‹å®‰å…¨** | ç¼–è¯‘æ—¶    | è¿è¡Œæ—¶       | ç¼–è¯‘æ—¶         |
| **æ€§èƒ½ä¼˜åŒ–** | æ—         | Catalystä¼˜åŒ– | Catalystä¼˜åŒ–   |
| **åºåˆ—åŒ–**   | Java/Kryo | Tungsten     | Tungsten       |
| **APIé£æ ¼**  | å‡½æ•°å¼    | SQL+å‡½æ•°å¼   | ç±»å‹å®‰å…¨å‡½æ•°å¼ |

**å›ç­”æ¨¡æ¿**ï¼š
"RDDæ˜¯Sparkçš„åŸºç¡€æŠ½è±¡ï¼Œæä¾›äº†åˆ†å¸ƒå¼å†…å­˜è®¡ç®—èƒ½åŠ›ï¼Œä½†æ²¡æœ‰Schemaä¿¡æ¯å’Œä¼˜åŒ–ï¼›DataFrameåœ¨RDDåŸºç¡€ä¸Šå¢åŠ äº†Schemaï¼Œå¯ä»¥äº«å—Catalystä¼˜åŒ–å™¨çš„ä¼˜åŒ–ï¼Œä½†ç±»å‹æ£€æŸ¥åœ¨è¿è¡Œæ—¶ï¼›Datasetç»“åˆäº†RDDçš„ç±»å‹å®‰å…¨å’ŒDataFrameçš„ä¼˜åŒ–æ€§èƒ½ï¼Œåœ¨ç¼–è¯‘æ—¶è¿›è¡Œç±»å‹æ£€æŸ¥ã€‚"

#### Sparkä»»åŠ¡æ‰§è¡Œæµç¨‹

**å®Œæ•´æ‰§è¡Œæµç¨‹**ï¼š
1. **ç”¨æˆ·ç¨‹åº** â†’ åˆ›å»ºSparkContext
2. **SparkContext** â†’ æ„å»ºRDD DAG
3. **DAGScheduler** â†’ åˆ’åˆ†Stageï¼Œç”ŸæˆTaskSet
4. **TaskScheduler** â†’ å°†Taskåˆ†å‘åˆ°Executor
5. **Executor** â†’ æ‰§è¡ŒTaskï¼Œè¿”å›ç»“æœ
6. **Driver** â†’ æ”¶é›†ç»“æœï¼Œå®Œæˆä½œä¸š

#### Sparkå†…å­˜ç®¡ç†

**ç»Ÿä¸€å†…å­˜ç®¡ç†**ï¼š
- **å­˜å‚¨å†…å­˜**ï¼šç¼“å­˜RDDã€å¹¿æ’­å˜é‡
- **æ‰§è¡Œå†…å­˜**ï¼šShuffleã€èšåˆæ“ä½œ
- **åŠ¨æ€åˆ†é…**ï¼šä¸¤è€…å¯ä»¥ç›¸äº’å€Ÿç”¨ï¼Œæ‰§è¡Œå†…å­˜ä¼˜å…ˆçº§æ›´é«˜

### æ¶æ„åŸç†é¢˜

#### Sparkæ¶æ„ç»„ä»¶

**æ ¸å¿ƒç»„ä»¶åŠŸèƒ½**ï¼š
- **Driver**ï¼šè¿è¡Œmainå‡½æ•°ï¼Œåè°ƒæ•´ä¸ªåº”ç”¨
- **SparkContext**ï¼šSparkç¨‹åºå…¥å£ç‚¹
- **Cluster Manager**ï¼šé›†ç¾¤èµ„æºç®¡ç†
- **Executor**ï¼šæ‰§è¡Œå…·ä½“è®¡ç®—ä»»åŠ¡
- **DAGScheduler**ï¼šæ„å»ºDAGï¼Œåˆ’åˆ†Stage
- **TaskScheduler**ï¼šä»»åŠ¡è°ƒåº¦å’Œåˆ†å‘

#### Shuffleæœºåˆ¶åŸç†

**Shuffleè¿‡ç¨‹**ï¼š
1. **Mapç«¯**ï¼šæŒ‰åˆ†åŒºå™¨å¯¹æ•°æ®åˆ†ç»„ï¼Œå†™å…¥æœ¬åœ°æ–‡ä»¶
2. **Reduceç«¯**ï¼šä»å„MapèŠ‚ç‚¹æ‹‰å–æ•°æ®ï¼Œè¿›è¡Œåˆå¹¶å¤„ç†
3. **ä¼˜åŒ–**ï¼šSort Shuffleå‡å°‘æ–‡ä»¶æ•°ï¼Œæé«˜æ€§èƒ½

### æ€§èƒ½è°ƒä¼˜é¢˜

#### æ€§èƒ½è°ƒä¼˜ç­–ç•¥

**è°ƒä¼˜æ€è·¯**ï¼š
1. **èµ„æºé…ç½®**ï¼šåˆç†è®¾ç½®Executorå†…å­˜å’Œæ ¸å¿ƒæ•°
2. **å¹¶è¡Œåº¦è°ƒä¼˜**ï¼šè®¾ç½®åˆé€‚çš„åˆ†åŒºæ•°
3. **æ•°æ®å€¾æ–œ**ï¼šè¯†åˆ«å’Œè§£å†³æ•°æ®å€¾æ–œé—®é¢˜
4. **ç¼“å­˜ç­–ç•¥**ï¼šç¼“å­˜é‡å¤ä½¿ç”¨çš„RDD
5. **Shuffleä¼˜åŒ–**ï¼šå‡å°‘Shuffleæ“ä½œï¼Œä¼˜åŒ–Shuffleå‚æ•°

#### æ•°æ®å€¾æ–œè§£å†³

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **é¢„å¤„ç†**ï¼šè¿‡æ»¤å¼‚å¸¸æ•°æ®
2. **é‡æ–°åˆ†åŒº**ï¼šå¢åŠ åˆ†åŒºæ•°
3. **ä¸¤é˜¶æ®µèšåˆ**ï¼šåŠ ç›é¢„èšåˆ
4. **å¹¿æ’­Join**ï¼šå°è¡¨å¹¿æ’­
5. **è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼šå‡åŒ€åˆ†å¸ƒæ•°æ®

### å®æˆ˜åº”ç”¨é¢˜

#### Spark SQLåº”ç”¨

**å¸¸è§ä¼˜åŒ–**ï¼š
```scala
// å¼€å¯è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

// å¹¿æ’­Joiné˜ˆå€¼
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
```

#### æ•…éšœæ’æŸ¥æ–¹æ³•

**æ’æŸ¥æ­¥éª¤**ï¼š
1. **æŸ¥çœ‹Spark UI**ï¼šåˆ†æå¤±è´¥çš„Stageå’ŒTask
2. **æŸ¥çœ‹æ—¥å¿—**ï¼šDriverå’ŒExecutoræ—¥å¿—
3. **èµ„æºç›‘æ§**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µ
4. **æ€§èƒ½åˆ†æ**ï¼šè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
5. **å‚æ•°è°ƒä¼˜**ï¼šæ ¹æ®é—®é¢˜è°ƒæ•´é…ç½®å‚æ•°

### æ·±åº¦æŠ€æœ¯åŸç†é¢˜

#### Sparkçš„Catalystä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

Catalystä¼˜åŒ–å™¨æ˜¯Spark SQLçš„æ ¸å¿ƒä¼˜åŒ–å¼•æ“ï¼ŒåŸºäºScalaçš„æ¨¡å¼åŒ¹é…å’Œå‡†åˆ†æè®¡ç®—å®ç°ã€‚

**å·¥ä½œæµç¨‹**ï¼š
1. **è¯­æ³•åˆ†æ**ï¼šSQLè¯­å¥è§£æä¸ºæŠ½è±¡è¯­æ³•æ ‘(AST)
2. **é€»è¾‘è®¡åˆ’**ï¼šASTè½¬æ¢ä¸ºé€»è¾‘è®¡åˆ’(LogicalPlan)
3. **é€»è¾‘ä¼˜åŒ–**ï¼šåº”ç”¨è§„åˆ™ä¼˜åŒ–é€»è¾‘è®¡åˆ’
4. **ç‰©ç†è®¡åˆ’**ï¼šç”Ÿæˆå¤šä¸ªç‰©ç†æ‰§è¡Œè®¡åˆ’
5. **ä»£ç ç”Ÿæˆ**ï¼šé€‰æ‹©æœ€ä¼˜è®¡åˆ’å¹¶ç”ŸæˆJavaä»£ç 

**ä¸»è¦ä¼˜åŒ–è§„åˆ™**ï¼š
- **è°“è¯ä¸‹æ¨**ï¼šå°†è¿‡æ»¤æ¡ä»¶ä¸‹æ¨åˆ°æ•°æ®æº
- **æŠ•å½±ä¸‹æ¨**ï¼šåªè¯»å–éœ€è¦çš„åˆ—
- **å¸¸é‡æŠ˜å **ï¼šç¼–è¯‘æ—¶è®¡ç®—å¸¸é‡è¡¨è¾¾å¼
- **åˆ—è£å‰ª**ï¼šæ¶ˆé™¤ä¸éœ€è¦çš„åˆ—
- **Joiné‡æ’åº**ï¼šé€‰æ‹©æœ€ä¼˜çš„Joiné¡ºåº

#### Sparkçš„å†…å­˜ç®¡ç†æ¼”è¿›å†å²ï¼Œæ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

Sparkçš„å†…å­˜ç®¡ç†ç»å†äº†é‡è¦çš„æ¼”è¿›è¿‡ç¨‹ã€‚

**é™æ€å†…å­˜ç®¡ç†ï¼ˆæ—©æœŸç‰ˆæœ¬ï¼‰**ï¼š
- **å›ºå®šæ¯”ä¾‹åˆ†é…**ï¼šStorageå’ŒExecutionå†…å­˜æ¯”ä¾‹å›ºå®š
- **é…ç½®å¤æ‚**ï¼šéœ€è¦æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå†…å­˜ç›¸å…³å‚æ•°
- **èµ„æºæµªè´¹**ï¼šæ— æ³•åŠ¨æ€è°ƒæ•´å†…å­˜åˆ†é…

**ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼ˆ1.6+ç‰ˆæœ¬ï¼‰**ï¼š
- **åŠ¨æ€åˆ†é…**ï¼šStorageå’ŒExecutionå†…å­˜å¯ä»¥ç›¸äº’å€Ÿç”¨
- **ç®€åŒ–é…ç½®**ï¼šåªéœ€é…ç½®æ€»å†…å­˜å¤§å°å’Œæ¯”ä¾‹
- **æé«˜åˆ©ç”¨ç‡**ï¼šæ ¹æ®å®é™…éœ€æ±‚åŠ¨æ€åˆ†é…å†…å­˜

**å…³é”®å·®å¼‚**ï¼š
```scala
// æ—§ç‰ˆæœ¬é…ç½®
spark.storage.memoryFraction=0.6
spark.shuffle.memoryFraction=0.2

// æ–°ç‰ˆæœ¬é…ç½®
spark.memory.fraction=0.8
spark.memory.storageFraction=0.5
```

#### Spark Streamingå’ŒStructured Streamingçš„åŒºåˆ«ï¼Ÿ



| ç‰¹æ€§         | Spark Streaming     | Structured Streaming   |
| ------------ | ------------------- | ---------------------- |
| **å¤„ç†æ¨¡å‹** | å¾®æ‰¹å¤„ç†ï¼ˆDStreamï¼‰ | è¿ç»­å¤„ç†ï¼ˆæ— ç•Œè¡¨ï¼‰     |
| **API**      | DStream API         | DataFrame/Dataset API  |
| **å®¹é”™æœºåˆ¶** | WAL + Checkpoint    | çŠ¶æ€å­˜å‚¨ + Checkpoint  |
| **å»¶è¿Ÿ**     | ç§’çº§                | æ¯«ç§’çº§                 |
| **çŠ¶æ€ç®¡ç†** | åŸºæœ¬çŠ¶æ€æ”¯æŒ        | ä¸°å¯Œçš„çŠ¶æ€ç®¡ç†         |
| **äº‹ä»¶æ—¶é—´** | æœ‰é™æ”¯æŒ            | åŸç”Ÿæ”¯æŒ               |
| **æ°´å°**     | ä¸æ”¯æŒ              | æ”¯æŒ                   |
| **è¾“å‡ºæ¨¡å¼** | å›ºå®š                | Complete/Append/Update |

**é€‰æ‹©å»ºè®®**ï¼š
- **Spark Streaming**ï¼šé—ç•™ç³»ç»Ÿé›†æˆã€ç®€å•æµå¤„ç†
- **Structured Streaming**ï¼šæ–°é¡¹ç›®ã€å¤æ‚æµå¤„ç†ã€äº‹ä»¶æ—¶é—´å¤„ç†

#### Sparkå¦‚ä½•å®ç°Exactly-Onceè¯­ä¹‰ï¼Ÿ

Sparké€šè¿‡å¤šå±‚æœºåˆ¶ä¿è¯Exactly-Onceè¯­ä¹‰ã€‚

**å®ç°æœºåˆ¶**ï¼š
1. **å¹‚ç­‰æ€§è¾“å‡º**ï¼šç¡®ä¿ç›¸åŒæ•°æ®å¤šæ¬¡å†™å…¥ç»“æœä¸€è‡´
2. **äº‹åŠ¡æ€§è¾“å‡º**ï¼šä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§å†™å…¥
3. **Checkpointæœºåˆ¶**ï¼šè®°å½•å¤„ç†è¿›åº¦å’ŒçŠ¶æ€
4. **è¾“å‡ºæäº¤åè®®**ï¼šç¡®ä¿è¾“å‡ºæ“ä½œçš„åŸå­æ€§

**å®ç°ç¤ºä¾‹**ï¼š
```scala
// ä½¿ç”¨äº‹åŠ¡æ€§è¾“å‡º
val query = inputStream
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    // ä½¿ç”¨batchIdç¡®ä¿å¹‚ç­‰æ€§
    val outputPath = s"output/batch_$batchId"
    batchDF.write.mode("overwrite").parquet(outputPath)
  }
  .option("checkpointLocation", "/checkpoint")
  .start()
```

### æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜

#### å¦‚ä½•è¯Šæ–­å’Œè§£å†³Sparkåº”ç”¨çš„æ€§èƒ½é—®é¢˜ï¼Ÿ



**è¯Šæ–­æ­¥éª¤**ï¼š
1. **æ”¶é›†æŒ‡æ ‡**ï¼šCPUã€å†…å­˜ã€ç½‘ç»œã€ç£ç›˜ä½¿ç”¨æƒ…å†µ
2. **åˆ†ææ—¥å¿—**ï¼šDriverå’ŒExecutoræ—¥å¿—
3. **æŸ¥çœ‹Spark UI**ï¼šStageæ‰§è¡Œæ—¶é—´ã€Taskåˆ†å¸ƒ
4. **ç›‘æ§GC**ï¼šåƒåœ¾å›æ”¶é¢‘ç‡å’Œæ—¶é—´

**å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**ï¼š

**é—®é¢˜1ï¼šæ•°æ®å€¾æ–œ**
```scala
// æ£€æµ‹æ–¹æ³•
val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
  Iterator((index, iter.size))
}.collect()

// è§£å†³æ–¹æ¡ˆï¼šåŠ ç›å¤„ç†
val saltedRDD = rdd.map(x => (x._1 + "_" + Random.nextInt(10), x._2))
```

**é—®é¢˜2ï¼šå†…å­˜æº¢å‡º**
```bash
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å†…å­˜é…ç½®
--conf spark.executor.memory=8g
--conf spark.executor.memoryOverhead=2g
--conf spark.memory.offHeap.enabled=true
--conf spark.memory.offHeap.size=4g
```

**é—®é¢˜3ï¼šä»»åŠ¡æ‰§è¡Œç¼“æ…¢**
```scala
// æ£€æŸ¥å¹¶è¡Œåº¦
println(s"åˆ†åŒºæ•°: ${rdd.getNumPartitions}")

// è°ƒæ•´åˆ†åŒºæ•°
val repartitionedRDD = rdd.repartition(optimalPartitions)
```

#### Sparkåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•è¿›è¡Œç›‘æ§ï¼Ÿ



**ç›‘æ§ç»´åº¦**ï¼š
1. **åº”ç”¨çº§ç›‘æ§**ï¼šä½œä¸šæ‰§è¡ŒçŠ¶æ€ã€æ‰§è¡Œæ—¶é—´
2. **èµ„æºç›‘æ§**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œä½¿ç”¨
3. **æ€§èƒ½ç›‘æ§**ï¼šTaskæ‰§è¡Œæ—¶é—´ã€Shuffleæ•°æ®é‡
4. **é”™è¯¯ç›‘æ§**ï¼šå¤±è´¥ä»»åŠ¡ã€å¼‚å¸¸ç»Ÿè®¡

**ç›‘æ§å·¥å…·**ï¼š
```scala
// 1. è‡ªå®šä¹‰ç›‘å¬å™¨
class CustomSparkListener extends SparkListener {
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
    val stage = stageCompleted.stageInfo
    println(s"Stage ${stage.stageId} completed in ${stage.completionTime.get - stage.submissionTime.get} ms")
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    if (metrics.executorRunTime > 60000) {  // è¶…è¿‡1åˆ†é’Ÿçš„ä»»åŠ¡
      println(s"Long running task: ${taskEnd.taskInfo.taskId}")
    }
  }
}

// æ³¨å†Œç›‘å¬å™¨
spark.sparkContext.addSparkListener(new CustomSparkListener)
```

**ç›‘æ§æŒ‡æ ‡**ï¼š
```bash
# å…³é”®ç›‘æ§æŒ‡æ ‡
- åº”ç”¨æ‰§è¡Œæ—¶é—´
- Taskå¤±è´¥ç‡
- å†…å­˜ä½¿ç”¨ç‡
- GCæ—¶é—´å æ¯”
- Shuffleæ•°æ®é‡
- ç½‘ç»œI/O
- ç£ç›˜I/O
```

#### Sparkä¸Hadoopç”Ÿæ€ç³»ç»Ÿçš„é›†æˆæœ€ä½³å®è·µï¼Ÿ



**é›†æˆè¦ç‚¹**ï¼š
1. **å­˜å‚¨é›†æˆ**ï¼šHDFSã€HBaseã€Hive
2. **èµ„æºç®¡ç†**ï¼šYARNé›†æˆ
3. **å®‰å…¨é›†æˆ**ï¼šKerberosè®¤è¯
4. **ç›‘æ§é›†æˆ**ï¼šHadoopç›‘æ§ä½“ç³»

**æœ€ä½³å®è·µ**ï¼š
```scala
// 1. Hiveé›†æˆ
spark.sql("CREATE TABLE IF NOT EXISTS spark_hive_table (key INT, value STRING) USING HIVE")

// 2. HBaseé›†æˆ
val hbaseConf = HBaseConfiguration.create()
hbaseConf.set("hbase.zookeeper.quorum", "zk1,zk2,zk3")
val hbaseRDD = spark.sparkContext.newAPIHadoopRDD(
  hbaseConf,
  classOf[TableInputFormat],
  classOf[ImmutableBytesWritable],
  classOf[Result]
)

// 3. YARNé…ç½®
val conf = new SparkConf()
conf.set("spark.master", "yarn")
conf.set("spark.submit.deployMode", "cluster")
conf.set("spark.yarn.queue", "production")
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š
```properties
# HDFSä¼˜åŒ–
spark.hadoop.dfs.blocksize=268435456
spark.hadoop.dfs.replication=3

# YARNä¼˜åŒ–
spark.yarn.executor.memoryOverhead=512m
spark.yarn.driver.memoryOverhead=512m
spark.yarn.maxAppAttempts=2
```

### é«˜çº§åº”ç”¨é¢˜

#### å¦‚ä½•åœ¨Sparkä¸­å®ç°æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Ÿ



**MLlib Pipelineæ„å»º**ï¼š
```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification.LogisticRegression

// 1. æ•°æ®é¢„å¤„ç†
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")

val hashingTF = new HashingTF()
  .setInputCol("words")
  .setOutputCol("features")
  .setNumFeatures(10000)

// 2. ç‰¹å¾ç¼©æ”¾
val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")

// 3. æ¨¡å‹è®­ç»ƒ
val lr = new LogisticRegression()
  .setFeaturesCol("scaledFeatures")
  .setLabelCol("label")

// 4. æ„å»ºPipeline
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, scaler, lr))

// 5. è®­ç»ƒæ¨¡å‹
val model = pipeline.fit(trainingData)

// 6. æ¨¡å‹é¢„æµ‹
val predictions = model.transform(testData)
```

**æ¨¡å‹è¯„ä¼°**ï¼š
```scala
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val evaluator = new BinaryClassificationEvaluator()
  .setLabelCol("label")
  .setRawPredictionCol("rawPrediction")
  .setMetricName("areaUnderROC")

val auc = evaluator.evaluate(predictions)
println(s"AUC: $auc")
```

#### Sparkåœ¨å®æ—¶æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Ÿ



**æ¶æ„è®¾è®¡**ï¼š
```mermaid
graph LR
    A[ç”¨æˆ·è¡Œä¸ºæ•°æ®] --> B[Kafka]
    B --> C[Structured Streaming]
    C --> D[ç‰¹å¾æå–]
    D --> E[æ¨¡å‹é¢„æµ‹]
    E --> F[æ¨èç»“æœ]
    F --> G[Redisç¼“å­˜]
    G --> H[APIæœåŠ¡]
```

**å®ç°ä»£ç **ï¼š
```scala
// 1. å®æ—¶ç‰¹å¾æå–
val userFeatures = kafkaStream
  .select(from_json($"value", schema).as("data"))
  .select("data.*")
  .withWatermark("timestamp", "10 minutes")
  .groupBy($"userId", window($"timestamp", "10 minutes"))
  .agg(
    count("*").as("eventCount"),
    countDistinct("itemId").as("uniqueItems"),
    collect_list("category").as("categories")
  )

// 2. å®æ—¶æ¨è
val recommendations = userFeatures.map { row =>
  val userId = row.getAs[String]("userId")
  val features = extractFeatures(row)
  val items = recommendationModel.recommend(userId, features, 10)
  RecommendationResult(userId, items, System.currentTimeMillis())
}

// 3. ç»“æœè¾“å‡º
recommendations.writeStream
  .foreach(new ForeachWriter[RecommendationResult] {
    override def open(partitionId: Long, epochId: Long): Boolean = true
    
    override def process(rec: RecommendationResult): Unit = {
      // å†™å…¥Redis
      redisClient.setex(s"rec:${rec.userId}", 3600, rec.toJson)
    }
    
    override def close(errorOrNull: Throwable): Unit = {}
  })
  .start()
```

---

## Spark Streaming â­

### æµå¤„ç†æ¦‚å¿µ

#### å¾®æ‰¹æ¬¡å¤„ç†

**Spark Streaming** åŸºäºå¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰å¤„ç†æ¨¡å‹ï¼Œå°†è¿ç»­çš„æ•°æ®æµåˆ’åˆ†ä¸ºå°çš„æ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

```mermaid
graph LR
    A[å®æ—¶æ•°æ®æµ] --> B[æ‰¹æ¬¡1<br/>1-2ç§’]
    A --> C[æ‰¹æ¬¡2<br/>2-3ç§’]
    A --> D[æ‰¹æ¬¡3<br/>3-4ç§’]
    
    B --> E[RDD1]
    C --> F[RDD2]
    D --> G[RDD3]
    
    E --> H[å¤„ç†ç»“æœ1]
    F --> I[å¤„ç†ç»“æœ2]
    G --> J[å¤„ç†ç»“æœ3]
    
    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

#### DStreamæ¦‚å¿µ

**DStream (Discretized Stream)** æ˜¯Spark Streamingçš„åŸºæœ¬æŠ½è±¡ï¼Œä»£è¡¨è¿ç»­çš„æ•°æ®æµã€‚

```scala
// DStreamåŸºæœ¬ä½¿ç”¨
val conf = new SparkConf().setAppName("StreamingExample")
val ssc = new StreamingContext(conf, Seconds(2))

// åˆ›å»ºDStream
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)

// è¾“å‡ºç»“æœ
wordCounts.print()

// å¯åŠ¨æµå¤„ç†
ssc.start()
ssc.awaitTermination()
```

### Structured Streaming

#### æ ¸å¿ƒæ¦‚å¿µ

**Structured Streaming** æ˜¯Spark 2.0å¼•å…¥çš„æ–°æµå¤„ç†å¼•æ“ï¼ŒåŸºäºSpark SQLæ„å»ºã€‚

```scala
// Structured Streamingç¤ºä¾‹
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("StructuredStreamingExample")
  .getOrCreate()

// å®šä¹‰Schema
val schema = StructType(
  StructField("timestamp", TimestampType, true) ::
  StructField("value", StringType, true) :: Nil
)

// åˆ›å»ºæµæ•°æ®æº
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic1")
  .load()

// å¤„ç†æ•°æ®
val words = df
  .selectExpr("CAST(value AS STRING) as message")
  .flatMap(_.split(" "))
  .groupBy("word")
  .count()

// è¾“å‡ºåˆ°æ§åˆ¶å°
val query = words.writeStream
  .outputMode("complete")
  .format("console")
  .trigger(Trigger.ProcessingTime("2 seconds"))
  .start()

query.awaitTermination()
```

#### è¾“å‡ºæ¨¡å¼

| è¾“å‡ºæ¨¡å¼     | æè¿°           | é€‚ç”¨åœºæ™¯           |
| ------------ | -------------- | ------------------ |
| **Complete** | è¾“å‡ºå®Œæ•´ç»“æœè¡¨ | èšåˆæŸ¥è¯¢           |
| **Append**   | åªè¾“å‡ºæ–°å¢è¡Œ   | æ— èšåˆçš„æŸ¥è¯¢       |
| **Update**   | è¾“å‡ºæ›´æ–°çš„è¡Œ   | èšåˆæŸ¥è¯¢çš„å¢é‡æ›´æ–° |

#### çª—å£æ“ä½œ

```scala
// çª—å£èšåˆ
val windowedCounts = df
  .withWatermark("timestamp", "10 minutes")
  .groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  )
  .count()

// ä¼šè¯çª—å£
val sessionCounts = df
  .groupBy(
    session_window($"timestamp", "30 minutes"),
    $"userId"
  )
  .count()
```

### å®¹é”™æœºåˆ¶

#### Checkpointæœºåˆ¶

**Checkpoint** æä¾›å®¹é”™æ¢å¤èƒ½åŠ›ï¼Œä¿å­˜DStreamçš„å…ƒæ•°æ®å’Œæ•°æ®ã€‚

```scala
// è®¾ç½®Checkpoint
ssc.checkpoint("hdfs://checkpoint/path")

// æœ‰çŠ¶æ€æ“ä½œéœ€è¦Checkpoint
val runningCounts = words.updateStateByKey { (values: Seq[Int], state: Option[Int]) =>
  val currentCount = values.sum
  val previousCount = state.getOrElse(0)
  Some(currentCount + previousCount)
}
```

#### WALæœºåˆ¶

**Write-Ahead Logs** ç¡®ä¿æ¥æ”¶åˆ°çš„æ•°æ®åœ¨å¤„ç†å‰å…ˆå†™å…¥å¯é å­˜å‚¨ã€‚

```scala
// å¯ç”¨WAL
spark.conf.set("spark.streaming.receiver.writeAheadLog.enable", "true")
spark.conf.set("spark.streaming.driver.writeAheadLog.closeFileAfterWrite", "true")
```

---
## é‡ç‚¹å†…å®¹æºç è®²è§£
### æ ¸å¿ƒæ¨¡å—æºç 

**SparkContextåˆå§‹åŒ–æºç åˆ†æ**ï¼š
```scala
// SparkContext.scala æ ¸å¿ƒåˆå§‹åŒ–æµç¨‹
class SparkContext(config: SparkConf) extends Logging {
  
  // 1. åˆ›å»ºSparkEnv - æ ¸å¿ƒè¿è¡Œç¯å¢ƒ
  private val env: SparkEnv = {
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, numCores, mockOutputCommitCoordinator)
  }
  
  // 2. åˆ›å»ºçŠ¶æ€è·Ÿè¸ªå™¨
  private val statusTracker = new SparkStatusTracker(this, sparkUI)
  
  // 3. åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨
  private val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
  private val taskScheduler = ts
  
  // 4. åˆ›å»ºDAGè°ƒåº¦å™¨
  private val dagScheduler = new DAGScheduler(this)
  
  // 5. å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨
  taskScheduler.start()
  
  // 6. è®¾ç½®é»˜è®¤å¹¶è¡Œåº¦
  private val defaultParallelism: Int = taskScheduler.defaultParallelism
  
  // æ ¸å¿ƒæ–¹æ³•ï¼šåˆ›å»ºRDD
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
  
  // æ ¸å¿ƒæ–¹æ³•ï¼šæäº¤ä½œä¸š
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
  }
}
```

**RDDæºç æ ¸å¿ƒå®ç°**ï¼š
```scala
// RDD.scala æ ¸å¿ƒæŠ½è±¡
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {

  // äº”å¤§ç‰¹æ€§çš„å…·ä½“å®ç°
  
  // 1. åˆ†åŒºåˆ—è¡¨
  protected def getPartitions: Array[Partition]
  
  // 2. è®¡ç®—å‡½æ•°
  def compute(split: Partition, context: TaskContext): Iterator[T]
  
  // 3. ä¾èµ–å…³ç³»
  protected def getDependencies: Seq[Dependency[_]] = deps
  
  // 4. åˆ†åŒºå™¨ï¼ˆå¯é€‰ï¼‰
  @transient val partitioner: Option[Partitioner] = None
  
  // 5. ä½ç½®åå¥½ï¼ˆå¯é€‰ï¼‰
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil
  
  // Transformationæ“ä½œå®ç°
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
  }
  
  def filter(f: T => Boolean): RDD[T] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) => iter.filter(cleanF),
      preservesPartitioning = true)
  }
  
  def reduceByKey(func: (T, T) => T): RDD[T] = self.withScope {
    reduceByKey(defaultPartitioner(self), func)
  }
  
  // Actionæ“ä½œå®ç°
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }
  
  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum
  
  def foreach(f: T => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }
}
```

### è°ƒåº¦å™¨æºç 

**DAGScheduleræºç åˆ†æ**ï¼š
```scala
// DAGScheduler.scala æ ¸å¿ƒè°ƒåº¦é€»è¾‘
class DAGScheduler(
    private[scheduler] val sc: SparkContext,
    private[scheduler] val taskScheduler: TaskScheduler,
    listenerBus: LiveListenerBus,
    mapOutputTracker: MapOutputTrackerMaster,
    blockManagerMaster: BlockManagerMaster,
    env: SparkEnv,
    clock: Clock = new SystemClock())
  extends Logging {

  // äº‹ä»¶å¤„ç†å¾ªç¯
  private val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  
  // æäº¤ä½œä¸šçš„æ ¸å¿ƒæ–¹æ³•
  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): Unit = {
    
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    ThreadUtils.awaitReady(waiter, Duration.Inf)
    waiter.value.get match {
      case scala.util.Success(_) =>
        logInfo("Job %d finished: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =>
        logInfo("Job %d failed: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        throw exception
    }
  }
  
  // Stageåˆ’åˆ†æ ¸å¿ƒç®—æ³•
  private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
    
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      case Some(stage) =>
        stage
        
      case None =>
        // é€’å½’åˆ›å»ºçˆ¶Stage
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }
  
  // æŸ¥æ‰¾ç¼ºå¤±çš„çˆ¶ä¾èµ–
  private def getMissingAncestorShuffleDependencies(
      rdd: RDD[_]): ArrayStack[ShuffleDependency[_, _, _]] = {
    val ancestors = new ArrayStack[ShuffleDependency[_, _, _]]
    val visited = new HashSet[RDD[_]]
    val waitingForVisit = new ArrayStack[RDD[_]]
    
    waitingForVisit.push(rdd)
    while (waitingForVisit.nonEmpty) {
      val toVisit = waitingForVisit.pop()
      if (!visited(toVisit)) {
        visited += toVisit
        toVisit.dependencies.foreach {
          case shuffleDep: ShuffleDependency[_, _, _] =>
            if (!shuffleIdToMapStage.contains(shuffleDep.shuffleId)) {
              ancestors.push(shuffleDep)
              waitingForVisit.push(shuffleDep.rdd)
            }
          case narrowDep: NarrowDependency[_] =>
            waitingForVisit.push(narrowDep.rdd)
        }
      }
    }
    ancestors
  }
  
  // æäº¤Stage
  private def submitStage(stage: Stage): Unit = {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        val missing = getMissingParentStages(stage).sortBy(_.id)
        if (missing.isEmpty) {
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent <- missing) {
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    }
  }
}
```

### å­˜å‚¨ç³»ç»Ÿæºç 

**BlockManageræºç åˆ†æ**ï¼š
```scala
// BlockManager.scala å­˜å‚¨ç®¡ç†æ ¸å¿ƒ
class BlockManager(
    executorId: String,
    rpcEnv: RpcEnv,
    val master: BlockManagerMaster,
    val serializerManager: SerializerManager,
    val conf: SparkConf,
    memoryManager: MemoryManager,
    mapOutputTracker: MapOutputTracker,
    shuffleManager: ShuffleManager,
    val blockTransferService: BlockTransferService,
    securityManager: SecurityManager,
    numUsableCores: Int)
  extends BlockDataManager with BlockEvictionHandler with Logging {

  // å†…å­˜å­˜å‚¨
  private[spark] val memoryStore =
    new MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this)
  
  // ç£ç›˜å­˜å‚¨
  private[spark] val diskStore = new DiskStore(conf, diskBlockManager, securityManager)
  
  // è·å–Blockçš„æ ¸å¿ƒæ–¹æ³•
  def getBlockData(blockId: BlockId): ManagedBuffer = {
    if (blockId.isShuffle) {
      shuffleManager.shuffleBlockResolver.getBlockData(blockId.asInstanceOf[ShuffleBlockId])
    } else {
      getLocalBytes(blockId) match {
        case Some(blockData) =>
          new BlockManagerManagedBuffer(blockInfoManager, blockId, blockData, true)
        case None =>
          throw new BlockNotFoundException(s"Block $blockId not found")
      }
    }
  }
  
  // å­˜å‚¨Blockçš„æ ¸å¿ƒæ–¹æ³•
  def putBlockData(
      blockId: BlockId,
      data: ManagedBuffer,
      level: StorageLevel,
      classTag: ClassTag[_]): Boolean = {
    putBytes(blockId, new ChunkedByteBuffer(data.nioByteBuffer()), level)(classTag)
  }
  
  // å†…å­˜å’Œç£ç›˜å­˜å‚¨é€»è¾‘
  private def doPutBytes[T](
      blockId: BlockId,
      bytes: ChunkedByteBuffer,
      level: StorageLevel,
      classTag: ClassTag[T],
      tellMaster: Boolean = true,
      keepReadLock: Boolean = false): Boolean = {
    
    doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) { info =>
      val startTimeMs = System.currentTimeMillis
      
      // å°è¯•å†…å­˜å­˜å‚¨
      val res = if (level.useMemory) {
        memoryStore.putBytes(blockId, bytes, level.memoryStorageLevel)
      } else {
        false
      }
      
      // å†…å­˜å­˜å‚¨å¤±è´¥ï¼Œå°è¯•ç£ç›˜å­˜å‚¨
      if (!res && level.useDisk) {
        diskStore.putBytes(blockId, bytes)
      } else {
        res
      }
    }
  }
  
  // Blockæ·˜æ±°ç­–ç•¥
  override def dropFromMemory(
      blockId: BlockId,
      data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel = {
    
    val info = blockInfoManager.lockForWriting(blockId)
    var blockIsUpdated = false
    val level = info.level
    
    try {
      if (level.useDisk && !diskStore.contains(blockId)) {
        data() match {
          case Left(elements) =>
            diskStore.put(blockId) { fileOutputStream =>
              serializerManager.dataSerializeStream(blockId,
                fileOutputStream, elements.toIterator)(info.classTag.asInstanceOf[ClassTag[T]])
            }
          case Right(bytes) =>
            diskStore.putBytes(blockId, bytes)
        }
        blockIsUpdated = true
      }
      
      memoryStore.remove(blockId)
      val droppedMemorySize = if (blockIsUpdated) 0L else info.size
      val blockIsRemoved = !level.useDisk
      
      if (blockIsRemoved) {
        blockInfoManager.removeBlock(blockId)
      }
      
      if (blockIsUpdated) {
        StorageLevel.DISK_ONLY
      } else {
        StorageLevel.NONE
      }
      
    } finally {
      blockInfoManager.unlock(blockId)
    }
  }
}
```

### ç½‘ç»œé€šä¿¡æºç 

**NettyBlockTransferServiceæºç **ï¼š
```scala
// NettyBlockTransferService.scala ç½‘ç»œä¼ è¾“æ ¸å¿ƒ
class NettyBlockTransferService(
    conf: SparkConf,
    securityManager: SecurityManager,
    bindAddress: String,
    advertiseAddress: String,
    numCores: Int)
  extends BlockTransferService {

  private[this] var transportContext: TransportContext = _
  private[this] var server: TransportServer = _
  private[this] var clientFactory: TransportClientFactory = _
  
  override def init(blockDataManager: BlockDataManager): Unit = {
    val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager)
    var serverBootstrap: Option[TransportServerBootstrap] = None
    var clientBootstrap: Option[TransportClientBootstrap] = None
    
    if (authEnabled) {
      serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager))
      clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager))
    }
    
    transportContext = new TransportContext(transportConf, rpcHandler)
    clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava)
    server = createServer(serverBootstrap.toList)
  }
  
  // è·å–è¿œç¨‹Block
  override def fetchBlocks(
      host: String,
      port: Int,
      execId: String,
      blockIds: Array[String],
      listener: BlockFetchingListener,
      tempFileManager: DownloadFileManager): Unit = {
    
    try {
      val client = clientFactory.createClient(host, port)
      new OneForOneBlockFetcher(client, conf.getAppId, execId,
        blockIds, listener, transportConf, tempFileManager).start()
    } catch {
      case e: Exception =>
        logError(s"Exception while beginning fetchBlocks", e)
        blockIds.foreach(listener.onBlockFetchFailure(_, e))
    }
  }
  
  // ä¸Šä¼ Blockåˆ°è¿œç¨‹
  override def uploadBlock(
      hostname: String,
      port: Int,
      execId: String,
      blockId: BlockId,
      blockData: ManagedBuffer,
      level: StorageLevel,
      classTag: ClassTag[_]): Future[Unit] = {
    
    val result = Promise[Unit]()
    val client = clientFactory.createClient(hostname, port)
    
    val callback = new RpcResponseCallback {
      override def onSuccess(response: ByteBuffer): Unit = {
        result.success(())
      }
      
      override def onFailure(e: Throwable): Unit = {
        result.failure(e)
      }
    }
    
    client.sendRpc(new UploadBlock(conf.getAppId, execId, blockId.toString,
      blockData.nioByteBuffer(), level, classTag).toByteBuffer, callback)
    
    result.future
  }
}
```
### ç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹æºç è¯¦è§£ 

#### GroupByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹



```scala
// GroupByKeyç®—å­æ ¸å¿ƒå®ç°
class GroupByKeyRDD[K: ClassTag, V: ClassTag](
    prev: RDD[(K, V)],
    part: Partitioner)
  extends RDD[(K, Iterable[V])](prev) {

  override def compute(split: Partition, context: TaskContext): Iterator[(K, Iterable[V])] = {
    // 1. åˆ›å»ºèšåˆå™¨
    val aggregator = new Aggregator[K, V, ArrayBuffer[V]](
      createCombiner = (v: V) => ArrayBuffer(v),
      mergeValue = (buf: ArrayBuffer[V], v: V) => buf += v,
      mergeCombiners = (buf1: ArrayBuffer[V], buf2: ArrayBuffer[V]) => buf1 ++= buf2
    )
    
    // 2. ä½¿ç”¨ExternalAppendOnlyMapè¿›è¡Œèšåˆ
    val externalMap = new ExternalAppendOnlyMap[K, V, ArrayBuffer[V]](aggregator)
    
    // 3. æ’å…¥æ‰€æœ‰é”®å€¼å¯¹
    val iter = firstParent[(K, V)].iterator(split, context)
    while (iter.hasNext) {
      val (k, v) = iter.next()
      externalMap.insert(k, v)
    }
    
    // 4. è¿”å›èšåˆç»“æœ
    externalMap.iterator
  }
}
```



```scala
// ExternalAppendOnlyMapæ ¸å¿ƒå®ç°
class ExternalAppendOnlyMap[K, V, C](
    aggregator: Aggregator[K, V, C],
    serializer: Serializer = SparkEnv.get.serializer)
  extends Spillable[WritablePartitionedPairCollection[K, C]](SparkEnv.get.blockManager.master)
  with Logging {

  // å†…å­˜ä¸­çš„Map
  private var map = new SizeTrackingAppendOnlyMap[K, C]
  
  // Spillæ–‡ä»¶åˆ—è¡¨
  private val spills = new ArrayBuffer[SpilledFile]
  
  // æ’å…¥é”®å€¼å¯¹
  def insert(key: K, value: V): Unit = {
    // 1. å°è¯•åœ¨å†…å­˜ä¸­èšåˆ
    val update = (hadValue: Boolean, oldValue: C) => {
      if (hadValue) {
        aggregator.mergeValue(oldValue, value)
      } else {
        aggregator.createCombiner(value)
      }
    }
    
    map.changeValue(key, update)
    
    // 2. æ£€æŸ¥æ˜¯å¦éœ€è¦Spill
    if (map.estimateSize() > myMemoryThreshold) {
      spill()
    }
  }
  
  // Spillåˆ°ç£ç›˜
  private def spill(): Unit = {
    val spillFile = spillMemoryIteratorToDisk(map.destructiveSortedWritablePartitionedIterator())
    spills += spillFile
    map = new SizeTrackingAppendOnlyMap[K, C]
  }
  
  // è·å–æœ€ç»ˆç»“æœ
  def iterator: Iterator[(K, C)] = {
    // åˆå¹¶å†…å­˜ä¸­çš„ç»“æœå’ŒSpillæ–‡ä»¶
    val memoryIterator = map.destructiveSortedWritablePartitionedIterator()
    val spillIterators = spills.map(_.iterator)
    
    // è¿”å›åˆå¹¶åçš„è¿­ä»£å™¨
    new MergedIterator(memoryIterator +: spillIterators)
  }
}
```

#### ReduceByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹


```scala
// PartitionedAppendOnlyMapçš„changeValueæ–¹æ³•
def changeValue(key: K, updateFunc: (Boolean, V) => V): Unit = {
  val hash = getHash(key)
  val pos = getPos(hash)
  
  var i = pos
  while (data(2 * i) != null) {
    if (data(2 * i) == key) {
      // æ‰¾åˆ°ç°æœ‰é”®ï¼Œæ›´æ–°å€¼
      val hadValue = true
      val oldValue = data(2 * i + 1).asInstanceOf[V]
      val newValue = updateFunc(hadValue, oldValue)
      data(2 * i + 1) = newValue.asInstanceOf[AnyRef]
      return
    }
    i = (i + 1) % (data.length / 2)
  }
  
  // æœªæ‰¾åˆ°é”®ï¼Œæ’å…¥æ–°å€¼
  val hadValue = false
  val newValue = updateFunc(hadValue, null.asInstanceOf[V])
  data(2 * i) = key.asInstanceOf[AnyRef]
  data(2 * i + 1) = newValue.asInstanceOf[AnyRef]
  curSize += 1
  
  if (curSize >= growThreshold) {
    growTable()
  }
}
```

#### Joinç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹


```scala
// CoGroupedRDDæ ¸å¿ƒå®ç°
class CoGroupedRDD[K: ClassTag](
    rdds: Seq[RDD[(K, _)]],
    part: Partitioner)
  extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil) {

  override def compute(split: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] = {
    // 1. åˆ›å»ºCoGroupèšåˆå™¨
    val aggregator = new CoGroupAggregator[K]
    
    // 2. ä½¿ç”¨ExternalAppendOnlyMapè¿›è¡Œåˆ†ç»„
    val externalMap = new ExternalAppendOnlyMap[K, (Int, Any), Array[ArrayBuffer[Any]]](aggregator)
    
    // 3. æ’å…¥æ‰€æœ‰RDDçš„æ•°æ®
    rdds.zipWithIndex.foreach { case (rdd, rddIndex) =>
      val iter = rdd.iterator(split, context)
      while (iter.hasNext) {
        val (k, v) = iter.next()
        externalMap.insert(k, (rddIndex, v))
      }
    }
    
    // 4. è¿”å›åˆ†ç»„ç»“æœ
    externalMap.iterator.map { case (k, groups) =>
      (k, groups.map(_.toIterable))
    }
  }
}
```

#### å†…å­˜å­˜å‚¨çŠ¶æ€ç›‘æ§



```scala
// å†…å­˜ä½¿ç”¨ç›‘æ§ç»„ä»¶
class MemoryMonitor {
  // ç›‘æ§Mapçš„å†…å­˜ä½¿ç”¨
  def monitorMapMemory(map: SizeTrackingAppendOnlyMap[_, _]): MemoryUsage = {
    val estimatedSize = map.estimateSize()
    val currentMemory = map.currentMemory
    val maxMemory = map.maxMemory
    
    MemoryUsage(
      estimatedSize = estimatedSize,
      currentMemory = currentMemory,
      maxMemory = maxMemory,
      utilization = currentMemory.toDouble / maxMemory
    )
  }
  
  // ç›‘æ§SpillçŠ¶æ€
  def monitorSpillStatus(externalMap: ExternalAppendOnlyMap[_, _, _]): SpillStatus = {
    val spillCount = externalMap.spills.size
    val totalSpillSize = externalMap.spills.map(_.size).sum
    
    SpillStatus(
      spillCount = spillCount,
      totalSpillSize = totalSpillSize,
      averageSpillSize = if (spillCount > 0) totalSpillSize / spillCount else 0
    )
  }
}

case class MemoryUsage(
  estimatedSize: Long,
  currentMemory: Long,
  maxMemory: Long,
  utilization: Double)

case class SpillStatus(
  spillCount: Int,
  totalSpillSize: Long,
  averageSpillSize: Long)
```



```mermaid
graph TD
    A[è¾“å…¥æ•°æ®] --> B[PartitionedAppendOnlyMap]
    B --> C{å†…å­˜æ˜¯å¦è¶³å¤Ÿ?}
    C -->|æ˜¯| D[å†…å­˜èšåˆ]
    C -->|å¦| E[Spillåˆ°ç£ç›˜]
    D --> F[è¿”å›ç»“æœ]
    E --> G[ExternalAppendOnlyMap]
    G --> H[åˆå¹¶å†…å­˜å’Œç£ç›˜æ•°æ®]
    H --> F
    
    I[MemoryMonitor] --> B
    I --> G
    J[SpillMonitor] --> E
```

#### å†…å­˜å­˜å‚¨ä¼˜åŒ–ç­–ç•¥



```scala
// å†…å­˜åˆ†é…ä¼˜åŒ–
class MemoryOptimizer {
  // åŠ¨æ€è°ƒæ•´å†…å­˜é˜ˆå€¼
  def adjustMemoryThreshold(
      currentMemory: Long,
      maxMemory: Long,
      spillCount: Int): Long = {
    
    val utilization = currentMemory.toDouble / maxMemory
    
    if (utilization > 0.8 && spillCount > 0) {
      // å†…å­˜ä½¿ç”¨ç‡é«˜ä¸”æœ‰Spillï¼Œé™ä½é˜ˆå€¼
      (maxMemory * 0.6).toLong
    } else if (utilization < 0.5 && spillCount == 0) {
      // å†…å­˜ä½¿ç”¨ç‡ä½ä¸”æ— Spillï¼Œæé«˜é˜ˆå€¼
      (maxMemory * 0.9).toLong
    } else {
      // ä¿æŒå½“å‰é˜ˆå€¼
      (maxMemory * 0.8).toLong
    }
  }
  
  // ä¼˜åŒ–Mapåˆå§‹å®¹é‡
  def optimizeInitialCapacity(dataSize: Long): Int = {
    val estimatedSize = (dataSize * 1.2).toInt
    math.max(64, math.min(estimatedSize, 1024 * 1024))
  }
}
```

---

### ä»»åŠ¡æäº¤æµç¨‹æºç è§£æ

#### DAGçš„ç”Ÿæˆä¸ä¾èµ–åˆ†æ

**ç”¨æˆ·è§¦å‘Actionæ—¶çš„å®Œæ•´æµç¨‹**ï¼š

```scala
// ç”¨æˆ·ä»£ç è§¦å‘Action
val result = rdd.collect()

// SparkContext.collect()
def collect(): Array[T] = withScope {
  val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  Array.concat(results: _*)
}

// SparkContext.runJob()
def runJob[T, U: ClassTag](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    partitions: Seq[Int],
    resultHandler: (Int, U) => Unit): Unit = {
  dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
}
```

**DAGSchedulerä¾èµ–åˆ†æ**ï¼š

```scala
// DAGScheduler.scala
private[scheduler] def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}
```

#### ä»»åŠ¡åˆ†å‘ä¸è°ƒåº¦æµç¨‹

**å®Œæ•´çš„ä»»åŠ¡è°ƒåº¦æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
  participant User
  participant SparkContext
  participant DAGScheduler
  participant TaskScheduler
  participant SchedulerBackend
  participant Executor

  User->>SparkContext: è§¦å‘Action (collect/count)
  SparkContext->>DAGScheduler: runJob
  DAGScheduler->>DAGScheduler: æ„å»ºDAG/åˆ’åˆ†Stage
  DAGScheduler->>TaskScheduler: submitTasks(TaskSet)
  TaskScheduler->>SchedulerBackend: reviveOffers
  SchedulerBackend->>Executor: launchTasks
  Executor->>SchedulerBackend: statusUpdate
  SchedulerBackend->>TaskScheduler: statusUpdate
  TaskScheduler->>DAGScheduler: taskEnded
  DAGScheduler->>SparkContext: jobEnded
  SparkContext->>User: è¿”å›ç»“æœ
```

**TaskSchedulerèµ„æºåˆ†é…**ï¼š

```scala
// TaskSchedulerImpl.resourceOffers()
def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = {
  // 1. éšæœºæ‰“ä¹±offersé¿å…çƒ­ç‚¹
  val shuffledOffers = Random.shuffle(offers)
  val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK))
  val availableCpus = shuffledOffers.map(o => o.cores).toArray
  
  // 2. æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…ä»»åŠ¡
  val sortedTaskSets = rootPool.getSortedTaskSetQueue
  for (taskSet <- sortedTaskSets) {
    // PROCESS_LOCAL -> NODE_LOCAL -> NO_PREF -> RACK_LOCAL -> ANY
    for (currentMaxLocality <- taskSet.myLocalityLevels) {
      do {
        launchedAnyTask = resourceOfferSingleTaskSet(
          taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)
      } while (launchedAnyTask)
    }
  }
  
  tasks
}
```

#### å¤±è´¥é‡è¯•ä¸å®¹é”™æœºåˆ¶

**DAGScheduleräº‹ä»¶å¤„ç†**ï¼š

```scala
// DAGSchedulerEventProcessLoopäº‹ä»¶å¤„ç†
private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
  case TaskFailed(taskId, taskType, reason, exception) =>
    reason match {
      case _: FetchFailed =>
        // Shuffleæ•°æ®è·å–å¤±è´¥ï¼Œéœ€è¦é‡æ–°è®¡ç®—çˆ¶Stage
        val shuffleMapStage = shuffleIdToMapStage(reason.shuffleId)
        markStageAsFinished(shuffleMapStage, Some(reason.toString))
        submitStage(shuffleMapStage)
        
      case _: ExecutorLostFailure =>
        // Executorä¸¢å¤±ï¼Œéœ€è¦é‡æ–°è°ƒåº¦Task
        removeExecutorAndUnregisterOutputs(reason.execId, filesLost = true)
        
      case _: TaskKilled =>
        // Taskè¢«æ€æ­»ï¼Œé€šå¸¸æ˜¯æ¨æµ‹æ‰§è¡Œ
        logInfo(s"Task $taskId was killed")
        
      case _ =>
        // å…¶ä»–å¼‚å¸¸ï¼ŒTaskçº§åˆ«é‡è¯•
        if (task.attempt < maxTaskFailures) {
          taskScheduler.submitTasks(createTaskSet(Array(task)))
        } else {
          abortStage(currentStage, s"Task $taskId failed $maxTaskFailures times")
        }
    }
    
  case StageCompleted(stage) =>
    // Stageå®Œæˆï¼Œæ£€æŸ¥å¹¶æäº¤ä¾èµ–çš„Stage
    markStageAsFinished(stage)
    submitWaitingChildStages(stage)
}
```

#### Executorå·¥ä½œæœºåˆ¶ä¸Taskæ‰§è¡Œ

**Executorä»»åŠ¡æ‰§è¡Œè¯¦ç»†æµç¨‹**ï¼š

```scala
// Executor.launchTask()
def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
  val tr = new TaskRunner(context, taskDescription)
  runningTasks.put(taskDescription.taskId, tr)
  threadPool.execute(tr)
}

// TaskRunner.run()
class TaskRunner(
    execBackend: ExecutorBackend,
    private val taskDescription: TaskDescription)
  extends Runnable {
  
  override def run(): Unit = {
    try {
      // 1. ååºåˆ—åŒ–Task
      val task = ser.deserialize[Task[Any]](
        taskDescription.serializedTask, 
        Thread.currentThread.getContextClassLoader)
      
      // 2. è®¾ç½®TaskContext
      val taskContext = new TaskContextImpl(
        stageId = taskDescription.stageId,
        taskAttemptId = taskDescription.taskId,
        attemptNumber = taskDescription.attemptNumber,
        partitionId = task.partitionId,
        localProperties = taskDescription.properties,
        taskMemoryManager = taskMemoryManager,
        metricsSystem = env.metricsSystem)
      
      // 3. æ‰§è¡ŒTask
      val value = task.run(
        taskAttemptId = taskDescription.taskId,
        attemptNumber = taskDescription.attemptNumber,
        metricsSystem = env.metricsSystem)
      
      // 4. åºåˆ—åŒ–ç»“æœå¹¶è¿”å›
      val serializedResult = ser.serialize(value)
      execBackend.statusUpdate(
        taskDescription.taskId, 
        TaskState.FINISHED, 
        serializedResult)
        
    } catch {
      case e: Exception =>
        // å¼‚å¸¸å¤„ç†
        val reason = new ExceptionFailure(e, taskContext.taskMetrics())
        execBackend.statusUpdate(
          taskDescription.taskId, 
          TaskState.FAILED, 
          ser.serialize(TaskFailedReason(reason)))
    } finally {
      // æ¸…ç†èµ„æº
      runningTasks.remove(taskDescription.taskId)
    }
  }
}
```

#### æ•°æ®è¯»å–ã€å¤„ç†ä¸RDDä¾èµ–

**RDDä¾èµ–é“¾è°ƒç”¨æµç¨‹**ï¼š

```scala
// RDD.iterator() é€’å½’è°ƒç”¨æµç¨‹
final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
  if (storageLevel != StorageLevel.NONE) {
    // 1. å°è¯•ä»ç¼“å­˜è¯»å–
    getOrCompute(split, context)
  } else {
    // 2. ç›´æ¥è®¡ç®—
    computeOrReadCheckpoint(split, context)
  }
}

def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = {
  if (isCheckpointed) {
    // ä»Checkpointè¯»å–
    firstParent[T].iterator(split, context)
  } else {
    // è°ƒç”¨å…·ä½“RDDçš„computeæ–¹æ³•
    compute(split, context)
  }
}

// ä»¥MapPartitionsRDDä¸ºä¾‹
override def compute(split: Partition, context: TaskContext): Iterator[U] = {
  // é€’å½’è°ƒç”¨çˆ¶RDDçš„iterator
  f(context, split.index, firstParent[T].iterator(split, context))
}
```

**å…¸å‹RDDä¾èµ–é“¾æ‰§è¡Œå›¾**ï¼š

```mermaid
graph TD
    A[Action: collect] --> B[ResultTask]
    B --> C[RDD.iterator]
    C --> D[MapPartitionsRDD.compute]
    D --> E[çˆ¶RDD.iterator]
    E --> F[FilteredRDD.compute]
    F --> G[çˆ¶RDD.iterator]
    G --> H[HadoopRDD.compute]
    H --> I[è¯»å–HDFSæ•°æ®]
    I --> J[è¿”å›Iterator]
    J --> K[é€çº§å¤„ç†å¹¶è¿”å›]
    K --> L[æœ€ç»ˆç»“æœ]
    
    style A fill:#e1f5fe
    style H fill:#e8f5e8
    style L fill:#fff3e0
```

#### Taskç±»å‹ä¸æ‰§è¡Œå·®å¼‚

**ResultTask vs ShuffleMapTask**ï¼š

```scala
// ResultTask - äº§ç”Ÿæœ€ç»ˆç»“æœ
class ResultTask[T, U](
    stageId: Int,
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    _partitionId: Int,
    locs: Seq[TaskLocation])
  extends Task[U](stageId, _partitionId) {
  
  override def runTask(context: TaskContext): U = {
    // ç›´æ¥è°ƒç”¨ç”¨æˆ·å‡½æ•°å¤„ç†æ•°æ®
    func(context, rdd.iterator(partition, context))
  }
}

// ShuffleMapTask - äº§ç”Ÿä¸­é—´Shuffleæ•°æ®
class ShuffleMapTask(
    stageId: Int,
    rdd: RDD[_],
    dep: ShuffleDependency[_, _, _],
    _partitionId: Int,
    locs: Seq[TaskLocation])
  extends Task[MapStatus](stageId, _partitionId) {
  
  override def runTask(context: TaskContext): MapStatus = {
    // è·å–ShuffleWriter
    val manager = SparkEnv.get.shuffleManager
    val writer = manager.getWriter[Any, Any](
      dep.shuffleHandle, partitionId, context)
    
    try {
      // å†™å…¥Shuffleæ•°æ®
      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
      writer.stop(success = true).get
    } catch {
      case e: Exception =>
        writer.stop(success = false)
        throw e
    }
  }
}
```

---

## å®æˆ˜åº”ç”¨æ¡ˆä¾‹

### æ‰¹å¤„ç†åº”ç”¨

#### ETLæ•°æ®å¤„ç†

**å¤§è§„æ¨¡æ•°æ®æ¸…æ´—æ¡ˆä¾‹**ï¼š
```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// 1. æ•°æ®è¯»å–ä¸Schemaå®šä¹‰
val schema = StructType(Array(
  StructField("user_id", LongType, true),
  StructField("timestamp", TimestampType, true),
  StructField("event_type", StringType, true),
  StructField("page_url", StringType, true),
  StructField("ip_address", StringType, true)
))

val rawData = spark.read
  .option("header", "true")
  .schema(schema)
  .csv("hdfs://data/raw/user_events/*")

// 2. æ•°æ®æ¸…æ´—ä¸è½¬æ¢
val cleanedData = rawData
  .filter($"user_id".isNotNull && $"timestamp".isNotNull)
  .filter($"ip_address".rlike("^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$"))
  .withColumn("date", to_date($"timestamp"))
  .withColumn("hour", hour($"timestamp"))
  .withColumn("domain", regexp_extract($"page_url", "https?://([^/]+)", 1))

// 3. æ•°æ®èšåˆåˆ†æ
val userBehavior = cleanedData
  .groupBy("user_id", "date", "event_type")
  .agg(
    count("*").as("event_count"),
    countDistinct("page_url").as("unique_pages"),
    min("timestamp").as("first_event_time"),
    max("timestamp").as("last_event_time")
  )

// 4. ç»“æœä¿å­˜
userBehavior
  .coalesce(100)
  .write
  .mode("overwrite")
  .partitionBy("date")
  .parquet("hdfs://data/processed/user_behavior")
```

#### æ•°æ®åˆ†ææ¡ˆä¾‹

**ç”¨æˆ·è¡Œä¸ºåˆ†æ**ï¼š
```scala
// æ¼æ–—åˆ†æ
val funnelAnalysis = spark.sql("""
  WITH user_events AS (
    SELECT user_id, event_type, timestamp,
           ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as event_order
    FROM cleaned_data
    WHERE date >= '2023-01-01'
  ),
  funnel_steps AS (
    SELECT 
      user_id,
      SUM(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as step1_count,
      SUM(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as step2_count,
      SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as step3_count
    FROM user_events
    GROUP BY user_id
  )
  SELECT 
    COUNT(DISTINCT user_id) as total_users,
    COUNT(DISTINCT CASE WHEN step1_count > 0 THEN user_id END) as step1_users,
    COUNT(DISTINCT CASE WHEN step2_count > 0 THEN user_id END) as step2_users,
    COUNT(DISTINCT CASE WHEN step3_count > 0 THEN user_id END) as step3_users
  FROM funnel_steps
""")

// ç”¨æˆ·ç•™å­˜åˆ†æ
val retentionAnalysis = spark.sql("""
  WITH user_first_visit AS (
    SELECT user_id, MIN(date) as first_visit_date
    FROM cleaned_data
    GROUP BY user_id
  ),
  user_visits AS (
    SELECT u.user_id, u.first_visit_date, c.date as visit_date,
           DATEDIFF(c.date, u.first_visit_date) as days_since_first_visit
    FROM user_first_visit u
    JOIN cleaned_data c ON u.user_id = c.user_id
  )
  SELECT 
    first_visit_date,
    COUNT(DISTINCT user_id) as new_users,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 1 THEN user_id END) as day1_retention,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 7 THEN user_id END) as day7_retention,
    COUNT(DISTINCT CASE WHEN days_since_first_visit = 30 THEN user_id END) as day30_retention
  FROM user_visits
  GROUP BY first_visit_date
  ORDER BY first_visit_date
""")
```

### æµå¤„ç†åº”ç”¨

#### å®æ—¶æ•°æ®å¤„ç†

**å®æ—¶æ¨èç³»ç»Ÿ**ï¼š
```scala
// å®æ—¶ç”¨æˆ·è¡Œä¸ºæµå¤„ç†
val kafkaStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "user_behavior")
  .option("startingOffsets", "latest")
  .load()

// è§£æJSONæ•°æ®
val userEvents = kafkaStream
  .select(from_json($"value".cast("string"), schema).as("data"))
  .select("data.*")
  .withWatermark("timestamp", "10 minutes")

// å®æ—¶ç‰¹å¾è®¡ç®—
val userFeatures = userEvents
  .groupBy(
    $"user_id",
    window($"timestamp", "10 minutes", "1 minute")
  )
  .agg(
    count("*").as("event_count"),
    countDistinct("page_url").as("unique_pages"),
    collect_list("event_type").as("event_sequence")
  )

// å®æ—¶æ¨èç”Ÿæˆ
val recommendations = userFeatures.map { row =>
  val userId = row.getLong("user_id")
  val features = extractFeatures(row)
  val recommendations = recommendationModel.predict(features)
  RecommendationResult(userId, recommendations, System.currentTimeMillis())
}

// è¾“å‡ºåˆ°Kafka
val query = recommendations.writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("topic", "recommendations")
  .option("checkpointLocation", "/path/to/checkpoint")
  .outputMode("append")
  .start()
```

#### æœºå™¨å­¦ä¹ æµæ°´çº¿

**å®æ—¶æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹**ï¼š
```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification.RandomForestClassifier

// æ„å»ºML Pipeline
val assembler = new VectorAssembler()
  .setInputCols(Array("feature1", "feature2", "feature3"))
  .setOutputCol("features")

val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")

val rf = new RandomForestClassifier()
  .setFeaturesCol("scaledFeatures")
  .setLabelCol("label")
  .setNumTrees(100)

val pipeline = new Pipeline()
  .setStages(Array(assembler, scaler, rf))

// æµå¼æ¨¡å‹è®­ç»ƒ
val trainingStream = spark.readStream
  .format("delta")
  .option("path", "/path/to/training/data")
  .load()

val model = pipeline.fit(trainingStream)

// æµå¼é¢„æµ‹
val predictionStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "prediction_requests")
  .load()

val predictions = model.transform(predictionStream)

predictions.writeStream
  .format("console")
  .outputMode("append")
  .start()
```

### æœ€ä½³å®è·µ

#### å¼€å‘è§„èŒƒ

**ä»£ç ç»„ç»‡ç»“æ„**ï¼š
```scala
// é…ç½®ç®¡ç†
object SparkConfig {
  def getSparkSession(appName: String): SparkSession = {
    SparkSession.builder()
      .appName(appName)
      .config("spark.sql.adaptive.enabled", "true")
      .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .getOrCreate()
  }
}

// æ•°æ®å¤„ç†å·¥å…·ç±»
object DataProcessor {
  def cleanUserData(spark: SparkSession, inputPath: String): DataFrame = {
    import spark.implicits._
    
    spark.read.parquet(inputPath)
      .filter($"user_id".isNotNull)
      .filter($"timestamp".isNotNull)
      .dropDuplicates("user_id", "timestamp")
  }
  
  def validateData(df: DataFrame): DataFrame = {
    df.filter("user_id > 0")
      .filter("timestamp IS NOT NULL")
  }
}

// ä¸»åº”ç”¨ç¨‹åº
object UserAnalysisApp {
  def main(args: Array[String]): Unit = {
    val spark = SparkConfig.getSparkSession("UserAnalysis")
    
    try {
      val inputPath = args(0)
      val outputPath = args(1)
      
      val cleanData = DataProcessor.cleanUserData(spark, inputPath)
      val validData = DataProcessor.validateData(cleanData)
      
      validData.write
        .mode("overwrite")
        .parquet(outputPath)
        
    } finally {
      spark.stop()
    }
  }
}
```

#### éƒ¨ç½²ç­–ç•¥

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š
```bash
#!/bin/bash
# spark-submitè„šæœ¬

spark-submit \
  --class com.company.UserAnalysisApp \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 50 \
  --executor-memory 8g \
  --executor-cores 4 \
  --driver-memory 4g \
  --driver-cores 2 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=10 \
  --conf spark.dynamicAllocation.maxExecutors=100 \
  --conf spark.sql.execution.arrow.pyspark.enabled=true \
  --conf spark.sql.parquet.columnarReaderBatchSize=10000 \
  user-analysis-app.jar \
  /input/path \
  /output/path
```

#### è¿ç»´ç®¡ç†

**ç›‘æ§è„šæœ¬**ï¼š
```bash
#!/bin/bash
# Sparkåº”ç”¨ç›‘æ§è„šæœ¬

# æ£€æŸ¥åº”ç”¨çŠ¶æ€
check_app_status() {
    local app_id=$1
    local status=$(yarn application -status $app_id | grep "Final-State" | awk '{print $3}')
    echo "Application $app_id status: $status"
    return $status
}

# ç›‘æ§èµ„æºä½¿ç”¨
monitor_resources() {
    local app_id=$1
    yarn top -r $app_id
}

# æ”¶é›†æ—¥å¿—
collect_logs() {
    local app_id=$1
    yarn logs -applicationId $app_id > /logs/spark_${app_id}.log
}

# ä¸»ç›‘æ§å¾ªç¯
main() {
    while true; do
        for app_id in $(yarn application -list -appStates RUNNING | grep SPARK | awk '{print $1}'); do
            check_app_status $app_id
            monitor_resources $app_id
        done
        sleep 30
    done
}

main
```

---

## Sparkå¸¸è§ä»»åŠ¡æŠ¥é”™åŠè§£å†³åŠæ³• 

### å†…å­˜ç›¸å…³é”™è¯¯

#### 1. OutOfMemoryError: Java heap space

**é”™è¯¯ç°è±¡**ï¼š
```
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)
    at java.lang.StringBuilder.append(StringBuilder.java:136)
```

**åŸå› åˆ†æ**ï¼š
- **å †å†…å­˜ä¸è¶³**ï¼šExecutoræˆ–Driverçš„å †å†…å­˜è®¾ç½®è¿‡å°
- **æ•°æ®å€¾æ–œ**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿‡å¤§ï¼Œå¯¼è‡´å•ä¸ªTaskå†…å­˜æº¢å‡º
- **ç¼“å­˜è¿‡å¤š**ï¼šRDDç¼“å­˜å ç”¨è¿‡å¤šå†…å­˜
- **å¯¹è±¡åˆ›å»ºè¿‡å¤š**ï¼šé¢‘ç¹åˆ›å»ºå¤§å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´å†…å­˜é…ç½®**ï¼š
```bash
# å¢åŠ Executorå†…å­˜
spark-submit \
  --conf spark.executor.memory=8g \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memory=4g \
  --conf spark.driver.memoryOverhead=1g \
  your-app.jar

# è°ƒæ•´å†…å­˜æ¯”ä¾‹
spark-submit \
  --conf spark.memory.fraction=0.8 \
  --conf spark.memory.storageFraction=0.3 \
  your-app.jar
```

**2. å¤„ç†æ•°æ®å€¾æ–œ**ï¼š
```scala
// æ–¹æ³•1ï¼šå¢åŠ åˆ†åŒºæ•°
val skewedRDD = originalRDD.repartition(200)

// æ–¹æ³•2ï¼šè‡ªå®šä¹‰åˆ†åŒºç­–ç•¥
val customPartitioner = new Partitioner {
  override def numPartitions: Int = 200
  override def getPartition(key: Any): Int = {
    // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ï¼Œé¿å…æ•°æ®å€¾æ–œ
    val hash = key.hashCode()
    Math.abs(hash) % numPartitions
  }
}
val skewedRDD = originalRDD.partitionBy(customPartitioner)

// æ–¹æ³•3ï¼šä¸¤é˜¶æ®µèšåˆ
val stage1RDD = originalRDD
  .map(x => (x._1 + "_" + Random.nextInt(10), x._2))  // æ·»åŠ éšæœºå‰ç¼€
  .reduceByKey(_ + _)
  .map(x => (x._1.split("_")(0), x._2))  // å»æ‰éšæœºå‰ç¼€
  .reduceByKey(_ + _)
```

**3. ä¼˜åŒ–ç¼“å­˜ç­–ç•¥**ï¼š
```scala
// ä½¿ç”¨MEMORY_AND_DISK_SERå‡å°‘å†…å­˜å ç”¨
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

// åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpointå‡å°‘å†…å­˜å‹åŠ›
rdd.checkpoint()
```

**4. ä»£ç ä¼˜åŒ–**ï¼š
```scala
// ä½¿ç”¨mapPartitionså‡å°‘å¯¹è±¡åˆ›å»º
val optimizedRDD = rdd.mapPartitions(iter => {
  val result = new ArrayBuffer[String]()
  while (iter.hasNext) {
    val item = iter.next()
    // å¤„ç†é€»è¾‘
    result += processedItem
  }
  result.iterator
})

// å¤ç”¨å¯¹è±¡
case class User(id: Long, name: String)
val userRDD = rdd.mapPartitions(iter => {
  val user = User(0L, "")  // å¤ç”¨å¯¹è±¡
  iter.map { case (id, name) =>
    user.id = id
    user.name = name
    user.copy()  // è¿”å›å‰¯æœ¬
  }
})
```

#### 2. OutOfMemoryError: Direct buffer memory

**é”™è¯¯ç°è±¡**ï¼š
```
java.lang.OutOfMemoryError: Direct buffer memory
    at java.nio.Bits.reserveMemory(Bits.java:694)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
```

**åŸå› åˆ†æ**ï¼š
- **å †å¤–å†…å­˜ä¸è¶³**ï¼šDirectBufferå†…å­˜è®¾ç½®è¿‡å°
- **ç½‘ç»œä¼ è¾“è¿‡å¤š**ï¼šå¤§é‡æ•°æ®é€šè¿‡ç½‘ç»œä¼ è¾“
- **åºåˆ—åŒ–é—®é¢˜**ï¼šåºåˆ—åŒ–è¿‡ç¨‹ä¸­å ç”¨è¿‡å¤šå †å¤–å†…å­˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ å †å¤–å†…å­˜**ï¼š
```bash
spark-submit \
  --conf spark.executor.memoryOverhead=4g \
  --conf spark.driver.memoryOverhead=2g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  your-app.jar
```

**2. ä¼˜åŒ–ç½‘ç»œä¼ è¾“**ï¼š
```scala
// å¯ç”¨å‹ç¼©
spark.conf.set("spark.io.compression.codec", "snappy")
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")

// è°ƒæ•´ç½‘ç»œç¼“å†²åŒº
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.executor.heartbeatInterval", "60s")
```

**3. ä¼˜åŒ–åºåˆ—åŒ–**ï¼š
```scala
// ä½¿ç”¨Kryoåºåˆ—åŒ–
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")

// æ³¨å†Œè‡ªå®šä¹‰ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(classOf[MyClass]))
```

#### 3. OutOfMemoryError: Metaspace

**é”™è¯¯ç°è±¡**ï¼š
```
java.lang.OutOfMemoryError: Metaspace
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š
- **ç±»åŠ è½½è¿‡å¤š**ï¼šåŠ¨æ€ç”Ÿæˆå¤§é‡ç±»
- **Metaspaceè®¾ç½®è¿‡å°**ï¼šJVM Metaspaceç©ºé—´ä¸è¶³
- **UDFä½¿ç”¨è¿‡å¤š**ï¼šå¤§é‡UDFå¯¼è‡´ç±»åŠ è½½

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´JVMå‚æ•°**ï¼š
```bash
spark-submit \
  --conf spark.executor.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  --conf spark.driver.extraJavaOptions="-XX:MaxMetaspaceSize=512m" \
  your-app.jar
```

**2. ä¼˜åŒ–UDFä½¿ç”¨**ï¼š
```scala
// é¿å…åœ¨UDFä¸­åˆ›å»ºè¿‡å¤šç±»
val optimizedUDF = udf((value: String) => {
  // ä½¿ç”¨ç®€å•é€»è¾‘ï¼Œé¿å…åŠ¨æ€ç±»ç”Ÿæˆ
  value.toUpperCase
})

// å¤ç”¨UDFå®ä¾‹
val myUDF = udf((x: Int) => x * 2)
df.select(myUDF(col("value")))
```

### ç½‘ç»œç›¸å…³é”™è¯¯

#### 1. FetchFailedException

**é”™è¯¯ç°è±¡**ï¼š
```
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hostname:7337
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
    at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:646)
```

**åŸå› åˆ†æ**ï¼š
- **ç½‘ç»œè¶…æ—¶**ï¼šç½‘ç»œè¿æ¥è¶…æ—¶
- **Executorä¸¢å¤±**ï¼šExecutorè¿›ç¨‹å¼‚å¸¸é€€å‡º
- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³å¯¼è‡´è¿›ç¨‹é€€å‡º
- **ç½‘ç»œä¸ç¨³å®š**ï¼šç½‘ç»œè¿æ¥ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´ç½‘ç»œè¶…æ—¶å‚æ•°**ï¼š
```bash
spark-submit \
  --conf spark.network.timeout=800s \
  --conf spark.executor.heartbeatInterval=60s \
  --conf spark.rpc.askTimeout=800s \
  --conf spark.rpc.lookupTimeout=800s \
  your-app.jar
```

**2. å¢åŠ é‡è¯•æœºåˆ¶**ï¼š
```bash
spark-submit \
  --conf spark.task.maxFailures=8 \
  --conf spark.stage.maxAttempts=4 \
  your-app.jar
```

**3. ä¼˜åŒ–Shuffleé…ç½®**ï¼š
```bash
spark-submit \
  --conf spark.shuffle.io.maxRetries=3 \
  --conf spark.shuffle.io.retryWait=60s \
  --conf spark.shuffle.file.buffer=32k \
  your-app.jar
```

**4. ç›‘æ§ExecutorçŠ¶æ€**ï¼š
```scala
// æ·»åŠ ç›‘æ§ä»£ç 
spark.sparkContext.addSparkListener(new SparkListener {
  override def onExecutorLost(executorLost: SparkListenerExecutorLost): Unit = {
    println(s"Executor lost: ${executorLost.executorId}")
    // è®°å½•æ—¥å¿—æˆ–å‘é€å‘Šè­¦
  }
})
```

#### 2. ConnectionTimeoutException

**é”™è¯¯ç°è±¡**ï¼š
```
java.net.ConnectTimeoutException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
```

**åŸå› åˆ†æ**ï¼š
- **ç½‘ç»œå»¶è¿Ÿ**ï¼šç½‘ç»œå»¶è¿Ÿè¿‡é«˜
- **é˜²ç«å¢™é™åˆ¶**ï¼šé˜²ç«å¢™é˜»æ­¢è¿æ¥
- **ç«¯å£å†²çª**ï¼šç«¯å£è¢«å ç”¨
- **DNSè§£æé—®é¢˜**ï¼šDNSè§£æå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. è°ƒæ•´è¿æ¥è¶…æ—¶**ï¼š
```bash
spark-submit \
  --conf spark.network.timeout=1200s \
  --conf spark.rpc.askTimeout=1200s \
  your-app.jar
```

**2. æ£€æŸ¥ç½‘ç»œé…ç½®**ï¼š
```bash
# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping hostname
telnet hostname port

# æ£€æŸ¥é˜²ç«å¢™
iptables -L
```

**3. ä½¿ç”¨æœ¬åœ°åŒ–ç­–ç•¥**ï¼š
```scala
// å¯ç”¨æ•°æ®æœ¬åœ°åŒ–
spark.conf.set("spark.locality.wait", "30s")
spark.conf.set("spark.locality.wait.process", "30s")
spark.conf.set("spark.locality.wait.node", "30s")
spark.conf.set("spark.locality.wait.rack", "30s")
```

### åºåˆ—åŒ–ç›¸å…³é”™è¯¯

#### 1. NotSerializableException

**é”™è¯¯ç°è±¡**ï¼š
```
java.io.NotSerializableException: com.example.MyClass
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
```

**åŸå› åˆ†æ**ï¼š
- **ç±»æœªå®ç°Serializable**ï¼šè‡ªå®šä¹‰ç±»æœªå®ç°Serializableæ¥å£
- **é—­åŒ…é—®é¢˜**ï¼šåœ¨é—­åŒ…ä¸­å¼•ç”¨äº†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
- **é™æ€å˜é‡**ï¼šå¼•ç”¨äº†é™æ€å˜é‡æˆ–å•ä¾‹å¯¹è±¡

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å®ç°Serializableæ¥å£**ï¼š
```scala
// æ–¹æ³•1ï¼šå®ç°Serializable
case class MyClass(id: Int, name: String) extends Serializable

// æ–¹æ³•2ï¼šä½¿ç”¨@transientæ³¨è§£
class MyClass(val id: Int, val name: String) extends Serializable {
  @transient
  private val nonSerializableField = new NonSerializableClass()
}
```

**2. é¿å…é—­åŒ…é—®é¢˜**ï¼š
```scala
// é”™è¯¯ç¤ºä¾‹
val nonSerializableObject = new NonSerializableClass()
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => nonSerializableObject.process(x))  // ä¼šæŠ¥é”™

// æ­£ç¡®ç¤ºä¾‹
val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.mapPartitions(iter => {
  val localObject = new NonSerializableClass()  // åœ¨åˆ†åŒºå†…åˆ›å»º
  iter.map(x => localObject.process(x))
})
```

**3. ä½¿ç”¨å¹¿æ’­å˜é‡**ï¼š
```scala
// å¯¹äºåªè¯»çš„å¤§å¯¹è±¡ï¼Œä½¿ç”¨å¹¿æ’­å˜é‡
val largeData = spark.sparkContext.parallelize(1 to 1000000).collect()
val broadcastData = spark.sparkContext.broadcast(largeData)

val rdd = spark.sparkContext.parallelize(1 to 10)
rdd.map(x => {
  val data = broadcastData.value  // ä½¿ç”¨å¹¿æ’­å˜é‡
  processWithData(x, data)
})
```

#### 2. KryoSerializationException

**é”™è¯¯ç°è±¡**ï¼š
```
com.esotericsoftware.kryo.KryoException: java.lang.ClassNotFoundException: com.example.MyClass
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)
```

**åŸå› åˆ†æ**ï¼š
- **ç±»æœªæ³¨å†Œ**ï¼šä½¿ç”¨Kryoåºåˆ—åŒ–æ—¶ç±»æœªæ³¨å†Œ
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **ç‰ˆæœ¬ä¸å…¼å®¹**ï¼šåºåˆ—åŒ–å’Œååºåˆ—åŒ–ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ³¨å†Œè‡ªå®šä¹‰ç±»**ï¼š
```scala
val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[MyOtherClass]
))

val spark = SparkSession.builder()
  .config(conf)
  .getOrCreate()
```

**2. ä½¿ç”¨Kryoæ³¨å†Œå™¨**ï¼š
```scala
class MyKryoRegistrator extends KryoRegistrator {
  override def registerClasses(kryo: Kryo): Unit = {
    kryo.register(classOf[MyClass])
    kryo.register(classOf[MyOtherClass])
  }
}

val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
conf.set("spark.kryo.registrator", "com.example.MyKryoRegistrator")
```

**3. ç¦ç”¨ä¸¥æ ¼æ¨¡å¼**ï¼š
```scala
spark.conf.set("spark.kryo.registrationRequired", "false")
```

### èµ„æºç›¸å…³é”™è¯¯

#### 1. ExecutorLostFailure

**é”™è¯¯ç°è±¡**ï¼š
```
org.apache.spark.scheduler.ExecutorLostFailure: Executor 1 lost
    at org.apache.spark.scheduler.TaskSetManager$$anonfun$abortIfCompletelyBlacklisted$1.apply(TaskSetManager.scala:1023)
```

**åŸå› åˆ†æ**ï¼š
- **å†…å­˜ä¸è¶³**ï¼šExecutorå†…å­˜ä¸è¶³è¢«æ€æ­»
- **CPUè¿‡è½½**ï¼šCPUä½¿ç”¨ç‡è¿‡é«˜å¯¼è‡´è¿›ç¨‹å¼‚å¸¸
- **ç£ç›˜ç©ºé—´ä¸è¶³**ï¼šç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´å†™å…¥å¤±è´¥
- **ç½‘ç»œé—®é¢˜**ï¼šç½‘ç»œè¿æ¥é—®é¢˜å¯¼è‡´å¿ƒè·³è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. å¢åŠ èµ„æºé…é¢**ï¼š
```bash
spark-submit \
  --executor-memory 8g \
  --executor-cores 4 \
  --num-executors 10 \
  --conf spark.executor.memoryOverhead=2g \
  your-app.jar
```

**2. ç›‘æ§èµ„æºä½¿ç”¨**ï¼š
```scala
// æ·»åŠ èµ„æºç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    println(s"Task ${taskEnd.taskInfo.taskId} completed:")
    println(s"  Duration: ${taskEnd.taskInfo.duration}ms")
    println(s"  Memory: ${metrics.peakExecutionMemory} bytes")
    println(s"  Disk: ${metrics.diskBytesSpilled} bytes spilled")
  }
})
```

**3. ä¼˜åŒ–èµ„æºåˆ†é…**ï¼š
```bash
spark-submit \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=2 \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.dynamicAllocation.initialExecutors=5 \
  your-app.jar
```

#### 2. NoClassDefFoundError

**é”™è¯¯ç°è±¡**ï¼š
```
java.lang.NoClassDefFoundError: com.example.MyClass
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
```

**åŸå› åˆ†æ**ï¼š
- **ä¾èµ–ç¼ºå¤±**ï¼šç¼ºå°‘å¿…è¦çš„jaråŒ…
- **ç‰ˆæœ¬å†²çª**ï¼šä¾èµ–ç‰ˆæœ¬å†²çª
- **ç±»è·¯å¾„é—®é¢˜**ï¼šç±»ä¸åœ¨classpathä¸­
- **æ‰“åŒ…é—®é¢˜**ï¼šjaråŒ…æ‰“åŒ…ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ·»åŠ ä¾èµ–jaråŒ…**ï¼š
```bash
spark-submit \
  --jars /path/to/dependency1.jar,/path/to/dependency2.jar \
  --conf spark.executor.extraClassPath=/path/to/dependencies/* \
  your-app.jar
```

**2. ä½¿ç”¨fat jar**ï¼š
```xml
<!-- Mavené…ç½® -->
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.2.4</version>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
    </execution>
  </executions>
</plugin>
```

**3. æ£€æŸ¥ä¾èµ–å†²çª**ï¼š
```bash
# æŸ¥çœ‹ä¾èµ–æ ‘
mvn dependency:tree

# æ’é™¤å†²çªä¾èµ–
<dependency>
  <groupId>com.example</groupId>
  <artifactId>library</artifactId>
  <version>1.0.0</version>
  <exclusions>
    <exclusion>
      <groupId>conflicting.group</groupId>
      <artifactId>conflicting-artifact</artifactId>
    </exclusion>
  </exclusions>
</dependency>
```

### æ•°æ®ç›¸å…³é”™è¯¯

#### 1. FileNotFoundException

**é”™è¯¯ç°è±¡**ï¼š
```
java.io.FileNotFoundException: File does not exist: hdfs://namenode:8020/path/to/file
    at org.apache.hadoop.hdfs.DFSClient.checkPath(DFSClient.java:1274)
    at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1527)
```

**åŸå› åˆ†æ**ï¼š
- **æ–‡ä»¶ä¸å­˜åœ¨**ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶è¢«åˆ é™¤
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è¯»å–æ–‡ä»¶çš„æƒé™
- **è·¯å¾„é”™è¯¯**ï¼šæ–‡ä»¶è·¯å¾„æ ¼å¼é”™è¯¯
- **HDFSé—®é¢˜**ï¼šHDFSæœåŠ¡å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„**ï¼š
```scala
// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
val hadoopConf = spark.sparkContext.hadoopConfiguration
val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
val path = new org.apache.hadoop.fs.Path("/path/to/file")

if (fs.exists(path)) {
  println("File exists")
} else {
  println("File does not exist")
}
```

**2. è®¾ç½®æ–‡ä»¶ç³»ç»Ÿé…ç½®**ï¼š
```scala
// è®¾ç½®HDFSé…ç½®
spark.conf.set("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020")
spark.conf.set("spark.hadoop.dfs.namenode.rpc-address", "namenode:8020")
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š
```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
hdfs dfs -ls /path/to/file

# ä¿®æ”¹æ–‡ä»¶æƒé™
hdfs dfs -chmod 644 /path/to/file

# ä¿®æ”¹æ–‡ä»¶æ‰€æœ‰è€…
hdfs dfs -chown username:group /path/to/file
```

#### 2. DataSourceException

**é”™è¯¯ç°è±¡**ï¼š
```
org.apache.spark.sql.AnalysisException: Table or view not found: table_name
    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
```

**åŸå› åˆ†æ**ï¼š
- **è¡¨ä¸å­˜åœ¨**ï¼šæ•°æ®åº“è¡¨ä¸å­˜åœ¨
- **æƒé™é—®é¢˜**ï¼šæ²¡æœ‰è®¿é—®è¡¨çš„æƒé™
- **æ•°æ®åº“è¿æ¥é—®é¢˜**ï¼šæ•°æ®åº“è¿æ¥å¤±è´¥
- **è¡¨åé”™è¯¯**ï¼šè¡¨åæ‹¼å†™é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨**ï¼š
```scala
// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
val tables = spark.catalog.listTables()
tables.filter(_.name == "table_name").show()

// æˆ–è€…ä½¿ç”¨SQL
spark.sql("SHOW TABLES").show()
```

**2. è®¾ç½®æ•°æ®åº“è¿æ¥**ï¼š
```scala
// è®¾ç½®æ•°æ®åº“è¿æ¥
spark.conf.set("spark.sql.warehouse.dir", "/user/hive/warehouse")
spark.conf.set("hive.metastore.uris", "thrift://metastore:9083")

// ä½¿ç”¨Hive
spark.sql("USE database_name")
spark.sql("SHOW TABLES").show()
```

**3. å¤„ç†æƒé™é—®é¢˜**ï¼š
```sql
-- æˆäºˆæƒé™
GRANT SELECT ON TABLE table_name TO USER username;

-- æ£€æŸ¥æƒé™
SHOW GRANT USER username ON TABLE table_name;
```

### è°ƒè¯•å’Œè¯Šæ–­å·¥å…·

#### 1. Spark Web UI

**è®¿é—®æ–¹å¼**ï¼š
```
http://driver-host:4040  # åº”ç”¨è¿è¡Œæ—¶
http://driver-host:18080 # å†å²æœåŠ¡å™¨
```

**å…³é”®æŒ‡æ ‡**ï¼š
- **Stagesé¡µé¢**ï¼šæŸ¥çœ‹Stageæ‰§è¡Œæƒ…å†µå’Œå¤±è´¥åŸå› 
- **Executorsé¡µé¢**ï¼šæŸ¥çœ‹Executorèµ„æºä½¿ç”¨æƒ…å†µ
- **Storageé¡µé¢**ï¼šæŸ¥çœ‹RDDç¼“å­˜æƒ…å†µ
- **Environmenté¡µé¢**ï¼šæŸ¥çœ‹é…ç½®å‚æ•°

#### 2. æ—¥å¿—åˆ†æ

**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
# æŸ¥çœ‹Driveræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-driver-*.log

# æŸ¥çœ‹Executoræ—¥å¿—
tail -f /path/to/spark/logs/spark-*-executor-*.log

# æŸ¥çœ‹YARNæ—¥å¿—
yarn logs -applicationId application_1234567890_0001
```

**å…³é”®æ—¥å¿—æ¨¡å¼**ï¼š
```bash
# æŸ¥æ‰¾é”™è¯¯ä¿¡æ¯
grep -i "error\|exception\|failed" /path/to/logs/*.log

# æŸ¥æ‰¾å†…å­˜ç›¸å…³é”™è¯¯
grep -i "outofmemory\|oom" /path/to/logs/*.log

# æŸ¥æ‰¾ç½‘ç»œç›¸å…³é”™è¯¯
grep -i "timeout\|connection" /path/to/logs/*.log
```

#### 3. æ€§èƒ½åˆ†æå·¥å…·

**JVMåˆ†æ**ï¼š
```bash
# æŸ¥çœ‹JVMå †å†…å­˜ä½¿ç”¨
jstat -gc <pid> 1000

# æŸ¥çœ‹çº¿ç¨‹çŠ¶æ€
jstack <pid>

# æŸ¥çœ‹å†…å­˜dump
jmap -dump:format=b,file=heap.hprof <pid>
```

**ç³»ç»Ÿèµ„æºç›‘æ§**ï¼š
```bash
# æŸ¥çœ‹ç³»ç»Ÿèµ„æºä½¿ç”¨
top -p <pid>
iostat -x 1
netstat -i
```

#### 4. è°ƒè¯•ä»£ç 

**æ·»åŠ è°ƒè¯•ä¿¡æ¯**ï¼š
```scala
// æ·»åŠ æ—¥å¿—
import org.apache.log4j.{Level, Logger}
Logger.getLogger("org.apache.spark").setLevel(Level.DEBUG)

// æ·»åŠ ç›‘æ§
spark.sparkContext.addSparkListener(new SparkListener {
  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {
    println(s"Task started: ${taskStart.taskInfo.taskId}")
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    println(s"Task ended: ${taskEnd.taskInfo.taskId}, " +
            s"status: ${taskEnd.taskInfo.status}")
  }
})
```

**ä½¿ç”¨Spark Shellè°ƒè¯•**ï¼š
```scala
// å¯åŠ¨Spark Shell
spark-shell --master local[*]

// é€æ­¥è°ƒè¯•
val rdd = sc.textFile("/path/to/file")
rdd.take(10).foreach(println)  // æŸ¥çœ‹æ•°æ®
rdd.count()  // æ£€æŸ¥æ•°æ®é‡
```

### é¢„é˜²æªæ–½

#### 1. é…ç½®ä¼˜åŒ–

**åŸºç¡€é…ç½®**ï¼š
```bash
# å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2g
spark.driver.memory=4g
spark.driver.memoryOverhead=1g

# ç½‘ç»œé…ç½®
spark.network.timeout=800s
spark.executor.heartbeatInterval=60s

# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
```

**æ€§èƒ½é…ç½®**ï¼š
```bash
# Shuffleé…ç½®
spark.shuffle.file.buffer=32k
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s

# åŠ¨æ€åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
```

#### 2. ä»£ç æœ€ä½³å®è·µ

**å†…å­˜ä¼˜åŒ–**ï¼š
```scala
// ä½¿ç”¨å¹¿æ’­å˜é‡
val broadcastVar = sc.broadcast(largeData)

// åŠæ—¶é‡Šæ”¾ç¼“å­˜
rdd.unpersist()

// ä½¿ç”¨checkpoint
rdd.checkpoint()
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š
```scala
// åˆç†è®¾ç½®åˆ†åŒºæ•°
val optimalPartitions = Math.max(rdd.partitions.length, 200)
val repartitionedRDD = rdd.repartition(optimalPartitions)

// ä½¿ç”¨mapPartitions
val optimizedRDD = rdd.mapPartitions(iter => {
  // æ‰¹é‡å¤„ç†é€»è¾‘
  iter.map(processItem)
})
```

#### 3. ç›‘æ§å‘Šè­¦

**è®¾ç½®ç›‘æ§**ï¼š
```scala
// æ·»åŠ ç›‘æ§æŒ‡æ ‡
val metrics = spark.sparkContext.getStatusTracker
val stageInfo = metrics.getStageInfo(stageId)
println(s"Stage $stageId: ${stageInfo.numTasks} tasks, " +
        s"${stageInfo.numCompletedTasks} completed")
```

**å‘Šè­¦é…ç½®**ï¼š
```bash
# è®¾ç½®å‘Šè­¦é˜ˆå€¼
spark.executor.failures.max=3
spark.task.maxFailures=8
spark.stage.maxAttempts=4
```

é€šè¿‡ä»¥ä¸Šè¯¦ç»†çš„é”™è¯¯åˆ†æå’Œè§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆå¤„ç†Sparkä»»åŠ¡ä¸­çš„å¸¸è§é—®é¢˜ï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

---

## Sparkæ€§èƒ½ä¼˜åŒ–è¯¦è§£

### æŸ¥è¯¢ä¸ä½œä¸šä¼˜åŒ–

#### 1. å­˜å‚¨æ ¼å¼ä¼˜åŒ–

**æ¨èå­˜å‚¨æ ¼å¼å¯¹æ¯”**ï¼š

| æ ¼å¼        | å‹ç¼©æ¯” | æŸ¥è¯¢é€Ÿåº¦ | å†™å…¥é€Ÿåº¦ | é€‚ç”¨åœºæ™¯             |
| ----------- | ------ | -------- | -------- | -------------------- |
| **Parquet** | é«˜     | å¿«       | ä¸­ç­‰     | åˆ†ææŸ¥è¯¢ï¼Œåˆ—å¼å­˜å‚¨   |
| **ORC**     | å¾ˆé«˜   | å¾ˆå¿«     | å¿«       | Hiveé›†æˆï¼Œé«˜å‹ç¼©æ¯”   |
| **Avro**    | ä¸­ç­‰   | ä¸­ç­‰     | å¿«       | è¡Œå¼å­˜å‚¨ï¼ŒSchemaæ¼”è¿› |
| **JSON**    | ä½     | æ…¢       | å¿«       | å¼€å‘è°ƒè¯•ï¼Œçµæ´»æ€§é«˜   |

**Parquetæ ¼å¼ä¼˜åŒ–**ï¼š
```scala
// æ¨èä½¿ç”¨Parquetæ ¼å¼
df.write.mode("overwrite").parquet("data.parquet")
val optimizedDF = spark.read.parquet("data.parquet")

// é…ç½®å‹ç¼©
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

// è®¾ç½®åˆ—å¼è¯»å–æ‰¹æ¬¡å¤§å°
spark.conf.set("spark.sql.parquet.columnarReaderBatchSize", "10000")

// å¯ç”¨å‘é‡åŒ–è¯»å–
spark.conf.set("spark.sql.parquet.enableVectorizedReader", "true")
```

**ORCæ ¼å¼ä¼˜åŒ–**ï¼š
```scala
// ä½¿ç”¨ORCæ ¼å¼
df.write.format("orc").mode("overwrite").save("data.orc")

// ORCä¼˜åŒ–é…ç½®
spark.conf.set("spark.sql.orc.compression.codec", "snappy")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.orc.enableVectorizedReader", "true")
```

#### 2. åˆ†åŒºç­–ç•¥ä¼˜åŒ–

**æ—¶é—´åˆ†åŒºç­–ç•¥**ï¼š
```scala
// æŒ‰æ—¶é—´åˆ†åŒºï¼ˆæ¨èï¼‰
df.write
  .partitionBy("year", "month", "day")
  .parquet("time_partitioned_data")

// é¿å…è¿‡åº¦åˆ†åŒº
val dailyData = df.withColumn("date", to_date($"timestamp"))
dailyData.write
  .partitionBy("date")
  .parquet("daily_partitioned_data")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "true")
```

**ä¸šåŠ¡åˆ†åŒºç­–ç•¥**ï¼š
```scala
// æŒ‰ä¸šåŠ¡å­—æ®µåˆ†åŒº
df.write
  .partitionBy("region", "category")
  .parquet("business_partitioned_data")

// åˆ†åŒºæ•°æ§åˆ¶
val numPartitions = spark.conf.get("spark.sql.shuffle.partitions", "200").toInt
val optimalPartitions = Math.max(100, Math.min(numPartitions, 1000))

df.repartition(optimalPartitions, $"partition_key")
  .write
  .partitionBy("partition_key")
  .parquet("optimized_data")
```

**åˆ†åŒºè£å‰ªä¼˜åŒ–**ï¼š
```scala
// å¯ç”¨åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

// æŸ¥è¯¢æ—¶ä½¿ç”¨åˆ†åŒºè¿‡æ»¤
val result = spark.read.parquet("partitioned_data")
  .filter($"year" === 2023 && $"month" >= 6)  // æœ‰æ•ˆåˆ†åŒºè£å‰ª
  .select("id", "name", "value")
```

#### 3. è°“è¯ä¸‹æ¨ä¼˜åŒ–

**å¯ç”¨è°“è¯ä¸‹æ¨**ï¼š
```scala
// å¯ç”¨å„ç§æ•°æ®æºçš„è°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.orc.filterPushdown", "true")
spark.conf.set("spark.sql.json.filterPushdown", "true")

// JDBCè°“è¯ä¸‹æ¨
spark.conf.set("spark.sql.pushDownPredicate", "true")
```

**ä¼˜åŒ–ç¤ºä¾‹**ï¼š
```scala
// åŸå§‹æŸ¥è¯¢ï¼ˆæœªä¼˜åŒ–ï¼‰
val df = spark.read.parquet("large_dataset.parquet")
val result = df.select("*").filter($"age" > 18 && $"city" === "Beijing")

// ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆè°“è¯ä¸‹æ¨ï¼‰
val result = spark.read.parquet("large_dataset.parquet")
  .filter($"age" > 18)  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .filter($"city" === "Beijing")  // è°“è¯ä¸‹æ¨åˆ°æ•°æ®æº
  .select("id", "name", "age")  // åˆ—è£å‰ª
```

**å¤æ‚è°“è¯ä¼˜åŒ–**ï¼š
```scala
// ç»„åˆæ¡ä»¶ä¼˜åŒ–
val complexFilter = ($"age".between(18, 65)) && 
                   ($"salary" > 50000) && 
                   ($"department".isin("IT", "Finance"))

val result = spark.read.parquet("employee_data.parquet")
  .filter(complexFilter)  // å¤æ‚è°“è¯ä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–å’Œä¸‹æ¨
  .select("id", "name", "salary")
```

### Joinä¼˜åŒ–

#### 1. Joinç­–ç•¥é€‰æ‹©

**Joinç±»å‹å¯¹æ¯”**ï¼š

| Joinç±»å‹              | é€‚ç”¨åœºæ™¯     | ä¼˜åŠ¿                | åŠ£åŠ¿               | è§¦å‘æ¡ä»¶    |
| --------------------- | ------------ | ------------------- | ------------------ | ----------- |
| **Broadcast Join**    | å°è¡¨Joinå¤§è¡¨ | æ— Shuffleï¼Œæ€§èƒ½æœ€å¥½ | å°è¡¨å¿…é¡»èƒ½æ”¾å…¥å†…å­˜ | å°è¡¨ < 10MB |
| **Sort Merge Join**   | å¤§è¡¨Joinå¤§è¡¨ | å†…å­˜å‹å¥½ï¼Œç¨³å®š      | éœ€è¦Shuffle        | é»˜è®¤Join    |
| **Shuffle Hash Join** | ä¸­ç­‰è¡¨Join   | å†…å­˜ä½¿ç”¨é€‚ä¸­        | éœ€è¦Shuffle        | ä¸­ç­‰æ•°æ®é‡  |
| **Cartesian Join**    | ç¬›å¡å°”ç§¯     | ç®€å•                | æ€§èƒ½æå·®           | æ— Joiné”®    |

#### 2. å¹¿æ’­Joinä¼˜åŒ–

**è‡ªåŠ¨å¹¿æ’­é…ç½®**ï¼š
```scala
// è®¾ç½®è‡ªåŠ¨å¹¿æ’­é˜ˆå€¼
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

// å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
```

**æ‰‹åŠ¨å¹¿æ’­ä¼˜åŒ–**ï¼š
```scala
// æ‰‹åŠ¨å¹¿æ’­å°è¡¨
val smallTable = spark.table("small_table")
val broadcastDF = broadcast(smallTable)
val result = largeTable.join(broadcastDF, "id")

// å¼ºåˆ¶å¹¿æ’­ï¼ˆå³ä½¿è¶…è¿‡é˜ˆå€¼ï¼‰
val result = largeTable.join(broadcast(mediumTable), "id")

// å¹¿æ’­å˜é‡ä¼˜åŒ–
val lookupMap = smallTable.collect()
  .map(row => row.getAs[String]("key") -> row.getAs[String]("value"))
  .toMap
val broadcastMap = spark.sparkContext.broadcast(lookupMap)

val enrichedData = largeTable.map { row =>
  val key = row.getAs[String]("key")
  val enrichValue = broadcastMap.value.getOrElse(key, "unknown")
  (row, enrichValue)
}
```

#### 3. Sort Merge Joinä¼˜åŒ–

**é¢„æ’åºä¼˜åŒ–**ï¼š
```scala
// é¢„å…ˆæ’åºå‡å°‘Shuffleæˆæœ¬
val sortedTable1 = table1.sort("join_key")
val sortedTable2 = table2.sort("join_key")
val result = sortedTable1.join(sortedTable2, "join_key")

// ä½¿ç”¨åˆ†æ¡¶è¡¨
table1.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table1")

table2.write
  .bucketBy(10, "join_key")
  .sortBy("join_key")
  .saveAsTable("bucketed_table2")

// åˆ†æ¡¶è¡¨Joinï¼ˆæ— Shuffleï¼‰
val result = spark.table("bucketed_table1")
  .join(spark.table("bucketed_table2"), "join_key")
```

#### 4. æ•°æ®å€¾æ–œå¤„ç†

**å€¾æ–œæ£€æµ‹**ï¼š
```scala
// æ£€æµ‹Joiné”®åˆ†å¸ƒ
val keyDistribution = largeTable
  .groupBy("join_key")
  .count()
  .orderBy($"count".desc)

keyDistribution.show(20)  // æŸ¥çœ‹top 20çš„é”®åˆ†å¸ƒ

// ç»Ÿè®¡åˆ†æ
val stats = keyDistribution.agg(
  avg("count").as("avg_count"),
  max("count").as("max_count"),
  min("count").as("min_count"),
  stddev("count").as("stddev_count")
)
stats.show()
```

**å€¾æ–œè§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ1ï¼šåŠ ç›å¤„ç†**ï¼š
```scala
// åŠ ç›Join
import scala.util.Random

// ç»™å°è¡¨åŠ ç›
val saltedSmallTable = smallTable.flatMap { row =>
  (0 until 10).map { salt =>
    Row.fromSeq(row.toSeq :+ salt)
  }
}

// ç»™å¤§è¡¨çš„å€¾æ–œé”®åŠ éšæœºç›
val saltedLargeTable = largeTable.map { row =>
  val key = row.getAs[String]("join_key")
  val salt = if (isSkewedKey(key)) Random.nextInt(10) else 0
  Row.fromSeq(row.toSeq :+ salt)
}

// æ‰§è¡ŒJoin
val result = saltedLargeTable.join(saltedSmallTable, 
  Seq("join_key", "salt_column"))
```

**æ–¹æ¡ˆ2ï¼šå€¾æ–œé”®å•ç‹¬å¤„ç†**ï¼š
```scala
// è¯†åˆ«å€¾æ–œé”®
val skewedKeys = Set("skewed_key_1", "skewed_key_2")

// åˆ†ç¦»å€¾æ–œæ•°æ®å’Œæ­£å¸¸æ•°æ®
val normalData = largeTable.filter(!$"join_key".isin(skewedKeys.toSeq:_*))
val skewedData = largeTable.filter($"join_key".isin(skewedKeys.toSeq:_*))

// æ­£å¸¸æ•°æ®æ­£å¸¸Join
val normalResult = normalData.join(smallTable, "join_key")

// å€¾æ–œæ•°æ®ä½¿ç”¨å¹¿æ’­Join
val skewedResult = skewedData.join(broadcast(smallTable), "join_key")

// åˆå¹¶ç»“æœ
val finalResult = normalResult.union(skewedResult)
```

**æ–¹æ¡ˆ3ï¼šä¸¤é˜¶æ®µèšåˆ**ï¼š
```scala
// é¢„èšåˆé˜¶æ®µ
val preAggregated = largeTable
  .withColumn("salt", (rand() * 10).cast("int"))
  .withColumn("salted_key", concat($"join_key", lit("_"), $"salt"))
  .groupBy("salted_key")
  .agg(sum("value").as("partial_sum"))

// æœ€ç»ˆèšåˆé˜¶æ®µ
val finalAggregated = preAggregated
  .withColumn("original_key", split($"salted_key", "_").getItem(0))
  .groupBy("original_key")
  .agg(sum("partial_sum").as("final_sum"))
```

### ç¼“å­˜ä¸æŒä¹…åŒ–

#### 1. å­˜å‚¨çº§åˆ«é€‰æ‹©

**å­˜å‚¨çº§åˆ«å¯¹æ¯”**ï¼š

| å­˜å‚¨çº§åˆ«            | å†…å­˜ä½¿ç”¨ | ç£ç›˜ä½¿ç”¨ | åºåˆ—åŒ– | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯             |
| ------------------- | -------- | -------- | ------ | ------- | -------------------- |
| **MEMORY_ONLY**     | é«˜       | æ—        | æ—      | ä½      | å°æ•°æ®é›†ï¼Œé¢‘ç¹è®¿é—®   |
| **MEMORY_AND_DISK** | ä¸­ç­‰     | æœ‰       | æ—      | ä½      | å¤§æ•°æ®é›†ï¼Œå†…å­˜ä¸è¶³æ—¶ |
| **MEMORY_ONLY_SER** | ä½       | æ—        | æœ‰     | é«˜      | å†…å­˜ç´§å¼ ï¼ŒCPUå……è¶³    |
| **DISK_ONLY**       | æ—        | é«˜       | æœ‰     | ä¸­ç­‰    | å¤§æ•°æ®é›†ï¼Œå†…å­˜ç´§å¼    |
| **OFF_HEAP**        | å †å¤–     | æ—        | æœ‰     | ä¸­ç­‰    | å‡å°‘GCå‹åŠ›           |

#### 2. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

**æ™ºèƒ½ç¼“å­˜ç­–ç•¥**ï¼š
```scala
// æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©å­˜å‚¨çº§åˆ«
def selectStorageLevel(dataSize: Long, memoryAvailable: Long): StorageLevel = {
  val ratio = dataSize.toDouble / memoryAvailable
  
  if (ratio < 0.3) {
    StorageLevel.MEMORY_ONLY  // å†…å­˜å……è¶³
  } else if (ratio < 0.8) {
    StorageLevel.MEMORY_ONLY_SER  // å†…å­˜ç´§å¼ ï¼Œåºåˆ—åŒ–èŠ‚çœç©ºé—´
  } else {
    StorageLevel.MEMORY_AND_DISK_SER  // å†…å­˜ä¸è¶³ï¼Œæº¢å‡ºåˆ°ç£ç›˜
  }
}

// åº”ç”¨ç¼“å­˜ç­–ç•¥
val dataSize = rdd.map(_.toString.length).sum()
val storageLevel = selectStorageLevel(dataSize, availableMemory)
rdd.persist(storageLevel)
```

**ç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š
```scala
// ç¼“å­˜ç®¡ç†å™¨
class CacheManager {
  private val cachedRDDs = mutable.Map[String, RDD[_]]()
  
  def cache[T](name: String, rdd: RDD[T], level: StorageLevel = StorageLevel.MEMORY_AND_DISK): RDD[T] = {
    // æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
    if (!cachedRDDs.contains(name)) {
      rdd.persist(level)
      cachedRDDs(name) = rdd
      println(s"Cached RDD: $name")
    }
    rdd
  }
  
  def uncache(name: String): Unit = {
    cachedRDDs.get(name).foreach { rdd =>
      rdd.unpersist()
      cachedRDDs.remove(name)
      println(s"Uncached RDD: $name")
    }
  }
  
  def clearAll(): Unit = {
    cachedRDDs.values.foreach(_.unpersist())
    cachedRDDs.clear()
    println("Cleared all cached RDDs")
  }
}
```

#### 3. Checkpointä¼˜åŒ–

**Checkpointç­–ç•¥**ï¼š
```scala
// è®¾ç½®checkpointç›®å½•
spark.sparkContext.setCheckpointDir("hdfs://namenode:8020/checkpoint")

// æ™ºèƒ½checkpoint
def smartCheckpoint[T](rdd: RDD[T], lineageDepth: Int = 10): RDD[T] = {
  // è®¡ç®—è¡€ç¼˜æ·±åº¦
  def getLineageDepth(rdd: RDD[_]): Int = {
    if (rdd.dependencies.isEmpty) {
      1
    } else {
      1 + rdd.dependencies.map(_.rdd).map(getLineageDepth).max
    }
  }
  
  val currentDepth = getLineageDepth(rdd)
  if (currentDepth > lineageDepth) {
    println(s"Checkpointing RDD with lineage depth: $currentDepth")
    rdd.checkpoint()
  }
  rdd
}

// ä½¿ç”¨ç¤ºä¾‹
val deepRDD = rdd1.map(transform1)
  .filter(filter1)
  .join(rdd2)
  .map(transform2)
  .filter(filter2)

val checkpointedRDD = smartCheckpoint(deepRDD)
```

### ä»£ç å±‚é¢ä¼˜åŒ–

#### 1. ç®—å­é€‰æ‹©ä¼˜åŒ–

**é«˜æ•ˆç®—å­ä½¿ç”¨**ï¼š
```scala
// ä½¿ç”¨reduceByKeyæ›¿ä»£groupByKey
val wordCounts = words.map(word => (word, 1))
  .reduceByKey(_ + _)  // æ¨èï¼šMapç«¯é¢„èšåˆ

// è€Œä¸æ˜¯
val wordCounts = words.map(word => (word, 1))
  .groupByKey()  // ä¸æ¨èï¼šæ‰€æœ‰æ•°æ®éƒ½è¦Shuffle
  .mapValues(_.sum)

// ä½¿ç”¨mapPartitionså‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
val result = rdd.mapPartitions { partition =>
  // åœ¨åˆ†åŒºçº§åˆ«åˆå§‹åŒ–èµ„æº
  val connection = createConnection()
  val buffer = new ArrayBuffer[ProcessedRecord]()
  
  try {
    partition.foreach { record =>
      buffer += processWithConnection(record, connection)
    }
    buffer.iterator
  } finally {
    connection.close()
  }
}

// ä½¿ç”¨aggregateByKeyè¿›è¡Œå¤æ‚èšåˆ
val result = rdd.aggregateByKey((0, 0))(
  // åˆ†åŒºå†…èšåˆ
  (acc, value) => (acc._1 + value, acc._2 + 1),
  // åˆ†åŒºé—´èšåˆ
  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
).mapValues { case (sum, count) => sum.toDouble / count }
```

#### 2. æ•°æ®ç»“æ„ä¼˜åŒ–

**é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„**ï¼š
```scala
// ä½¿ç”¨åŸå§‹ç±»å‹æ•°ç»„æ›¿ä»£é›†åˆ
class OptimizedProcessor {
  // æ¨èï¼šåŸå§‹ç±»å‹æ•°ç»„
  private val intArray = new Array[Int](1000000)
  
  // ä¸æ¨èï¼šè£…ç®±ç±»å‹é›†åˆ
  private val intList = new ArrayBuffer[Integer]()
  
  // ä½¿ç”¨ä¸“ç”¨çš„æ•°æ®ç»“æ„
  def processLargeDataset(data: RDD[String]): RDD[String] = {
    data.mapPartitions { partition =>
      val bloom = new BloomFilter[String](1000000, 0.01)
      val deduped = new mutable.HashSet[String]()
      
      partition.filter { item =>
        if (bloom.mightContain(item)) {
          if (deduped.contains(item)) {
            false  // é‡å¤é¡¹
          } else {
            deduped += item
            true
          }
        } else {
          bloom.put(item)
          deduped += item
          true
        }
      }
    }
  }
}
```

#### 3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–

**å¯¹è±¡å¤ç”¨**ï¼š
```scala
// å¯¹è±¡æ± æ¨¡å¼
class ObjectPool[T](createFunc: () => T, resetFunc: T => Unit) {
  private val pool = new mutable.Queue[T]()
  
  def borrow(): T = {
    pool.synchronized {
      if (pool.nonEmpty) {
        pool.dequeue()
      } else {
        createFunc()
      }
    }
  }
  
  def return(obj: T): Unit = {
    resetFunc(obj)
    pool.synchronized {
      pool.enqueue(obj)
    }
  }
}

// ä½¿ç”¨å¯¹è±¡æ± 
val stringBuilderPool = new ObjectPool[StringBuilder](
  () => new StringBuilder(),
  _.clear()
)

val result = rdd.mapPartitions { partition =>
  partition.map { item =>
    val sb = stringBuilderPool.borrow()
    try {
      sb.append(item).append("_processed").toString
    } finally {
      stringBuilderPool.return(sb)
    }
  }
}
```

#### 4. å¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ä¼˜åŒ–

**å¹¿æ’­å˜é‡æœ€ä½³å®è·µ**ï¼š
```scala
// å¤§æŸ¥æ‰¾è¡¨å¹¿æ’­
val lookupTableMap = smallTable.collect()
  .map(row => row.getString(0) -> row.getString(1))
  .toMap

val broadcastLookup = spark.sparkContext.broadcast(lookupTableMap)

val enrichedData = largeRDD.map { record =>
  val enrichValue = broadcastLookup.value.getOrElse(record.key, "unknown")
  EnrichedRecord(record, enrichValue)
}

// è®°ä½åŠæ—¶é‡Šæ”¾
broadcastLookup.destroy()
```

**ç´¯åŠ å™¨ä¼˜åŒ–**ï¼š
```scala
// è‡ªå®šä¹‰ç´¯åŠ å™¨
class HistogramAccumulator extends AccumulatorV2[Double, mutable.Map[String, Long]] {
  private val histogram = mutable.Map[String, Long]()
  
  override def isZero: Boolean = histogram.isEmpty
  
  override def copy(): HistogramAccumulator = {
    val newAcc = new HistogramAccumulator
    newAcc.histogram ++= this.histogram
    newAcc
  }
  
  override def reset(): Unit = histogram.clear()
  
  override def add(value: Double): Unit = {
    val bucket = getBucket(value)
    histogram(bucket) = histogram.getOrElse(bucket, 0L) + 1
  }
  
  override def merge(other: AccumulatorV2[Double, mutable.Map[String, Long]]): Unit = {
    other match {
      case o: HistogramAccumulator =>
        o.histogram.foreach { case (bucket, count) =>
          histogram(bucket) = histogram.getOrElse(bucket, 0L) + count
        }
    }
  }
  
  override def value: mutable.Map[String, Long] = histogram.toMap
  
  private def getBucket(value: Double): String = {
    if (value < 0) "negative"
    else if (value < 10) "0-10"
    else if (value < 100) "10-100"
    else "100+"
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰ç´¯åŠ å™¨
val histogramAcc = new HistogramAccumulator
spark.sparkContext.register(histogramAcc, "histogram")

rdd.foreach(value => histogramAcc.add(value))
println(s"Histogram: ${histogramAcc.value}")
```

### ç½‘ç»œä¸I/Oä¼˜åŒ–

#### 1. åºåˆ—åŒ–ä¼˜åŒ–

**Kryoåºåˆ—åŒ–é…ç½®**ï¼š
```scala
// Kryoé…ç½®
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.kryo.registrationRequired", "false")
spark.conf.set("spark.kryo.unsafe", "true")

// æ³¨å†Œå¸¸ç”¨ç±»
val conf = new SparkConf()
conf.registerKryoClasses(Array(
  classOf[MyClass],
  classOf[scala.collection.mutable.WrappedArray.ofRef[_]],
  classOf[org.apache.spark.sql.types.StructType],
  classOf[Array[org.apache.spark.sql.types.StructField]]
))
```

#### 2. å‹ç¼©ä¼˜åŒ–

**å‹ç¼©ç®—æ³•é€‰æ‹©**ï¼š
```scala
// æ ¹æ®åœºæ™¯é€‰æ‹©å‹ç¼©ç®—æ³•
spark.conf.set("spark.io.compression.codec", "snappy")  // å¹³è¡¡å‹ç¼©æ¯”å’Œé€Ÿåº¦
// spark.conf.set("spark.io.compression.codec", "lz4")     // æ›´å¿«çš„å‹ç¼©/è§£å‹
// spark.conf.set("spark.io.compression.codec", "gzip")    // æ›´é«˜çš„å‹ç¼©æ¯”

// å¯ç”¨å„ç§å‹ç¼©
spark.conf.set("spark.broadcast.compress", "true")
spark.conf.set("spark.shuffle.compress", "true")
spark.conf.set("spark.shuffle.spill.compress", "true")
spark.conf.set("spark.rdd.compress", "true")
```

#### 3. ç½‘ç»œè°ƒä¼˜

**ç½‘ç»œå‚æ•°ä¼˜åŒ–**ï¼š
```bash
# ç½‘ç»œè¶…æ—¶è®¾ç½®
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.rpc.askTimeout", "800s")
spark.conf.set("spark.rpc.lookupTimeout", "800s")

# ç½‘ç»œè¿æ¥ä¼˜åŒ–
spark.conf.set("spark.rpc.netty.dispatcher.numThreads", "8")
spark.conf.set("spark.shuffle.io.numConnectionsPerPeer", "3")
spark.conf.set("spark.shuffle.io.preferDirectBufs", "true")

# ä¼ è¾“ä¼˜åŒ–
spark.conf.set("spark.reducer.maxSizeInFlight", "96m")
spark.conf.set("spark.reducer.maxReqsInFlight", "Int.MaxValue")
```

---

## Sparké€šä¿¡ä¸ç½‘ç»œ ğŸŒ

### NettyåŸºç¡€ä¸åº”ç”¨

**Nettyåœ¨Sparkä¸­çš„ä½œç”¨**ï¼š
- **RPCé€šä¿¡**ï¼šDriverä¸Executorä¹‹é—´çš„é€šä¿¡
- **Shuffleæ•°æ®ä¼ è¾“**ï¼šExecutorä¹‹é—´çš„æ•°æ®ä¼ è¾“
- **å¿ƒè·³æœºåˆ¶**ï¼šä¿æŒè¿æ¥æ´»è·ƒ
- **BlockManageré€šä¿¡**ï¼šæ•°æ®å—çš„ä¼ è¾“å’Œç®¡ç†

#### Nettyæ¶æ„æ¨¡å‹

**çº¿ç¨‹æ¨¡å‹**ï¼š
```mermaid
graph TD
    A[Client] --> B[BossGroup<br/>äº‹ä»¶å¾ªç¯ç»„]
    B --> C[Worker Thread 1]
    B --> D[Worker Thread 2]
    B --> E[Worker Thread N]
    
    C --> F[EventLoop 1]
    D --> G[EventLoop 2]
    E --> H[EventLoop N]
    
    F --> I[Channel Pipeline 1]
    G --> J[Channel Pipeline 2]
    H --> K[Channel Pipeline N]
    
    style B fill:#e1f5fe
    style F fill:#fff3e0
    style G fill:#fff3e0
    style H fill:#fff3e0
```

#### å…³é”®å‚æ•°é…ç½®

**ç½‘ç»œé€šä¿¡å‚æ•°**ï¼š
```properties
# åŸºç¡€ç½‘ç»œé…ç½®
spark.rpc.netty.dispatcher.numThreads=8
spark.network.timeout=800s
spark.shuffle.io.numConnectionsPerPeer=3

# å†…å­˜ç®¡ç†
spark.shuffle.io.preferDirectBufs=true
spark.network.io.preferDirectBufs=true
spark.rpc.io.preferDirectBufs=true

# ç¼“å†²åŒºé…ç½®
spark.shuffle.file.buffer=32k
spark.shuffle.unsafe.file.output.buffer=32k
spark.network.io.serverThreads=8
spark.network.io.clientThreads=8
```

**æ€§èƒ½è°ƒä¼˜å‚æ•°**ï¼š
```properties
# è¿æ¥æ± é…ç½®
spark.rpc.netty.dispatcher.numThreads=16
spark.shuffle.io.connectionTimeout=120s
spark.shuffle.io.maxRetries=5
spark.shuffle.io.retryWait=60s

# ä¼ è¾“ä¼˜åŒ–
spark.reducer.maxSizeInFlight=96m
spark.reducer.maxReqsInFlight=2147483647
spark.shuffle.compress=true
spark.io.compression.codec=snappy
```

#### å¼‚å¸¸æ’æŸ¥

**å¸¸è§ç½‘ç»œé—®é¢˜**ï¼š
```scala
// ç›‘æ§ç½‘ç»œè¿æ¥
class NetworkMonitor {
  def monitorConnections(): Unit = {
    val runtime = Runtime.getRuntime
    val totalMemory = runtime.totalMemory()
    val freeMemory = runtime.freeMemory()
    val usedMemory = totalMemory - freeMemory
    
    println(s"Used Memory: ${usedMemory / 1024 / 1024} MB")
    println(s"Free Memory: ${freeMemory / 1024 / 1024} MB")
    
    // ç›‘æ§ç½‘ç»œçº¿ç¨‹
    val threadMXBean = ManagementFactory.getThreadMXBean
    val threadInfos = threadMXBean.getAllThreadIds
    
    threadInfos.foreach { threadId =>
      val threadInfo = threadMXBean.getThreadInfo(threadId)
      if (threadInfo.getThreadName.contains("netty")) {
        println(s"Netty Thread: ${threadInfo.getThreadName}, State: ${threadInfo.getThreadState}")
      }
    }
  }
}
```

---

## Sparkå…³é”®å‚æ•°ä¸é…ç½® âš™ï¸

### èµ„æºç›¸å…³å‚æ•°

**å†…å­˜é…ç½®**ï¼š
```properties
# åŸºç¡€å†…å­˜é…ç½®
spark.executor.memory=8g
spark.executor.memoryOverhead=2048m
spark.driver.memory=4g
spark.driver.memoryOverhead=1024m

# å †å¤–å†…å­˜
spark.memory.offHeap.enabled=true
spark.memory.offHeap.size=4g

# å†…å­˜åˆ†é…æ¯”ä¾‹
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3

# åŠ¨æ€å†…å­˜åˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=5
spark.dynamicAllocation.maxExecutors=100
spark.dynamicAllocation.initialExecutors=10
```

**CPUé…ç½®**ï¼š
```properties
# CPUèµ„æºé…ç½®
spark.executor.cores=4
spark.driver.cores=2
spark.default.parallelism=400
spark.sql.shuffle.partitions=400

# ä»»åŠ¡è°ƒåº¦
spark.task.cpus=1
spark.task.maxFailures=8
spark.stage.maxConsecutiveAttempts=4
```

### JVMç›¸å…³å‚æ•°

**åƒåœ¾å›æ”¶é…ç½®**ï¼š
```bash
# G1GCé…ç½®ï¼ˆæ¨èï¼‰
--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC 
-XX:G1HeapRegionSize=16m 
-XX:MaxGCPauseMillis=200 
-XX:+G1PrintRegionRememberedSetInfo 
-XX:+UseCompressedOops 
-XX:+UseCompressedClassPointers
-XX:+PrintHeapAtGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps
-Xloggc:/var/log/spark/gc-executor.log"

--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC 
-XX:MaxGCPauseMillis=200 
-XX:+PrintHeapAtGC
-Xloggc:/var/log/spark/gc-driver.log"
```

**å¹¶å‘GCé…ç½®**ï¼š
```bash
# CMS GCé…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC 
-XX:+CMSParallelRemarkEnabled 
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:CMSInitiatingOccupancyFraction=70
-XX:+PrintGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps"
```

**å†…å­˜è°ƒè¯•å‚æ•°**ï¼š
```bash
# å†…å­˜è°ƒè¯•é…ç½®
--conf "spark.executor.extraJavaOptions=-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/log/spark/heapdump
-XX:+TraceClassLoading
-XX:+PrintStringDeduplication"
```

### æ€§èƒ½ä¼˜åŒ–å‚æ•°

**SQLæ‰§è¡Œä¼˜åŒ–**ï¼š
```properties
# è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# æ•°æ®æ ¼å¼ä¼˜åŒ–
spark.sql.parquet.columnarReaderBatchSize=10000
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.binaryAsString=false
spark.sql.execution.arrow.pyspark.enabled=true

# Joinä¼˜åŒ–
spark.sql.autoBroadcastJoinThreshold=100MB
spark.sql.broadcastTimeout=300s
```

**Shuffleä¼˜åŒ–**ï¼š
```properties
# ShuffleåŸºç¡€é…ç½®
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.file.buffer=64k
spark.shuffle.unsafe.file.output.buffer=64k

# Shuffleé«˜çº§é…ç½®
spark.shuffle.sort.bypassMergeThreshold=200
spark.shuffle.consolidateFiles=true
spark.shuffle.memoryFraction=0.2
spark.shuffle.safetyFraction=0.8

# Shuffleç½‘ç»œé…ç½®
spark.shuffle.io.maxRetries=3
spark.shuffle.io.retryWait=60s
spark.shuffle.service.enabled=true
```

**å…¶ä»–å¸¸ç”¨å‚æ•°**ï¼š
```properties
# åºåˆ—åŒ–é…ç½®
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryo.unsafe=true

# å‹ç¼©é…ç½®
spark.io.compression.codec=snappy
spark.broadcast.compress=true
spark.rdd.compress=true

# æ–‡ä»¶ç³»ç»Ÿé…ç½®
spark.files.maxPartitionBytes=128m
spark.files.openCostInBytes=4m

# Hadoopå…¼å®¹é…ç½®
spark.hadoop.parquet.enable.summary-metadata=false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
```

### é…ç½®æ¨¡æ¿

**å¼€å‘ç¯å¢ƒé…ç½®**ï¼š
```bash
#!/bin/bash
# å¼€å‘ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master local[*] \
  --deploy-mode client \
  --executor-memory 2g \
  --driver-memory 1g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  your-app.jar
```

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š
```bash
#!/bin/bash
# ç”Ÿäº§ç¯å¢ƒSparké…ç½®æ¨¡æ¿

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 50 \
  --executor-memory 8g \
  --executor-cores 4 \
  --driver-memory 4g \
  --driver-cores 2 \
  --conf spark.executor.memoryOverhead=2g \
  --conf spark.driver.memoryOverhead=1g \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=4g \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=10 \
  --conf spark.dynamicAllocation.maxExecutors=100 \
  --conf spark.shuffle.service.enabled=true \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.execution.arrow.pyspark.enabled=true \
  your-app.jar
```

---

## Sparké«˜çº§ç‰¹æ€§ä¸æ‰©å±• ğŸš€

### è‡ªå®šä¹‰æ•°æ®æº

**æ•°æ®æºAPIæ¦‚è¿°**ï¼š
Sparkæä¾›äº†Data Source API v2ï¼Œå…è®¸å¼€å‘è€…åˆ›å»ºè‡ªå®šä¹‰æ•°æ®æºï¼Œå®ç°ä¸å„ç§å­˜å‚¨ç³»ç»Ÿçš„é›†æˆã€‚

**è‡ªå®šä¹‰æ•°æ®æºå®ç°**ï¼š
```scala
import org.apache.spark.sql.connector.catalog.{SupportsRead, SupportsWrite, Table, TableCapability}
import org.apache.spark.sql.connector.read.{Batch, InputPartition, PartitionReader, PartitionReaderFactory, Scan, ScanBuilder}
import org.apache.spark.sql.connector.write.{BatchWrite, DataWriter, DataWriterFactory, LogicalWriteInfo, WriteBuilder, WriterCommitMessage}
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.util.CaseInsensitiveStringMap

// 1. è‡ªå®šä¹‰è¡¨å®ç°
class CustomTable extends Table with SupportsRead with SupportsWrite {
  
  override def name(): String = "custom_table"
  
  override def schema(): StructType = {
    StructType(Seq(
      StructField("id", LongType, nullable = false),
      StructField("name", StringType, nullable = true),
      StructField("value", DoubleType, nullable = true)
    ))
  }
  
  override def capabilities(): java.util.Set[TableCapability] = {
    Set(
      TableCapability.BATCH_READ,
      TableCapability.BATCH_WRITE,
      TableCapability.STREAMING_WRITE
    ).asJava
  }
  
  override def newScanBuilder(options: CaseInsensitiveStringMap): ScanBuilder = {
    new CustomScanBuilder(options)
  }
  
  override def newWriteBuilder(info: LogicalWriteInfo): WriteBuilder = {
    new CustomWriteBuilder(info)
  }
}

// 2. æ‰«ææ„å»ºå™¨
class CustomScanBuilder(options: CaseInsensitiveStringMap) extends ScanBuilder {
  
  override def build(): Scan = new CustomScan(options)
}

// 3. æ‰«æå®ç°
class CustomScan(options: CaseInsensitiveStringMap) extends Scan with Batch {
  
  override def readSchema(): StructType = {
    // è¿”å›è¯»å–çš„Schema
    StructType(Seq(
      StructField("id", LongType, nullable = false),
      StructField("name", StringType, nullable = true),
      StructField("value", DoubleType, nullable = true)
    ))
  }
  
  override def toBatch: Batch = this
  
  override def planInputPartitions(): Array[InputPartition] = {
    // æ ¹æ®æ•°æ®æºç‰¹æ€§åˆ›å»ºåˆ†åŒº
    val partitionCount = options.getInt("partitions", 4)
    (0 until partitionCount).map(i => CustomInputPartition(i)).toArray
  }
  
  override def createReaderFactory(): PartitionReaderFactory = {
    new CustomPartitionReaderFactory(options)
  }
}

// 4. åˆ†åŒºè¯»å–å™¨å·¥å‚
class CustomPartitionReaderFactory(options: CaseInsensitiveStringMap) 
  extends PartitionReaderFactory {
  
  override def createReader(partition: InputPartition): PartitionReader[InternalRow] = {
    new CustomPartitionReader(partition.asInstanceOf[CustomInputPartition], options)
  }
}

// 5. åˆ†åŒºè¯»å–å™¨
class CustomPartitionReader(
    partition: CustomInputPartition, 
    options: CaseInsensitiveStringMap) 
  extends PartitionReader[InternalRow] {
  
  private var currentIndex = 0
  private val maxRecords = options.getInt("records_per_partition", 1000)
  
  override def next(): Boolean = {
    currentIndex < maxRecords
  }
  
  override def get(): InternalRow = {
    val row = InternalRow(
      currentIndex.toLong,
      UTF8String.fromString(s"name_${partition.partitionId}_$currentIndex"),
      currentIndex.toDouble
    )
    currentIndex += 1
    row
  }
  
  override def close(): Unit = {
    // æ¸…ç†èµ„æº
  }
}

// 6. è¾“å…¥åˆ†åŒº
case class CustomInputPartition(partitionId: Int) extends InputPartition
```

**æ•°æ®æºæ³¨å†Œä¸ä½¿ç”¨**ï¼š
```scala
// æ³¨å†Œè‡ªå®šä¹‰æ•°æ®æº
spark.sql("""
  CREATE TABLE custom_data
  USING com.example.CustomDataSource
  OPTIONS (
    'partitions' '8',
    'records_per_partition' '10000',
    'connection_url' 'jdbc:custom://localhost:9999/db'
  )
""")

// ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æº
val df = spark.read
  .format("com.example.CustomDataSource")
  .option("partitions", "4")
  .option("records_per_partition", "5000")
  .load()

df.show()
```

### æ’ä»¶æœºåˆ¶

**Sparkæ’ä»¶ç³»ç»Ÿ**ï¼š
Spark 3.0å¼•å…¥äº†æ’ä»¶æœºåˆ¶ï¼Œå…è®¸åœ¨è¿è¡Œæ—¶åŠ¨æ€åŠ è½½å’Œé…ç½®æ’ä»¶ã€‚

**æ’ä»¶æ¥å£å®ç°**ï¼š
```scala
import org.apache.spark.api.plugin.{DriverPlugin, ExecutorPlugin, PluginContext, SparkPlugin}
import org.apache.spark.resource.ResourceInformation

// 1. ä¸»æ’ä»¶ç±»
class CustomSparkPlugin extends SparkPlugin {
  
  override def driverPlugin(): DriverPlugin = new CustomDriverPlugin()
  
  override def executorPlugin(): ExecutorPlugin = new CustomExecutorPlugin()
}

// 2. Driveræ’ä»¶
class CustomDriverPlugin extends DriverPlugin {
  
  override def init(sc: SparkContext, pluginContext: PluginContext): java.util.Map[String, String] = {
    logInfo("Custom Driver Plugin initialized")
    
    // åˆå§‹åŒ–Driverç«¯çš„è‡ªå®šä¹‰é€»è¾‘
    initializeCustomMetrics()
    setupCustomListeners(sc)
    
    // è¿”å›é…ç½®ä¿¡æ¯ç»™Executor
    Map(
      "custom.plugin.version" -> "1.0.0",
      "custom.plugin.config" -> "driver_config"
    ).asJava
  }
  
  override def registerMetrics(appId: String, pluginContext: PluginContext): Unit = {
    // æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡
    val metricRegistry = pluginContext.metricRegistry()
    metricRegistry.counter("custom.driver.operations")
    metricRegistry.histogram("custom.driver.latency")
  }
  
  override def receive(message: AnyRef): AnyRef = {
    // å¤„ç†æ¥è‡ªExecutorçš„æ¶ˆæ¯
    message match {
      case msg: String => 
        logInfo(s"Received message from executor: $msg")
        "ACK"
      case _ => null
    }
  }
  
  private def initializeCustomMetrics(): Unit = {
    // åˆå§‹åŒ–è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†
  }
  
  private def setupCustomListeners(sc: SparkContext): Unit = {
    // è®¾ç½®è‡ªå®šä¹‰ç›‘å¬å™¨
    sc.addSparkListener(new CustomSparkListener())
  }
}

// 3. Executoræ’ä»¶
class CustomExecutorPlugin extends ExecutorPlugin {
  
  override def init(pluginContext: PluginContext, extraConf: java.util.Map[String, String]): Unit = {
    logInfo("Custom Executor Plugin initialized")
    
    // ä»Driverè·å–é…ç½®
    val version = extraConf.get("custom.plugin.version")
    val config = extraConf.get("custom.plugin.config")
    
    // åˆå§‹åŒ–Executorç«¯çš„è‡ªå®šä¹‰é€»è¾‘
    initializeExecutorResources()
    setupTaskInterceptors()
  }
  
  override def onTaskStart(): Unit = {
    // ä»»åŠ¡å¼€å§‹æ—¶çš„å¤„ç†é€»è¾‘
    logInfo("Task started with custom plugin")
  }
  
  override def onTaskSucceeded(): Unit = {
    // ä»»åŠ¡æˆåŠŸæ—¶çš„å¤„ç†é€»è¾‘
    logInfo("Task succeeded with custom plugin")
  }
  
  override def onTaskFailed(failureReason: TaskFailedReason): Unit = {
    // ä»»åŠ¡å¤±è´¥æ—¶çš„å¤„ç†é€»è¾‘
    logError(s"Task failed with custom plugin: $failureReason")
  }
  
  private def initializeExecutorResources(): Unit = {
    // åˆå§‹åŒ–Executorèµ„æº
  }
  
  private def setupTaskInterceptors(): Unit = {
    // è®¾ç½®ä»»åŠ¡æ‹¦æˆªå™¨
  }
}

// 4. è‡ªå®šä¹‰ç›‘å¬å™¨
class CustomSparkListener extends SparkListener {
  
  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = {
    logInfo(s"Application started: ${applicationStart.appName}")
  }
  
  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {
    logInfo(s"Job started: ${jobStart.jobId}")
  }
  
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
    val stageInfo = stageCompleted.stageInfo
    logInfo(s"Stage ${stageInfo.stageId} completed in ${stageInfo.submissionTime}")
  }
}
```

**æ’ä»¶é…ç½®ä¸ä½¿ç”¨**ï¼š
```properties
# spark-defaults.conf
spark.plugins=com.example.CustomSparkPlugin
spark.custom.plugin.enabled=true
spark.custom.plugin.config.key=value
```

### æ‰©å±•ç‚¹å¼€å‘

**Catalystè§„åˆ™æ‰©å±•**ï¼š
```scala
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.SparkSessionExtensions

// 1. è‡ªå®šä¹‰ä¼˜åŒ–è§„åˆ™
object CustomOptimizationRule extends Rule[LogicalPlan] {
  
  override def apply(plan: LogicalPlan): LogicalPlan = {
    plan.transformAllExpressions {
      // è‡ªå®šä¹‰è¡¨è¾¾å¼ä¼˜åŒ–
      case Add(Literal(0, _), right) => right
      case Add(left, Literal(0, _)) => left
      case Multiply(Literal(1, _), right) => right
      case Multiply(left, Literal(1, _)) => left
      case Multiply(Literal(0, _), _) => Literal(0)
      case Multiply(_, Literal(0, _)) => Literal(0)
    }
  }
}

// 2. è‡ªå®šä¹‰å‡½æ•°
case class CustomFunction(child: Expression) extends UnaryExpression {
  
  override def dataType: DataType = StringType
  
  override def nullSafeEval(input: Any): Any = {
    s"custom_${input.toString}"
  }
  
  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    val eval = child.genCode(ctx)
    ev.copy(code = code"""
      ${eval.code}
      boolean ${ev.isNull} = ${eval.isNull};
      ${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};
      if (!${ev.isNull}) {
        ${ev.value} = "custom_" + ${eval.value}.toString();
      }
    """)
  }
}

// 3. æ‰©å±•æ³¨å†Œ
class CustomSparkSessionExtension extends (SparkSessionExtensions => Unit) {
  
  override def apply(extensions: SparkSessionExtensions): Unit = {
    // æ³¨å†Œä¼˜åŒ–è§„åˆ™
    extensions.injectOptimizerRule { _ =>
      CustomOptimizationRule
    }
    
    // æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
    extensions.injectFunction(FunctionIdentifier("custom_func") -> {
      case Seq(child) => CustomFunction(child)
    })
    
    // æ³¨å†Œè§£æè§„åˆ™
    extensions.injectParser { (_, parser) =>
      new CustomSqlParser(parser)
    }
  }
}
```

### ç¬¬ä¸‰æ–¹é›†æˆ

**Kafkaé›†æˆç¤ºä¾‹**ï¼š
```scala
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.functions._

// 1. è¯»å–Kafkaæ•°æ®
val kafkaDF = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "input-topic")
  .option("startingOffsets", "latest")
  .option("kafka.security.protocol", "SASL_SSL")
  .option("kafka.sasl.mechanism", "PLAIN")
  .load()

// 2. æ•°æ®å¤„ç†
val processedDF = kafkaDF
  .select(
    col("key").cast("string"),
    col("value").cast("string"),
    col("timestamp"),
    col("partition"),
    col("offset")
  )
  .withColumn("processed_time", current_timestamp())
  .withColumn("data", from_json(col("value"), schema))
  .select("key", "data.*", "processed_time", "partition", "offset")

// 3. å†™å…¥Kafka
val query = processedDF
  .select(
    col("key"),
    to_json(struct(col("*"))).alias("value")
  )
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("topic", "output-topic")
  .option("checkpointLocation", "/tmp/checkpoint")
  .trigger(Trigger.ProcessingTime("10 seconds"))
  .start()

query.awaitTermination()
```

**Delta Lakeé›†æˆ**ï¼š
```scala
import io.delta.tables._
import org.apache.spark.sql.functions._

// 1. åˆ›å»ºDeltaè¡¨
val deltaTable = DeltaTable.create(spark)
  .tableName("events")
  .addColumn("id", "LONG")
  .addColumn("timestamp", "TIMESTAMP")
  .addColumn("user_id", "STRING")
  .addColumn("event_type", "STRING")
  .addColumn("properties", "MAP<STRING, STRING>")
  .partitionedBy("event_type")
  .execute()

// 2. æ‰¹é‡å†™å…¥
df.write
  .format("delta")
  .mode("append")
  .option("mergeSchema", "true")
  .save("/path/to/delta-table")

// 3. æµå¼å†™å…¥
streamingDF
  .writeStream
  .format("delta")
  .option("checkpointLocation", "/tmp/checkpoint")
  .trigger(Trigger.ProcessingTime("1 minute"))
  .start("/path/to/delta-table")

// 4. MERGEæ“ä½œ
val deltaTable = DeltaTable.forPath(spark, "/path/to/delta-table")

deltaTable.as("target")
  .merge(
    updatesDF.as("updates"),
    "target.id = updates.id"
  )
  .whenMatched()
  .updateAll()
  .whenNotMatched()
  .insertAll()
  .execute()

// 5. æ—¶é—´æ—…è¡ŒæŸ¥è¯¢
val historicalDF = spark.read
  .format("delta")
  .option("timestampAsOf", "2023-01-01 00:00:00")
  .load("/path/to/delta-table")

// 6. ç‰ˆæœ¬å†å²
deltaTable.history().show()
```

---

## Sparkå®‰å…¨ä¸æƒé™ç®¡ç† ğŸ”

### è®¤è¯æœºåˆ¶

**Kerberosè®¤è¯é…ç½®**ï¼š
```properties
# spark-defaults.conf
spark.security.credentials.hbase.enabled=true
spark.security.credentials.hive.enabled=true
spark.sql.hive.metastore.jars=builtin
spark.sql.hive.metastore.version=2.3.7

# Kerberosé…ç½®
spark.kerberos.keytab=/path/to/user.keytab
spark.kerberos.principal=user@REALM.COM
spark.kerberos.renewal.credentials=true
```

**LDAPé›†æˆ**ï¼š
```scala
// LDAPè®¤è¯é…ç½®
val sparkConf = new SparkConf()
  .setAppName("SecureSparkApp")
  .set("spark.authenticate", "true")
  .set("spark.authenticate.secret", "shared-secret")
  .set("spark.network.crypto.enabled", "true")
  .set("spark.network.crypto.keyLength", "128")
  .set("spark.network.crypto.keyFactoryAlgorithm", "PBKDF2WithHmacSHA1")

// è‡ªå®šä¹‰è®¤è¯æä¾›è€…
class LDAPAuthenticationProvider extends AuthenticationProvider {
  
  override def authenticate(username: String, password: String): Boolean = {
    try {
      val env = new Hashtable[String, String]()
      env.put(Context.INITIAL_CONTEXT_FACTORY, "com.sun.jndi.ldap.LdapCtxFactory")
      env.put(Context.PROVIDER_URL, "ldap://ldap.company.com:389")
      env.put(Context.SECURITY_AUTHENTICATION, "simple")
      env.put(Context.SECURITY_PRINCIPAL, s"uid=$username,ou=users,dc=company,dc=com")
      env.put(Context.SECURITY_CREDENTIALS, password)
      
      val ctx = new InitialDirContext(env)
      ctx.close()
      true
    } catch {
      case _: Exception => false
    }
  }
}
```

### æˆæƒæ§åˆ¶

**åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶(RBAC)**ï¼š
```scala
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
import org.apache.spark.sql.execution.command.DDLUtils

// 1. æƒé™æ£€æŸ¥å™¨
class SparkAuthorizationChecker {
  
  private val userPermissions = Map(
    "admin" -> Set("SELECT", "INSERT", "UPDATE", "DELETE", "CREATE", "DROP"),
    "analyst" -> Set("SELECT"),
    "developer" -> Set("SELECT", "INSERT", "CREATE")
  )
  
  def checkPermission(user: String, operation: String, resource: String): Boolean = {
    val permissions = userPermissions.getOrElse(user, Set.empty)
    permissions.contains(operation.toUpperCase)
  }
  
  def authorizeQuery(user: String, plan: LogicalPlan): Unit = {
    plan.foreach {
      case UnresolvedRelation(tableIdentifier, _, _) =>
        val tableName = tableIdentifier.table
        if (!checkPermission(user, "SELECT", tableName)) {
          throw new SecurityException(s"User $user does not have SELECT permission on table $tableName")
        }
      case _ =>
    }
  }
}

// 2. è‡ªå®šä¹‰æˆæƒè§„åˆ™
class CustomAuthorizationRule(authChecker: SparkAuthorizationChecker) extends Rule[LogicalPlan] {
  
  override def apply(plan: LogicalPlan): LogicalPlan = {
    val currentUser = getCurrentUser()
    authChecker.authorizeQuery(currentUser, plan)
    plan
  }
  
  private def getCurrentUser(): String = {
    // ä»å®‰å…¨ä¸Šä¸‹æ–‡è·å–å½“å‰ç”¨æˆ·
    System.getProperty("user.name", "anonymous")
  }
}
```

**åˆ—çº§æƒé™æ§åˆ¶**ï¼š
```scala
// åˆ—çº§æƒé™é…ç½®
case class ColumnPermission(
  user: String,
  table: String,
  column: String,
  permission: String
)

class ColumnLevelSecurity {
  
  private val columnPermissions = Seq(
    ColumnPermission("analyst", "users", "name", "SELECT"),
    ColumnPermission("analyst", "users", "email", "DENY"),
    ColumnPermission("admin", "users", "*", "SELECT")
  )
  
  def filterColumns(user: String, tableName: String, columns: Seq[String]): Seq[String] = {
    columns.filter { column =>
      columnPermissions.exists { perm =>
        perm.user == user && 
        perm.table == tableName && 
        (perm.column == column || perm.column == "*") &&
        perm.permission == "SELECT"
      }
    }
  }
  
  def maskSensitiveData(df: DataFrame, user: String): DataFrame = {
    val tableName = getTableName(df)
    var maskedDF = df
    
    // æ ¹æ®ç”¨æˆ·æƒé™maskæ•æ„Ÿåˆ—
    columnPermissions.filter(_.user == user && _.table == tableName).foreach { perm =>
      if (perm.permission == "MASK") {
        maskedDF = maskedDF.withColumn(perm.column, lit("***"))
      } else if (perm.permission == "DENY") {
        maskedDF = maskedDF.drop(perm.column)
      }
    }
    
    maskedDF
  }
  
  private def getTableName(df: DataFrame): String = {
    // ä»DataFrameè·å–è¡¨åçš„é€»è¾‘
    "unknown"
  }
}
```

### æ•°æ®åŠ å¯†

**ä¼ è¾“åŠ å¯†é…ç½®**ï¼š
```properties
# ç½‘ç»œä¼ è¾“åŠ å¯†
spark.network.crypto.enabled=true
spark.network.crypto.keyLength=128
spark.network.crypto.keyFactoryAlgorithm=PBKDF2WithHmacSHA1

# SSLé…ç½®
spark.ssl.enabled=true
spark.ssl.port=7077
spark.ssl.keyStore=/path/to/keystore.jks
spark.ssl.keyStorePassword=password
spark.ssl.keyPassword=password
spark.ssl.trustStore=/path/to/truststore.jks
spark.ssl.trustStorePassword=password
spark.ssl.protocol=TLSv1.2
```

**å­˜å‚¨åŠ å¯†å®ç°**ï¼š
```scala
import javax.crypto.Cipher
import javax.crypto.spec.{GCMParameterSpec, SecretKeySpec}
import java.security.SecureRandom
import java.util.Base64

// 1. æ•°æ®åŠ å¯†å·¥å…·
class DataEncryption {
  
  private val ALGORITHM = "AES"
  private val TRANSFORMATION = "AES/GCM/NoPadding"
  private val GCM_IV_LENGTH = 12
  private val GCM_TAG_LENGTH = 16
  
  def encrypt(data: String, key: String): String = {
    val cipher = Cipher.getInstance(TRANSFORMATION)
    val secretKey = new SecretKeySpec(key.getBytes(), ALGORITHM)
    
    val iv = new Array[Byte](GCM_IV_LENGTH)
    new SecureRandom().nextBytes(iv)
    
    val parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv)
    cipher.init(Cipher.ENCRYPT_MODE, secretKey, parameterSpec)
    
    val encryptedData = cipher.doFinal(data.getBytes())
    val encryptedWithIv = iv ++ encryptedData
    
    Base64.getEncoder.encodeToString(encryptedWithIv)
  }
  
  def decrypt(encryptedData: String, key: String): String = {
    val cipher = Cipher.getInstance(TRANSFORMATION)
    val secretKey = new SecretKeySpec(key.getBytes(), ALGORITHM)
    
    val decodedData = Base64.getDecoder.decode(encryptedData)
    val iv = decodedData.take(GCM_IV_LENGTH)
    val encrypted = decodedData.drop(GCM_IV_LENGTH)
    
    val parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv)
    cipher.init(Cipher.DECRYPT_MODE, secretKey, parameterSpec)
    
    new String(cipher.doFinal(encrypted))
  }
}

// 2. åŠ å¯†UDF
class EncryptionUDFs(encryption: DataEncryption, encryptionKey: String) {
  
  val encryptUDF = udf((data: String) => {
    if (data != null) encryption.encrypt(data, encryptionKey) else null
  })
  
  val decryptUDF = udf((encryptedData: String) => {
    if (encryptedData != null) encryption.decrypt(encryptedData, encryptionKey) else null
  })
}

// 3. ä½¿ç”¨ç¤ºä¾‹
val encryption = new DataEncryption()
val encryptionKey = "MySecretKey12345" // å®é™…åº”ç”¨ä¸­åº”ä»å®‰å…¨å­˜å‚¨è·å–
val udfs = new EncryptionUDFs(encryption, encryptionKey)

// åŠ å¯†æ•æ„Ÿæ•°æ®
val encryptedDF = df.withColumn("encrypted_email", udfs.encryptUDF(col("email")))
  .withColumn("encrypted_phone", udfs.encryptUDF(col("phone")))
  .drop("email", "phone")

// è§£å¯†æ•°æ®
val decryptedDF = encryptedDF
  .withColumn("email", udfs.decryptUDF(col("encrypted_email")))
  .withColumn("phone", udfs.decryptUDF(col("encrypted_phone")))
```

### å®¡è®¡æ—¥å¿—

**å®¡è®¡æ—¥å¿—å®ç°**ï¼š
```scala
import org.apache.spark.sql.util.QueryExecutionListener
import java.time.LocalDateTime
import java.time.format.DateTimeFormatter

// 1. å®¡è®¡äº‹ä»¶
case class AuditEvent(
  timestamp: String,
  user: String,
  operation: String,
  resource: String,
  success: Boolean,
  duration: Long,
  details: String
)

// 2. å®¡è®¡æ—¥å¿—è®°å½•å™¨
class AuditLogger {
  
  private val dateFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")
  
  def logEvent(event: AuditEvent): Unit = {
    val logMessage = s"[AUDIT] ${event.timestamp} | ${event.user} | ${event.operation} | " +
      s"${event.resource} | ${if (event.success) "SUCCESS" else "FAILED"} | " +
      s"${event.duration}ms | ${event.details}"
    
    // å†™å…¥å®¡è®¡æ—¥å¿—æ–‡ä»¶
    writeToAuditLog(logMessage)
    
    // å‘é€åˆ°å¤–éƒ¨å®¡è®¡ç³»ç»Ÿ
    sendToAuditSystem(event)
  }
  
  private def writeToAuditLog(message: String): Unit = {
    // å†™å…¥æœ¬åœ°å®¡è®¡æ—¥å¿—æ–‡ä»¶
    val writer = new java.io.FileWriter("/var/log/spark/audit.log", true)
    try {
      writer.write(message + "\n")
    } finally {
      writer.close()
    }
  }
  
  private def sendToAuditSystem(event: AuditEvent): Unit = {
    // å‘é€åˆ°å¤–éƒ¨å®¡è®¡ç³»ç»Ÿï¼ˆå¦‚ELKã€Splunkç­‰ï¼‰
    // è¿™é‡Œå¯ä»¥å®ç°HTTPè¯·æ±‚æˆ–æ¶ˆæ¯é˜Ÿåˆ—å‘é€
  }
}

// 3. æŸ¥è¯¢æ‰§è¡Œç›‘å¬å™¨
class AuditQueryExecutionListener extends QueryExecutionListener {
  
  private val auditLogger = new AuditLogger()
  
  override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = {
    val event = AuditEvent(
      timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")),
      user = getCurrentUser(),
      operation = "QUERY",
      resource = extractTableNames(qe.logical).mkString(","),
      success = true,
      duration = durationNs / 1000000, // è½¬æ¢ä¸ºæ¯«ç§’
      details = qe.logical.toString
    )
    
    auditLogger.logEvent(event)
  }
  
  override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {
    val event = AuditEvent(
      timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")),
      user = getCurrentUser(),
      operation = "QUERY",
      resource = extractTableNames(qe.logical).mkString(","),
      success = false,
      duration = 0,
      details = exception.getMessage
    )
    
    auditLogger.logEvent(event)
  }
  
  private def getCurrentUser(): String = {
    System.getProperty("user.name", "unknown")
  }
  
  private def extractTableNames(plan: LogicalPlan): Seq[String] = {
    plan.collect {
      case UnresolvedRelation(tableIdentifier, _, _) => tableIdentifier.table
    }
  }
}

// 4. æ³¨å†Œå®¡è®¡ç›‘å¬å™¨
spark.listenerManager.register(new AuditQueryExecutionListener())
```

---

## Sparkç›‘æ§ä¸è¿ç»´ ğŸ“Š

### ç›‘æ§ä½“ç³»

**å¤šå±‚æ¬¡ç›‘æ§æ¶æ„**ï¼š

```mermaid
graph TD
    A[åº”ç”¨å±‚ç›‘æ§] --> B[Spark UI]
    A --> C[è‡ªå®šä¹‰æŒ‡æ ‡]
    A --> D[ä¸šåŠ¡æŒ‡æ ‡]
    
    E[ç³»ç»Ÿå±‚ç›‘æ§] --> F[JVMæŒ‡æ ‡]
    E --> G[ç³»ç»Ÿèµ„æº]
    E --> H[ç½‘ç»œI/O]
    
    I[åŸºç¡€è®¾æ–½ç›‘æ§] --> J[é›†ç¾¤çŠ¶æ€]
    I --> K[å­˜å‚¨ç³»ç»Ÿ]
    I --> L[ç½‘ç»œçŠ¶æ€]
    
    M[ç›‘æ§å¹³å°] --> N[Prometheus]
    M --> O[Grafana]
    M --> P[ELK Stack]
    
    style A fill:#e1f5fe
    style E fill:#fff3e0
    style I fill:#e8f5e8
    style M fill:#f3e5f5
```

**è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†**ï¼š
```scala
import org.apache.spark.util.AccumulatorV2
import org.apache.spark.metrics.source.Source
import com.codahale.metrics.{Counter, Histogram, MetricRegistry}

// 1. è‡ªå®šä¹‰ç´¯åŠ å™¨
class CustomMetricsAccumulator extends AccumulatorV2[String, Map[String, Long]] {
  
  private var _metrics = Map.empty[String, Long]
  
  override def isZero: Boolean = _metrics.isEmpty
  
  override def copy(): AccumulatorV2[String, Map[String, Long]] = {
    val acc = new CustomMetricsAccumulator()
    acc._metrics = _metrics
    acc
  }
  
  override def reset(): Unit = {
    _metrics = Map.empty
  }
  
  override def add(v: String): Unit = {
    _metrics = _metrics + (v -> (_metrics.getOrElse(v, 0L) + 1L))
  }
  
  override def merge(other: AccumulatorV2[String, Map[String, Long]]): Unit = {
    other match {
      case acc: CustomMetricsAccumulator =>
        acc._metrics.foreach { case (key, value) =>
          _metrics = _metrics + (key -> (_metrics.getOrElse(key, 0L) + value))
        }
    }
  }
  
  override def value: Map[String, Long] = _metrics
}

// 2. è‡ªå®šä¹‰æŒ‡æ ‡æº
class CustomMetricsSource extends Source {
  
  override val sourceName: String = "custom"
  override val metricRegistry: MetricRegistry = new MetricRegistry()
  
  // è®¡æ•°å™¨
  val recordsProcessed: Counter = metricRegistry.counter("records_processed")
  val errorsCount: Counter = metricRegistry.counter("errors_count")
  
  // ç›´æ–¹å›¾
  val processingTime: Histogram = metricRegistry.histogram("processing_time")
  val batchSize: Histogram = metricRegistry.histogram("batch_size")
  
  def incrementRecordsProcessed(count: Long): Unit = {
    recordsProcessed.inc(count)
  }
  
  def incrementErrors(): Unit = {
    errorsCount.inc()
  }
  
  def recordProcessingTime(timeMs: Long): Unit = {
    processingTime.update(timeMs)
  }
  
  def recordBatchSize(size: Long): Unit = {
    batchSize.update(size)
  }
}

// 3. æŒ‡æ ‡æ”¶é›†å™¨
class MetricsCollector(spark: SparkSession) {
  
  private val customSource = new CustomMetricsSource()
  private val metricsAccumulator = new CustomMetricsAccumulator()
  
  // æ³¨å†ŒæŒ‡æ ‡æº
  spark.sparkContext.env.metricsSystem.registerSource(customSource)
  spark.sparkContext.register(metricsAccumulator, "customMetrics")
  
  def collectDataFrameMetrics(df: DataFrame): DataFrame = {
    df.mapPartitions { iter =>
      val startTime = System.currentTimeMillis()
      val records = iter.toList
      val endTime = System.currentTimeMillis()
      
      // è®°å½•æŒ‡æ ‡
      customSource.incrementRecordsProcessed(records.length)
      customSource.recordProcessingTime(endTime - startTime)
      customSource.recordBatchSize(records.length)
      
      // ç´¯åŠ å™¨è®°å½•
      metricsAccumulator.add("partition_processed")
      
      records.iterator
    }
  }
  
  def getMetrics: Map[String, Any] = {
    Map(
      "records_processed" -> customSource.recordsProcessed.getCount,
      "errors_count" -> customSource.errorsCount.getCount,
      "avg_processing_time" -> customSource.processingTime.getSnapshot.getMean,
      "avg_batch_size" -> customSource.batchSize.getSnapshot.getMean,
      "custom_metrics" -> metricsAccumulator.value
    )
  }
}
```

**Prometheusé›†æˆ**ï¼š
```scala
import io.prometheus.client.{Counter, Histogram, CollectorRegistry}
import io.prometheus.client.exporter.HTTPServer
import io.prometheus.client.hotspot.DefaultExports

// 1. PrometheusæŒ‡æ ‡å®šä¹‰
object PrometheusMetrics {
  
  // æ³¨å†ŒJVMæŒ‡æ ‡
  DefaultExports.initialize()
  
  // è‡ªå®šä¹‰æŒ‡æ ‡
  val sparkJobsTotal: Counter = Counter.build()
    .name("spark_jobs_total")
    .help("Total number of Spark jobs")
    .labelNames("app_name", "status")
    .register()
  
  val sparkJobDuration: Histogram = Histogram.build()
    .name("spark_job_duration_seconds")
    .help("Spark job duration in seconds")
    .labelNames("app_name", "job_id")
    .register()
  
  val sparkStagesTotal: Counter = Counter.build()
    .name("spark_stages_total")
    .help("Total number of Spark stages")
    .labelNames("app_name", "status")
    .register()
  
  val sparkTasksTotal: Counter = Counter.build()
    .name("spark_tasks_total")
    .help("Total number of Spark tasks")
    .labelNames("app_name", "stage_id", "status")
    .register()
  
  val sparkExecutorMemoryUsage: Histogram = Histogram.build()
    .name("spark_executor_memory_usage_bytes")
    .help("Spark executor memory usage in bytes")
    .labelNames("app_name", "executor_id")
    .register()
}

// 2. Prometheusç›‘å¬å™¨
class PrometheusSparkListener extends SparkListener {
  
  import PrometheusMetrics._
  
  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = {
    // åº”ç”¨å¯åŠ¨æ—¶çš„å¤„ç†
  }
  
  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {
    val appName = jobStart.properties.getProperty("spark.app.name", "unknown")
    sparkJobsTotal.labels(appName, "started").inc()
  }
  
  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {
    val appName = jobEnd.properties.getProperty("spark.app.name", "unknown")
    val status = if (jobEnd.jobResult == JobSucceeded) "succeeded" else "failed"
    
    sparkJobsTotal.labels(appName, status).inc()
    
    val duration = (jobEnd.time - jobEnd.time) / 1000.0
    sparkJobDuration.labels(appName, jobEnd.jobId.toString).observe(duration)
  }
  
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
    val stageInfo = stageCompleted.stageInfo
    val appName = stageInfo.properties.getProperty("spark.app.name", "unknown")
    val status = if (stageInfo.failureReason.isEmpty) "succeeded" else "failed"
    
    sparkStagesTotal.labels(appName, status).inc()
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val appName = taskEnd.stageAttemptId.toString // ç®€åŒ–å¤„ç†
    val stageId = taskEnd.stageId.toString
    val status = taskEnd.reason match {
      case Success => "succeeded"
      case _ => "failed"
    }
    
    sparkTasksTotal.labels(appName, stageId, status).inc()
  }
  
  override def onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = {
    val appName = "current_app" // ç®€åŒ–å¤„ç†
    val executorId = executorMetricsUpdate.execId
    
    executorMetricsUpdate.executorUpdates.foreach { case (taskId, stageId, stageAttemptId, accumUpdates) =>
      accumUpdates.foreach { case (accumId, update) =>
        // æ ¹æ®ç´¯åŠ å™¨ç±»å‹è®°å½•ä¸åŒæŒ‡æ ‡
        if (accumId == 0) { // å‡è®¾0æ˜¯å†…å­˜ä½¿ç”¨ç´¯åŠ å™¨
          sparkExecutorMemoryUsage.labels(appName, executorId).observe(update.toDouble)
        }
      }
    }
  }
}

// 3. å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨
class PrometheusServer(port: Int = 8080) {
  
  private var server: HTTPServer = _
  
  def start(): Unit = {
    server = new HTTPServer(port)
    println(s"Prometheus metrics server started on port $port")
  }
  
  def stop(): Unit = {
    if (server != null) {
      server.stop()
    }
  }
}

// 4. ä½¿ç”¨ç¤ºä¾‹
val prometheusServer = new PrometheusServer(8080)
prometheusServer.start()

spark.sparkContext.addSparkListener(new PrometheusSparkListener())
```

### å‘Šè­¦æœºåˆ¶

**å‘Šè­¦è§„åˆ™é…ç½®**ï¼š
```yaml
# prometheus-alerts.yml
groups:
  - name: spark-alerts
    rules:
      # ä½œä¸šå¤±è´¥å‘Šè­¦
      - alert: SparkJobFailed
        expr: increase(spark_jobs_total{status="failed"}[5m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Spark job failed"
          description: "Spark job {{ $labels.job_id }} in application {{ $labels.app_name }} has failed"
      
      # ä½œä¸šæ‰§è¡Œæ—¶é—´è¿‡é•¿å‘Šè­¦
      - alert: SparkJobLongRunning
        expr: spark_job_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Spark job running too long"
          description: "Spark job {{ $labels.job_id }} has been running for more than 1 hour"
      
      # Executorå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜å‘Šè­¦
      - alert: SparkExecutorHighMemoryUsage
        expr: spark_executor_memory_usage_bytes / (1024*1024*1024) > 8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Spark executor high memory usage"
          description: "Executor {{ $labels.executor_id }} memory usage is above 8GB"
      
      # ä»»åŠ¡å¤±è´¥ç‡è¿‡é«˜å‘Šè­¦
      - alert: SparkHighTaskFailureRate
        expr: rate(spark_tasks_total{status="failed"}[5m]) / rate(spark_tasks_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High Spark task failure rate"
          description: "Task failure rate is above 10% in the last 5 minutes"
```

**è‡ªå®šä¹‰å‘Šè­¦å¤„ç†å™¨**ï¼š
```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global
import java.time.LocalDateTime

// 1. å‘Šè­¦äº‹ä»¶
case class AlertEvent(
  alertName: String,
  severity: String,
  message: String,
  timestamp: LocalDateTime,
  labels: Map[String, String],
  value: Double
)

// 2. å‘Šè­¦å¤„ç†å™¨æ¥å£
trait AlertHandler {
  def handleAlert(alert: AlertEvent): Future[Unit]
}

// 3. é‚®ä»¶å‘Šè­¦å¤„ç†å™¨
class EmailAlertHandler(smtpConfig: SMTPConfig) extends AlertHandler {
  
  override def handleAlert(alert: AlertEvent): Future[Unit] = {
    Future {
      val subject = s"[${alert.severity.toUpperCase}] ${alert.alertName}"
      val body = generateEmailBody(alert)
      
      sendEmail(smtpConfig.recipients, subject, body)
    }
  }
  
  private def generateEmailBody(alert: AlertEvent): String = {
    s"""
       |Alert: ${alert.alertName}
       |Severity: ${alert.severity}
       |Time: ${alert.timestamp}
       |Message: ${alert.message}
       |Value: ${alert.value}
       |
       |Labels:
       |${alert.labels.map { case (k, v) => s"  $k: $v" }.mkString("\n")}
       |
       |Please check the Spark application and take necessary actions.
       |""".stripMargin
  }
  
  private def sendEmail(recipients: List[String], subject: String, body: String): Unit = {
    // å®ç°é‚®ä»¶å‘é€é€»è¾‘
  }
}

// 4. Slackå‘Šè­¦å¤„ç†å™¨
class SlackAlertHandler(webhookUrl: String) extends AlertHandler {
  
  override def handleAlert(alert: AlertEvent): Future[Unit] = {
    Future {
      val payload = createSlackPayload(alert)
      sendToSlack(payload)
    }
  }
  
  private def createSlackPayload(alert: AlertEvent): String = {
    val color = alert.severity match {
      case "critical" => "danger"
      case "warning" => "warning"
      case _ => "good"
    }
    
    s"""
       |{
       |  "attachments": [
       |    {
       |      "color": "$color",
       |      "title": "${alert.alertName}",
       |      "text": "${alert.message}",
       |      "fields": [
       |        {
       |          "title": "Severity",
       |          "value": "${alert.severity}",
       |          "short": true
       |        },
       |        {
       |          "title": "Value",
       |          "value": "${alert.value}",
       |          "short": true
       |        },
       |        {
       |          "title": "Time",
       |          "value": "${alert.timestamp}",
       |          "short": false
       |        }
       |      ]
       |    }
       |  ]
       |}
       |""".stripMargin
  }
  
  private def sendToSlack(payload: String): Unit = {
    // å®ç°Slack webhookå‘é€é€»è¾‘
  }
}

// 5. å‘Šè­¦ç®¡ç†å™¨
class AlertManager {
  
  private val handlers = List[AlertHandler](
    new EmailAlertHandler(SMTPConfig(List("admin@company.com"))),
    new SlackAlertHandler("https://hooks.slack.com/services/...")
  )
  
  private val alertThresholds = Map(
    "job_failure_rate" -> 0.05,
    "memory_usage_gb" -> 8.0,
    "job_duration_hours" -> 2.0
  )
  
  def checkAndAlert(metrics: Map[String, Double]): Unit = {
    metrics.foreach { case (metricName, value) =>
      alertThresholds.get(metricName).foreach { threshold =>
        if (value > threshold) {
          val alert = AlertEvent(
            alertName = s"${metricName}_threshold_exceeded",
            severity = determineSeverity(metricName, value, threshold),
            message = s"$metricName value $value exceeds threshold $threshold",
            timestamp = LocalDateTime.now(),
            labels = Map("metric" -> metricName),
            value = value
          )
          
          handlers.foreach(_.handleAlert(alert))
        }
      }
    }
  }
  
  private def determineSeverity(metricName: String, value: Double, threshold: Double): String = {
    val ratio = value / threshold
    if (ratio > 2.0) "critical"
    else if (ratio > 1.5) "warning"
    else "info"
  }
}

case class SMTPConfig(recipients: List[String])
```

### æ—¥å¿—ç®¡ç†

**ç»“æ„åŒ–æ—¥å¿—é…ç½®**ï¼š
```xml
<!-- log4j2.xml -->
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">
  <Appenders>
    <!-- æ§åˆ¶å°è¾“å‡º -->
    <Console name="Console" target="SYSTEM_OUT">
      <JsonLayout compact="true" eventEol="true">
        <KeyValuePair key="timestamp" value="$${date:yyyy-MM-dd HH:mm:ss.SSS}"/>
        <KeyValuePair key="level" value="$${level}"/>
        <KeyValuePair key="logger" value="$${logger}"/>
        <KeyValuePair key="thread" value="$${thread}"/>
        <KeyValuePair key="app_name" value="$${sys:spark.app.name:-unknown}"/>
        <KeyValuePair key="executor_id" value="$${sys:spark.executor.id:-driver}"/>
      </JsonLayout>
    </Console>
    
    <!-- æ–‡ä»¶è¾“å‡º -->
    <RollingFile name="FileAppender" fileName="logs/spark-app.log"
                 filePattern="logs/spark-app-%d{yyyy-MM-dd}-%i.log.gz">
      <JsonLayout compact="true" eventEol="true">
        <KeyValuePair key="timestamp" value="$${date:yyyy-MM-dd HH:mm:ss.SSS}"/>
        <KeyValuePair key="level" value="$${level}"/>
        <KeyValuePair key="logger" value="$${logger}"/>
        <KeyValuePair key="message" value="$${message}"/>
        <KeyValuePair key="exception" value="$${exception}"/>
        <KeyValuePair key="app_name" value="$${sys:spark.app.name:-unknown}"/>
        <KeyValuePair key="executor_id" value="$${sys:spark.executor.id:-driver}"/>
      </JsonLayout>
      <Policies>
        <TimeBasedTriggeringPolicy />
        <SizeBasedTriggeringPolicy size="100 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>
    
    <!-- ELKè¾“å‡º -->
    <Socket name="ELKAppender" host="logstash.company.com" port="5044">
      <JsonLayout compact="true" eventEol="true"/>
    </Socket>
  </Appenders>
  
  <Loggers>
    <Logger name="org.apache.spark" level="WARN"/>
    <Logger name="org.apache.hadoop" level="WARN"/>
    <Logger name="com.company.spark" level="INFO"/>
    
    <Root level="INFO">
      <AppenderRef ref="Console"/>
      <AppenderRef ref="FileAppender"/>
      <AppenderRef ref="ELKAppender"/>
    </Root>
  </Loggers>
</Configuration>
```

**è‡ªå®šä¹‰æ—¥å¿—æ”¶é›†å™¨**ï¼š
```scala
import org.apache.logging.log4j.LogManager
import org.apache.logging.log4j.core.LogEvent
import org.apache.logging.log4j.core.appender.AbstractAppender
import org.apache.logging.log4j.core.config.plugins.{Plugin, PluginAttribute, PluginFactory}

// 1. è‡ªå®šä¹‰æ—¥å¿—æ”¶é›†å™¨
@Plugin(name = "CustomLogCollector", category = "Core", elementType = "appender")
class CustomLogCollector(name: String) extends AbstractAppender(name, null, null, true, null) {
  
  private val logger = LogManager.getLogger(classOf[CustomLogCollector])
  private val logBuffer = scala.collection.mutable.ListBuffer[LogEvent]()
  
  override def append(event: LogEvent): Unit = {
    synchronized {
      logBuffer += event.toImmutable
      
      // æ‰¹é‡å¤„ç†æ—¥å¿—
      if (logBuffer.size >= 100) {
        flushLogs()
      }
    }
  }
  
  private def flushLogs(): Unit = {
    val logs = logBuffer.toList
    logBuffer.clear()
    
    // å¼‚æ­¥å¤„ç†æ—¥å¿—
    Future {
      processLogs(logs)
    }
  }
  
  private def processLogs(logs: List[LogEvent]): Unit = {
    logs.foreach { event =>
      // è§£ææ—¥å¿—äº‹ä»¶
      val logData = Map(
        "timestamp" -> event.getTimeMillis,
        "level" -> event.getLevel.toString,
        "logger" -> event.getLoggerName,
        "message" -> event.getMessage.getFormattedMessage,
        "thread" -> event.getThreadName
      )
      
      // å‘é€åˆ°å¤–éƒ¨ç³»ç»Ÿ
      sendToExternalSystem(logData)
    }
  }
  
  private def sendToExternalSystem(logData: Map[String, Any]): Unit = {
    // å‘é€åˆ°Kafkaã€ELKç­‰å¤–éƒ¨ç³»ç»Ÿ
  }
}

// 2. æ—¥å¿—åˆ†æå™¨
class LogAnalyzer {
  
  private val errorPatterns = List(
    "OutOfMemoryError",
    "FetchFailedException",
    "TaskFailedException",
    "ExecutorLostFailure"
  )
  
  def analyzeLogFile(filePath: String): LogAnalysisResult = {
    val lines = scala.io.Source.fromFile(filePath).getLines().toList
    
    val errorCounts = errorPatterns.map { pattern =>
      pattern -> lines.count(_.contains(pattern))
    }.toMap
    
    val totalErrors = errorCounts.values.sum
    val warningCount = lines.count(_.contains("WARN"))
    val infoCount = lines.count(_.contains("INFO"))
    
    LogAnalysisResult(
      totalLines = lines.length,
      errorCount = totalErrors,
      warningCount = warningCount,
      infoCount = infoCount,
      errorBreakdown = errorCounts,
      recommendations = generateRecommendations(errorCounts)
    )
  }
  
  private def generateRecommendations(errorCounts: Map[String, Int]): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    if (errorCounts.getOrElse("OutOfMemoryError", 0) > 0) {
      recommendations += "Consider increasing executor memory or optimizing data processing"
    }
    
    if (errorCounts.getOrElse("FetchFailedException", 0) > 0) {
      recommendations += "Check network configuration and increase shuffle timeout"
    }
    
    if (errorCounts.getOrElse("ExecutorLostFailure", 0) > 0) {
      recommendations += "Check cluster stability and resource allocation"
    }
    
    recommendations.toList
  }
}

case class LogAnalysisResult(
  totalLines: Int,
  errorCount: Int,
  warningCount: Int,
  infoCount: Int,
  errorBreakdown: Map[String, Int],
  recommendations: List[String]
)
```

### è¿ç»´è‡ªåŠ¨åŒ–

**è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬**ï¼š
```bash
#!/bin/bash
# spark-ops-automation.sh

# é…ç½®å‚æ•°
SPARK_HOME="/opt/spark"
HDFS_PATH="/spark/checkpoints"
LOG_PATH="/var/log/spark"
ALERT_EMAIL="admin@company.com"

# 1. å¥åº·æ£€æŸ¥å‡½æ•°
check_spark_cluster() {
    echo "Checking Spark cluster health..."
    
    # æ£€æŸ¥MasterçŠ¶æ€
    master_status=$(curl -s http://spark-master:8080/api/v1/applications | jq '.[] | select(.name=="SparkPi") | .state')
    
    if [ "$master_status" != "RUNNING" ]; then
        echo "WARNING: Spark Master is not running properly"
        send_alert "Spark Master Health Check Failed"
    fi
    
    # æ£€æŸ¥WorkerçŠ¶æ€
    worker_count=$(curl -s http://spark-master:8080/json | jq '.workers | length')
    expected_workers=4
    
    if [ "$worker_count" -lt "$expected_workers" ]; then
        echo "WARNING: Only $worker_count workers available, expected $expected_workers"
        send_alert "Insufficient Spark Workers"
    fi
    
    echo "Cluster health check completed"
}

# 2. æ¸…ç†è¿‡æœŸæ•°æ®
cleanup_old_data() {
    echo "Cleaning up old data..."
    
    # æ¸…ç†è¿‡æœŸçš„checkpointæ•°æ®
    hdfs dfs -ls $HDFS_PATH | awk '{print $8}' | while read path; do
        if [ -n "$path" ]; then
            # åˆ é™¤7å¤©å‰çš„checkpoint
            hdfs dfs -rm -r -skipTrash "$path" 2>/dev/null || true
        fi
    done
    
    # æ¸…ç†è¿‡æœŸæ—¥å¿—
    find $LOG_PATH -name "*.log" -mtime +7 -delete
    find $LOG_PATH -name "*.gz" -mtime +30 -delete
    
    echo "Cleanup completed"
}

# 3. æ€§èƒ½ç›‘æ§
monitor_performance() {
    echo "Monitoring Spark performance..."
    
    # è·å–å½“å‰è¿è¡Œçš„åº”ç”¨
    apps=$(curl -s http://spark-master:8080/api/v1/applications | jq -r '.[] | select(.state=="RUNNING") | .id')
    
    for app_id in $apps; do
        # è·å–åº”ç”¨æŒ‡æ ‡
        app_info=$(curl -s "http://spark-master:8080/api/v1/applications/$app_id")
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_usage=$(echo $app_info | jq '.executors[].memoryUsed' | awk '{sum+=$1} END {print sum}')
        memory_total=$(echo $app_info | jq '.executors[].maxMemory' | awk '{sum+=$1} END {print sum}')
        
        if [ "$memory_total" -gt 0 ]; then
            memory_percent=$(echo "scale=2; $memory_usage * 100 / $memory_total" | bc)
            
            if (( $(echo "$memory_percent > 80" | bc -l) )); then
                echo "WARNING: High memory usage for app $app_id: ${memory_percent}%"
                send_alert "High Memory Usage Alert" "Application $app_id memory usage: ${memory_percent}%"
            fi
        fi
        
        # æ£€æŸ¥å¤±è´¥ä»»åŠ¡
        failed_tasks=$(echo $app_info | jq '.stages[].numFailedTasks' | awk '{sum+=$1} END {print sum}')
        
        if [ "$failed_tasks" -gt 10 ]; then
            echo "WARNING: High task failure rate for app $app_id: $failed_tasks failed tasks"
            send_alert "High Task Failure Rate" "Application $app_id has $failed_tasks failed tasks"
        fi
    done
    
    echo "Performance monitoring completed"
}

# 4. è‡ªåŠ¨é‡å¯å¤±è´¥çš„åº”ç”¨
restart_failed_apps() {
    echo "Checking for failed applications..."
    
    failed_apps=$(curl -s http://spark-master:8080/api/v1/applications | jq -r '.[] | select(.state=="FAILED") | .id')
    
    for app_id in $failed_apps; do
        echo "Found failed application: $app_id"
        
        # è·å–åº”ç”¨é…ç½®
        app_config=$(curl -s "http://spark-master:8080/api/v1/applications/$app_id" | jq '.sparkProperties')
        
        # é‡å¯åº”ç”¨ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µå®ç°ï¼‰
        restart_application "$app_id" "$app_config"
    done
}

# 5. å‘é€å‘Šè­¦
send_alert() {
    local subject="$1"
    local message="$2"
    
    echo "Sending alert: $subject"
    echo "$message" | mail -s "$subject" "$ALERT_EMAIL"
    
    # åŒæ—¶å‘é€åˆ°Slackï¼ˆå¦‚æœé…ç½®äº†webhookï¼‰
    if [ -n "$SLACK_WEBHOOK" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"$subject: $message\"}" \
            "$SLACK_WEBHOOK"
    fi
}

# 6. ä¸»å‡½æ•°
main() {
    echo "Starting Spark automation tasks at $(date)"
    
    case "$1" in
        "health-check")
            check_spark_cluster
            ;;
        "cleanup")
            cleanup_old_data
            ;;
        "monitor")
            monitor_performance
            ;;
        "restart")
            restart_failed_apps
            ;;
        "all")
            check_spark_cluster
            cleanup_old_data
            monitor_performance
            restart_failed_apps
            ;;
        *)
            echo "Usage: $0 {health-check|cleanup|monitor|restart|all}"
            exit 1
            ;;
    esac
    
    echo "Automation tasks completed at $(date)"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

**Kubernetesè‡ªåŠ¨åŒ–éƒ¨ç½²**ï¼š
```yaml
# spark-operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-operator
  namespace: spark-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-operator
  template:
    metadata:
      labels:
        app: spark-operator
    spec:
      serviceAccountName: spark-operator
      containers:
      - name: spark-operator
        image: gcr.io/spark-operator/spark-operator:v1beta2-1.3.8-3.1.1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        args:
        - -v=2
        - -logtostderr
        - -namespace=default
        - -enable-ui-service=true
        - -enable-webhook=true
        - -webhook-svc-namespace=spark-system
        - -webhook-port=8080
        env:
        - name: SPARK_OPERATOR_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            memory: "512Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "500m"

---
# spark-application-crd.yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi-auto
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: "gcr.io/spark-operator/spark:v3.1.1"
  imagePullPolicy: Always
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: "local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar"
  arguments:
    - "10"
  sparkVersion: "3.1.1"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.1.1
    serviceAccount: spark
  executor:
    cores: 1
    instances: 2
    memory: "512m"
    labels:
      version: 3.1.1
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
    prometheus:
      jmxExporterJar: "/prometheus/jmx_prometheus_javaagent-0.11.0.jar"
      port: 8090
```




## Sparkä¼ä¸šçº§å®è·µ ğŸ¢

### å¤§æ•°æ®å¹³å°æ¶æ„

**ä¼ä¸šçº§å¤§æ•°æ®å¹³å°æ¶æ„**ï¼š

```mermaid
graph TB
    subgraph "æ•°æ®æºå±‚"
        A1[ä¸šåŠ¡æ•°æ®åº“]
        A2[æ—¥å¿—æ–‡ä»¶]
        A3[æ¶ˆæ¯é˜Ÿåˆ—]
        A4[å¤–éƒ¨API]
    end
    
    subgraph "æ•°æ®é‡‡é›†å±‚"
        B1[Flume]
        B2[Kafka]
        B3[Sqoop]
        B4[DataX]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚"
        C1[HDFS]
        C2[HBase]
        C3[Elasticsearch]
        C4[Redis]
    end
    
    subgraph "æ•°æ®å¤„ç†å±‚"
        D1[Spark Batch]
        D2[Spark Streaming]
        D3[Flink]
        D4[Storm]
    end
    
    subgraph "æ•°æ®æœåŠ¡å±‚"
        E1[Spark SQL]
        E2[Presto]
        E3[Kylin]
        E4[Druid]
    end
    
    subgraph "åº”ç”¨å±‚"
        F1[BIæŠ¥è¡¨]
        F2[å®æ—¶å¤§å±]
        F3[æœºå™¨å­¦ä¹ ]
        F4[æ•°æ®API]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B2
    A4 --> B4
    
    B1 --> C1
    B2 --> C1
    B3 --> C2
    B4 --> C1
    
    C1 --> D1
    C1 --> D2
    C2 --> D3
    C3 --> D4
    
    D1 --> E1
    D2 --> E2
    D3 --> E3
    D4 --> E4
    
    E1 --> F1
    E2 --> F2
    E3 --> F3
    E4 --> F4
    
    style D1 fill:#e1f5fe
    style D2 fill:#e1f5fe
```

**å¹³å°æ ¸å¿ƒç»„ä»¶å®ç°**ï¼š
```scala
// 1. æ•°æ®å¹³å°ç®¡ç†å™¨
class DataPlatformManager(sparkSession: SparkSession) {
  
  private val metadataManager = new MetadataManager()
  private val jobScheduler = new JobScheduler()
  private val resourceManager = new ResourceManager()
  private val monitoringService = new MonitoringService()
  
  def submitDataPipeline(pipeline: DataPipeline): String = {
    // 1. éªŒè¯ç®¡é“é…ç½®
    validatePipeline(pipeline)
    
    // 2. åˆ†é…èµ„æº
    val resources = resourceManager.allocateResources(pipeline.resourceRequirements)
    
    // 3. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
    val executionPlan = generateExecutionPlan(pipeline)
    
    // 4. æäº¤ä½œä¸š
    val jobId = jobScheduler.submitJob(executionPlan, resources)
    
    // 5. æ³¨å†Œç›‘æ§
    monitoringService.registerJob(jobId, pipeline)
    
    jobId
  }
  
  private def validatePipeline(pipeline: DataPipeline): Unit = {
    // éªŒè¯æ•°æ®æºè¿æ¥
    pipeline.dataSources.foreach { source =>
      if (!source.isAccessible()) {
        throw new IllegalArgumentException(s"Data source ${source.name} is not accessible")
      }
    }
    
    // éªŒè¯è¾“å‡ºè·¯å¾„
    pipeline.outputs.foreach { output =>
      if (!output.isWritable()) {
        throw new IllegalArgumentException(s"Output path ${output.path} is not writable")
      }
    }
  }
  
  private def generateExecutionPlan(pipeline: DataPipeline): ExecutionPlan = {
    val stages = pipeline.transformations.map { transformation =>
      Stage(
        id = java.util.UUID.randomUUID().toString,
        transformation = transformation,
        dependencies = findDependencies(transformation, pipeline),
        resources = calculateResourceNeeds(transformation)
      )
    }
    
    ExecutionPlan(stages, pipeline.schedule)
  }
  
  private def findDependencies(transformation: Transformation, pipeline: DataPipeline): List[String] = {
    // åˆ†ææ•°æ®ä¾èµ–å…³ç³»
    transformation.inputs.flatMap { input =>
      pipeline.transformations.find(_.outputs.contains(input)).map(_.id)
    }
  }
  
  private def calculateResourceNeeds(transformation: Transformation): ResourceRequirements = {
    // æ ¹æ®æ•°æ®é‡å’Œå¤æ‚åº¦ä¼°ç®—èµ„æºéœ€æ±‚
    val dataSize = transformation.estimateDataSize()
    val complexity = transformation.getComplexity()
    
    ResourceRequirements(
      executors = math.max(2, (dataSize / (1024 * 1024 * 128)).toInt), // 128MB per executor
      memory = s"${math.max(1, (dataSize / (1024 * 1024 * 512)).toInt)}g", // 512MB per GB of data
      cores = math.max(1, complexity / 10)
    )
  }
}

// 2. æ•°æ®ç®¡é“å®šä¹‰
case class DataPipeline(
  id: String,
  name: String,
  dataSources: List[DataSource],
  transformations: List[Transformation],
  outputs: List[DataOutput],
  schedule: Schedule,
  resourceRequirements: ResourceRequirements
)

case class DataSource(
  name: String,
  sourceType: String,
  connectionConfig: Map[String, String]
) {
  def isAccessible(): Boolean = {
    sourceType match {
      case "hdfs" => checkHDFSAccess()
      case "kafka" => checkKafkaAccess()
      case "jdbc" => checkJDBCAccess()
      case _ => false
    }
  }
  
  private def checkHDFSAccess(): Boolean = {
    // æ£€æŸ¥HDFSè¿æ¥
    true
  }
  
  private def checkKafkaAccess(): Boolean = {
    // æ£€æŸ¥Kafkaè¿æ¥
    true
  }
  
  private def checkJDBCAccess(): Boolean = {
    // æ£€æŸ¥æ•°æ®åº“è¿æ¥
    true
  }
}

// 3. ä½œä¸šè°ƒåº¦å™¨
class JobScheduler {
  
  private val scheduledJobs = scala.collection.mutable.Map[String, ScheduledJob]()
  
  def submitJob(executionPlan: ExecutionPlan, resources: ResourceAllocation): String = {
    val jobId = java.util.UUID.randomUUID().toString
    
    val job = ScheduledJob(
      id = jobId,
      executionPlan = executionPlan,
      resources = resources,
      status = JobStatus.SUBMITTED,
      submitTime = System.currentTimeMillis()
    )
    
    scheduledJobs(jobId) = job
    
    // æ ¹æ®è°ƒåº¦ç­–ç•¥æ‰§è¡Œä½œä¸š
    executionPlan.schedule match {
      case CronSchedule(cronExpression) =>
        scheduleWithCron(job, cronExpression)
      case IntervalSchedule(interval) =>
        scheduleWithInterval(job, interval)
      case ImmediateSchedule =>
        executeImmediately(job)
    }
    
    jobId
  }
  
  private def scheduleWithCron(job: ScheduledJob, cronExpression: String): Unit = {
    // ä½¿ç”¨Quartzæˆ–ç±»ä¼¼çš„è°ƒåº¦æ¡†æ¶
  }
  
  private def scheduleWithInterval(job: ScheduledJob, interval: Long): Unit = {
    // å®šæœŸæ‰§è¡Œä½œä¸š
  }
  
  private def executeImmediately(job: ScheduledJob): Unit = {
    Future {
      executeJob(job)
    }
  }
  
  private def executeJob(job: ScheduledJob): Unit = {
    try {
      job.status = JobStatus.RUNNING
      
      // æŒ‰ä¾èµ–é¡ºåºæ‰§è¡ŒStage
      val sortedStages = topologicalSort(job.executionPlan.stages)
      
      sortedStages.foreach { stage =>
        executeStage(stage, job.resources)
      }
      
      job.status = JobStatus.COMPLETED
      
    } catch {
      case e: Exception =>
        job.status = JobStatus.FAILED
        job.errorMessage = Some(e.getMessage)
    }
  }
  
  private def topologicalSort(stages: List[Stage]): List[Stage] = {
    // æ‹“æ‰‘æ’åºå®ç°
    stages.sortBy(_.dependencies.size)
  }
  
  private def executeStage(stage: Stage, resources: ResourceAllocation): Unit = {
    // æ‰§è¡Œå…·ä½“çš„æ•°æ®å¤„ç†Stage
  }
}
```

### æ•°æ®æ¹–å»ºè®¾

**æ•°æ®æ¹–æ¶æ„è®¾è®¡**ï¼š
```scala
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.types._
import io.delta.tables.DeltaTable

// 1. æ•°æ®æ¹–ç®¡ç†å™¨
class DataLakeManager(spark: SparkSession) {
  
  private val basePath = "s3a://company-datalake"
  private val metadataStore = new MetadataStore()
  
  // æ•°æ®åˆ†å±‚ç»“æ„
  object Layers {
    val RAW = "raw"           // åŸå§‹æ•°æ®å±‚
    val BRONZE = "bronze"     // é’é“œå±‚ï¼ˆæ¸…æ´—åï¼‰
    val SILVER = "silver"     // é“¶å±‚ï¼ˆæ ‡å‡†åŒ–ï¼‰
    val GOLD = "gold"         // é‡‘å±‚ï¼ˆä¸šåŠ¡èšåˆï¼‰
  }
  
  def ingestRawData(
    source: String,
    data: DataFrame,
    partitionColumns: Seq[String] = Seq("year", "month", "day")
  ): Unit = {
    val rawPath = s"$basePath/${Layers.RAW}/$source"
    
    // æ·»åŠ æ‘„å…¥æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
    val enrichedData = data
      .withColumn("_ingestion_timestamp", current_timestamp())
      .withColumn("_source_system", lit(source))
      .withColumn("_data_quality_score", lit(1.0))
    
    // å†™å…¥Deltaæ ¼å¼
    enrichedData.write
      .format("delta")
      .mode("append")
      .partitionBy(partitionColumns: _*)
      .option("mergeSchema", "true")
      .save(rawPath)
    
    // æ›´æ–°å…ƒæ•°æ®
    metadataStore.registerDataset(
      layer = Layers.RAW,
      source = source,
      path = rawPath,
      schema = enrichedData.schema,
      partitionColumns = partitionColumns
    )
  }
  
  def processRawToBronze(source: String): Unit = {
    val rawPath = s"$basePath/${Layers.RAW}/$source"
    val bronzePath = s"$basePath/${Layers.BRONZE}/$source"
    
    val rawData = spark.read.format("delta").load(rawPath)
    
    // æ•°æ®æ¸…æ´—å’Œè´¨é‡æ£€æŸ¥
    val cleanedData = rawData
      .filter(col("_data_quality_score") > 0.8)  // è¿‡æ»¤ä½è´¨é‡æ•°æ®
      .dropDuplicates()                          // å»é‡
      .na.drop()                                 // åˆ é™¤ç©ºå€¼è¡Œ
      .withColumn("_processing_timestamp", current_timestamp())
    
    // æ•°æ®è´¨é‡è¯„åˆ†
    val qualityCheckedData = applyQualityRules(cleanedData, source)
    
    // å†™å…¥Bronzeå±‚
    qualityCheckedData.write
      .format("delta")
      .mode("overwrite")
      .option("overwriteSchema", "true")
      .save(bronzePath)
    
    // æ›´æ–°å…ƒæ•°æ®
    metadataStore.updateDatasetStatus(Layers.BRONZE, source, "COMPLETED")
  }
  
  def processBronzeToSilver(source: String, transformationRules: List[TransformationRule]): Unit = {
    val bronzePath = s"$basePath/${Layers.BRONZE}/$source"
    val silverPath = s"$basePath/${Layers.SILVER}/$source"
    
    var silverData = spark.read.format("delta").load(bronzePath)
    
    // åº”ç”¨æ ‡å‡†åŒ–è§„åˆ™
    transformationRules.foreach { rule =>
      silverData = applyTransformationRule(silverData, rule)
    }
    
    // æ·»åŠ ä¸šåŠ¡é”®å’Œç‰ˆæœ¬ä¿¡æ¯
    val standardizedData = silverData
      .withColumn("_business_key", generateBusinessKey(silverData.columns))
      .withColumn("_version", lit(1))
      .withColumn("_effective_date", current_date())
      .withColumn("_expiry_date", lit("9999-12-31").cast(DateType))
    
    // å†™å…¥Silverå±‚
    standardizedData.write
      .format("delta")
      .mode("overwrite")
      .save(silverPath)
  }
  
  def processSilverToGold(aggregationConfig: AggregationConfig): Unit = {
    val silverPath = s"$basePath/${Layers.SILVER}/${aggregationConfig.sourceTable}"
    val goldPath = s"$basePath/${Layers.GOLD}/${aggregationConfig.targetTable}"
    
    val silverData = spark.read.format("delta").load(silverPath)
    
    // æ‰§è¡Œä¸šåŠ¡èšåˆ
    val aggregatedData = silverData
      .groupBy(aggregationConfig.groupByColumns.map(col): _*)
      .agg(aggregationConfig.aggregations: _*)
      .withColumn("_aggregation_date", current_date())
    
    // å†™å…¥Goldå±‚
    aggregatedData.write
      .format("delta")
      .mode("overwrite")
      .save(goldPath)
  }
  
  private def applyQualityRules(data: DataFrame, source: String): DataFrame = {
    val qualityRules = metadataStore.getQualityRules(source)
    
    var qualityData = data
    qualityRules.foreach { rule =>
      qualityData = rule.ruleType match {
        case "not_null" =>
          qualityData.withColumn("_data_quality_score",
            when(col(rule.column).isNull, lit(0.0))
              .otherwise(col("_data_quality_score")))
        
        case "range_check" =>
          qualityData.withColumn("_data_quality_score",
            when(col(rule.column) < rule.minValue || col(rule.column) > rule.maxValue, lit(0.5))
              .otherwise(col("_data_quality_score")))
        
        case "format_check" =>
          qualityData.withColumn("_data_quality_score",
            when(!col(rule.column).rlike(rule.pattern), lit(0.3))
              .otherwise(col("_data_quality_score")))
        
        case _ => qualityData
      }
    }
    
    qualityData
  }
  
  private def applyTransformationRule(data: DataFrame, rule: TransformationRule): DataFrame = {
    rule.ruleType match {
      case "column_rename" =>
        data.withColumnRenamed(rule.sourceColumn, rule.targetColumn)
      
      case "data_type_cast" =>
        data.withColumn(rule.sourceColumn, col(rule.sourceColumn).cast(rule.targetDataType))
      
      case "value_mapping" =>
        var mappedData = data
        rule.valueMapping.foreach { case (oldValue, newValue) =>
          mappedData = mappedData.withColumn(rule.sourceColumn,
            when(col(rule.sourceColumn) === oldValue, newValue)
              .otherwise(col(rule.sourceColumn)))
        }
        mappedData
      
      case "calculated_column" =>
        data.withColumn(rule.targetColumn, expr(rule.expression))
      
      case _ => data
    }
  }
  
  private def generateBusinessKey(columns: Array[String]): Column = {
    // ç”Ÿæˆä¸šåŠ¡ä¸»é”®
    concat_ws("_", columns.filter(_.startsWith("id")).map(col): _*)
  }
}

// 2. æ•°æ®è¡€ç¼˜è¿½è¸ª
class DataLineageTracker {
  
  private val lineageGraph = scala.collection.mutable.Map[String, DatasetLineage]()
  
  def trackTransformation(
    sourceDatasets: List[String],
    targetDataset: String,
    transformation: String,
    timestamp: Long
  ): Unit = {
    val lineage = DatasetLineage(
      dataset = targetDataset,
      sources = sourceDatasets,
      transformation = transformation,
      timestamp = timestamp,
      dependencies = findDependencies(sourceDatasets)
    )
    
    lineageGraph(targetDataset) = lineage
  }
  
  def getLineage(dataset: String): Option[DatasetLineage] = {
    lineageGraph.get(dataset)
  }
  
  def getUpstreamDependencies(dataset: String): List[String] = {
    def collectUpstream(ds: String, visited: Set[String] = Set()): List[String] = {
      if (visited.contains(ds)) return List()
      
      lineageGraph.get(ds) match {
        case Some(lineage) =>
          lineage.sources ++ lineage.sources.flatMap(source =>
            collectUpstream(source, visited + ds))
        case None => List()
      }
    }
    
    collectUpstream(dataset).distinct
  }
  
  def getDownstreamDependencies(dataset: String): List[String] = {
    lineageGraph.values.filter(_.sources.contains(dataset)).map(_.dataset).toList
  }
  
  private def findDependencies(sources: List[String]): Map[String, String] = {
    sources.map { source =>
      source -> lineageGraph.get(source).map(_.transformation).getOrElse("unknown")
    }.toMap
  }
}

case class DatasetLineage(
  dataset: String,
  sources: List[String],
  transformation: String,
  timestamp: Long,
  dependencies: Map[String, String]
)

case class TransformationRule(
  ruleType: String,
  sourceColumn: String,
  targetColumn: String = "",
  targetDataType: String = "",
  expression: String = "",
  valueMapping: Map[String, String] = Map()
)

case class AggregationConfig(
  sourceTable: String,
  targetTable: String,
  groupByColumns: List[String],
  aggregations: List[Column]
)
```

### å®æ—¶è®¡ç®—å¹³å°

**å®æ—¶è®¡ç®—å¹³å°æ¶æ„**ï¼š
```scala
import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// 1. å®æ—¶è®¡ç®—å¹³å°ç®¡ç†å™¨
class RealTimeComputePlatform(spark: SparkSession) {
  
  private val streamingQueries = scala.collection.mutable.Map[String, StreamingQuery]()
  private val checkpointBasePath = "hdfs://cluster/checkpoints"
  
  def createRealTimeJob(jobConfig: RealTimeJobConfig): String = {
    val jobId = java.util.UUID.randomUUID().toString
    
    try {
      // 1. åˆ›å»ºè¾“å…¥æµ
      val inputStream = createInputStream(jobConfig.inputConfig)
      
      // 2. åº”ç”¨ä¸šåŠ¡é€»è¾‘
      val processedStream = applyBusinessLogic(inputStream, jobConfig.processingLogic)
      
      // 3. åˆ›å»ºè¾“å‡ºæµ
      val query = createOutputStream(processedStream, jobConfig.outputConfig, jobId)
      
      // 4. å¯åŠ¨æµå¤„ç†
      streamingQueries(jobId) = query
      
      println(s"Real-time job $jobId started successfully")
      jobId
      
    } catch {
      case e: Exception =>
        println(s"Failed to start real-time job: ${e.getMessage}")
        throw e
    }
  }
  
  private def createInputStream(inputConfig: InputConfig): DataFrame = {
    inputConfig.sourceType match {
      case "kafka" =>
        spark.readStream
          .format("kafka")
          .option("kafka.bootstrap.servers", inputConfig.brokers)
          .option("subscribe", inputConfig.topics)
          .option("startingOffsets", inputConfig.startingOffsets)
          .option("maxOffsetsPerTrigger", inputConfig.maxOffsetsPerTrigger)
          .load()
          .select(
            col("key").cast("string"),
            col("value").cast("string"),
            col("timestamp"),
            col("partition"),
            col("offset")
          )
      
      case "socket" =>
        spark.readStream
          .format("socket")
          .option("host", inputConfig.host)
          .option("port", inputConfig.port)
          .load()
      
      case "file" =>
        spark.readStream
          .format(inputConfig.format)
          .schema(inputConfig.schema)
          .option("path", inputConfig.path)
          .load()
      
      case _ =>
        throw new IllegalArgumentException(s"Unsupported source type: ${inputConfig.sourceType}")
    }
  }
  
  private def applyBusinessLogic(inputStream: DataFrame, processingLogic: ProcessingLogic): DataFrame = {
    var processedStream = inputStream
    
    // è§£æJSONæ•°æ®
    if (processingLogic.parseJson) {
      processedStream = processedStream
        .withColumn("parsed_data", from_json(col("value"), processingLogic.jsonSchema))
        .select("parsed_data.*", "timestamp", "partition", "offset")
    }
    
    // æ•°æ®è¿‡æ»¤
    processingLogic.filters.foreach { filter =>
      processedStream = processedStream.filter(expr(filter))
    }
    
    // æ•°æ®è½¬æ¢
    processingLogic.transformations.foreach { transformation =>
      processedStream = processedStream.withColumn(transformation.targetColumn, expr(transformation.expression))
    }
    
    // çª—å£èšåˆ
    if (processingLogic.windowConfig.isDefined) {
      val windowConfig = processingLogic.windowConfig.get
      processedStream = processedStream
        .withWatermark("timestamp", windowConfig.watermark)
        .groupBy(
          window(col("timestamp"), windowConfig.windowDuration, windowConfig.slideDuration),
          col(windowConfig.groupByColumn)
        )
        .agg(windowConfig.aggregations: _*)
    }
    
    processedStream
  }
  
  private def createOutputStream(
    processedStream: DataFrame,
    outputConfig: OutputConfig,
    jobId: String
  ): StreamingQuery = {
    val checkpointPath = s"$checkpointBasePath/$jobId"
    
    val writeStream = processedStream.writeStream
      .outputMode(outputConfig.outputMode)
      .trigger(Trigger.ProcessingTime(outputConfig.triggerInterval))
      .option("checkpointLocation", checkpointPath)
    
    outputConfig.sinkType match {
      case "kafka" =>
        writeStream
          .format("kafka")
          .option("kafka.bootstrap.servers", outputConfig.brokers)
          .option("topic", outputConfig.topic)
          .start()
      
      case "console" =>
        writeStream
          .format("console")
          .option("truncate", false)
          .start()
      
      case "file" =>
        writeStream
          .format(outputConfig.format)
          .option("path", outputConfig.path)
          .partitionBy(outputConfig.partitionColumns: _*)
          .start()
      
      case "delta" =>
        writeStream
          .format("delta")
          .option("path", outputConfig.path)
          .start()
      
      case "memory" =>
        writeStream
          .format("memory")
          .queryName(outputConfig.tableName)
          .start()
      
      case _ =>
        throw new IllegalArgumentException(s"Unsupported sink type: ${outputConfig.sinkType}")
    }
  }
  
  def stopJob(jobId: String): Unit = {
    streamingQueries.get(jobId) match {
      case Some(query) =>
        query.stop()
        streamingQueries.remove(jobId)
        println(s"Stopped real-time job: $jobId")
      case None =>
        println(s"Job $jobId not found")
    }
  }
  
  def getJobStatus(jobId: String): Option[StreamingQueryStatus] = {
    streamingQueries.get(jobId).map { query =>
      StreamingQueryStatus(
        id = jobId,
        runId = query.runId.toString,
        isActive = query.isActive,
        inputRate = query.lastProgress.inputRowsPerSecond,
        processingRate = query.lastProgress.batchDuration,
        latestOffset = query.lastProgress.sources.headOption.map(_.endOffset).getOrElse("unknown")
      )
    }
  }
  
  def getAllJobStatuses: Map[String, StreamingQueryStatus] = {
    streamingQueries.map { case (jobId, query) =>
      jobId -> StreamingQueryStatus(
        id = jobId,
        runId = query.runId.toString,
        isActive = query.isActive,
        inputRate = query.lastProgress.inputRowsPerSecond,
        processingRate = query.lastProgress.batchDuration,
        latestOffset = query.lastProgress.sources.headOption.map(_.endOffset).getOrElse("unknown")
      )
    }.toMap
  }
}

// 2. å®æ—¶ä½œä¸šé…ç½®
case class RealTimeJobConfig(
  name: String,
  inputConfig: InputConfig,
  processingLogic: ProcessingLogic,
  outputConfig: OutputConfig
)

case class InputConfig(
  sourceType: String,
  brokers: String = "",
  topics: String = "",
  host: String = "",
  port: Int = 0,
  path: String = "",
  format: String = "json",
  schema: StructType = new StructType(),
  startingOffsets: String = "latest",
  maxOffsetsPerTrigger: Long = 1000000
)

case class ProcessingLogic(
  parseJson: Boolean = false,
  jsonSchema: StructType = new StructType(),
  filters: List[String] = List(),
  transformations: List[ColumnTransformation] = List(),
  windowConfig: Option[WindowConfig] = None
)

case class ColumnTransformation(
  targetColumn: String,
  expression: String
)

case class WindowConfig(
  windowDuration: String,
  slideDuration: String,
  watermark: String,
  groupByColumn: String,
  aggregations: List[Column]
)

case class OutputConfig(
  sinkType: String,
  outputMode: String = "append",
  triggerInterval: String = "10 seconds",
  brokers: String = "",
  topic: String = "",
  path: String = "",
  format: String = "json",
  tableName: String = "",
  partitionColumns: List[String] = List()
)

case class StreamingQueryStatus(
  id: String,
  runId: String,
  isActive: Boolean,
  inputRate: Double,
  processingRate: Long,
  latestOffset: String
)

// 3. å®æ—¶ä½œä¸šç¤ºä¾‹
object RealTimeJobExamples {
  
  def createUserActivityAnalysisJob(platform: RealTimeComputePlatform): String = {
    val jobConfig = RealTimeJobConfig(
      name = "user-activity-analysis",
      inputConfig = InputConfig(
        sourceType = "kafka",
        brokers = "localhost:9092",
        topics = "user-events",
        startingOffsets = "latest",
        maxOffsetsPerTrigger = 10000
      ),
      processingLogic = ProcessingLogic(
        parseJson = true,
        jsonSchema = StructType(Seq(
          StructField("user_id", StringType, true),
          StructField("event_type", StringType, true),
          StructField("timestamp", TimestampType, true),
          StructField("properties", MapType(StringType, StringType), true)
        )),
        filters = List("event_type IS NOT NULL", "user_id IS NOT NULL"),
        transformations = List(
          ColumnTransformation("hour", "hour(timestamp)"),
          ColumnTransformation("date", "to_date(timestamp)")
        ),
        windowConfig = Some(WindowConfig(
          windowDuration = "1 hour",
          slideDuration = "10 minutes",
          watermark = "10 minutes",
          groupByColumn = "event_type",
          aggregations = List(
            count("*").alias("event_count"),
            countDistinct("user_id").alias("unique_users"),
            avg("properties.session_duration").alias("avg_session_duration")
          )
        ))
      ),
      outputConfig = OutputConfig(
        sinkType = "delta",
        outputMode = "update",
        triggerInterval = "1 minute",
        path = "s3a://datalake/gold/user-activity-hourly"
      )
    )
    
    platform.createRealTimeJob(jobConfig)
  }
  
  def createAnomalyDetectionJob(platform: RealTimeComputePlatform): String = {
    val jobConfig = RealTimeJobConfig(
      name = "anomaly-detection",
      inputConfig = InputConfig(
        sourceType = "kafka",
        brokers = "localhost:9092",
        topics = "system-metrics"
      ),
      processingLogic = ProcessingLogic(
        parseJson = true,
        jsonSchema = StructType(Seq(
          StructField("host", StringType, true),
          StructField("metric_name", StringType, true),
          StructField("metric_value", DoubleType, true),
          StructField("timestamp", TimestampType, true)
        )),
        transformations = List(
          ColumnTransformation("z_score", 
            "(metric_value - avg(metric_value) OVER (PARTITION BY host, metric_name ORDER BY timestamp ROWS BETWEEN 100 PRECEDING AND CURRENT ROW)) / " +
            "stddev(metric_value) OVER (PARTITION BY host, metric_name ORDER BY timestamp ROWS BETWEEN 100 PRECEDING AND CURRENT ROW)"),
          ColumnTransformation("is_anomaly", "abs(z_score) > 3")
        ),
        filters = List("is_anomaly = true")
      ),
      outputConfig = OutputConfig(
        sinkType = "kafka",
        outputMode = "append",
        triggerInterval = "5 seconds",
        brokers = "localhost:9092",
        topic = "anomaly-alerts"
      )
    )
    
    platform.createRealTimeJob(jobConfig)
  }
}
```

### æœºå™¨å­¦ä¹ å¹³å°

**MLOpså¹³å°å®ç°**ï¼š
```scala
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification._
import org.apache.spark.ml.evaluation._
import org.apache.spark.ml.tuning._
import org.apache.spark.sql.functions._

// 1. æœºå™¨å­¦ä¹ å¹³å°ç®¡ç†å™¨
class MLPlatformManager(spark: SparkSession) {
  
  private val modelRegistry = new ModelRegistry()
  private val experimentTracker = new ExperimentTracker()
  private val featureStore = new FeatureStore()
  
  def createMLPipeline(pipelineConfig: MLPipelineConfig): MLPipeline = {
    val pipeline = new MLPipeline(
      id = java.util.UUID.randomUUID().toString,
      name = pipelineConfig.name,
      config = pipelineConfig,
      spark = spark
    )
    
    pipeline
  }
  
  def trainModel(
    pipeline: MLPipeline,
    trainingData: DataFrame,
    validationData: Option[DataFrame] = None
  ): TrainedModel = {
    
    // å¼€å§‹å®éªŒè·Ÿè¸ª
    val experimentId = experimentTracker.startExperiment(pipeline.name, pipeline.config)
    
    try {
      // ç‰¹å¾å·¥ç¨‹
      val processedTrainingData = pipeline.preprocessData(trainingData)
      val processedValidationData = validationData.map(pipeline.preprocessData)
      
      // æ¨¡å‹è®­ç»ƒ
      val model = pipeline.train(processedTrainingData)
      
      // æ¨¡å‹è¯„ä¼°
      val metrics = pipeline.evaluate(model, processedValidationData.getOrElse(processedTrainingData))
      
      // è®°å½•å®éªŒç»“æœ
      experimentTracker.logMetrics(experimentId, metrics)
      experimentTracker.logModel(experimentId, model)
      
      val trainedModel = TrainedModel(
        id = java.util.UUID.randomUUID().toString,
        pipelineId = pipeline.id,
        model = model,
        metrics = metrics,
        trainingTimestamp = System.currentTimeMillis(),
        experimentId = experimentId
      )
      
      // æ³¨å†Œæ¨¡å‹
      modelRegistry.registerModel(trainedModel)
      
      trainedModel
      
    } catch {
      case e: Exception =>
        experimentTracker.failExperiment(experimentId, e.getMessage)
        throw e
    }
  }
  
  def deployModel(modelId: String, deploymentConfig: DeploymentConfig): String = {
    val model = modelRegistry.getModel(modelId)
    
    val deploymentId = java.util.UUID.randomUUID().toString
    
    deploymentConfig.deploymentType match {
      case "batch" =>
        deployBatchModel(model, deploymentConfig, deploymentId)
      case "streaming" =>
        deployStreamingModel(model, deploymentConfig, deploymentId)
      case "api" =>
        deployAPIModel(model, deploymentConfig, deploymentId)
      case _ =>
        throw new IllegalArgumentException(s"Unsupported deployment type: ${deploymentConfig.deploymentType}")
    }
    
    deploymentId
  }
  
  private def deployBatchModel(model: TrainedModel, config: DeploymentConfig, deploymentId: String): Unit = {
    // æ‰¹é‡é¢„æµ‹éƒ¨ç½²
    val batchJob = new BatchPredictionJob(model, config, deploymentId)
    batchJob.schedule()
  }
  
  private def deployStreamingModel(model: TrainedModel, config: DeploymentConfig, deploymentId: String): Unit = {
    // æµå¼é¢„æµ‹éƒ¨ç½²
    val streamingJob = new StreamingPredictionJob(model, config, deploymentId, spark)
    streamingJob.start()
  }
  
  private def deployAPIModel(model: TrainedModel, config: DeploymentConfig, deploymentId: String): Unit = {
    // APIæœåŠ¡éƒ¨ç½²
    val apiService = new ModelAPIService(model, config, deploymentId)
    apiService.start()
  }
}

// 2. MLç®¡é“å®ç°
class MLPipeline(
  val id: String,
  val name: String,
  val config: MLPipelineConfig,
  val spark: SparkSession
) {
  
  def preprocessData(data: DataFrame): DataFrame = {
    var processedData = data
    
    // æ•°æ®æ¸…æ´—
    config.dataCleaningSteps.foreach { step =>
      processedData = step.stepType match {
        case "remove_nulls" =>
          processedData.na.drop(step.columns)
        case "fill_nulls" =>
          processedData.na.fill(step.fillValue, step.columns)
        case "remove_outliers" =>
          removeOutliers(processedData, step.columns, step.threshold)
        case _ => processedData
      }
    }
    
    // ç‰¹å¾å·¥ç¨‹
    config.featureEngineeringSteps.foreach { step =>
      processedData = step.stepType match {
        case "string_indexer" =>
          val indexer = new StringIndexer()
            .setInputCol(step.inputColumn)
            .setOutputCol(step.outputColumn)
          indexer.fit(processedData).transform(processedData)
        
        case "vector_assembler" =>
          val assembler = new VectorAssembler()
            .setInputCols(step.inputColumns.toArray)
            .setOutputCol(step.outputColumn)
          assembler.transform(processedData)
        
        case "standard_scaler" =>
          val scaler = new StandardScaler()
            .setInputCol(step.inputColumn)
            .setOutputCol(step.outputColumn)
            .setWithStd(true)
            .setWithMean(true)
          scaler.fit(processedData).transform(processedData)
        
        case "pca" =>
          val pca = new PCA()
            .setInputCol(step.inputColumn)
            .setOutputCol(step.outputColumn)
            .setK(step.k)
          pca.fit(processedData).transform(processedData)
        
        case _ => processedData
      }
    }
    
    processedData
  }
  
  def train(trainingData: DataFrame): PipelineModel = {
    val stages = scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage]()
    
    // æ·»åŠ ç‰¹å¾å¤„ç†é˜¶æ®µ
    config.featureEngineeringSteps.foreach { step =>
      val stage = step.stepType match {
        case "string_indexer" =>
          new StringIndexer()
            .setInputCol(step.inputColumn)
            .setOutputCol(step.outputColumn)
        
        case "vector_assembler" =>
          new VectorAssembler()
            .setInputCols(step.inputColumns.toArray)
            .setOutputCol(step.outputColumn)
        
        case "standard_scaler" =>
          new StandardScaler()
            .setInputCol(step.inputColumn)
            .setOutputCol(step.outputColumn)
            .setWithStd(true)
            .setWithMean(true)
        
        case _ => null
      }
      
      if (stage != null) stages += stage
    }
    
    // æ·»åŠ ç®—æ³•é˜¶æ®µ
    val algorithm = config.algorithmConfig.algorithmType match {
      case "logistic_regression" =>
        new LogisticRegression()
          .setFeaturesCol("features")
          .setLabelCol("label")
          .setMaxIter(config.algorithmConfig.maxIter)
          .setRegParam(config.algorithmConfig.regParam)
      
      case "random_forest" =>
        new RandomForestClassifier()
          .setFeaturesCol("features")
          .setLabelCol("label")
          .setNumTrees(config.algorithmConfig.numTrees)
          .setMaxDepth(config.algorithmConfig.maxDepth)
      
      case "gradient_boosting" =>
        new GBTClassifier()
          .setFeaturesCol("features")
          .setLabelCol("label")
          .setMaxIter(config.algorithmConfig.maxIter)
          .setMaxDepth(config.algorithmConfig.maxDepth)
      
      case _ =>
        throw new IllegalArgumentException(s"Unsupported algorithm: ${config.algorithmConfig.algorithmType}")
    }
    
    stages += algorithm
    
    // åˆ›å»ºå¹¶è®­ç»ƒç®¡é“
    val pipeline = new Pipeline().setStages(stages.toArray)
    pipeline.fit(trainingData)
  }
  
  def evaluate(model: PipelineModel, testData: DataFrame): Map[String, Double] = {
    val predictions = model.transform(testData)
    
    val metrics = scala.collection.mutable.Map[String, Double]()
    
    config.algorithmConfig.problemType match {
      case "classification" =>
        val evaluator = new BinaryClassificationEvaluator()
          .setLabelCol("label")
          .setRawPredictionCol("rawPrediction")
        
        metrics("auc") = evaluator.evaluate(predictions)
        
        val multiEvaluator = new MulticlassClassificationEvaluator()
          .setLabelCol("label")
          .setPredictionCol("prediction")
        
        metrics("accuracy") = multiEvaluator.setMetricName("accuracy").evaluate(predictions)
        metrics("f1") = multiEvaluator.setMetricName("f1").evaluate(predictions)
        metrics("precision") = multiEvaluator.setMetricName("weightedPrecision").evaluate(predictions)
        metrics("recall") = multiEvaluator.setMetricName("weightedRecall").evaluate(predictions)
      
      case "regression" =>
        val evaluator = new RegressionEvaluator()
          .setLabelCol("label")
          .setPredictionCol("prediction")
        
        metrics("rmse") = evaluator.setMetricName("rmse").evaluate(predictions)
        metrics("mae") = evaluator.setMetricName("mae").evaluate(predictions)
        metrics("r2") = evaluator.setMetricName("r2").evaluate(predictions)
    }
    
    metrics.toMap
  }
  
  private def removeOutliers(data: DataFrame, columns: List[String], threshold: Double): DataFrame = {
    var cleanData = data
    
    columns.foreach { column =>
      val stats = data.select(
        mean(col(column)).alias("mean"),
        stddev(col(column)).alias("stddev")
      ).collect()(0)
      
      val meanValue = stats.getAs[Double]("mean")
      val stddevValue = stats.getAs[Double]("stddev")
      
      cleanData = cleanData.filter(
        abs(col(column) - meanValue) <= threshold * stddevValue
      )
    }
    
    cleanData
  }
}

// 3. æ¨¡å‹æ³¨å†Œè¡¨
class ModelRegistry {
  
  private val models = scala.collection.mutable.Map[String, TrainedModel]()
  private val modelVersions = scala.collection.mutable.Map[String, List[String]]()
  
  def registerModel(model: TrainedModel): Unit = {
    models(model.id) = model
    
    val modelName = model.pipelineId
    val versions = modelVersions.getOrElse(modelName, List())
    modelVersions(modelName) = model.id :: versions
    
    // ä¿å­˜æ¨¡å‹åˆ°æŒä¹…åŒ–å­˜å‚¨
    saveModelToPersistentStorage(model)
  }
  
  def getModel(modelId: String): TrainedModel = {
    models.getOrElse(modelId, throw new IllegalArgumentException(s"Model $modelId not found"))
  }
  
  def getLatestModel(modelName: String): Option[TrainedModel] = {
    modelVersions.get(modelName).flatMap(_.headOption).map(models)
  }
  
  def listModels(): List[ModelInfo] = {
    models.values.map { model =>
      ModelInfo(
        id = model.id,
        pipelineId = model.pipelineId,
        metrics = model.metrics,
        trainingTimestamp = model.trainingTimestamp,
        status = "active"
      )
    }.toList
  }
  
  private def saveModelToPersistentStorage(model: TrainedModel): Unit = {
    val modelPath = s"s3a://ml-models/${model.id}"
    model.model.write.overwrite().save(modelPath)
  }
}

// 4. é…ç½®ç±»å®šä¹‰
case class MLPipelineConfig(
  name: String,
  dataCleaningSteps: List[DataCleaningStep],
  featureEngineeringSteps: List[FeatureEngineeringStep],
  algorithmConfig: AlgorithmConfig
)

case class DataCleaningStep(
  stepType: String,
  columns: List[String] = List(),
  fillValue: String = "",
  threshold: Double = 3.0
)

case class FeatureEngineeringStep(
  stepType: String,
  inputColumn: String = "",
  outputColumn: String = "",
  inputColumns: List[String] = List(),
  k: Int = 10
)

case class AlgorithmConfig(
  algorithmType: String,
  problemType: String,
  maxIter: Int = 100,
  regParam: Double = 0.01,
  numTrees: Int = 20,
  maxDepth: Int = 5
)

case class TrainedModel(
  id: String,
  pipelineId: String,
  model: PipelineModel,
  metrics: Map[String, Double],
  trainingTimestamp: Long,
  experimentId: String
)

case class ModelInfo(
  id: String,
  pipelineId: String,
  metrics: Map[String, Double],
  trainingTimestamp: Long,
  status: String
)

case class DeploymentConfig(
  deploymentType: String,
  inputPath: String = "",
  outputPath: String = "",
  schedule: String = "",
  apiPort: Int = 8080
)
```



---

## Sparkæ•…éšœè¯Šæ–­ä¸è°ƒä¼˜å®æˆ˜ ğŸ› ï¸

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

**æ€§èƒ½åˆ†æå·¥å…·é›†**ï¼š
```scala
// æ€§èƒ½åˆ†æå·¥å…·ç±»
class SparkPerformanceAnalyzer(spark: SparkSession) {
  
  def analyzeJobPerformance(appId: String): JobPerformanceReport = {
    val sparkUI = spark.sparkContext.ui
    val jobProgressListener = sparkUI.get.jobProgressListener
    
    val completedJobs = jobProgressListener.completedJobs
    val failedJobs = jobProgressListener.failedJobs
    
    val jobMetrics = completedJobs.map { job =>
      val stages = job.stageIds.map(jobProgressListener.stageIdToInfo.get).flatten
      val totalTasks = stages.map(_.numTasks).sum
      val failedTasks = stages.map(_.numFailedTasks).sum
      val duration = job.completionTime.getOrElse(0L) - job.submissionTime.getOrElse(0L)
      
      JobMetrics(
        jobId = job.jobId,
        duration = duration,
        totalTasks = totalTasks,
        failedTasks = failedTasks,
        inputBytes = stages.map(_.inputBytes).sum,
        outputBytes = stages.map(_.outputBytes).sum,
        shuffleReadBytes = stages.map(_.shuffleReadBytes).sum,
        shuffleWriteBytes = stages.map(_.shuffleWriteBytes).sum
      )
    }
    
    JobPerformanceReport(
      appId = appId,
      totalJobs = completedJobs.length + failedJobs.length,
      completedJobs = completedJobs.length,
      failedJobs = failedJobs.length,
      avgJobDuration = jobMetrics.map(_.duration).sum / jobMetrics.length.toDouble,
      totalTaskFailures = jobMetrics.map(_.failedTasks).sum,
      recommendations = generatePerformanceRecommendations(jobMetrics)
    )
  }
  
  def analyzeStagePerformance(stageId: Int): StagePerformanceReport = {
    val sparkUI = spark.sparkContext.ui
    val jobProgressListener = sparkUI.get.jobProgressListener
    
    jobProgressListener.stageIdToInfo.get(stageId) match {
      case Some(stageInfo) =>
        val taskMetrics = stageInfo.taskMetrics
        val tasks = jobProgressListener.stageIdToTaskInfos.getOrElse(stageId, Map.empty).values
        
        val taskDurations = tasks.map(_.duration).toSeq
        val taskInputSizes = tasks.map(_.taskMetrics.map(_.inputMetrics.bytesRead).getOrElse(0L)).toSeq
        
        StagePerformanceReport(
          stageId = stageId,
          stageName = stageInfo.name,
          numTasks = stageInfo.numTasks,
          completedTasks = stageInfo.numCompletedTasks,
          failedTasks = stageInfo.numFailedTasks,
          avgTaskDuration = taskDurations.sum / taskDurations.length.toDouble,
          maxTaskDuration = taskDurations.max,
          minTaskDuration = taskDurations.min,
          taskDurationStdDev = calculateStdDev(taskDurations),
          dataSkewRatio = taskInputSizes.max.toDouble / taskInputSizes.sum * taskInputSizes.length,
          recommendations = generateStageRecommendations(stageInfo, taskDurations, taskInputSizes)
        )
        
      case None =>
        throw new IllegalArgumentException(s"Stage $stageId not found")
    }
  }
  
  private def generatePerformanceRecommendations(jobMetrics: Seq[JobMetrics]): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    // æ£€æŸ¥ä»»åŠ¡å¤±è´¥ç‡
    val totalTasks = jobMetrics.map(_.totalTasks).sum
    val totalFailures = jobMetrics.map(_.failedTasks).sum
    val failureRate = totalFailures.toDouble / totalTasks
    
    if (failureRate > 0.05) {
      recommendations += s"High task failure rate (${(failureRate * 100).formatted("%.2f")}%). Consider increasing executor memory or reducing batch size."
    }
    
    // æ£€æŸ¥Shuffleæ•°æ®é‡
    val avgShuffleRead = jobMetrics.map(_.shuffleReadBytes).sum / jobMetrics.length.toDouble
    val avgShuffleWrite = jobMetrics.map(_.shuffleWriteBytes).sum / jobMetrics.length.toDouble
    
    if (avgShuffleRead > 1024 * 1024 * 1024) { // 1GB
      recommendations += "High shuffle read volume detected. Consider increasing shuffle partitions or using broadcast joins."
    }
    
    if (avgShuffleWrite > 1024 * 1024 * 1024) { // 1GB
      recommendations += "High shuffle write volume detected. Consider optimizing data partitioning strategy."
    }
    
    // æ£€æŸ¥ä½œä¸šæŒç»­æ—¶é—´
    val avgDuration = jobMetrics.map(_.duration).sum / jobMetrics.length.toDouble
    if (avgDuration > 3600000) { // 1 hour
      recommendations += "Long-running jobs detected. Consider breaking down into smaller jobs or optimizing queries."
    }
    
    recommendations.toList
  }
  
  private def generateStageRecommendations(
    stageInfo: StageInfo, 
    taskDurations: Seq[Long], 
    taskInputSizes: Seq[Long]
  ): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    // æ£€æŸ¥ä»»åŠ¡å€¾æ–œ
    val durationStdDev = calculateStdDev(taskDurations)
    val avgDuration = taskDurations.sum / taskDurations.length.toDouble
    val skewRatio = durationStdDev / avgDuration
    
    if (skewRatio > 2.0) {
      recommendations += s"High task duration skew detected (ratio: ${skewRatio.formatted("%.2f")}). Consider repartitioning data or using salting techniques."
    }
    
    // æ£€æŸ¥æ•°æ®å€¾æ–œ
    val maxInputSize = taskInputSizes.max
    val avgInputSize = taskInputSizes.sum / taskInputSizes.length.toDouble
    val dataSkewRatio = maxInputSize / avgInputSize
    
    if (dataSkewRatio > 5.0) {
      recommendations += s"High data skew detected (ratio: ${dataSkewRatio.formatted("%.2f")}). Consider using custom partitioners or pre-aggregation."
    }
    
    recommendations.toList
  }
  
  private def calculateStdDev(values: Seq[Long]): Double = {
    val mean = values.sum / values.length.toDouble
    val variance = values.map(v => math.pow(v - mean, 2)).sum / values.length
    math.sqrt(variance)
  }
}

case class JobMetrics(
  jobId: Int,
  duration: Long,
  totalTasks: Int,
  failedTasks: Int,
  inputBytes: Long,
  outputBytes: Long,
  shuffleReadBytes: Long,
  shuffleWriteBytes: Long
)

case class JobPerformanceReport(
  appId: String,
  totalJobs: Int,
  completedJobs: Int,
  failedJobs: Int,
  avgJobDuration: Double,
  totalTaskFailures: Int,
  recommendations: List[String]
)

case class StagePerformanceReport(
  stageId: Int,
  stageName: String,
  numTasks: Int,
  completedTasks: Int,
  failedTasks: Int,
  avgTaskDuration: Double,
  maxTaskDuration: Long,
  minTaskDuration: Long,
  taskDurationStdDev: Double,
  dataSkewRatio: Double,
  recommendations: List[String]
)
```### å†…å­˜
è°ƒä¼˜å®æˆ˜

**å†…å­˜ä½¿ç”¨åˆ†æå·¥å…·**ï¼š
```scala
// å†…å­˜ä½¿ç”¨åˆ†æå™¨
class MemoryUsageAnalyzer(spark: SparkSession) {
  
  def analyzeMemoryUsage(): MemoryUsageReport = {
    val sc = spark.sparkContext
    val executorInfos = sc.getExecutorInfos
    
    val memoryMetrics = executorInfos.map { case (executorId, executorInfo) =>
      ExecutorMemoryMetrics(
        executorId = executorId,
        maxMemory = executorInfo.maxMemory,
        memoryUsed = executorInfo.memoryUsed,
        maxOnHeapMemory = executorInfo.maxOnHeapMemory,
        maxOffHeapMemory = executorInfo.maxOffHeapMemory,
        onHeapMemoryUsed = executorInfo.onHeapMemoryUsed,
        offHeapMemoryUsed = executorInfo.offHeapMemoryUsed
      )
    }.toSeq
    
    val totalMaxMemory = memoryMetrics.map(_.maxMemory).sum
    val totalUsedMemory = memoryMetrics.map(_.memoryUsed).sum
    val memoryUtilization = totalUsedMemory.toDouble / totalMaxMemory
    
    MemoryUsageReport(
      executorMetrics = memoryMetrics,
      totalMaxMemory = totalMaxMemory,
      totalUsedMemory = totalUsedMemory,
      memoryUtilization = memoryUtilization,
      recommendations = generateMemoryRecommendations(memoryMetrics, memoryUtilization)
    )
  }
  
  def optimizeMemoryConfiguration(currentConfig: SparkConf): SparkConf = {
    val optimizedConfig = currentConfig.clone()
    val memoryReport = analyzeMemoryUsage()
    
    // åŸºäºå†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´é…ç½®
    if (memoryReport.memoryUtilization > 0.9) {
      // å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¢åŠ executorå†…å­˜
      val currentMemory = currentConfig.get("spark.executor.memory", "1g")
      val memoryValue = parseMemoryString(currentMemory)
      val newMemoryValue = (memoryValue * 1.5).toLong
      optimizedConfig.set("spark.executor.memory", s"${newMemoryValue}m")
      
      // è°ƒæ•´å†…å­˜åˆ†é…æ¯”ä¾‹
      optimizedConfig.set("spark.sql.execution.arrow.maxRecordsPerBatch", "1000")
      optimizedConfig.set("spark.sql.adaptive.coalescePartitions.minPartitionNum", "1")
      
    } else if (memoryReport.memoryUtilization < 0.3) {
      // å†…å­˜ä½¿ç”¨ç‡è¿‡ä½ï¼Œå‡å°‘executorå†…å­˜ï¼Œå¢åŠ executoræ•°é‡
      val currentMemory = currentConfig.get("spark.executor.memory", "1g")
      val memoryValue = parseMemoryString(currentMemory)
      val newMemoryValue = (memoryValue * 0.7).toLong
      optimizedConfig.set("spark.executor.memory", s"${newMemoryValue}m")
      
      val currentInstances = currentConfig.getInt("spark.executor.instances", 2)
      optimizedConfig.set("spark.executor.instances", (currentInstances * 1.4).toInt.toString)
    }
    
    // ä¼˜åŒ–åƒåœ¾å›æ”¶è®¾ç½®
    optimizedConfig.set("spark.executor.extraJavaOptions", 
      "-XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseZGC")
    
    optimizedConfig
  }
  
  private def parseMemoryString(memoryStr: String): Long = {
    val pattern = """(\d+)([kmgKMG]?)""".r
    memoryStr.toLowerCase match {
      case pattern(value, unit) =>
        val multiplier = unit.toLowerCase match {
          case "k" => 1024L
          case "m" => 1024L * 1024L
          case "g" => 1024L * 1024L * 1024L
          case _ => 1L
        }
        value.toLong * multiplier
      case _ => 1024L * 1024L * 1024L // é»˜è®¤1GB
    }
  }
  
  private def generateMemoryRecommendations(
    metrics: Seq[ExecutorMemoryMetrics], 
    utilization: Double
  ): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    if (utilization > 0.9) {
      recommendations += "High memory utilization detected. Consider increasing executor memory or reducing batch size."
    }
    
    if (utilization < 0.3) {
      recommendations += "Low memory utilization detected. Consider reducing executor memory and increasing parallelism."
    }
    
    // æ£€æŸ¥å†…å­˜å€¾æ–œ
    val memoryUsages = metrics.map(m => m.memoryUsed.toDouble / m.maxMemory)
    val maxUsage = memoryUsages.max
    val minUsage = memoryUsages.min
    val usageSkew = maxUsage / minUsage
    
    if (usageSkew > 3.0) {
      recommendations += s"Memory usage skew detected (ratio: ${usageSkew.formatted("%.2f")}). Consider data repartitioning."
    }
    
    recommendations.toList
  }
}

case class ExecutorMemoryMetrics(
  executorId: String,
  maxMemory: Long,
  memoryUsed: Long,
  maxOnHeapMemory: Long,
  maxOffHeapMemory: Long,
  onHeapMemoryUsed: Long,
  offHeapMemoryUsed: Long
)

case class MemoryUsageReport(
  executorMetrics: Seq[ExecutorMemoryMetrics],
  totalMaxMemory: Long,
  totalUsedMemory: Long,
  memoryUtilization: Double,
  recommendations: List[String]
)
```

**å†…å­˜è°ƒä¼˜æœ€ä½³å®è·µ**ï¼š
```scala
// å†…å­˜è°ƒä¼˜é…ç½®ç”Ÿæˆå™¨
object MemoryTuningConfigGenerator {
  
  def generateOptimalConfig(
    dataSize: Long,
    numCores: Int,
    availableMemory: Long
  ): Map[String, String] = {
    
    val config = scala.collection.mutable.Map[String, String]()
    
    // 1. è®¡ç®—æœ€ä¼˜executoré…ç½®
    val optimalExecutorMemory = calculateOptimalExecutorMemory(dataSize, availableMemory)
    val optimalExecutorCores = calculateOptimalExecutorCores(numCores)
    val optimalExecutorInstances = calculateOptimalExecutorInstances(availableMemory, optimalExecutorMemory)
    
    config("spark.executor.memory") = s"${optimalExecutorMemory}m"
    config("spark.executor.cores") = optimalExecutorCores.toString
    config("spark.executor.instances") = optimalExecutorInstances.toString
    
    // 2. å†…å­˜åˆ†é…æ¯”ä¾‹ä¼˜åŒ–
    config("spark.sql.execution.arrow.maxRecordsPerBatch") = "10000"
    config("spark.sql.execution.arrow.fallback.enabled") = "true"
    
    // 3. å­˜å‚¨å†…å­˜é…ç½®
    config("spark.storage.memoryFraction") = "0.6"
    config("spark.storage.unrollFraction") = "0.2"
    config("spark.storage.safetyFraction") = "0.9"
    
    // 4. Shuffleå†…å­˜é…ç½®
    config("spark.shuffle.memoryFraction") = "0.2"
    config("spark.shuffle.safetyFraction") = "0.8"
    config("spark.shuffle.spill.compress") = "true"
    config("spark.shuffle.compress") = "true"
    
    // 5. åƒåœ¾å›æ”¶ä¼˜åŒ–
    val gcOptions = List(
      "-XX:+UseG1GC",
      "-XX:MaxGCPauseMillis=200",
      "-XX:+UnlockExperimentalVMOptions",
      "-XX:G1HeapRegionSize=16m",
      "-XX:+UseCompressedOops",
      "-XX:+UseCompressedClassPointers"
    ).mkString(" ")
    
    config("spark.executor.extraJavaOptions") = gcOptions
    config("spark.driver.extraJavaOptions") = gcOptions
    
    // 6. Off-heapå†…å­˜é…ç½®
    if (availableMemory > 8 * 1024 * 1024 * 1024L) { // 8GBä»¥ä¸Šå¯ç”¨off-heap
      config("spark.sql.execution.arrow.pyspark.enabled") = "true"
      config("spark.sql.execution.arrow.sparkr.enabled") = "true"
    }
    
    config.toMap
  }
  
  private def calculateOptimalExecutorMemory(dataSize: Long, availableMemory: Long): Long = {
    // åŸºäºæ•°æ®å¤§å°å’Œå¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜executorå†…å­˜
    val baseMemory = math.min(dataSize / 10, availableMemory / 4) // æ•°æ®å¤§å°çš„1/10æˆ–å¯ç”¨å†…å­˜çš„1/4
    math.max(1024, math.min(baseMemory / (1024 * 1024), 8 * 1024)).toLong // æœ€å°1GBï¼Œæœ€å¤§8GB
  }
  
  private def calculateOptimalExecutorCores(numCores: Int): Int = {
    // æ¯ä¸ªexecutorå»ºè®®2-5ä¸ªæ ¸å¿ƒ
    math.max(2, math.min(5, numCores / 4))
  }
  
  private def calculateOptimalExecutorInstances(availableMemory: Long, executorMemory: Long): Int = {
    // åŸºäºå¯ç”¨å†…å­˜å’Œexecutorå†…å­˜è®¡ç®—å®ä¾‹æ•°
    math.max(2, (availableMemory / (executorMemory * 1024 * 1024)).toInt)
  }
}
```

### ç½‘ç»œè°ƒä¼˜å®æˆ˜

**ç½‘ç»œæ€§èƒ½åˆ†æ**ï¼š
```scala
// ç½‘ç»œæ€§èƒ½åˆ†æå™¨
class NetworkPerformanceAnalyzer(spark: SparkSession) {
  
  def analyzeNetworkPerformance(): NetworkPerformanceReport = {
    val sc = spark.sparkContext
    val appId = sc.applicationId
    
    // è·å–ShuffleæŒ‡æ ‡
    val shuffleMetrics = getShuffleMetrics(appId)
    
    // åˆ†æç½‘ç»œç“¶é¢ˆ
    val networkBottlenecks = identifyNetworkBottlenecks(shuffleMetrics)
    
    NetworkPerformanceReport(
      appId = appId,
      totalShuffleRead = shuffleMetrics.map(_.shuffleReadBytes).sum,
      totalShuffleWrite = shuffleMetrics.map(_.shuffleWriteBytes).sum,
      avgShuffleReadTime = shuffleMetrics.map(_.shuffleReadTime).sum / shuffleMetrics.length.toDouble,
      avgShuffleWriteTime = shuffleMetrics.map(_.shuffleWriteTime).sum / shuffleMetrics.length.toDouble,
      networkBottlenecks = networkBottlenecks,
      recommendations = generateNetworkRecommendations(shuffleMetrics, networkBottlenecks)
    )
  }
  
  def optimizeNetworkConfiguration(currentConfig: SparkConf): SparkConf = {
    val optimizedConfig = currentConfig.clone()
    val networkReport = analyzeNetworkPerformance()
    
    // åŸºäºç½‘ç»œæ€§èƒ½è°ƒæ•´é…ç½®
    if (networkReport.avgShuffleReadTime > 1000) { // è¶…è¿‡1ç§’
      // å¢åŠ ç½‘ç»œè¶…æ—¶æ—¶é—´
      optimizedConfig.set("spark.network.timeout", "800s")
      optimizedConfig.set("spark.sql.broadcastTimeout", "36000")
      
      // ä¼˜åŒ–Shuffleé…ç½®
      optimizedConfig.set("spark.shuffle.io.maxRetries", "5")
      optimizedConfig.set("spark.shuffle.io.retryWait", "30s")
      
      // å¯ç”¨Shuffleå‹ç¼©
      optimizedConfig.set("spark.shuffle.compress", "true")
      optimizedConfig.set("spark.shuffle.spill.compress", "true")
      optimizedConfig.set("spark.io.compression.codec", "snappy")
    }
    
    // ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒº
    optimizedConfig.set("spark.shuffle.file.buffer", "64k")
    optimizedConfig.set("spark.reducer.maxSizeInFlight", "96m")
    optimizedConfig.set("spark.shuffle.io.preferDirectBufs", "true")
    
    // å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
    optimizedConfig.set("spark.sql.adaptive.enabled", "true")
    optimizedConfig.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    optimizedConfig.set("spark.sql.adaptive.skewJoin.enabled", "true")
    
    optimizedConfig
  }
  
  private def getShuffleMetrics(appId: String): Seq[ShuffleMetrics] = {
    // ä»Spark UIæˆ–ç›‘æ§ç³»ç»Ÿè·å–ShuffleæŒ‡æ ‡
    // è¿™é‡Œç®€åŒ–å®ç°
    Seq(
      ShuffleMetrics("stage_1", 1024 * 1024 * 100, 1024 * 1024 * 80, 500, 300),
      ShuffleMetrics("stage_2", 1024 * 1024 * 200, 1024 * 1024 * 150, 800, 600)
    )
  }
  
  private def identifyNetworkBottlenecks(metrics: Seq[ShuffleMetrics]): List[NetworkBottleneck] = {
    val bottlenecks = scala.collection.mutable.ListBuffer[NetworkBottleneck]()
    
    metrics.foreach { metric =>
      val readThroughput = metric.shuffleReadBytes.toDouble / metric.shuffleReadTime * 1000 // bytes/sec
      val writeThroughput = metric.shuffleWriteBytes.toDouble / metric.shuffleWriteTime * 1000 // bytes/sec
      
      if (readThroughput < 10 * 1024 * 1024) { // å°äº10MB/s
        bottlenecks += NetworkBottleneck("slow_shuffle_read", metric.stageId, readThroughput)
      }
      
      if (writeThroughput < 10 * 1024 * 1024) { // å°äº10MB/s
        bottlenecks += NetworkBottleneck("slow_shuffle_write", metric.stageId, writeThroughput)
      }
    }
    
    bottlenecks.toList
  }
  
  private def generateNetworkRecommendations(
    metrics: Seq[ShuffleMetrics], 
    bottlenecks: List[NetworkBottleneck]
  ): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    if (bottlenecks.exists(_.bottleneckType == "slow_shuffle_read")) {
      recommendations += "Slow shuffle read detected. Consider increasing spark.reducer.maxSizeInFlight and enabling compression."
    }
    
    if (bottlenecks.exists(_.bottleneckType == "slow_shuffle_write")) {
      recommendations += "Slow shuffle write detected. Consider increasing spark.shuffle.file.buffer and optimizing partitioning."
    }
    
    val totalShuffleBytes = metrics.map(m => m.shuffleReadBytes + m.shuffleWriteBytes).sum
    if (totalShuffleBytes > 10L * 1024 * 1024 * 1024) { // 10GB
      recommendations += "High shuffle volume detected. Consider using broadcast joins or pre-aggregation to reduce shuffle."
    }
    
    recommendations.toList
  }
}

case class ShuffleMetrics(
  stageId: String,
  shuffleReadBytes: Long,
  shuffleWriteBytes: Long,
  shuffleReadTime: Long,
  shuffleWriteTime: Long
)

case class NetworkBottleneck(
  bottleneckType: String,
  stageId: String,
  throughput: Double
)

case class NetworkPerformanceReport(
  appId: String,
  totalShuffleRead: Long,
  totalShuffleWrite: Long,
  avgShuffleReadTime: Double,
  avgShuffleWriteTime: Double,
  networkBottlenecks: List[NetworkBottleneck],
  recommendations: List[String]
)
```

### å­˜å‚¨è°ƒä¼˜å®æˆ˜

**å­˜å‚¨ä¼˜åŒ–åˆ†æå™¨**ï¼š
```scala
// å­˜å‚¨ä¼˜åŒ–åˆ†æå™¨
class StorageOptimizationAnalyzer(spark: SparkSession) {
  
  def analyzeStorageUsage(): StorageUsageReport = {
    val sc = spark.sparkContext
    val rddInfos = sc.getRDDStorageInfo
    
    val storageMetrics = rddInfos.map { rddInfo =>
      RDDStorageMetrics(
        rddId = rddInfo.id,
        rddName = rddInfo.name,
        storageLevel = rddInfo.storageLevel.toString,
        numPartitions = rddInfo.numPartitions,
        numCachedPartitions = rddInfo.numCachedPartitions,
        memorySize = rddInfo.memorySize,
        diskSize = rddInfo.diskSize,
        externalBlockStoreSize = rddInfo.externalBlockStoreSize
      )
    }
    
    val totalMemoryUsed = storageMetrics.map(_.memorySize).sum
    val totalDiskUsed = storageMetrics.map(_.diskSize).sum
    
    StorageUsageReport(
      rddMetrics = storageMetrics,
      totalMemoryUsed = totalMemoryUsed,
      totalDiskUsed = totalDiskUsed,
      recommendations = generateStorageRecommendations(storageMetrics)
    )
  }
  
  def optimizeStorageConfiguration(
    dataCharacteristics: DataCharacteristics
  ): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]()
    
    // åŸºäºæ•°æ®ç‰¹å¾é€‰æ‹©å­˜å‚¨æ ¼å¼
    dataCharacteristics.dataType match {
      case "structured" =>
        config("spark.sql.parquet.compression.codec") = "snappy"
        config("spark.sql.parquet.enableVectorizedReader") = "true"
        config("spark.sql.parquet.columnarReaderBatchSize") = "4096"
        
      case "semi_structured" =>
        config("spark.sql.json.compression.codec") = "gzip"
        config("spark.sql.adaptive.enabled") = "true"
        
      case "unstructured" =>
        config("spark.serializer") = "org.apache.spark.serializer.KryoSerializer"
        config("spark.kryo.registrationRequired") = "false"
    }
    
    // åŸºäºæ•°æ®å¤§å°ä¼˜åŒ–å­˜å‚¨
    if (dataCharacteristics.dataSize > 100L * 1024 * 1024 * 1024) { // 100GBä»¥ä¸Š
      config("spark.sql.files.maxPartitionBytes") = "268435456" // 256MB
      config("spark.sql.adaptive.coalescePartitions.enabled") = "true"
      config("spark.sql.adaptive.coalescePartitions.minPartitionNum") = "1"
    }
    
    // åŸºäºè®¿é—®æ¨¡å¼ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
    dataCharacteristics.accessPattern match {
      case "frequent_read" =>
        config("spark.storage.memoryFraction") = "0.8"
        config("spark.storage.unrollFraction") = "0.2"
        
      case "write_heavy" =>
        config("spark.storage.memoryFraction") = "0.4"
        config("spark.shuffle.memoryFraction") = "0.4"
        
      case "mixed" =>
        config("spark.storage.memoryFraction") = "0.6"
        config("spark.shuffle.memoryFraction") = "0.2"
    }
    
    config.toMap
  }
  
  private def generateStorageRecommendations(metrics: Seq[RDDStorageMetrics]): List[String] = {
    val recommendations = scala.collection.mutable.ListBuffer[String]()
    
    // æ£€æŸ¥æœªå……åˆ†åˆ©ç”¨çš„ç¼“å­˜
    val underutilizedCache = metrics.filter { metric =>
      metric.numCachedPartitions.toDouble / metric.numPartitions < 0.5
    }
    
    if (underutilizedCache.nonEmpty) {
      recommendations += s"Found ${underutilizedCache.length} RDDs with underutilized cache. Consider unpersisting unused RDDs."
    }
    
    // æ£€æŸ¥ç£ç›˜æº¢å‡º
    val diskSpillRDDs = metrics.filter(_.diskSize > 0)
    if (diskSpillRDDs.nonEmpty) {
      recommendations += s"Found ${diskSpillRDDs.length} RDDs spilled to disk. Consider increasing memory or using more efficient storage levels."
    }
    
    // æ£€æŸ¥å­˜å‚¨çº§åˆ«ä¼˜åŒ–
    val memoryOnlyRDDs = metrics.filter(_.storageLevel.contains("MEMORY_ONLY"))
    if (memoryOnlyRDDs.length > 5) {
      recommendations += "Many RDDs using MEMORY_ONLY storage. Consider using MEMORY_AND_DISK for better fault tolerance."
    }
    
    recommendations.toList
  }
}

case class RDDStorageMetrics(
  rddId: Int,
  rddName: String,
  storageLevel: String,
  numPartitions: Int,
  numCachedPartitions: Int,
  memorySize: Long,
  diskSize: Long,
  externalBlockStoreSize: Long
)

case class StorageUsageReport(
  rddMetrics: Seq[RDDStorageMetrics],
  totalMemoryUsed: Long,
  totalDiskUsed: Long,
  recommendations: List[String]
)

case class DataCharacteristics(
  dataType: String, // structured, semi_structured, unstructured
  dataSize: Long,
  accessPattern: String // frequent_read, write_heavy, mixed
)
```

**ç»¼åˆè°ƒä¼˜å·¥å…·**ï¼š
```scala
// ç»¼åˆæ€§èƒ½è°ƒä¼˜å·¥å…·
class ComprehensiveSparkTuner(spark: SparkSession) {
  
  private val performanceAnalyzer = new SparkPerformanceAnalyzer(spark)
  private val memoryAnalyzer = new MemoryUsageAnalyzer(spark)
  private val networkAnalyzer = new NetworkPerformanceAnalyzer(spark)
  private val storageAnalyzer = new StorageOptimizationAnalyzer(spark)
  
  def generateOptimalConfiguration(
    workloadCharacteristics: WorkloadCharacteristics
  ): OptimizedSparkConfig = {
    
    // 1. åˆ†æå½“å‰æ€§èƒ½
    val performanceReport = performanceAnalyzer.analyzeJobPerformance(spark.sparkContext.applicationId)
    val memoryReport = memoryAnalyzer.analyzeMemoryUsage()
    val networkReport = networkAnalyzer.analyzeNetworkPerformance()
    val storageReport = storageAnalyzer.analyzeStorageUsage()
    
    // 2. ç”ŸæˆåŸºç¡€é…ç½®
    val baseConfig = generateBaseConfiguration(workloadCharacteristics)
    
    // 3. åº”ç”¨æ€§èƒ½ä¼˜åŒ–
    val performanceOptimizedConfig = applyPerformanceOptimizations(baseConfig, performanceReport)
    
    // 4. åº”ç”¨å†…å­˜ä¼˜åŒ–
    val memoryOptimizedConfig = applyMemoryOptimizations(performanceOptimizedConfig, memoryReport)
    
    // 5. åº”ç”¨ç½‘ç»œä¼˜åŒ–
    val networkOptimizedConfig = applyNetworkOptimizations(memoryOptimizedConfig, networkReport)
    
    // 6. åº”ç”¨å­˜å‚¨ä¼˜åŒ–
    val finalConfig = applyStorageOptimizations(networkOptimizedConfig, storageReport, workloadCharacteristics)
    
    OptimizedSparkConfig(
      configuration = finalConfig,
      performanceReport = performanceReport,
      memoryReport = memoryReport,
      networkReport = networkReport,
      storageReport = storageReport,
      optimizationSummary = generateOptimizationSummary(baseConfig, finalConfig)
    )
  }
  
  private def generateBaseConfiguration(workload: WorkloadCharacteristics): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]()
    
    // åŸºäºå·¥ä½œè´Ÿè½½ç±»å‹è®¾ç½®åŸºç¡€é…ç½®
    workload.workloadType match {
      case "batch_processing" =>
        config("spark.sql.adaptive.enabled") = "true"
        config("spark.sql.adaptive.coalescePartitions.enabled") = "true"
        config("spark.dynamicAllocation.enabled") = "true"
        
      case "streaming" =>
        config("spark.streaming.backpressure.enabled") = "true"
        config("spark.streaming.dynamicAllocation.enabled") = "true"
        config("spark.streaming.kafka.consumer.cache.enabled") = "true"
        
      case "machine_learning" =>
        config("spark.ml.cache.enabled") = "true"
        config("spark.sql.execution.arrow.pyspark.enabled") = "true"
        config("spark.serializer") = "org.apache.spark.serializer.KryoSerializer"
        
      case "interactive_analytics" =>
        config("spark.sql.adaptive.enabled") = "true"
        config("spark.sql.adaptive.skewJoin.enabled") = "true"
        config("spark.sql.cbo.enabled") = "true"
    }
    
    config.toMap
  }
  
  private def applyPerformanceOptimizations(
    baseConfig: Map[String, String], 
    report: JobPerformanceReport
  ): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]() ++ baseConfig
    
    if (report.totalTaskFailures > report.totalJobs * 0.1) {
      config("spark.task.maxAttempts") = "5"
      config("spark.stage.maxConsecutiveAttempts") = "8"
    }
    
    if (report.avgJobDuration > 3600000) { // 1å°æ—¶ä»¥ä¸Š
      config("spark.sql.adaptive.coalescePartitions.enabled") = "true"
      config("spark.sql.adaptive.coalescePartitions.minPartitionNum") = "1"
    }
    
    config.toMap
  }
  
  private def applyMemoryOptimizations(
    baseConfig: Map[String, String], 
    report: MemoryUsageReport
  ): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]() ++ baseConfig
    
    if (report.memoryUtilization > 0.9) {
      config("spark.executor.memory") = increaseMemory(baseConfig.getOrElse("spark.executor.memory", "1g"))
      config("spark.sql.execution.arrow.maxRecordsPerBatch") = "1000"
    }
    
    config.toMap
  }
  
  private def applyNetworkOptimizations(
    baseConfig: Map[String, String], 
    report: NetworkPerformanceReport
  ): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]() ++ baseConfig
    
    if (report.avgShuffleReadTime > 1000) {
      config("spark.network.timeout") = "800s"
      config("spark.shuffle.compress") = "true"
      config("spark.io.compression.codec") = "snappy"
    }
    
    config.toMap
  }
  
  private def applyStorageOptimizations(
    baseConfig: Map[String, String], 
    report: StorageUsageReport,
    workload: WorkloadCharacteristics
  ): Map[String, String] = {
    val config = scala.collection.mutable.Map[String, String]() ++ baseConfig
    
    if (report.totalDiskUsed > 0) {
      config("spark.storage.memoryFraction") = "0.8"
    }
    
    config.toMap
  }
  
  private def increaseMemory(currentMemory: String): String = {
    val pattern = """(\d+)([kmgKMG]?)""".r
    currentMemory.toLowerCase match {
      case pattern(value, unit) =>
        val newValue = (value.toInt * 1.5).toInt
        s"$newValue$unit"
      case _ => "2g"
    }
  }
  
  private def generateOptimizationSummary(
    baseConfig: Map[String, String], 
    finalConfig: Map[String, String]
  ): List[String] = {
    val changes = finalConfig.filter { case (key, value) =>
      baseConfig.get(key) != Some(value)
    }
    
    changes.map { case (key, value) =>
      s"$key: ${baseConfig.getOrElse(key, "default")} -> $value"
    }.toList
  }
}

case class WorkloadCharacteristics(
  workloadType: String, // batch_processing, streaming, machine_learning, interactive_analytics
  dataSize: Long,
  computeIntensity: String, // low, medium, high
  ioPattern: String // read_heavy, write_heavy, balanced
)

case class OptimizedSparkConfig(
  configuration: Map[String, String],
  performanceReport: JobPerformanceReport,
  memoryReport: MemoryUsageReport,
  networkReport: NetworkPerformanceReport,
  storageReport: StorageUsageReport,
  optimizationSummary: List[String]
)
```

é€šè¿‡è¿™äº›å®Œæ•´çš„è°ƒä¼˜å®æˆ˜å·¥å…·å’Œæ–¹æ³•ï¼Œæˆ‘å·²ç»ä¸ºSparkæ–‡æ¡£æ·»åŠ äº†ï¼š

1. **æ€§èƒ½ç“¶é¢ˆåˆ†æ**ï¼šå…¨é¢çš„æ€§èƒ½åˆ†æå·¥å…·å’ŒæŠ¥å‘Šç”Ÿæˆ
2. **å†…å­˜è°ƒä¼˜å®æˆ˜**ï¼šå†…å­˜ä½¿ç”¨åˆ†æã€é…ç½®ä¼˜åŒ–å’Œæœ€ä½³å®è·µ
3. **ç½‘ç»œè°ƒä¼˜å®æˆ˜**ï¼šç½‘ç»œæ€§èƒ½åˆ†æã€ç“¶é¢ˆè¯†åˆ«å’Œé…ç½®ä¼˜åŒ–
4. **å­˜å‚¨è°ƒä¼˜å®æˆ˜**ï¼šå­˜å‚¨ä½¿ç”¨åˆ†æã€æ ¼å¼ä¼˜åŒ–å’Œç¼“å­˜ç­–ç•¥
5. **ç»¼åˆè°ƒä¼˜å·¥å…·**ï¼šé›†æˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥çš„ç»¼åˆè°ƒä¼˜æ¡†æ¶

ç°åœ¨Sparkæ–‡æ¡£å·²ç»éå¸¸å®Œæ•´å’Œå®ç”¨ï¼Œæ¶µç›–äº†ä»åŸºç¡€æ¦‚å¿µåˆ°ä¼ä¸šçº§å®è·µçš„æ‰€æœ‰é‡è¦å†…å®¹ï¼Œå®Œå…¨ç¬¦åˆ.cursorrulesæ–‡ä»¶çš„è¦æ±‚ã€‚