## Sparké‡ç‚¹å†…å®¹æºç è®²è§£

### æ ¸å¿ƒæ¨¡å—æºç 

#### SparkContextåˆå§‹åŒ–æµç¨‹å›¾

```mermaid
graph TD
    A[SparkContextæ„é€ ] --> B[åˆ›å»ºSparkConfé…ç½®]
    B --> C[åˆ›å»ºSparkEnvè¿è¡Œç¯å¢ƒ]
    C --> D[åˆ›å»ºStatusTrackerçŠ¶æ€è·Ÿè¸ªå™¨]
    D --> E[åˆ›å»ºTaskSchedulerä»»åŠ¡è°ƒåº¦å™¨]
    E --> F[åˆ›å»ºDAGScheduler DAGè°ƒåº¦å™¨]
    F --> G[å¯åŠ¨TaskScheduler]
    G --> H[è®¾ç½®é»˜è®¤å¹¶è¡Œåº¦]
    H --> I[SparkContextåˆå§‹åŒ–å®Œæˆ]
    
    C --> C1[åˆ›å»ºSerializerManager]
    C --> C2[åˆ›å»ºBlockManager]
    C --> C3[åˆ›å»ºMemoryManager]
    C --> C4[åˆ›å»ºMetricsSystem]
    
    E --> E1[æ ¹æ®masteråˆ›å»ºè°ƒåº¦å™¨]
    E1 --> E2[Standaloneæ¨¡å¼]
    E1 --> E3[YARNæ¨¡å¼]
    E1 --> E4[Localæ¨¡å¼]
    
    style A fill:#e1f5fe
    style I fill:#e8f5e8
    style C fill:#fff3e0
    style F fill:#f3e5f5
```

#### SparkContextåˆå§‹åŒ–æºç åˆ†æ
```scala
// SparkContext.scala æ ¸å¿ƒåˆå§‹åŒ–æµç¨‹
class SparkContext(config: SparkConf) extends Logging {
  
  // 1. åˆ›å»ºSparkEnv - æ ¸å¿ƒè¿è¡Œç¯å¢ƒ
  private val env: SparkEnv = {
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, numCores, mockOutputCommitCoordinator)
  }
  
  // 2. åˆ›å»ºçŠ¶æ€è·Ÿè¸ªå™¨
  private val statusTracker = new SparkStatusTracker(this, sparkUI)
  
  // 3. åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨
  private val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
  private val taskScheduler = ts
  
  // 4. åˆ›å»ºDAGè°ƒåº¦å™¨
  private val dagScheduler = new DAGScheduler(this)
  
  // 5. å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨
  taskScheduler.start()
  
  // 6. è®¾ç½®é»˜è®¤å¹¶è¡Œåº¦
  private val defaultParallelism: Int = taskScheduler.defaultParallelism
  
  // æ ¸å¿ƒæ–¹æ³•ï¼šåˆ›å»ºRDD
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
  
  // æ ¸å¿ƒæ–¹æ³•ï¼šæäº¤ä½œä¸š
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
  }
}
```

#### RDDæ“ä½œæ‰§è¡Œæµç¨‹å›¾

```mermaid
graph TD
    A[RDDæ“ä½œè°ƒç”¨] --> B{æ“ä½œç±»å‹}
    B -->|Transformation| C[åˆ›å»ºæ–°RDD]
    B -->|Action| D[è§¦å‘ä½œä¸šæ‰§è¡Œ]
    
    C --> C1[æ„å»ºRDDè¡€ç»Ÿ]
    C1 --> C2[è®¾ç½®ä¾èµ–å…³ç³»]
    C2 --> C3[è¿”å›æ–°RDDå¯¹è±¡]
    C3 --> E[ç­‰å¾…Actionè§¦å‘]
    
    D --> D1[è°ƒç”¨SparkContext.runJob]
    D1 --> D2[DAGScheduler.runJob]
    D2 --> D3[æ„å»ºDAGå›¾]
    D3 --> D4[åˆ’åˆ†Stage]
    D4 --> D5[æäº¤Task]
    D5 --> D6[Executoræ‰§è¡Œ]
    D6 --> D7[è¿”å›ç»“æœ]
    
    style C fill:#e8f5e8
    style D fill:#ffebee
    style D3 fill:#fff3e0
    style D6 fill:#e1f5fe
```

#### RDDäº”å¤§ç‰¹æ€§å®ç°æµç¨‹

```mermaid
graph LR
    A[RDDå®ä¾‹åŒ–] --> B[getPartitions<br/>è·å–åˆ†åŒºåˆ—è¡¨]
    B --> C[compute<br/>å®šä¹‰è®¡ç®—å‡½æ•°]
    C --> D[getDependencies<br/>è®¾ç½®ä¾èµ–å…³ç³»]
    D --> E[partitioner<br/>è®¾ç½®åˆ†åŒºå™¨]
    E --> F[getPreferredLocations<br/>ä½ç½®åå¥½]
    F --> G[RDDåˆ›å»ºå®Œæˆ]
    
    style A fill:#e1f5fe
    style G fill:#e8f5e8
```

#### RDDæºç æ ¸å¿ƒå®ç°
```scala
// RDD.scala æ ¸å¿ƒæŠ½è±¡
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {

  // äº”å¤§ç‰¹æ€§çš„å…·ä½“å®ç°
  
  // 1. åˆ†åŒºåˆ—è¡¨
  protected def getPartitions: Array[Partition]
  
  // 2. è®¡ç®—å‡½æ•°
  def compute(split: Partition, context: TaskContext): Iterator[T]
  
  // 3. ä¾èµ–å…³ç³»
  protected def getDependencies: Seq[Dependency[_]] = deps
  
  // 4. åˆ†åŒºå™¨ï¼ˆå¯é€‰ï¼‰
  @transient val partitioner: Option[Partitioner] = None
  
  // 5. ä½ç½®åå¥½ï¼ˆå¯é€‰ï¼‰
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil
  
  // Transformationæ“ä½œå®ç°
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
  }
  
  def filter(f: T => Boolean): RDD[T] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) => iter.filter(cleanF),
      preservesPartitioning = true)
  }
  
  def reduceByKey(func: (T, T) => T): RDD[T] = self.withScope {
    reduceByKey(defaultPartitioner(self), func)
  }
  
  // Actionæ“ä½œå®ç°
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }
  
  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum
  
  def foreach(f: T => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }
}
```

### è°ƒåº¦å™¨æºç 

#### DAGSchedulerä½œä¸šæäº¤æµç¨‹å›¾

```mermaid
graph TD
    A[ç”¨æˆ·è°ƒç”¨Action] --> B[SparkContext.runJob]
    B --> C[DAGScheduler.runJob]
    C --> D[åˆ›å»ºActiveJob]
    D --> E[submitJob]
    E --> F[æ„å»ºDAGå›¾]
    F --> G[findMissingPartitions]
    G --> H[getMissingParentStages]
    H --> I{æ˜¯å¦æœ‰çˆ¶Stage}
    I -->|æœ‰| J[é€’å½’æäº¤çˆ¶Stage]
    I -->|æ— | K[submitMissingTasks]
    J --> L[ç­‰å¾…çˆ¶Stageå®Œæˆ]
    L --> K
    K --> M[åˆ›å»ºTaskSet]
    M --> N[TaskScheduler.submitTasks]
    N --> O[åˆ†å‘Taskåˆ°Executor]
    O --> P[Taskæ‰§è¡Œå®Œæˆ]
    P --> Q[Stageå®Œæˆ]
    Q --> R[æ£€æŸ¥åç»­Stage]
    R --> S[Jobå®Œæˆ]
    
    style A fill:#e1f5fe
    style F fill:#fff3e0
    style K fill:#e8f5e8
    style S fill:#c8e6c9
```

#### Stageåˆ’åˆ†ç®—æ³•æµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹Stageåˆ’åˆ†] --> B[ä»æœ€ç»ˆRDDå¼€å§‹]
    B --> C[éå†RDDä¾èµ–]
    C --> D{ä¾èµ–ç±»å‹}
    D -->|çª„ä¾èµ–| E[åŠ å…¥å½“å‰Stage]
    D -->|å®½ä¾èµ–| F[åˆ›å»ºæ–°Stageè¾¹ç•Œ]
    E --> G[ç»§ç»­éå†çˆ¶RDD]
    F --> H[åˆ›å»ºShuffleMapStage]
    G --> C
    H --> I[é€’å½’å¤„ç†çˆ¶RDD]
    I --> C
    C --> J{æ˜¯å¦è¿˜æœ‰æœªå¤„ç†RDD}
    J -->|æ˜¯| C
    J -->|å¦| K[Stageåˆ’åˆ†å®Œæˆ]
    
    style A fill:#e1f5fe
    style F fill:#ffebee
    style H fill:#fff3e0
    style K fill:#e8f5e8
```

#### DAGScheduleræºç åˆ†æ
```scala
// DAGScheduler.scala æ ¸å¿ƒè°ƒåº¦é€»è¾‘
class DAGScheduler(
    private[scheduler] val sc: SparkContext,
    private[scheduler] val taskScheduler: TaskScheduler,
    listenerBus: LiveListenerBus,
    mapOutputTracker: MapOutputTrackerMaster,
    blockManagerMaster: BlockManagerMaster,
    env: SparkEnv,
    clock: Clock = new SystemClock())
  extends Logging {

  // äº‹ä»¶å¤„ç†å¾ªç¯
  private val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  
  // æäº¤ä½œä¸šçš„æ ¸å¿ƒæ–¹æ³•
  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): Unit = {
    
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    ThreadUtils.awaitReady(waiter, Duration.Inf)
    waiter.value.get match {
      case scala.util.Success(_) =>
        logInfo("Job %d finished: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =>
        logInfo("Job %d failed: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        throw exception
    }
  }
  
  // Stageåˆ’åˆ†æ ¸å¿ƒç®—æ³•
  private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
    
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      case Some(stage) =>
        stage
        
      case None =>
        // é€’å½’åˆ›å»ºçˆ¶Stage
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }
  
  // æŸ¥æ‰¾ç¼ºå¤±çš„çˆ¶ä¾èµ–
  private def getMissingAncestorShuffleDependencies(
      rdd: RDD[_]): ArrayStack[ShuffleDependency[_, _, _]] = {
    val ancestors = new ArrayStack[ShuffleDependency[_, _, _]]
    val visited = new HashSet[RDD[_]]
    val waitingForVisit = new ArrayStack[RDD[_]]
    
    waitingForVisit.push(rdd)
    while (waitingForVisit.nonEmpty) {
      val toVisit = waitingForVisit.pop()
      if (!visited(toVisit)) {
        visited += toVisit
        toVisit.dependencies.foreach {
          case shuffleDep: ShuffleDependency[_, _, _] =>
            if (!shuffleIdToMapStage.contains(shuffleDep.shuffleId)) {
              ancestors.push(shuffleDep)
              waitingForVisit.push(shuffleDep.rdd)
            }
          case narrowDep: NarrowDependency[_] =>
            waitingForVisit.push(narrowDep.rdd)
        }
      }
    }
    ancestors
  }
  
  // æäº¤Stage
  private def submitStage(stage: Stage): Unit = {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        val missing = getMissingParentStages(stage).sortBy(_.id)
        if (missing.isEmpty) {
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent <- missing) {
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    }
  }
}
```

### å­˜å‚¨ç³»ç»Ÿæºç 

#### BlockManageræºç åˆ†æ
```scala
// BlockManager.scala å­˜å‚¨ç®¡ç†æ ¸å¿ƒ
class BlockManager(
    executorId: String,
    rpcEnv: RpcEnv,
    val master: BlockManagerMaster,
    val serializerManager: SerializerManager,
    val conf: SparkConf,
    memoryManager: MemoryManager,
    mapOutputTracker: MapOutputTracker,
    shuffleManager: ShuffleManager,
    val blockTransferService: BlockTransferService,
    securityManager: SecurityManager,
    numUsableCores: Int)
  extends BlockDataManager with BlockEvictionHandler with Logging {

  // å†…å­˜å­˜å‚¨
  private[spark] val memoryStore =
    new MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this)
  
  // ç£ç›˜å­˜å‚¨
  private[spark] val diskStore = new DiskStore(conf, diskBlockManager, securityManager)
  
  // è·å–Blockçš„æ ¸å¿ƒæ–¹æ³•
  def getBlockData(blockId: BlockId): ManagedBuffer = {
    if (blockId.isShuffle) {
      shuffleManager.shuffleBlockResolver.getBlockData(blockId.asInstanceOf[ShuffleBlockId])
    } else {
      getLocalBytes(blockId) match {
        case Some(blockData) =>
          new BlockManagerManagedBuffer(blockInfoManager, blockId, blockData, true)
        case None =>
          throw new BlockNotFoundException(s"Block $blockId not found")
      }
    }
  }
  
  // å­˜å‚¨Blockçš„æ ¸å¿ƒæ–¹æ³•
  def putBlockData(
      blockId: BlockId,
      data: ManagedBuffer,
      level: StorageLevel,
      classTag: ClassTag[_]): Boolean = {
    putBytes(blockId, new ChunkedByteBuffer(data.nioByteBuffer()), level)(classTag)
  }
  
  // å†…å­˜å’Œç£ç›˜å­˜å‚¨é€»è¾‘
  private def doPutBytes[T](
      blockId: BlockId,
      bytes: ChunkedByteBuffer,
      level: StorageLevel,
      classTag: ClassTag[T],
      tellMaster: Boolean = true,
      keepReadLock: Boolean = false): Boolean = {
    
    doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) { info =>
      val startTimeMs = System.currentTimeMillis
      
      // å°è¯•å†…å­˜å­˜å‚¨
      val res = if (level.useMemory) {
        memoryStore.putBytes(blockId, bytes, level.memoryStorageLevel)
      } else {
        false
      }
      
      // å†…å­˜å­˜å‚¨å¤±è´¥ï¼Œå°è¯•ç£ç›˜å­˜å‚¨
      if (!res && level.useDisk) {
        diskStore.putBytes(blockId, bytes)
      } else {
        res
      }
    }
  }
  
  // Blockæ·˜æ±°ç­–ç•¥
  override def dropFromMemory(
      blockId: BlockId,
      data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel = {
    
    val info = blockInfoManager.lockForWriting(blockId)
    var blockIsUpdated = false
    val level = info.level
    
    try {
      if (level.useDisk && !diskStore.contains(blockId)) {
        data() match {
          case Left(elements) =>
            diskStore.put(blockId) { fileOutputStream =>
              serializerManager.dataSerializeStream(blockId,
                fileOutputStream, elements.toIterator)(info.classTag.asInstanceOf[ClassTag[T]])
            }
          case Right(bytes) =>
            diskStore.putBytes(blockId, bytes)
        }
        blockIsUpdated = true
      }
      
      memoryStore.remove(blockId)
      val droppedMemorySize = if (blockIsUpdated) 0L else info.size
      val blockIsRemoved = !level.useDisk
      
      if (blockIsRemoved) {
        blockInfoManager.removeBlock(blockId)
      }
      
      if (blockIsUpdated) {
        StorageLevel.DISK_ONLY
      } else {
        StorageLevel.NONE
      }
      
    } finally {
      blockInfoManager.unlock(blockId)
    }
  }
}
```

### ç½‘ç»œé€šä¿¡æºç 

#### NettyBlockTransferServiceæºç 
```scala
// NettyBlockTransferService.scala ç½‘ç»œä¼ è¾“æ ¸å¿ƒ
class NettyBlockTransferService(
    conf: SparkConf,
    securityManager: SecurityManager,
    bindAddress: String,
    advertiseAddress: String,
    numCores: Int)
  extends BlockTransferService {

  private[this] var transportContext: TransportContext = _
  private[this] var server: TransportServer = _
  private[this] var clientFactory: TransportClientFactory = _
  
  override def init(blockDataManager: BlockDataManager): Unit = {
    val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager)
    var serverBootstrap: Option[TransportServerBootstrap] = None
    var clientBootstrap: Option[TransportClientBootstrap] = None
    
    if (authEnabled) {
      serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager))
      clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager))
    }
    
    transportContext = new TransportContext(transportConf, rpcHandler)
    clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava)
    server = createServer(serverBootstrap.toList)
  }
  
  // è·å–è¿œç¨‹Block
  override def fetchBlocks(
      host: String,
      port: Int,
      execId: String,
      blockIds: Array[String],
      listener: BlockFetchingListener,
      tempFileManager: DownloadFileManager): Unit = {
    
    try {
      val client = clientFactory.createClient(host, port)
      new OneForOneBlockFetcher(client, conf.getAppId, execId,
        blockIds, listener, transportConf, tempFileManager).start()
    } catch {
      case e: Exception =>
        logError(s"Exception while beginning fetchBlocks", e)
        blockIds.foreach(listener.onBlockFetchFailure(_, e))
    }
  }
  
  // ä¸Šä¼ Blockåˆ°è¿œç¨‹
  override def uploadBlock(
      hostname: String,
      port: Int,
      execId: String,
      blockId: BlockId,
      blockData: ManagedBuffer,
      level: StorageLevel,
      classTag: ClassTag[_]): Future[Unit] = {
    
    val result = Promise[Unit]()
    val client = clientFactory.createClient(hostname, port)
    
    val callback = new RpcResponseCallback {
      override def onSuccess(response: ByteBuffer): Unit = {
        result.success(())
      }
      
      override def onFailure(e: Throwable): Unit = {
        result.failure(e)
      }
    }
    
    client.sendRpc(new UploadBlock(conf.getAppId, execId, blockId.toString,
      blockData.nioByteBuffer(), level, classTag).toByteBuffer, callback)
    
    result.future
  }
}
```
### ç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹æºç è¯¦è§£ 

#### GroupByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹

**GroupByKeyå†…å­˜å­˜å‚¨æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[GroupByKeyç®—å­è°ƒç”¨] --> B[åˆ›å»ºGroupByKeyRDD]
    B --> C[computeæ–¹æ³•æ‰§è¡Œ]
    C --> D[åˆ›å»ºAggregatorèšåˆå™¨]
    D --> E[åˆ›å»ºExternalAppendOnlyMap]
    E --> F[éå†è¾“å…¥æ•°æ®]
    F --> G[æ’å…¥é”®å€¼å¯¹åˆ°Map]
    G --> H{å†…å­˜æ˜¯å¦è¶…é™}
    H -->|å¦| I[ç»§ç»­æ’å…¥æ•°æ®]
    H -->|æ˜¯| J[è§¦å‘Spillæ“ä½œ]
    J --> K[å°†å†…å­˜æ•°æ®å†™å…¥ç£ç›˜]
    K --> L[æ¸…ç©ºå†…å­˜Map]
    L --> I
    I --> M{æ˜¯å¦è¿˜æœ‰æ•°æ®}
    M -->|æ˜¯| F
    M -->|å¦| N[åˆå¹¶å†…å­˜å’Œç£ç›˜æ•°æ®]
    N --> O[è¿”å›èšåˆç»“æœ]
    
    style A fill:#e1f5fe
    style J fill:#ffebee
    style N fill:#fff3e0
    style O fill:#e8f5e8
```

**ExternalAppendOnlyMapå†…å­˜ç®¡ç†æµç¨‹**ï¼š

```mermaid
graph TD
    A[æ•°æ®æ’å…¥è¯·æ±‚] --> B[SizeTrackingAppendOnlyMap.insert]
    B --> C[è®¡ç®—Hashå€¼å®šä½]
    C --> D{é”®æ˜¯å¦å­˜åœ¨}
    D -->|å­˜åœ¨| E[åˆå¹¶å€¼ mergeValue]
    D -->|ä¸å­˜åœ¨| F[åˆ›å»ºæ–°å€¼ createCombiner]
    E --> G[æ›´æ–°å†…å­˜ä½¿ç”¨é‡]
    F --> G
    G --> H{å†…å­˜ä½¿ç”¨ > é˜ˆå€¼}
    H -->|å¦| I[æ’å…¥å®Œæˆ]
    H -->|æ˜¯| J[æ‰§è¡ŒSpillæ“ä½œ]
    J --> K[æ’åºå†…å­˜æ•°æ®]
    K --> L[å†™å…¥ä¸´æ—¶æ–‡ä»¶]
    L --> M[æ·»åŠ åˆ°spillsåˆ—è¡¨]
    M --> N[é‡ç½®å†…å­˜Map]
    N --> I
    
    style A fill:#e1f5fe
    style J fill:#ffebee
    style K fill:#fff3e0
    style I fill:#e8f5e8
```

```scala
// GroupByKeyç®—å­æ ¸å¿ƒå®ç°
class GroupByKeyRDD[K: ClassTag, V: ClassTag](
    prev: RDD[(K, V)],
    part: Partitioner)
  extends RDD[(K, Iterable[V])](prev) {

  override def compute(split: Partition, context: TaskContext): Iterator[(K, Iterable[V])] = {
    // 1. åˆ›å»ºèšåˆå™¨
    val aggregator = new Aggregator[K, V, ArrayBuffer[V]](
      createCombiner = (v: V) => ArrayBuffer(v),
      mergeValue = (buf: ArrayBuffer[V], v: V) => buf += v,
      mergeCombiners = (buf1: ArrayBuffer[V], buf2: ArrayBuffer[V]) => buf1 ++= buf2
    )
    
    // 2. ä½¿ç”¨ExternalAppendOnlyMapè¿›è¡Œèšåˆ
    val externalMap = new ExternalAppendOnlyMap[K, V, ArrayBuffer[V]](aggregator)
    
    // 3. æ’å…¥æ‰€æœ‰é”®å€¼å¯¹
    val iter = firstParent[(K, V)].iterator(split, context)
    while (iter.hasNext) {
      val (k, v) = iter.next()
      externalMap.insert(k, v)
    }
    
    // 4. è¿”å›èšåˆç»“æœ
    externalMap.iterator
  }
}
```



```scala
// ExternalAppendOnlyMapæ ¸å¿ƒå®ç°
class ExternalAppendOnlyMap[K, V, C](
    aggregator: Aggregator[K, V, C],
    serializer: Serializer = SparkEnv.get.serializer)
  extends Spillable[WritablePartitionedPairCollection[K, C]](SparkEnv.get.blockManager.master)
  with Logging {

  // å†…å­˜ä¸­çš„Map
  private var map = new SizeTrackingAppendOnlyMap[K, C]
  
  // Spillæ–‡ä»¶åˆ—è¡¨
  private val spills = new ArrayBuffer[SpilledFile]
  
  // æ’å…¥é”®å€¼å¯¹
  def insert(key: K, value: V): Unit = {
    // 1. å°è¯•åœ¨å†…å­˜ä¸­èšåˆ
    val update = (hadValue: Boolean, oldValue: C) => {
      if (hadValue) {
        aggregator.mergeValue(oldValue, value)
      } else {
        aggregator.createCombiner(value)
      }
    }
    
    map.changeValue(key, update)
    
    // 2. æ£€æŸ¥æ˜¯å¦éœ€è¦Spill
    if (map.estimateSize() > myMemoryThreshold) {
      spill()
    }
  }
  
  // Spillåˆ°ç£ç›˜
  private def spill(): Unit = {
    val spillFile = spillMemoryIteratorToDisk(map.destructiveSortedWritablePartitionedIterator())
    spills += spillFile
    map = new SizeTrackingAppendOnlyMap[K, C]
  }
  
  // è·å–æœ€ç»ˆç»“æœ
  def iterator: Iterator[(K, C)] = {
    // åˆå¹¶å†…å­˜ä¸­çš„ç»“æœå’ŒSpillæ–‡ä»¶
    val memoryIterator = map.destructiveSortedWritablePartitionedIterator()
    val spillIterators = spills.map(_.iterator)
    
    // è¿”å›åˆå¹¶åçš„è¿­ä»£å™¨
    new MergedIterator(memoryIterator +: spillIterators)
  }
}
```

#### ReduceByKeyç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹

**ReduceByKeyå†…å­˜å­˜å‚¨æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[ReduceByKeyç®—å­è°ƒç”¨] --> B[åˆ›å»ºShuffledRDD]
    B --> C[Mapç«¯é¢„èšåˆ]
    C --> D[åˆ›å»ºPartitionedAppendOnlyMap]
    D --> E[éå†è¾“å…¥æ•°æ®]
    E --> F[è®¡ç®—Hashå€¼å®šä½]
    F --> G{é”®æ˜¯å¦å­˜åœ¨}
    G -->|å­˜åœ¨| H[åˆå¹¶å€¼ mergeValue]
    G -->|ä¸å­˜åœ¨| I[åˆ›å»ºæ–°å€¼ createCombiner]
    H --> J[æ›´æ–°å†…å­˜ä½¿ç”¨é‡]
    I --> J
    J --> K{å†…å­˜ä½¿ç”¨ > é˜ˆå€¼}
    K -->|å¦| L[ç»§ç»­å¤„ç†æ•°æ®]
    K -->|æ˜¯| M[æ‰§è¡ŒSpillæ“ä½œ]
    M --> N[æ’åºå¹¶å†™å…¥ç£ç›˜]
    N --> O[é‡ç½®å†…å­˜Map]
    O --> L
    L --> P{æ˜¯å¦è¿˜æœ‰æ•°æ®}
    P -->|æ˜¯| E
    P -->|å¦| Q[Shuffle Writeé˜¶æ®µ]
    Q --> R[åˆå¹¶å†…å­˜å’Œç£ç›˜æ•°æ®]
    R --> S[å†™å…¥Shuffleæ–‡ä»¶]
    
    style A fill:#e1f5fe
    style M fill:#ffebee
    style Q fill:#fff3e0
    style S fill:#e8f5e8
```

**PartitionedAppendOnlyMapæ“ä½œæµç¨‹**ï¼š

```mermaid
graph TD
    A[changeValueè°ƒç”¨] --> B[è®¡ç®—Hashå€¼]
    B --> C[è·å–ä½ç½®pos]
    C --> D[çº¿æ€§æ¢æµ‹æŸ¥æ‰¾]
    D --> E{æ‰¾åˆ°é”®}
    E -->|æ˜¯| F[æ›´æ–°ç°æœ‰å€¼]
    E -->|å¦| G[æ’å…¥æ–°é”®å€¼å¯¹]
    F --> H[è°ƒç”¨updateFunc]
    G --> I[è°ƒç”¨updateFuncåˆ›å»ºæ–°å€¼]
    H --> J[æ›´æ–°å®Œæˆ]
    I --> K[å¢åŠ curSize]
    K --> L{curSize >= growThreshold}
    L -->|æ˜¯| M[æ‰©å®¹Hashè¡¨]
    L -->|å¦| J
    M --> N[é‡æ–°Hashæ‰€æœ‰å…ƒç´ ]
    N --> J
    
    style A fill:#e1f5fe
    style M fill:#ffebee
    style J fill:#e8f5e8
```

```scala
// PartitionedAppendOnlyMapçš„changeValueæ–¹æ³•
def changeValue(key: K, updateFunc: (Boolean, V) => V): Unit = {
  val hash = getHash(key)
  val pos = getPos(hash)
  
  var i = pos
  while (data(2 * i) != null) {
    if (data(2 * i) == key) {
      // æ‰¾åˆ°ç°æœ‰é”®ï¼Œæ›´æ–°å€¼
      val hadValue = true
      val oldValue = data(2 * i + 1).asInstanceOf[V]
      val newValue = updateFunc(hadValue, oldValue)
      data(2 * i + 1) = newValue.asInstanceOf[AnyRef]
      return
    }
    i = (i + 1) % (data.length / 2)
  }
  
  // æœªæ‰¾åˆ°é”®ï¼Œæ’å…¥æ–°å€¼
  val hadValue = false
  val newValue = updateFunc(hadValue, null.asInstanceOf[V])
  data(2 * i) = key.asInstanceOf[AnyRef]
  data(2 * i + 1) = newValue.asInstanceOf[AnyRef]
  curSize += 1
  
  if (curSize >= growThreshold) {
    growTable()
  }
}
```

#### Joinç®—å­å†…å­˜å­˜å‚¨è¿‡ç¨‹

**Joinç®—å­å†…å­˜å­˜å‚¨æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[Joinç®—å­è°ƒç”¨] --> B[åˆ›å»ºCoGroupedRDD]
    B --> C[computeæ–¹æ³•æ‰§è¡Œ]
    C --> D[åˆ›å»ºCoGroupAggregator]
    D --> E[åˆ›å»ºExternalAppendOnlyMap]
    E --> F[å¤„ç†ç¬¬ä¸€ä¸ªRDDæ•°æ®]
    F --> G[æ’å…¥æ•°æ®æ ‡è®°rddIndex=0]
    G --> H{å†…å­˜æ˜¯å¦è¶…é™}
    H -->|æ˜¯| I[æ‰§è¡ŒSpillæ“ä½œ]
    H -->|å¦| J[å¤„ç†ç¬¬äºŒä¸ªRDDæ•°æ®]
    I --> K[å†™å…¥ç£ç›˜æ–‡ä»¶]
    K --> J
    J --> L[æ’å…¥æ•°æ®æ ‡è®°rddIndex=1]
    L --> M{å†…å­˜æ˜¯å¦è¶…é™}
    M -->|æ˜¯| N[æ‰§è¡ŒSpillæ“ä½œ]
    M -->|å¦| O[åˆå¹¶æ‰€æœ‰æ•°æ®]
    N --> P[å†™å…¥ç£ç›˜æ–‡ä»¶]
    P --> O
    O --> Q[æŒ‰Keyåˆ†ç»„]
    Q --> R[ç”ŸæˆJoinç»“æœ]
    
    style A fill:#e1f5fe
    style I fill:#ffebee
    style N fill:#ffebee
    style R fill:#e8f5e8
```

**CoGroupèšåˆè¿‡ç¨‹æµç¨‹**ï¼š

```mermaid
graph TD
    A[CoGroupèšåˆå¼€å§‹] --> B[åˆå§‹åŒ–ç»“æœæ•°ç»„]
    B --> C[éå†è¾“å…¥æ•°æ®]
    C --> D[è§£ærddIndexå’Œvalue]
    D --> E{Keyæ˜¯å¦å­˜åœ¨}
    E -->|å­˜åœ¨| F[è·å–ç°æœ‰æ•°ç»„ç»„]
    E -->|ä¸å­˜åœ¨| G[åˆ›å»ºæ–°æ•°ç»„ç»„]
    F --> H[å°†valueæ·»åŠ åˆ°å¯¹åº”rddIndexçš„æ•°ç»„]
    G --> I[åˆå§‹åŒ–æ‰€æœ‰RDDçš„ç©ºæ•°ç»„]
    I --> H
    H --> J{æ˜¯å¦è¿˜æœ‰æ•°æ®}
    J -->|æ˜¯| C
    J -->|å¦| K[è¿”å›åˆ†ç»„ç»“æœ]
    K --> L[è½¬æ¢ä¸ºIterableæ ¼å¼]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style K fill:#e8f5e8
```

```scala
// CoGroupedRDDæ ¸å¿ƒå®ç°
class CoGroupedRDD[K: ClassTag](
    rdds: Seq[RDD[(K, _)]],
    part: Partitioner)
  extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil) {

  override def compute(split: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] = {
    // 1. åˆ›å»ºCoGroupèšåˆå™¨
    val aggregator = new CoGroupAggregator[K]
    
    // 2. ä½¿ç”¨ExternalAppendOnlyMapè¿›è¡Œåˆ†ç»„
    val externalMap = new ExternalAppendOnlyMap[K, (Int, Any), Array[ArrayBuffer[Any]]](aggregator)
    
    // 3. æ’å…¥æ‰€æœ‰RDDçš„æ•°æ®
    rdds.zipWithIndex.foreach { case (rdd, rddIndex) =>
      val iter = rdd.iterator(split, context)
      while (iter.hasNext) {
        val (k, v) = iter.next()
        externalMap.insert(k, (rddIndex, v))
      }
    }
    
    // 4. è¿”å›åˆ†ç»„ç»“æœ
    externalMap.iterator.map { case (k, groups) =>
      (k, groups.map(_.toIterable))
    }
  }
}
```

#### å†…å­˜å­˜å‚¨çŠ¶æ€ç›‘æ§



```scala
// å†…å­˜ä½¿ç”¨ç›‘æ§ç»„ä»¶
class MemoryMonitor {
  // ç›‘æ§Mapçš„å†…å­˜ä½¿ç”¨
  def monitorMapMemory(map: SizeTrackingAppendOnlyMap[_, _]): MemoryUsage = {
    val estimatedSize = map.estimateSize()
    val currentMemory = map.currentMemory
    val maxMemory = map.maxMemory
    
    MemoryUsage(
      estimatedSize = estimatedSize,
      currentMemory = currentMemory,
      maxMemory = maxMemory,
      utilization = currentMemory.toDouble / maxMemory
    )
  }
  
  // ç›‘æ§SpillçŠ¶æ€
  def monitorSpillStatus(externalMap: ExternalAppendOnlyMap[_, _, _]): SpillStatus = {
    val spillCount = externalMap.spills.size
    val totalSpillSize = externalMap.spills.map(_.size).sum
    
    SpillStatus(
      spillCount = spillCount,
      totalSpillSize = totalSpillSize,
      averageSpillSize = if (spillCount > 0) totalSpillSize / spillCount else 0
    )
  }
}

case class MemoryUsage(
  estimatedSize: Long,
  currentMemory: Long,
  maxMemory: Long,
  utilization: Double)

case class SpillStatus(
  spillCount: Int,
  totalSpillSize: Long,
  averageSpillSize: Long)
```



```mermaid
graph TD
    A[è¾“å…¥æ•°æ®] --> B[PartitionedAppendOnlyMap]
    B --> C{å†…å­˜æ˜¯å¦è¶³å¤Ÿ?}
    C -->|æ˜¯| D[å†…å­˜èšåˆ]
    C -->|å¦| E[Spillåˆ°ç£ç›˜]
    D --> F[è¿”å›ç»“æœ]
    E --> G[ExternalAppendOnlyMap]
    G --> H[åˆå¹¶å†…å­˜å’Œç£ç›˜æ•°æ®]
    H --> F
    
    I[MemoryMonitor] --> B
    I --> G
    J[SpillMonitor] --> E
```

#### å†…å­˜å­˜å‚¨ä¼˜åŒ–ç­–ç•¥



```scala
// å†…å­˜åˆ†é…ä¼˜åŒ–
class MemoryOptimizer {
  // åŠ¨æ€è°ƒæ•´å†…å­˜é˜ˆå€¼
  def adjustMemoryThreshold(
      currentMemory: Long,
      maxMemory: Long,
      spillCount: Int): Long = {
    
    val utilization = currentMemory.toDouble / maxMemory
    
    if (utilization > 0.8 && spillCount > 0) {
      // å†…å­˜ä½¿ç”¨ç‡é«˜ä¸”æœ‰Spillï¼Œé™ä½é˜ˆå€¼
      (maxMemory * 0.6).toLong
    } else if (utilization < 0.5 && spillCount == 0) {
      // å†…å­˜ä½¿ç”¨ç‡ä½ä¸”æ— Spillï¼Œæé«˜é˜ˆå€¼
      (maxMemory * 0.9).toLong
    } else {
      // ä¿æŒå½“å‰é˜ˆå€¼
      (maxMemory * 0.8).toLong
    }
  }
  
  // ä¼˜åŒ–Mapåˆå§‹å®¹é‡
  def optimizeInitialCapacity(dataSize: Long): Int = {
    val estimatedSize = (dataSize * 1.2).toInt
    math.max(64, math.min(estimatedSize, 1024 * 1024))
  }
}
```

---

### ä»»åŠ¡æäº¤æµç¨‹æºç è§£æ

#### DAGçš„ç”Ÿæˆä¸ä¾èµ–åˆ†æ

**ä»»åŠ¡æäº¤å®Œæ•´æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[ç”¨æˆ·è°ƒç”¨Action] --> B[SparkContext.runJob]
    B --> C[DAGScheduler.runJob]
    C --> D[åˆ›å»ºActiveJob]
    D --> E[submitJobäº‹ä»¶]
    E --> F[handleJobSubmitted]
    F --> G[åˆ›å»ºResultStage]
    G --> H[getOrCreateParentStages]
    H --> I[é€’å½’åˆ†æRDDä¾èµ–]
    I --> J{ä¾èµ–ç±»å‹åˆ¤æ–­}
    J -->|çª„ä¾èµ–| K[ç»§ç»­å‘ä¸Šéå†]
    J -->|å®½ä¾èµ–| L[åˆ›å»ºShuffleMapStage]
    K --> I
    L --> M[submitStage]
    M --> N[getMissingParentStages]
    N --> O{çˆ¶Stageæ˜¯å¦å®Œæˆ}
    O -->|æœªå®Œæˆ| P[é€’å½’æäº¤çˆ¶Stage]
    O -->|å·²å®Œæˆ| Q[submitMissingTasks]
    P --> M
    Q --> R[åˆ›å»ºTaskSet]
    R --> S[TaskScheduler.submitTasks]
    S --> T[èµ„æºåˆ†é…ä¸ä»»åŠ¡åˆ†å‘]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style L fill:#ffebee
    style T fill:#e8f5e8
```

**DAGä¾èµ–åˆ†ææµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[å¼€å§‹ä¾èµ–åˆ†æ] --> B[ä»ç›®æ ‡RDDå¼€å§‹]
    B --> C[è·å–RDDä¾èµ–åˆ—è¡¨]
    C --> D[éå†æ¯ä¸ªä¾èµ–]
    D --> E{ä¾èµ–ç±»å‹}
    E -->|NarrowDependency| F[çª„ä¾èµ–å¤„ç†]
    E -->|ShuffleDependency| G[å®½ä¾èµ–å¤„ç†]
    F --> H[é€’å½’åˆ†æçˆ¶RDD]
    G --> I[åˆ›å»ºStageè¾¹ç•Œ]
    I --> J[åˆ›å»ºShuffleMapStage]
    J --> K[é€’å½’åˆ†æShuffleçˆ¶RDD]
    H --> L{æ˜¯å¦è¿˜æœ‰ä¾èµ–}
    K --> L
    L -->|æ˜¯| D
    L -->|å¦| M[ä¾èµ–åˆ†æå®Œæˆ]
    M --> N[è¿”å›Stageåˆ—è¡¨]
    
    style A fill:#e1f5fe
    style I fill:#ffebee
    style N fill:#e8f5e8
```

**ç”¨æˆ·è§¦å‘Actionæ—¶çš„å®Œæ•´æµç¨‹**ï¼š

```scala
// ç”¨æˆ·ä»£ç è§¦å‘Action
val result = rdd.collect()

// SparkContext.collect()
def collect(): Array[T] = withScope {
  val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  Array.concat(results: _*)
}

// SparkContext.runJob()
def runJob[T, U: ClassTag](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    partitions: Seq[Int],
    resultHandler: (Int, U) => Unit): Unit = {
  dagScheduler.runJob(rdd, func, partitions, callSite, resultHandler, localProperties.get)
}
```

**DAGSchedulerä¾èµ–åˆ†æ**ï¼š

```scala
// DAGScheduler.scala
private[scheduler] def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  val parents = new ArrayBuffer[Stage]()
  val visited = new HashSet[RDD[_]]
  
  def visit(r: RDD[_]): Unit = {
    if (!visited(r)) {
      visited += r
      for (dep <- r.dependencies) {
        dep match {
          case shufDep: ShuffleDependency[_, _, _] =>
            // å®½ä¾èµ–ï¼Œåˆ›å»ºæ–°çš„ShuffleMapStage
            parents += getOrCreateShuffleMapStage(shufDep, firstJobId)
          case _ =>
            // çª„ä¾èµ–ï¼Œé€’å½’è®¿é—®çˆ¶RDD
            visit(dep.rdd)
        }
      }
    }
  }
  
  visit(rdd)
  parents.toList
}
```

#### ä»»åŠ¡åˆ†å‘ä¸è°ƒåº¦æµç¨‹

**ä»»åŠ¡è°ƒåº¦å®Œæ•´æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[TaskScheduler.submitTasks] --> B[åˆ›å»ºTaskSetManager]
    B --> C[æ·»åŠ åˆ°è°ƒåº¦æ± ]
    C --> D[resourceOffersè°ƒç”¨]
    D --> E[è·å–å¯ç”¨èµ„æº]
    E --> F[æŒ‰æœ¬åœ°æ€§çº§åˆ«æ’åº]
    F --> G[PROCESS_LOCALä¼˜å…ˆ]
    G --> H{æ˜¯å¦æœ‰æœ¬åœ°ä»»åŠ¡}
    H -->|æœ‰| I[åˆ†é…æœ¬åœ°ä»»åŠ¡]
    H -->|æ— | J[NODE_LOCALçº§åˆ«]
    J --> K{æ˜¯å¦æœ‰èŠ‚ç‚¹æœ¬åœ°ä»»åŠ¡}
    K -->|æœ‰| L[åˆ†é…èŠ‚ç‚¹æœ¬åœ°ä»»åŠ¡]
    K -->|æ— | M[RACK_LOCALçº§åˆ«]
    M --> N{æ˜¯å¦æœ‰æœºæ¶æœ¬åœ°ä»»åŠ¡}
    N -->|æœ‰| O[åˆ†é…æœºæ¶æœ¬åœ°ä»»åŠ¡]
    N -->|æ— | P[ANYçº§åˆ«åˆ†é…]
    I --> Q[åˆ›å»ºTaskDescription]
    L --> Q
    O --> Q
    P --> Q
    Q --> R[å‘é€åˆ°Executor]
    R --> S[Taskæ‰§è¡Œ]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style Q fill:#e8f5e8
    style S fill:#c8e6c9
```

**èµ„æºåˆ†é…æœ¬åœ°æ€§ä¼˜åŒ–æµç¨‹**ï¼š

```mermaid
graph TD
    A[å¼€å§‹èµ„æºåˆ†é…] --> B[è·å–TaskSeté˜Ÿåˆ—]
    B --> C[éå†æ¯ä¸ªTaskSet]
    C --> D[è·å–æœ¬åœ°æ€§çº§åˆ«åˆ—è¡¨]
    D --> E[PROCESS_LOCAL]
    E --> F{æœ‰å¯ç”¨Executor}
    F -->|æ˜¯| G[åˆ†é…Taskåˆ°Executor]
    F -->|å¦| H[NODE_LOCAL]
    H --> I{æœ‰å¯ç”¨Node}
    I -->|æ˜¯| J[åˆ†é…Taskåˆ°Node]
    I -->|å¦| K[NO_PREF]
    K --> L[æ— ä½ç½®åå¥½åˆ†é…]
    L --> M[RACK_LOCAL]
    M --> N{æœ‰å¯ç”¨Rack}
    N -->|æ˜¯| O[åˆ†é…Taskåˆ°Rack]
    N -->|å¦| P[ANY]
    P --> Q[ä»»æ„ä½ç½®åˆ†é…]
    G --> R[æ›´æ–°èµ„æºçŠ¶æ€]
    J --> R
    O --> R
    Q --> R
    R --> S{è¿˜æœ‰Task}
    S -->|æ˜¯| C
    S -->|å¦| T[åˆ†é…å®Œæˆ]
    
    style A fill:#e1f5fe
    style E fill:#fff3e0
    style T fill:#e8f5e8
```

**å®Œæ•´çš„ä»»åŠ¡è°ƒåº¦æ—¶åºå›¾**ï¼š

```mermaid
sequenceDiagram
  participant User
  participant SparkContext
  participant DAGScheduler
  participant TaskScheduler
  participant SchedulerBackend
  participant Executor

  User->>SparkContext: è§¦å‘Action (collect/count)
  SparkContext->>DAGScheduler: runJob
  DAGScheduler->>DAGScheduler: æ„å»ºDAG/åˆ’åˆ†Stage
  DAGScheduler->>TaskScheduler: submitTasks(TaskSet)
  TaskScheduler->>SchedulerBackend: reviveOffers
  SchedulerBackend->>Executor: launchTasks
  Executor->>SchedulerBackend: statusUpdate
  SchedulerBackend->>TaskScheduler: statusUpdate
  TaskScheduler->>DAGScheduler: taskEnded
  DAGScheduler->>SparkContext: jobEnded
  SparkContext->>User: è¿”å›ç»“æœ
```

**TaskSchedulerèµ„æºåˆ†é…**ï¼š

```scala
// TaskSchedulerImpl.resourceOffers()
def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = {
  // 1. éšæœºæ‰“ä¹±offersé¿å…çƒ­ç‚¹
  val shuffledOffers = Random.shuffle(offers)
  val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK))
  val availableCpus = shuffledOffers.map(o => o.cores).toArray
  
  // 2. æŒ‰æœ¬åœ°æ€§çº§åˆ«åˆ†é…ä»»åŠ¡
  val sortedTaskSets = rootPool.getSortedTaskSetQueue
  for (taskSet <- sortedTaskSets) {
    // PROCESS_LOCAL -> NODE_LOCAL -> NO_PREF -> RACK_LOCAL -> ANY
    for (currentMaxLocality <- taskSet.myLocalityLevels) {
      do {
        launchedAnyTask = resourceOfferSingleTaskSet(
          taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)
      } while (launchedAnyTask)
    }
  }
  
  tasks
}
```

#### å¤±è´¥é‡è¯•ä¸å®¹é”™æœºåˆ¶

**å®¹é”™æœºåˆ¶æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[Taskæ‰§è¡Œå¤±è´¥] --> B{å¤±è´¥åŸå› åˆ†æ}
    B -->|FetchFailed| C[Shuffleæ•°æ®è·å–å¤±è´¥]
    B -->|TaskKilled| D[ä»»åŠ¡è¢«æ€æ­»]
    B -->|ExceptionFailure| E[ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸]
    B -->|ExecutorLostFailure| F[Executorä¸¢å¤±]
    
    C --> G[æ ‡è®°çˆ¶Stageå¤±è´¥]
    G --> H[é‡æ–°æäº¤çˆ¶Stage]
    H --> I[é‡æ–°è®¡ç®—Shuffleæ•°æ®]
    
    D --> J[æ£€æŸ¥é‡è¯•æ¬¡æ•°]
    E --> J
    J --> K{æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•}
    K -->|å¦| L[é‡æ–°è°ƒåº¦Task]
    K -->|æ˜¯| M[æ ‡è®°Stageå¤±è´¥]
    
    F --> N[ç§»é™¤ä¸¢å¤±çš„Executor]
    N --> O[é‡æ–°åˆ†é…èµ„æº]
    O --> P[é‡æ–°æäº¤æ‰€æœ‰Task]
    
    L --> Q[Taské‡æ–°æ‰§è¡Œ]
    I --> Q
    P --> Q
    M --> R[Jobå¤±è´¥]
    Q --> S[æ‰§è¡ŒæˆåŠŸ]
    
    style A fill:#ffebee
    style C fill:#fff3e0
    style R fill:#ffcdd2
    style S fill:#e8f5e8
```

**RDDè¡€ç»Ÿæ¢å¤æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[RDDåˆ†åŒºä¸¢å¤±] --> B[æŸ¥æ‰¾RDDè¡€ç»Ÿä¿¡æ¯]
    B --> C[éå†ä¾èµ–é“¾]
    C --> D{ä¾èµ–ç±»å‹}
    D -->|çª„ä¾èµ–| E[ç›´æ¥ä»çˆ¶RDDé‡ç®—]
    D -->|å®½ä¾èµ–| F[æ£€æŸ¥Shuffleæ•°æ®]
    F --> G{Shuffleæ•°æ®æ˜¯å¦å¯ç”¨}
    G -->|å¯ç”¨| H[ä»Shuffleæ•°æ®æ¢å¤]
    G -->|ä¸å¯ç”¨| I[é€’å½’é‡ç®—çˆ¶Stage]
    E --> J[é‡æ–°è®¡ç®—åˆ†åŒº]
    H --> J
    I --> K[é‡ç®—çˆ¶RDDåˆ†åŒº]
    K --> L[é‡æ–°æ‰§è¡ŒShuffle]
    L --> J
    J --> M[åˆ†åŒºæ¢å¤å®Œæˆ]
    
    style A fill:#ffebee
    style I fill:#fff3e0
    style M fill:#e8f5e8
```

**DAGScheduleräº‹ä»¶å¤„ç†**ï¼š

```scala
// DAGSchedulerEventProcessLoopäº‹ä»¶å¤„ç†
private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
  case TaskFailed(taskId, taskType, reason, exception) =>
    reason match {
      case _: FetchFailed =>
        // Shuffleæ•°æ®è·å–å¤±è´¥ï¼Œéœ€è¦é‡æ–°è®¡ç®—çˆ¶Stage
        val shuffleMapStage = shuffleIdToMapStage(reason.shuffleId)
        markStageAsFinished(shuffleMapStage, Some(reason.toString))
        submitStage(shuffleMapStage)
        
      case _: ExecutorLostFailure =>
        // Executorä¸¢å¤±ï¼Œéœ€è¦é‡æ–°è°ƒåº¦Task
        removeExecutorAndUnregisterOutputs(reason.execId, filesLost = true)
        
      case _: TaskKilled =>
        // Taskè¢«æ€æ­»ï¼Œé€šå¸¸æ˜¯æ¨æµ‹æ‰§è¡Œ
        logInfo(s"Task $taskId was killed")
        
      case _ =>
        // å…¶ä»–å¼‚å¸¸ï¼ŒTaskçº§åˆ«é‡è¯•
        if (task.attempt < maxTaskFailures) {
          taskScheduler.submitTasks(createTaskSet(Array(task)))
        } else {
          abortStage(currentStage, s"Task $taskId failed $maxTaskFailures times")
        }
    }
    
  case StageCompleted(stage) =>
    // Stageå®Œæˆï¼Œæ£€æŸ¥å¹¶æäº¤ä¾èµ–çš„Stage
    markStageAsFinished(stage)
    submitWaitingChildStages(stage)
}
```

#### Executorå·¥ä½œæœºåˆ¶ä¸Taskæ‰§è¡Œ

**Executorä»»åŠ¡æ‰§è¡Œæµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[æ¥æ”¶TaskDescription] --> B[åˆ›å»ºTaskRunner]
    B --> C[æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ]
    C --> D[TaskRunner.runå¼€å§‹]
    D --> E[ååºåˆ—åŒ–Taskå¯¹è±¡]
    E --> F[åˆ›å»ºTaskContext]
    F --> G[è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡]
    G --> H[è°ƒç”¨Task.runæ–¹æ³•]
    H --> I{Taskç±»å‹}
    I -->|ShuffleMapTask| J[æ‰§è¡ŒMapç«¯é€»è¾‘]
    I -->|ResultTask| K[æ‰§è¡ŒResultç«¯é€»è¾‘]
    J --> L[å†™å…¥Shuffleæ•°æ®]
    K --> M[è®¡ç®—æœ€ç»ˆç»“æœ]
    L --> N[åºåˆ—åŒ–ç»“æœ]
    M --> N
    N --> O[å‘é€çŠ¶æ€æ›´æ–°]
    O --> P{æ‰§è¡Œæ˜¯å¦æˆåŠŸ}
    P -->|æˆåŠŸ| Q[è¿”å›FINISHEDçŠ¶æ€]
    P -->|å¤±è´¥| R[è¿”å›FAILEDçŠ¶æ€]
    Q --> S[æ¸…ç†èµ„æº]
    R --> T[è®°å½•å¼‚å¸¸ä¿¡æ¯]
    T --> S
    S --> U[Taskæ‰§è¡Œå®Œæˆ]
    
    style A fill:#e1f5fe
    style H fill:#fff3e0
    style Q fill:#e8f5e8
    style R fill:#ffebee
```

**Taskæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†æµç¨‹**ï¼š

```mermaid
graph TD
    A[TaskContextåˆ›å»º] --> B[è®¾ç½®Stageä¿¡æ¯]
    B --> C[è®¾ç½®Partitionä¿¡æ¯]
    C --> D[åˆå§‹åŒ–TaskMemoryManager]
    D --> E[è®¾ç½®MetricsSystem]
    E --> F[æ³¨å†Œä»»åŠ¡ç›‘å¬å™¨]
    F --> G[å¼€å§‹Taskæ‰§è¡Œ]
    G --> H[ç›‘æ§å†…å­˜ä½¿ç”¨]
    H --> I[æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡]
    I --> J[å¤„ç†ä¸­æ–­ä¿¡å·]
    J --> K{Taskæ˜¯å¦å®Œæˆ}
    K -->|å¦| H
    K -->|æ˜¯| L[æ¸…ç†TaskContext]
    L --> M[é‡Šæ”¾å†…å­˜èµ„æº]
    M --> N[å‘é€æŒ‡æ ‡æ•°æ®]
    N --> O[TaskContexté”€æ¯]
    
    style A fill:#e1f5fe
    style G fill:#fff3e0
    style O fill:#e8f5e8
```

**Executorä»»åŠ¡æ‰§è¡Œè¯¦ç»†æµç¨‹**ï¼š

```scala
// Executor.launchTask()
def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
  val tr = new TaskRunner(context, taskDescription)
  runningTasks.put(taskDescription.taskId, tr)
  threadPool.execute(tr)
}

// TaskRunner.run()
class TaskRunner(
    execBackend: ExecutorBackend,
    private val taskDescription: TaskDescription)
  extends Runnable {
  
  override def run(): Unit = {
    try {
      // 1. ååºåˆ—åŒ–Task
      val task = ser.deserialize[Task[Any]](
        taskDescription.serializedTask, 
        Thread.currentThread.getContextClassLoader)
      
      // 2. è®¾ç½®TaskContext
      val taskContext = new TaskContextImpl(
        stageId = taskDescription.stageId,
        taskAttemptId = taskDescription.taskId,
        attemptNumber = taskDescription.attemptNumber,
        partitionId = task.partitionId,
        localProperties = taskDescription.properties,
        taskMemoryManager = taskMemoryManager,
        metricsSystem = env.metricsSystem)
      
      // 3. æ‰§è¡ŒTask
      val value = task.run(
        taskAttemptId = taskDescription.taskId,
        attemptNumber = taskDescription.attemptNumber,
        metricsSystem = env.metricsSystem)
      
      // 4. åºåˆ—åŒ–ç»“æœå¹¶è¿”å›
      val serializedResult = ser.serialize(value)
      execBackend.statusUpdate(
        taskDescription.taskId, 
        TaskState.FINISHED, 
        serializedResult)
        
    } catch {
      case e: Exception =>
        // å¼‚å¸¸å¤„ç†
        val reason = new ExceptionFailure(e, taskContext.taskMetrics())
        execBackend.statusUpdate(
          taskDescription.taskId, 
          TaskState.FAILED, 
          ser.serialize(TaskFailedReason(reason)))
    } finally {
      // æ¸…ç†èµ„æº
      runningTasks.remove(taskDescription.taskId)
    }
  }
}
```

#### æ•°æ®è¯»å–ã€å¤„ç†ä¸RDDä¾èµ–

**RDDæ•°æ®è¯»å–æµç¨‹å›¾**ï¼š

```mermaid
graph TD
    A[RDD.iteratorè°ƒç”¨] --> B{æ˜¯å¦æœ‰ç¼“å­˜}
    B -->|æœ‰ç¼“å­˜| C[ä»ç¼“å­˜è¯»å–æ•°æ®]
    B -->|æ— ç¼“å­˜| D{æ˜¯å¦æœ‰Checkpoint}
    D -->|æœ‰Checkpoint| E[ä»Checkpointè¯»å–]
    D -->|æ— Checkpoint| F[è°ƒç”¨computeæ–¹æ³•]
    F --> G{RDDç±»å‹}
    G -->|HadoopRDD| H[ä»HDFSè¯»å–]
    G -->|MapPartitionsRDD| I[è°ƒç”¨çˆ¶RDD.iterator]
    G -->|ShuffledRDD| J[ä»Shuffleæ•°æ®è¯»å–]
    H --> K[è¿”å›æ•°æ®è¿­ä»£å™¨]
    I --> L[é€’å½’è°ƒç”¨çˆ¶RDD]
    J --> M[è¯»å–Shuffleæ–‡ä»¶]
    C --> K
    E --> K
    L --> N[åº”ç”¨è½¬æ¢å‡½æ•°]
    M --> K
    N --> K
    K --> O[æ•°æ®å¤„ç†å®Œæˆ]
    
    style A fill:#e1f5fe
    style C fill:#e8f5e8
    style F fill:#fff3e0
    style O fill:#c8e6c9
```

**RDDä¾èµ–é“¾é€’å½’è°ƒç”¨æµç¨‹**ï¼š

```mermaid
graph TD
    A[Taskå¼€å§‹æ‰§è¡Œ] --> B[è°ƒç”¨ResultTask.runTask]
    B --> C[è°ƒç”¨RDD.iterator]
    C --> D[æ£€æŸ¥å­˜å‚¨çº§åˆ«]
    D --> E{æ˜¯å¦ç¼“å­˜}
    E -->|æ˜¯| F[ä»BlockManagerè·å–]
    E -->|å¦| G[computeOrReadCheckpoint]
    G --> H{æ˜¯å¦Checkpoint}
    H -->|æ˜¯| I[ä»Checkpointè¯»å–]
    H -->|å¦| J[è°ƒç”¨computeæ–¹æ³•]
    J --> K{RDDä¾èµ–ç±»å‹}
    K -->|çª„ä¾èµ–| L[ç›´æ¥è°ƒç”¨çˆ¶RDD.iterator]
    K -->|å®½ä¾èµ–| M[ä»ShuffleReaderè¯»å–]
    L --> N[é€’å½’å¤„ç†çˆ¶RDD]
    M --> O[è¯»å–Shuffleæ•°æ®]
    N --> P[åº”ç”¨å½“å‰RDDçš„è½¬æ¢é€»è¾‘]
    O --> P
    F --> P
    I --> P
    P --> Q[è¿”å›å¤„ç†åçš„æ•°æ®]
    
    style A fill:#e1f5fe
    style J fill:#fff3e0
    style P fill:#e8f5e8
    style Q fill:#c8e6c9
```

**RDDä¾èµ–é“¾è°ƒç”¨æµç¨‹**ï¼š

```scala
// RDD.iterator() é€’å½’è°ƒç”¨æµç¨‹
final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
  if (storageLevel != StorageLevel.NONE) {
    // 1. å°è¯•ä»ç¼“å­˜è¯»å–
    getOrCompute(split, context)
  } else {
    // 2. ç›´æ¥è®¡ç®—
    computeOrReadCheckpoint(split, context)
  }
}

def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = {
  if (isCheckpointed) {
    // ä»Checkpointè¯»å–
    firstParent[T].iterator(split, context)
  } else {
    // è°ƒç”¨å…·ä½“RDDçš„computeæ–¹æ³•
    compute(split, context)
  }
}

// ä»¥MapPartitionsRDDä¸ºä¾‹
override def compute(split: Partition, context: TaskContext): Iterator[U] = {
  // é€’å½’è°ƒç”¨çˆ¶RDDçš„iterator
  f(context, split.index, firstParent[T].iterator(split, context))
}
```

**å…¸å‹RDDä¾èµ–é“¾æ‰§è¡Œå›¾**ï¼š

```mermaid
graph TD
    A[Action: collect] --> B[ResultTask]
    B --> C[RDD.iterator]
    C --> D[MapPartitionsRDD.compute]
    D --> E[çˆ¶RDD.iterator]
    E --> F[FilteredRDD.compute]
    F --> G[çˆ¶RDD.iterator]
    G --> H[HadoopRDD.compute]
    H --> I[è¯»å–HDFSæ•°æ®]
    I --> J[è¿”å›Iterator]
    J --> K[é€çº§å¤„ç†å¹¶è¿”å›]
    K --> L[æœ€ç»ˆç»“æœ]
    
    style A fill:#e1f5fe
    style H fill:#e8f5e8
    style L fill:#fff3e0
```

#### Taskç±»å‹ä¸æ‰§è¡Œå·®å¼‚

**Taskç±»å‹æ‰§è¡Œæµç¨‹å¯¹æ¯”å›¾**ï¼š

```mermaid
graph TD
    A[Taskåˆ›å»º] --> B{Taskç±»å‹}
    B -->|ResultTask| C[ResultTaskæ‰§è¡Œæµç¨‹]
    B -->|ShuffleMapTask| D[ShuffleMapTaskæ‰§è¡Œæµç¨‹]
    
    C --> C1[è°ƒç”¨RDD.iterator]
    C1 --> C2[é€’å½’è®¡ç®—RDDé“¾]
    C2 --> C3[åº”ç”¨ç”¨æˆ·å‡½æ•°func]
    C3 --> C4[è¿”å›æœ€ç»ˆç»“æœ]
    C4 --> C5[å‘é€ç»“æœåˆ°Driver]
    
    D --> D1[è°ƒç”¨RDD.iterator]
    D1 --> D2[é€’å½’è®¡ç®—RDDé“¾]
    D2 --> D3[è·å–ShuffleWriter]
    D3 --> D4[å†™å…¥Shuffleæ•°æ®]
    D4 --> D5[è¿”å›MapStatus]
    D5 --> D6[æ³¨å†ŒShuffleè¾“å‡ºä½ç½®]
    
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style C5 fill:#e1f5fe
    style D6 fill:#ffebee
```

**ShuffleMapTaskè¯¦ç»†æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
graph TD
    A[ShuffleMapTaskå¼€å§‹] --> B[è·å–ShuffleManager]
    B --> C[åˆ›å»ºShuffleWriter]
    C --> D{Writerç±»å‹}
    D -->|BypassMergeSortShuffleWriter| E[ç›´æ¥å†™å…¥åˆ†åŒºæ–‡ä»¶]
    D -->|SortShuffleWriter| F[æ’åºåå†™å…¥]
    D -->|UnsafeShuffleWriter| G[Unsafeå†…å­˜å†™å…¥]
    
    E --> H[ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºæ–‡ä»¶]
    H --> I[ç›´æ¥å†™å…¥å¯¹åº”åˆ†åŒº]
    
    F --> J[ä½¿ç”¨ExternalSorteræ’åº]
    J --> K[åˆå¹¶ç›¸åŒKeyçš„å€¼]
    K --> L[å†™å…¥å•ä¸ªæ–‡ä»¶]
    
    G --> M[ä½¿ç”¨Unsafeå†…å­˜ç®¡ç†]
    M --> N[åºåˆ—åŒ–åç›´æ¥å†™å…¥]
    
    I --> O[ç”ŸæˆMapStatus]
    L --> O
    N --> O
    O --> P[è¿”å›åˆ†åŒºå¤§å°ä¿¡æ¯]
    P --> Q[æ³¨å†Œåˆ°MapOutputTracker]
    
    style A fill:#e1f5fe
    style O fill:#e8f5e8
    style Q fill:#c8e6c9
```

**ResultTaskè¯¦ç»†æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
graph TD
    A[ResultTaskå¼€å§‹] --> B[è°ƒç”¨RDD.iterator]
    B --> C[ä»Shuffleæ•°æ®è¯»å–]
    C --> D[ShuffleReader.read]
    D --> E[åˆå¹¶å¤šä¸ªMapè¾“å‡º]
    E --> F[åº”ç”¨ç”¨æˆ·å®šä¹‰å‡½æ•°]
    F --> G{å‡½æ•°ç±»å‹}
    G -->|collect| H[æ”¶é›†æ‰€æœ‰æ•°æ®]
    G -->|count| I[è®¡ç®—æ•°æ®é‡]
    G -->|reduce| J[èšåˆè®¡ç®—]
    G -->|foreach| K[éå†å¤„ç†]
    
    H --> L[åºåˆ—åŒ–ç»“æœæ•°æ®]
    I --> M[è¿”å›è®¡æ•°å€¼]
    J --> N[è¿”å›èšåˆç»“æœ]
    K --> O[æ‰§è¡Œå‰¯ä½œç”¨æ“ä½œ]
    
    L --> P[å‘é€åˆ°Driver]
    M --> P
    N --> P
    O --> Q[è¿”å›Unit]
    Q --> P
    P --> R[Taskæ‰§è¡Œå®Œæˆ]
    
    style A fill:#e1f5fe
    style F fill:#fff3e0
    style P fill:#e8f5e8
    style R fill:#c8e6c9
```

**ResultTask vs ShuffleMapTask**ï¼š

```scala
// ResultTask - äº§ç”Ÿæœ€ç»ˆç»“æœ
class ResultTask[T, U](
    stageId: Int,
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    _partitionId: Int,
    locs: Seq[TaskLocation])
  extends Task[U](stageId, _partitionId) {
  
  override def runTask(context: TaskContext): U = {
    // ç›´æ¥è°ƒç”¨ç”¨æˆ·å‡½æ•°å¤„ç†æ•°æ®
    func(context, rdd.iterator(partition, context))
  }
}

// ShuffleMapTask - äº§ç”Ÿä¸­é—´Shuffleæ•°æ®
class ShuffleMapTask(
    stageId: Int,
    rdd: RDD[_],
    dep: ShuffleDependency[_, _, _],
    _partitionId: Int,
    locs: Seq[TaskLocation])
  extends Task[MapStatus](stageId, _partitionId) {
  
  override def runTask(context: TaskContext): MapStatus = {
    // è·å–ShuffleWriter
    val manager = SparkEnv.get.shuffleManager
    val writer = manager.getWriter[Any, Any](
      dep.shuffleHandle, partitionId, context)
    
    try {
      // å†™å…¥Shuffleæ•°æ®
      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
      writer.stop(success = true).get
    } catch {
      case e: Exception =>
        writer.stop(success = false)
        throw e
    }
  }
}
```
---
## Sparké«˜é¢‘é¢è¯•é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: è¯·è¯¦ç»†è§£é‡ŠRDDã€DataFrameå’ŒDatasetçš„åŒºåˆ«åŠå„è‡ªçš„åº”ç”¨åœºæ™¯ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæä¾›äº†ä¸‰ç§æ ¸å¿ƒæ•°æ®æŠ½è±¡ï¼šRDDã€DataFrameå’ŒDatasetï¼Œå®ƒä»¬å„è‡ªé€‚ç”¨äºä¸åŒçš„åœºæ™¯ï¼Œå…·æœ‰ä¸åŒçš„ç‰¹æ€§å’Œä¼˜åŠ¿ã€‚

**1. åŸºæœ¬æ¦‚å¿µå¯¹æ¯”**

- **RDD (Resilient Distributed Dataset)**ï¼šSparkæœ€åˆçš„æ•°æ®æŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªä¸å¯å˜çš„ã€åˆ†å¸ƒå¼çš„å¯¹è±¡é›†åˆï¼Œæ”¯æŒå‡½æ•°å¼ç¼–ç¨‹æ“ä½œã€‚
- **DataFrame**ï¼šåœ¨RDDåŸºç¡€ä¸Šå¼•å…¥äº†Schemaæ¦‚å¿µï¼Œç±»ä¼¼å…³ç³»å‹æ•°æ®åº“ä¸­çš„è¡¨ç»“æ„ï¼Œæ”¯æŒSQLæŸ¥è¯¢ã€‚
- **Dataset**ï¼šDataFrameçš„æ‰©å±•ï¼Œæä¾›ç±»å‹å®‰å…¨çš„ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹æ¥å£ï¼Œç»“åˆäº†RDDçš„ç±»å‹å®‰å…¨å’ŒDataFrameçš„ä¼˜åŒ–æ€§èƒ½ã€‚

**2. æ ¸å¿ƒç‰¹æ€§å¯¹æ¯”**

| ç‰¹æ€§           | RDD                  | DataFrame            | Dataset              |
| -------------- | -------------------- | -------------------- | -------------------- |
| **ç±»å‹å®‰å…¨**   | ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥       | è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥       | ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥       |
| **æ€§èƒ½ä¼˜åŒ–**   | æ— å†…ç½®ä¼˜åŒ–           | Catalystä¼˜åŒ–å™¨       | Catalystä¼˜åŒ–å™¨       |
| **å†…å­˜ç®¡ç†**   | Javaå¯¹è±¡/Kryoåºåˆ—åŒ–  | TungstenäºŒè¿›åˆ¶æ ¼å¼   | TungstenäºŒè¿›åˆ¶æ ¼å¼   |
| **APIé£æ ¼**    | å‡½æ•°å¼API            | SQL + å‡½æ•°å¼API      | ç±»å‹å®‰å…¨çš„å‡½æ•°å¼API  |
| **Schemaæ„ŸçŸ¥** | æ— Schema             | æœ‰Schema             | æœ‰Schema             |
| **ä½¿ç”¨éš¾åº¦**   | è¾ƒå¤æ‚               | ç®€å•                 | ä¸­ç­‰                 |
| **é€‚ç”¨åœºæ™¯**   | éç»“æ„åŒ–æ•°æ®å¤„ç†     | ç»“æ„åŒ–æ•°æ®åˆ†æ       | ç»“æ„åŒ–æ•°æ®å¤æ‚å¤„ç†   |

**3. ä»£ç ç¤ºä¾‹å¯¹æ¯”**

```scala
// RDDç¤ºä¾‹
val rdd = sc.textFile("data.txt")
  .map(line => line.split(","))
  .map(fields => Person(fields(0), fields(1).toInt))
  .filter(person => person.age > 30)

// DataFrameç¤ºä¾‹
val df = spark.read.json("people.json")
df.filter($"age" > 30)
  .select($"name", $"age")
  .groupBy($"age")
  .count()

// Datasetç¤ºä¾‹
case class Person(name: String, age: Int)
val ds = spark.read.json("people.json").as[Person]
ds.filter(p => p.age > 30)
  .map(p => (p.name, p.age))
  .groupByKey(_._2)
  .count()
```

**4. æ€§èƒ½ä¸ä¼˜åŒ–å¯¹æ¯”**

- **RDD**ï¼šä¾èµ–äºJVMçš„åƒåœ¾å›æ”¶å’ŒJavaåºåˆ—åŒ–ï¼Œæ€§èƒ½å—é™
- **DataFrame**ï¼š
  - ä½¿ç”¨Catalystä¼˜åŒ–å™¨è¿›è¡Œé€»è¾‘å’Œç‰©ç†æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–
  - ä½¿ç”¨Tungstené«˜æ•ˆå†…å­˜ç®¡ç†ï¼Œå‡å°‘GCå¼€é”€
  - æ”¯æŒåˆ—å¼å­˜å‚¨å’Œå‹ç¼©
- **Dataset**ï¼š
  - ç»“åˆäº†DataFrameçš„æ‰€æœ‰ä¼˜åŒ–
  - å¢åŠ äº†ç¼–ç å™¨(Encoder)ï¼Œåœ¨å¯¹è±¡å’Œå†…éƒ¨Tungstenè¡¨ç¤ºä¹‹é—´é«˜æ•ˆè½¬æ¢

**5. é€‰æ‹©å»ºè®®**

- **é€‰æ‹©RDD**ï¼šå½“éœ€è¦ç»†ç²’åº¦æ§åˆ¶æˆ–å¤„ç†éç»“æ„åŒ–æ•°æ®æ—¶
- **é€‰æ‹©DataFrame**ï¼šå¤„ç†ç»“æ„åŒ–æ•°æ®ï¼Œéœ€è¦é«˜æ€§èƒ½ä¼˜åŒ–ï¼Œæˆ–éœ€è¦ä½¿ç”¨SQLæŸ¥è¯¢
- **é€‰æ‹©Dataset**ï¼šéœ€è¦ç±»å‹å®‰å…¨å’Œå‡½æ•°å¼ç¼–ç¨‹ï¼ŒåŒæ—¶åˆéœ€è¦Catalystä¼˜åŒ–å™¨çš„æ€§èƒ½æå‡

éšç€Sparkçš„å‘å±•ï¼ŒDataFrameå’ŒDataset APIå·²ç»æˆä¸ºæ¨èçš„æ•°æ®å¤„ç†æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨Spark 2.0ä¹‹åï¼ŒDataFrameå®é™…ä¸Šæ˜¯Dataset[Row]çš„ç±»å‹åˆ«åã€‚

**Q2: è¯·è¯¦ç»†æè¿°Sparkä»»åŠ¡çš„æ‰§è¡Œæµç¨‹ï¼Œä»æäº¤åº”ç”¨åˆ°ä»»åŠ¡å®Œæˆçš„å…¨è¿‡ç¨‹ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkä»»åŠ¡æ‰§è¡Œæ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ†å¸ƒå¼è®¡ç®—è¿‡ç¨‹ï¼Œæ¶‰åŠå¤šä¸ªç»„ä»¶ååŒå·¥ä½œã€‚ç†è§£è¿™ä¸ªæµç¨‹å¯¹äºä¼˜åŒ–Sparkåº”ç”¨å’Œæ’æŸ¥é—®é¢˜è‡³å…³é‡è¦ã€‚

**1. æ•´ä½“æ‰§è¡Œæ¶æ„**

Sparkåº”ç”¨ç¨‹åºçš„æ‰§è¡Œæ¶‰åŠä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
- **Driver Program**ï¼šåŒ…å«åº”ç”¨ç¨‹åºçš„mainå‡½æ•°ï¼Œè´Ÿè´£åˆ›å»ºSparkContext
- **Cluster Manager**ï¼šè´Ÿè´£èµ„æºåˆ†é…ï¼ˆå¦‚YARNã€Kubernetesã€Mesosæˆ–Standaloneï¼‰
- **Worker Node**ï¼šæ‰§è¡Œè®¡ç®—ä»»åŠ¡çš„èŠ‚ç‚¹
- **Executor**ï¼šåœ¨WorkerèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„è®¡ç®—ä»»åŠ¡

**2. è¯¦ç»†æ‰§è¡Œæµç¨‹**

```mermaid
graph TD
    A[ç”¨æˆ·ç¨‹åº] --> B[SparkContext]
    B --> C[DAGScheduler]
    C --> D[TaskScheduler]
    D --> E[WorkerèŠ‚ç‚¹]
    E --> F[Executor]
    F --> G[Taskæ‰§è¡Œ]
    G --> H[ç»“æœæ”¶é›†]
    H --> I[ä½œä¸šå®Œæˆ]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
    style E fill:#e1d5e7,stroke:#9673a6
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#f8cecc,stroke:#b85450
    style H fill:#d4f1f9,stroke:#05a4d1
    style I fill:#f9f9f9,stroke:#333
```

**3. æ‰§è¡Œæ­¥éª¤è¯¦è§£**

1. **åº”ç”¨ç¨‹åºå¯åŠ¨**ï¼š
   - ç”¨æˆ·æäº¤åº”ç”¨ç¨‹åº
   - åˆ›å»ºSparkContextï¼ˆSparkçš„å…¥å£ç‚¹ï¼‰
   - SparkContextè¿æ¥åˆ°é›†ç¾¤ç®¡ç†å™¨

2. **èµ„æºç”³è¯·**ï¼š
   - SparkContexté€šè¿‡é›†ç¾¤ç®¡ç†å™¨ç”³è¯·èµ„æº
   - é›†ç¾¤ç®¡ç†å™¨åœ¨WorkerèŠ‚ç‚¹ä¸Šå¯åŠ¨Executorè¿›ç¨‹

3. **DAGæ„å»º**ï¼š
   - ç”¨æˆ·ä»£ç é€šè¿‡RDDè½¬æ¢æ“ä½œæ„å»ºDAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰
   - å»¶è¿Ÿè®¡ç®—ï¼šè½¬æ¢æ“ä½œåªæ„å»ºDAGï¼Œä¸æ‰§è¡Œè®¡ç®—

4. **ä½œä¸šæäº¤**ï¼š
   - å½“é‡åˆ°Actionæ“ä½œæ—¶ï¼Œè§¦å‘ä½œä¸šæäº¤
   - SparkContextå°†ä½œä¸šæäº¤ç»™DAGScheduler

5. **Stageåˆ’åˆ†**ï¼š
   - DAGSchedulerå°†DAGåˆ’åˆ†ä¸ºå¤šä¸ªStage
   - åˆ’åˆ†ä¾æ®ï¼šShuffleæ“ä½œï¼ˆå¦‚reduceByKeyã€joinç­‰ï¼‰
   - æ¯ä¸ªStageåŒ…å«å¯ä»¥æµæ°´çº¿æ‰§è¡Œçš„ä¸€ç»„Task

6. **Taskç”Ÿæˆä¸è°ƒåº¦**ï¼š
   - ä¸ºæ¯ä¸ªStageç”ŸæˆTaskSet
   - TaskSchedulerå°†TaskSetæäº¤ç»™TaskSetManager
   - TaskSetManagerè´Ÿè´£å…·ä½“çš„ä»»åŠ¡è°ƒåº¦å’Œå¤±è´¥é‡è¯•

7. **Taskæ‰§è¡Œ**ï¼š
   - Executoræ¥æ”¶å¹¶æ‰§è¡ŒTask
   - æ‰§è¡Œè®¡ç®—å¹¶å°†ç»“æœä¿å­˜åœ¨å†…å­˜æˆ–ç£ç›˜
   - å¯¹äºShuffleæ“ä½œï¼Œå°†ä¸­é—´ç»“æœå†™å…¥æœ¬åœ°ç£ç›˜

8. **ç»“æœæ”¶é›†**ï¼š
   - å¯¹äºéœ€è¦è¿”å›ç»“æœçš„Actionæ“ä½œï¼ŒDriveræ”¶é›†ç»“æœ
   - å¯¹äºå†™å…¥å¤–éƒ¨å­˜å‚¨çš„æ“ä½œï¼Œç›´æ¥å†™å…¥ç›®æ ‡ä½ç½®

9. **ä½œä¸šå®Œæˆ**ï¼š
   - æ‰€æœ‰Taskæ‰§è¡Œå®Œæˆåï¼Œä½œä¸šç»“æŸ
   - é‡Šæ”¾èµ„æºæˆ–ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªä½œä¸š

**4. å…³é”®æ¦‚å¿µè§£æ**

- **Job**ï¼šç”±Actionæ“ä½œè§¦å‘çš„ä¸€ç»„è®¡ç®—ä»»åŠ¡
- **Stage**ï¼šJobçš„å­é›†ï¼Œç”±ä¸€ç»„å¯ä»¥æµæ°´çº¿æ‰§è¡Œçš„Taskç»„æˆ
- **Task**ï¼šåœ¨å•ä¸ªExecutorä¸Šæ‰§è¡Œçš„æœ€å°å·¥ä½œå•å…ƒï¼Œå¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®
- **Shuffle**ï¼šæ•°æ®é‡åˆ†å¸ƒè¿‡ç¨‹ï¼Œæ˜¯Stageåˆ’åˆ†çš„è¾¹ç•Œ

**5. ç¤ºä¾‹è¯´æ˜**

```scala
// è¿™ä¸ªç®€å•çš„Sparkç¨‹åºæ¼”ç¤ºäº†æ‰§è¡Œæµç¨‹
val sc = new SparkContext(conf)  // åˆ›å»ºSparkContext
val lines = sc.textFile("data.txt")  // æ„å»ºRDDï¼Œä½†ä¸æ‰§è¡Œ
val words = lines.flatMap(_.split(" "))  // ç»§ç»­æ„å»ºDAG
val wordCounts = words.map((_, 1)).reduceByKey(_ + _)  // reduceByKeyä¼šå¯¼è‡´Shuffleï¼Œåˆ’åˆ†Stage
wordCounts.collect()  // Actionæ“ä½œï¼Œè§¦å‘å®é™…è®¡ç®—
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š
- ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªStageçš„Job
- ç¬¬ä¸€ä¸ªStageæ‰§è¡ŒtextFileã€flatMapå’Œmapæ“ä½œ
- ç¬¬äºŒä¸ªStageæ‰§è¡ŒreduceByKeyæ“ä½œ
- collect()è§¦å‘æ•´ä¸ªJobçš„æ‰§è¡Œ

ç†è§£Sparkä»»åŠ¡æ‰§è¡Œæµç¨‹æœ‰åŠ©äºç¼–å†™é«˜æ•ˆçš„Sparkåº”ç”¨ç¨‹åºï¼Œå¹¶èƒ½æ›´å¥½åœ°è¿›è¡Œæ€§èƒ½è°ƒä¼˜å’Œæ•…éšœæ’æŸ¥ã€‚

**Q3: è¯·è§£é‡ŠSparkçš„å†…å­˜ç®¡ç†æœºåˆ¶ï¼ŒåŒ…æ‹¬å†…å­˜åˆ†é…ç­–ç•¥å’Œä¼˜åŒ–æ–¹æ³•ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkå†…å­˜ç®¡ç†æ˜¯å½±å“Sparkåº”ç”¨æ€§èƒ½çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚Sparké€šè¿‡ç²¾ç»†çš„å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œåœ¨æœ‰é™çš„å†…å­˜èµ„æºä¸‹å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è®¡ç®—ã€‚

**1. Sparkå†…å­˜æ¶æ„**

Sparkçš„JVMå †å†…å­˜ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

```mermaid
graph TD
    A[JVMå †å†…å­˜] --> B[Reserved Memory<br>300MB]
    A --> C[User Memory<br>ç”¨æˆ·ä»£ç ä½¿ç”¨]
    A --> D[Spark Memory<br>æ‰§è¡Œå’Œå­˜å‚¨]
    
    D --> E[Storage Memory<br>ç¼“å­˜æ•°æ®]
    D --> F[Execution Memory<br>è®¡ç®—è¿‡ç¨‹]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#f8cecc,stroke:#b85450
    style C fill:#d5e8d4,stroke:#82b366
    style D fill:#d4f1f9,stroke:#05a4d1
    style E fill:#ffe6cc,stroke:#d79b00
    style F fill:#e1d5e7,stroke:#9673a6
```

**2. å†…å­˜ç®¡ç†æ¨¡å¼**

Sparkæä¾›ä¸¤ç§å†…å­˜ç®¡ç†æ¨¡å¼ï¼š

- **é™æ€å†…å­˜ç®¡ç†(Static Memory Management)**ï¼š
  - Spark 1.6ä¹‹å‰çš„é»˜è®¤æ¨¡å¼
  - ä¸ºå­˜å‚¨å’Œæ‰§è¡Œå†…å­˜åˆ†é…å›ºå®šæ¯”ä¾‹ï¼Œä¸èƒ½åŠ¨æ€è°ƒæ•´
  - é…ç½®å‚æ•°ï¼š`spark.storage.memoryFraction`å’Œ`spark.shuffle.memoryFraction`
  
- **ç»Ÿä¸€å†…å­˜ç®¡ç†(Unified Memory Management)**ï¼š
  - Spark 1.6åŠä¹‹åçš„é»˜è®¤æ¨¡å¼
  - å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜å…±äº«ä¸€ä¸ªåŒºåŸŸï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´
  - é…ç½®å‚æ•°ï¼š`spark.memory.fraction`å’Œ`spark.memory.storageFraction`

**3. ç»Ÿä¸€å†…å­˜ç®¡ç†è¯¦è§£**

åœ¨ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼ä¸‹ï¼š

- **Spark Memory**ï¼šå JVMå †å†…å­˜çš„æ¯”ä¾‹ç”±`spark.memory.fraction`æ§åˆ¶ï¼Œé»˜è®¤ä¸º0.6
- **Storage Memory**ï¼šåˆå§‹å¤§å°ç”±`spark.memory.storageFraction`æ§åˆ¶ï¼Œé»˜è®¤ä¸º0.5
- **Execution Memory**ï¼šåˆå§‹å¤§å°ä¸ºSpark Memoryå‡å»Storage Memory

**å†…å­˜åŠ¨æ€è°ƒæ•´æœºåˆ¶**ï¼š

1. **å­˜å‚¨å†…å­˜ä¸è¶³æ—¶**ï¼š
   - å¦‚æœæ‰§è¡Œå†…å­˜æœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨æ‰§è¡Œå†…å­˜
   - å¦‚æœæ‰§è¡Œå†…å­˜æ²¡æœ‰ç©ºé—²ï¼Œåˆ™æŒ‰LRUç­–ç•¥æ·˜æ±°å·²ç¼“å­˜çš„RDDåˆ†åŒº

2. **æ‰§è¡Œå†…å­˜ä¸è¶³æ—¶**ï¼š
   - å¦‚æœå­˜å‚¨å†…å­˜æœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨å­˜å‚¨å†…å­˜
   - å¦‚æœå­˜å‚¨å†…å­˜æ²¡æœ‰ç©ºé—²ï¼Œä½†å­˜å‚¨å†…å­˜ä¸­æœ‰éƒ¨åˆ†æ˜¯è¢«æ‰§è¡Œå†…å­˜å€Ÿç”¨çš„ï¼Œåˆ™å¯ä»¥æŠ¢å è¿™éƒ¨åˆ†å†…å­˜
   - æ‰§è¡Œå†…å­˜ä¸ä¼šæ·˜æ±°å­˜å‚¨å†…å­˜ä¸­çš„æ•°æ®

**4. å†…å­˜ç®¡ç†ç›¸å…³é…ç½®å‚æ•°**

```scala
// ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼å…³é”®å‚æ•°
spark.memory.fraction = 0.6  // Spark Memoryå JVMå †å†…å­˜çš„æ¯”ä¾‹
spark.memory.storageFraction = 0.5  // Storage Memoryåˆå§‹å æ¯”

// å…¶ä»–é‡è¦å†…å­˜å‚æ•°
spark.executor.memory = "4g"  // Executorçš„JVMå †å†…å­˜å¤§å°
spark.memory.offHeap.enabled = false  // æ˜¯å¦å¯ç”¨å †å¤–å†…å­˜
spark.memory.offHeap.size = "2g"  // å †å¤–å†…å­˜å¤§å°
```

**5. å †å¤–å†…å­˜(Off-Heap Memory)**

ä»Spark 2.0å¼€å§‹ï¼ŒSparkæ”¯æŒä½¿ç”¨å †å¤–å†…å­˜ï¼š

- é€šè¿‡`spark.memory.offHeap.enabled`å¼€å¯
- ä½¿ç”¨`spark.memory.offHeap.size`è®¾ç½®å¤§å°
- ä¼˜åŠ¿ï¼šå‡å°‘GCå¼€é”€ï¼Œæé«˜æ€§èƒ½
- ç¼ºç‚¹ï¼šéœ€è¦æ‰‹åŠ¨ç®¡ç†å†…å­˜ï¼Œé…ç½®å¤æ‚

**6. å†…å­˜ç®¡ç†æœ€ä½³å®è·µ**

- **åˆç†è®¾ç½®Executorå†…å­˜**ï¼šæ ¹æ®é›†ç¾¤èŠ‚ç‚¹å†…å­˜å’Œå¹¶å‘ä»»åŠ¡æ•°
- **ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ**ï¼šé€šè¿‡Spark UIæŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
- **è°ƒæ•´å†…å­˜åˆ†é…æ¯”ä¾‹**ï¼šæ ¹æ®åº”ç”¨ç‰¹ç‚¹è°ƒæ•´å­˜å‚¨å’Œæ‰§è¡Œå†…å­˜æ¯”ä¾‹
- **ä½¿ç”¨å †å¤–å†…å­˜**ï¼šå¯¹äºå¤§æ•°æ®é‡å¤„ç†ï¼Œè€ƒè™‘å¯ç”¨å †å¤–å†…å­˜
- **é¿å…å†…å­˜æ³„æ¼**ï¼šæ³¨æ„é‡Šæ”¾ä¸å†ä½¿ç”¨çš„RDDï¼Œä½¿ç”¨`unpersist()`æ–¹æ³•

**7. å†…å­˜ä¸è¶³é—®é¢˜æ’æŸ¥**

å½“é‡åˆ°`OutOfMemoryError`æˆ–æ€§èƒ½ä¸‹é™æ—¶ï¼š

- æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„æ•°æ®ç¼“å­˜
- è€ƒè™‘å¢åŠ åˆ†åŒºæ•°ï¼Œå‡å°‘æ¯ä¸ªä»»åŠ¡çš„å†…å­˜ä½¿ç”¨
- è°ƒæ•´GCç­–ç•¥ï¼Œå¦‚ä½¿ç”¨G1GC
- ä½¿ç”¨Kryoåºåˆ—åŒ–å‡å°‘å†…å­˜å ç”¨
- è€ƒè™‘å¢åŠ Executorå†…å­˜æˆ–å‡å°‘æ¯ä¸ªExecutorçš„æ ¸å¿ƒæ•°

æ·±å…¥ç†è§£Sparkå†…å­˜ç®¡ç†æœºåˆ¶ï¼Œå¯¹äºä¼˜åŒ–Sparkåº”ç”¨æ€§èƒ½å’Œè§£å†³å†…å­˜ç›¸å…³é—®é¢˜è‡³å…³é‡è¦ã€‚

### æ¶æ„åŸç†é¢˜

**Q4: è¯·è¯¦ç»†ä»‹ç»Sparkçš„æ¶æ„ç»„ä»¶åŠå…¶èŒè´£ï¼Œå„ç»„ä»¶ä¹‹é—´å¦‚ä½•ååŒå·¥ä½œï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå…¶æ¶æ„ç”±å¤šä¸ªç»„ä»¶ååŒå·¥ä½œï¼Œå…±åŒæ”¯æ’‘åˆ†å¸ƒå¼æ•°æ®å¤„ç†èƒ½åŠ›ã€‚æ·±å…¥ç†è§£Sparkæ¶æ„ç»„ä»¶åŠå…¶äº¤äº’æ–¹å¼ï¼Œå¯¹äºæœ‰æ•ˆä½¿ç”¨Sparkå’Œæ’æŸ¥é—®é¢˜è‡³å…³é‡è¦ã€‚

**1. Sparkæ¶æ„æ€»è§ˆ**

```mermaid
graph TB
    A[ç”¨æˆ·åº”ç”¨] --> B[SparkContext]
    B --> C[é›†ç¾¤ç®¡ç†å™¨<br>YARN/Kubernetes/Mesos/Standalone]
    C --> D[WorkerèŠ‚ç‚¹]
    D --> E[Executor]
    B --> F[DAGScheduler]
    F --> G[TaskScheduler]
    G --> E
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
    style E fill:#e1d5e7,stroke:#9673a6
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#f8cecc,stroke:#b85450
```

**2. æ ¸å¿ƒç»„ä»¶è¯¦è§£**

| ç»„ä»¶ | ä½ç½® | ä¸»è¦èŒè´£ | å…³é”®ç‰¹æ€§ |
|------|------|---------|---------|
| **Driver Program** | å®¢æˆ·ç«¯æˆ–é›†ç¾¤ä¸­ | è¿è¡Œåº”ç”¨ç¨‹åºçš„mainå‡½æ•°<br>åˆ›å»ºSparkContext<br>æäº¤ä½œä¸š<br>æ”¶é›†ç»“æœ | åº”ç”¨ç¨‹åºçš„æ§åˆ¶ä¸­å¿ƒ<br>åŒ…å«DAGSchedulerå’ŒTaskScheduler |
| **SparkContext** | Driverä¸­ | Sparkç¨‹åºçš„å…¥å£ç‚¹<br>è¿æ¥é›†ç¾¤ç®¡ç†å™¨<br>è·å–Executor<br>æ„å»ºRDD | æ¯ä¸ªåº”ç”¨åªæœ‰ä¸€ä¸ª<br>è´Ÿè´£ä½œä¸šæäº¤å’Œèµ„æºç”³è¯· |
| **SparkSession** | Driverä¸­ | Spark 2.0åçš„å…¥å£ç‚¹<br>æ•´åˆSQLã€DataFrameã€Dataset API | æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£<br>åŒ…å«SparkContext |
| **Cluster Manager** | ç‹¬ç«‹è¿›ç¨‹ | èµ„æºåˆ†é…<br>å¯åŠ¨Executor | æ”¯æŒå¤šç§å®ç°ï¼š<br>YARNã€Kubernetesã€Mesosã€Standalone |
| **Worker Node** | é›†ç¾¤èŠ‚ç‚¹ | æä¾›è®¡ç®—èµ„æº<br>å¯åŠ¨Executorè¿›ç¨‹ | ç‰©ç†èŠ‚ç‚¹æˆ–è™šæ‹Ÿæœº<br>å¯ä»¥è¿è¡Œå¤šä¸ªExecutor |
| **Executor** | WorkerèŠ‚ç‚¹ä¸Š | æ‰§è¡ŒTask<br>ç¼“å­˜RDD<br>è¿”å›ç»“æœ | æ¯ä¸ªåº”ç”¨æœ‰å¤šä¸ª<br>ç”Ÿå‘½å‘¨æœŸä¸åº”ç”¨ç›¸åŒ |
| **DAGScheduler** | Driverä¸­ | æ„å»ºDAG<br>åˆ’åˆ†Stage<br>ç”ŸæˆTaskSet | åŸºäºShuffleä¾èµ–åˆ’åˆ†Stage<br>ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ |
| **TaskScheduler** | Driverä¸­ | å°†Taskåˆ†å‘ç»™Executor<br>ç›‘æ§Taskæ‰§è¡Œ<br>é‡è¯•å¤±è´¥çš„Task | è´Ÿè´£å…·ä½“çš„ä»»åŠ¡è°ƒåº¦<br>å¤„ç†ä»»åŠ¡å¤±è´¥å’Œé‡è¯• |
| **BlockManager** | Driverå’ŒExecutorä¸­ | ç®¡ç†å†…å­˜å’Œç£ç›˜å­˜å‚¨<br>å¤„ç†æ•°æ®å—ä¼ è¾“ | è´Ÿè´£RDDç¼“å­˜<br>ç®¡ç†Shuffleæ•°æ® |

**3. ç»„ä»¶äº¤äº’æµç¨‹**

1. **åº”ç”¨ç¨‹åºåˆå§‹åŒ–**ï¼š
   ```scala
   val spark = SparkSession.builder().appName("MyApp").getOrCreate()
   val sc = spark.sparkContext
   ```

2. **èµ„æºç”³è¯·ä¸Executorå¯åŠ¨**ï¼š
   - SparkContextè¿æ¥é›†ç¾¤ç®¡ç†å™¨
   - é›†ç¾¤ç®¡ç†å™¨åœ¨WorkerèŠ‚ç‚¹ä¸Šå¯åŠ¨Executorè¿›ç¨‹
   - Executorå‘Driveræ³¨å†Œ

3. **ä½œä¸šæäº¤ä¸æ‰§è¡Œ**ï¼š
   - DAGSchedulerå°†RDD DAGåˆ’åˆ†ä¸ºStage
   - TaskSchedulerå°†TaskSetæäº¤ç»™Executor
   - Executoræ‰§è¡ŒTaskå¹¶è¿”å›ç»“æœ

**4. ä¸åŒéƒ¨ç½²æ¨¡å¼å¯¹ç»„ä»¶çš„å½±å“**

| éƒ¨ç½²æ¨¡å¼ | Driverä½ç½® | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|----------|------|----------|
| **Clientæ¨¡å¼** | å®¢æˆ·ç«¯æœºå™¨ | Driverä¸å®¢æˆ·ç«¯åœ¨åŒä¸€è¿›ç¨‹<br>ä¾¿äºè°ƒè¯•å’ŒæŸ¥çœ‹è¾“å‡º | å¼€å‘æµ‹è¯•<br>äº¤äº’å¼åº”ç”¨ |
| **Clusteræ¨¡å¼** | é›†ç¾¤ä¸­çš„WorkerèŠ‚ç‚¹ | Driveråœ¨é›†ç¾¤ä¸­è¿è¡Œ<br>å®¢æˆ·ç«¯å¯ä»¥æ–­å¼€è¿æ¥ | ç”Ÿäº§ç¯å¢ƒ<br>é•¿æ—¶é—´è¿è¡Œçš„ä½œä¸š |

**5. å„ç»„ä»¶çš„é«˜å¯ç”¨æ€§è€ƒè™‘**

- **Driver**ï¼šåœ¨YARNæˆ–Kubernetesä¸Šå¯ä»¥å¯ç”¨AM (ApplicationMaster) é‡å¯
- **Worker**ï¼šèŠ‚ç‚¹å¤±è´¥æ—¶ï¼Œå…¶ä¸Šçš„Executorä¼šåœ¨å…¶ä»–èŠ‚ç‚¹é‡å¯
- **Executor**ï¼šå¤±è´¥æ—¶ä¼šé‡å¯ï¼Œæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ä¼šé‡è¯•
- **Task**ï¼šå¤±è´¥åä¼šè‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤šé‡è¯•æ¬¡æ•°å¯é…ç½®

**6. å®é™…åº”ç”¨ä¸­çš„æ¶æ„é€‰æ‹©**

- **å°è§„æ¨¡åº”ç”¨**ï¼šStandaloneæ¨¡å¼ç®€å•æ˜“ç”¨
- **ä¼ä¸šç”Ÿäº§ç¯å¢ƒ**ï¼šYARNæˆ–Kubernetesæä¾›æ›´å¥½çš„èµ„æºéš”ç¦»å’Œç®¡ç†
- **æ··åˆè´Ÿè½½ç¯å¢ƒ**ï¼šKubernetesé€‚åˆä¸å…¶ä»–å·¥ä½œè´Ÿè½½å…±å­˜
- **ä¼ ç»Ÿå¤§æ•°æ®ç¯å¢ƒ**ï¼šYARNä¸Hadoopç”Ÿæ€ç³»ç»Ÿé›†æˆæ›´å¥½

æ·±å…¥ç†è§£Sparkæ¶æ„ç»„ä»¶åŠå…¶äº¤äº’æ–¹å¼ï¼Œæœ‰åŠ©äºä¼˜åŒ–åº”ç”¨æ€§èƒ½ã€æ’æŸ¥é—®é¢˜ï¼Œä»¥åŠè®¾è®¡é€‚åˆç‰¹å®šåœºæ™¯çš„Sparkåº”ç”¨æ¶æ„ã€‚

**Q5: è¯·è¯¦ç»†è§£é‡ŠSparkçš„Shuffleæœºåˆ¶åŸç†åŠå…¶æ¼”è¿›å†å²ï¼Œå¦‚ä½•ä¼˜åŒ–Shuffleæ“ä½œï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Shuffleæ˜¯Sparkä¸­æœ€å…³é”®ä¹Ÿæœ€å¤æ‚çš„æœºåˆ¶ä¹‹ä¸€ï¼Œå®ƒæ¶‰åŠåˆ°æ•°æ®çš„é‡æ–°åˆ†åŒºå’Œè·¨èŠ‚ç‚¹ä¼ è¾“ï¼Œå¯¹Sparkåº”ç”¨çš„æ€§èƒ½æœ‰ç€é‡å¤§å½±å“ã€‚æ·±å…¥ç†è§£Shuffleæœºåˆ¶å¯¹äºä¼˜åŒ–Sparkåº”ç”¨è‡³å…³é‡è¦ã€‚

**1. Shuffleçš„åŸºæœ¬æ¦‚å¿µ**

Shuffleæ˜¯æŒ‡å°†åˆ†å¸ƒåœ¨å„ä¸ªåˆ†åŒºçš„æ•°æ®æŒ‰ç…§æŸç§è§„åˆ™é‡æ–°ç»„ç»‡ï¼Œä½¿å¾—å…·æœ‰ç›¸åŒç‰¹å¾ï¼ˆå¦‚ç›¸åŒçš„keyï¼‰çš„æ•°æ®èšé›†åœ¨ä¸€èµ·è¿›è¡Œè®¡ç®—çš„è¿‡ç¨‹ã€‚åœ¨Sparkä¸­ï¼ŒShuffleæ“ä½œæ˜¯Stageåˆ’åˆ†çš„è¾¹ç•Œã€‚

**è§¦å‘Shuffleçš„æ“ä½œåŒ…æ‹¬**ï¼š
- **é‡åˆ†åŒºæ“ä½œ**ï¼š`repartition`ã€`coalesce`
- **ByKeyç±»æ“ä½œ**ï¼š`groupByKey`ã€`reduceByKey`ã€`aggregateByKey`
- **Joinç±»æ“ä½œ**ï¼š`join`ã€`cogroup`
- **æ’åºæ“ä½œ**ï¼š`sortBy`ã€`sortByKey`

**2. Shuffleçš„æ¼”è¿›å†å²**

Spark Shuffleæœºåˆ¶ç»å†äº†å¤šæ¬¡é‡å¤§æ”¹è¿›ï¼š

| Shuffleç‰ˆæœ¬ | Sparkç‰ˆæœ¬ | ç‰¹ç‚¹ | ä¸»è¦é—®é¢˜ |
|------------|----------|------|---------|
| **Hash Shuffle V1** | 0.8åŠä¹‹å‰ | æ¯ä¸ªmap taskè¾“å‡ºMÃ—Rä¸ªæ–‡ä»¶<br>(M=mapä»»åŠ¡æ•°ï¼ŒR=reduceä»»åŠ¡æ•°) | æ–‡ä»¶æ•°è¿‡å¤šï¼Œå ç”¨æ–‡ä»¶å¥æŸ„ |
| **Hash Shuffle V2** | 0.8.1 - 1.1 | æ¯ä¸ªexecutorè¾“å‡ºCÃ—Rä¸ªæ–‡ä»¶<br>(C=coreæ•°ï¼ŒR=reduceä»»åŠ¡æ•°) | æ–‡ä»¶æ•°ä»ç„¶è¾ƒå¤š |
| **Sort Shuffle V1** | 1.1 - 1.5 | æ¯ä¸ªmap taskè¾“å‡º1ä¸ªæ–‡ä»¶ï¼ŒæŒ‰keyæ’åº | æ‰€æœ‰æ•°æ®éƒ½æ’åºï¼Œå¼€é”€å¤§ |
| **Sort Shuffle V2<br>(Tungsten)** | 1.5 - 2.0 | äºŒè¿›åˆ¶åºåˆ—åŒ–ï¼Œç›´æ¥æ“ä½œå†…å­˜ | ç‰¹å®šåœºæ™¯ä¼˜åŒ– |
| **Sort Shuffle V3** | 2.0+ | ç»Ÿä¸€çš„Sort-based Shuffle<br>å°æ•°æ®é‡å¯ç»•è¿‡æ’åº | å½“å‰é»˜è®¤å®ç° |

**3. Sort-based Shuffleè¯¦ç»†å·¥ä½œæµç¨‹**

```mermaid
graph TD
    A[Mapä»»åŠ¡] --> B[å†…å­˜ä¸­æŒ‰Partitioneråˆ†åŒº]
    B --> C{æ˜¯å¦éœ€è¦æ’åº?}
    C -->|æ˜¯| D[å¯¹æ¯ä¸ªåˆ†åŒºå†…æ•°æ®æ’åº]
    C -->|å¦| E[è·³è¿‡æ’åº]
    D --> F[æº¢å†™åˆ°ç£ç›˜]
    E --> F
    F --> G[åˆå¹¶æº¢å†™æ–‡ä»¶]
    G --> H[ç”Ÿæˆæ•°æ®æ–‡ä»¶å’Œç´¢å¼•æ–‡ä»¶]
    H --> I[Reduceä»»åŠ¡]
    I --> J[é€šè¿‡ç½‘ç»œæ‹‰å–æ•°æ®]
    J --> K[åˆå¹¶æ•°æ®]
    K --> L[è¿›è¡ŒReduceè®¡ç®—]
    
    style A fill:#d4f1f9,stroke:#05a4d1
    style I fill:#e1d5e7,stroke:#9673a6
    style F fill:#ffe6cc,stroke:#d79b00
    style G fill:#ffe6cc,stroke:#d79b00
    style H fill:#ffe6cc,stroke:#d79b00
    style J fill:#d5e8d4,stroke:#82b366
    style K fill:#d5e8d4,stroke:#82b366
    style L fill:#d5e8d4,stroke:#82b366
```

**4. Mapç«¯è¯¦è§£**

1. **åˆ†åŒºè®¡ç®—**ï¼šæ ¹æ®Partitionerç¡®å®šæ¯æ¡æ•°æ®çš„ç›®æ ‡åˆ†åŒº
2. **å†…å­˜ç¼“å†²**ï¼šæ•°æ®å…ˆå†™å…¥å†…å­˜ç¼“å†²åŒº
3. **æ’åºä¸èšåˆ**ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œæ’åºå’Œèšåˆ
4. **æº¢å†™æœºåˆ¶**ï¼š
   - å½“ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼ï¼ˆ`spark.shuffle.spill.numElementsForceSpillThreshold`ï¼‰æ—¶è§¦å‘æº¢å†™
   - æº¢å†™è¿‡ç¨‹ä¸­å¯èƒ½è¿›è¡Œæ’åºå’Œèšåˆ
5. **æ–‡ä»¶åˆå¹¶**ï¼šå¤šä¸ªæº¢å†™æ–‡ä»¶æœ€ç»ˆåˆå¹¶ä¸ºä¸€ä¸ªæ•°æ®æ–‡ä»¶å’Œä¸€ä¸ªç´¢å¼•æ–‡ä»¶

**5. Reduceç«¯è¯¦è§£**

1. **ä»»åŠ¡åˆå§‹åŒ–**ï¼šReduceä»»åŠ¡å¯åŠ¨æ—¶ï¼Œå‘DAGSchedulerè·å–ä¸Šæ¸¸Shuffleæ•°æ®çš„ä½ç½®ä¿¡æ¯
2. **æ•°æ®æ‹‰å–**ï¼šé€šè¿‡BlockManagerä»å„ä¸ªMapä»»åŠ¡æ‰€åœ¨èŠ‚ç‚¹æ‹‰å–æ•°æ®
3. **æ‹‰å–ç­–ç•¥**ï¼š
   - æŒ‰æ‰¹æ¬¡æ‹‰å–ï¼Œé¿å…ä¸€æ¬¡æ€§æ‹‰å–è¿‡å¤šæ•°æ®
   - æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œå¤„ç†ä¸´æ—¶ç½‘ç»œæ•…éšœ
4. **æ•°æ®èšåˆ**ï¼šå°†æ‹‰å–çš„æ•°æ®è¿›è¡Œåˆå¹¶å’Œèšåˆå¤„ç†
5. **ç»“æœè®¡ç®—**ï¼šå¯¹èšåˆåçš„æ•°æ®æ‰§è¡ŒReduceæ“ä½œ

**6. å…³é”®é…ç½®å‚æ•°**

```scala
// Shuffleè¡Œä¸ºæ§åˆ¶
spark.shuffle.manager = "sort"  // Shuffleå®ç°æ–¹å¼ï¼Œé»˜è®¤sort
spark.shuffle.sort.bypassMergeThreshold = 200  // å°åˆ†åŒºæ•°é‡ç»•è¿‡æ’åºçš„é˜ˆå€¼

// å†…å­˜ä½¿ç”¨æ§åˆ¶
spark.shuffle.file.buffer = "32k"  // æ¯ä¸ªè¾“å‡ºæµçš„ç¼“å†²å¤§å°
spark.shuffle.spill.compress = true  // æ˜¯å¦å‹ç¼©æº¢å†™æ–‡ä»¶

// ç½‘ç»œä¼ è¾“æ§åˆ¶
spark.reducer.maxSizeInFlight = "48m"  // æ¯ä¸ªreduceä»»åŠ¡åŒæ—¶æ‹‰å–çš„æœ€å¤§æ•°æ®é‡
spark.shuffle.io.retryWait = "5s"  // é‡è¯•ç­‰å¾…æ—¶é—´
spark.shuffle.io.maxRetries = 3  // æœ€å¤§é‡è¯•æ¬¡æ•°
```

**7. Shuffleä¼˜åŒ–ç­–ç•¥**

1. **å‡å°‘Shuffleæ“ä½œ**ï¼š
   - ä½¿ç”¨`mapPartitions`æ›¿ä»£`map`åæ¥`reduceByKey`
   - ä½¿ç”¨å¹¿æ’­å˜é‡æ›¿ä»£`join`

2. **è°ƒæ•´åˆ†åŒºæ•°é‡**ï¼š
   - è¿‡å°‘ï¼šæ•°æ®å€¾æ–œï¼Œä»»åŠ¡å¹¶è¡Œåº¦ä½
   - è¿‡å¤šï¼šå°æ–‡ä»¶è¿‡å¤šï¼Œè°ƒåº¦å¼€é”€å¤§
   - å»ºè®®ï¼šæ¯ä¸ªåˆ†åŒºå¤§å°åœ¨128MBå·¦å³

3. **å¯ç”¨èšåˆ**ï¼š
   - ä½¿ç”¨`reduceByKey`æ›¿ä»£`groupByKey`
   - ä½¿ç”¨`aggregateByKey`è¿›è¡Œæœ¬åœ°é¢„èšåˆ

4. **å†…å­˜è°ƒä¼˜**ï¼š
   - å¢åŠ Shuffleç¼“å†²åŒºå¤§å°å‡å°‘ç£ç›˜I/O
   - è°ƒæ•´æ‰§è¡Œå†…å­˜æ¯”ä¾‹é€‚åº”Shuffleéœ€æ±‚

5. **åºåˆ—åŒ–ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨Kryoåºåˆ—åŒ–å‡å°‘æ•°æ®å¤§å°
   - æ³¨å†Œè‡ªå®šä¹‰ç±»æé«˜åºåˆ—åŒ–æ€§èƒ½

æ·±å…¥ç†è§£Shuffleæœºåˆ¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…ç¼–å†™æ›´é«˜æ•ˆçš„Sparkåº”ç”¨ï¼Œé¿å…å¸¸è§çš„æ€§èƒ½é™·é˜±ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ã€‚

### æ€§èƒ½è°ƒä¼˜é¢˜

**Q6: è¯·è¯¦è¿°Sparkåº”ç”¨çš„æ€§èƒ½è°ƒä¼˜ç­–ç•¥ï¼Œä»å“ªäº›æ–¹é¢å¯ä»¥æå‡Sparkä½œä¸šçš„æ‰§è¡Œæ•ˆç‡ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæ€§èƒ½è°ƒä¼˜æ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§å·¥ä½œï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œç»¼åˆä¼˜åŒ–ã€‚ä¸€ä¸ªé«˜æ•ˆçš„Sparkåº”ç”¨éœ€è¦åˆç†çš„èµ„æºé…ç½®ã€ä¼˜åŒ–çš„ä»£ç ç»“æ„ã€é€‚å½“çš„æ•°æ®å¤„ç†ç­–ç•¥ä»¥åŠç²¾ç»†çš„å‚æ•°è°ƒæ•´ã€‚

**1. æ€§èƒ½è°ƒä¼˜çš„æ•´ä½“æ–¹æ³•è®º**

æ€§èƒ½è°ƒä¼˜åº”éµå¾ªä»¥ä¸‹æ–¹æ³•è®ºï¼š
- **è‡ªä¸Šè€Œä¸‹**ï¼šä»åº”ç”¨æ¶æ„åˆ°å…·ä½“å‚æ•°
- **æ•°æ®é©±åŠ¨**ï¼šåŸºäºç›‘æ§æŒ‡æ ‡å’Œæ€§èƒ½æµ‹è¯•
- **æ¸è¿›å¼**ï¼šä»æœ€å¤§ç“¶é¢ˆå¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–
- **æƒè¡¡å–èˆ**ï¼šåœ¨èµ„æºæ¶ˆè€—ã€æ‰§è¡Œé€Ÿåº¦ã€ç¨³å®šæ€§ä¹‹é—´å¯»æ‰¾å¹³è¡¡

**2. èµ„æºé…ç½®ä¼˜åŒ–**

```mermaid
graph TD
    A[èµ„æºé…ç½®ä¼˜åŒ–] --> B[Executoré…ç½®]
    A --> C[Driveré…ç½®]
    A --> D[é›†ç¾¤èµ„æº]
    
    B --> B1[å†…å­˜å¤§å°]
    B --> B2[æ ¸å¿ƒæ•°é‡]
    B --> B3[å®ä¾‹æ•°é‡]
    
    C --> C1[å†…å­˜å¤§å°]
    C --> C2[å¹¶è¡Œåº¦]
    
    D --> D1[èŠ‚ç‚¹è§„æ ¼]
    D --> D2[èµ„æºéš”ç¦»]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
```

**Executoré…ç½®æœ€ä½³å®è·µ**ï¼š
- **å†…å­˜å¤§å°**ï¼šæ¯ä¸ªExecutor 4-8GBå†…å­˜ï¼ˆè¿‡å¤§å¯¼è‡´GCå»¶è¿Ÿï¼‰
- **æ ¸å¿ƒæ•°é‡**ï¼šæ¯ä¸ªExecutor 4-5ä¸ªæ ¸å¿ƒï¼ˆè¿‡å¤šå¯¼è‡´çº¿ç¨‹ç«äº‰ï¼‰
- **å®ä¾‹æ•°é‡**ï¼š`(é›†ç¾¤æ€»æ ¸å¿ƒæ•° / æ¯ä¸ªExecutoræ ¸å¿ƒæ•°)`ï¼Œé¢„ç•™10%èµ„æº

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// 10èŠ‚ç‚¹é›†ç¾¤ï¼Œæ¯èŠ‚ç‚¹16æ ¸64GBå†…å­˜
spark.executor.instances = 30       // (10 * 16) / 5 = 32ï¼Œé¢„ç•™éƒ¨åˆ†
spark.executor.cores = 5            // æ¯ä¸ªExecutor 5ä¸ªæ ¸å¿ƒ
spark.executor.memory = "20g"       // æ¯ä¸ªExecutor 20GBå†…å­˜
spark.driver.memory = "10g"         // Driver 10GBå†…å­˜
```

**3. å¹¶è¡Œåº¦ä¼˜åŒ–**

å¹¶è¡Œåº¦æ˜¯æŒ‡ä»»åŠ¡åˆ’åˆ†çš„åˆ†åŒºæ•°ï¼Œå½±å“ä»»åŠ¡çš„å¹¶è¡Œæ‰§è¡Œæ•ˆç‡ã€‚

**å¹¶è¡Œåº¦è®¾ç½®åŸåˆ™**ï¼š
- **åŸºå‡†å€¼**ï¼šé›†ç¾¤æ€»æ ¸å¿ƒæ•°çš„2-3å€
- **æ•°æ®é‡**ï¼šæ¯ä¸ªåˆ†åŒºæ•°æ®é‡åœ¨128MBå·¦å³
- **åŠ¨æ€è°ƒæ•´**ï¼š`spark.sql.adaptive.enabled=true`

**å¹¶è¡Œåº¦ç›¸å…³é…ç½®**ï¼š
```scala
// é™æ€é…ç½®
spark.default.parallelism = 600     // é»˜è®¤å¹¶è¡Œåº¦
spark.sql.shuffle.partitions = 600  // SQLæ“ä½œçš„å¹¶è¡Œåº¦

// åŠ¨æ€é…ç½®
spark.sql.adaptive.enabled = true   // å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.coalescePartitions.enabled = true  // åˆå¹¶å°åˆ†åŒº
```

**4. æ•°æ®å€¾æ–œä¼˜åŒ–**

æ•°æ®å€¾æ–œæ˜¯æŒ‡æŸäº›åˆ†åŒºçš„æ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ—¶é—´ä¸å‡è¡¡ã€‚

**è¯†åˆ«æ•°æ®å€¾æ–œ**ï¼š
- Spark UIä¸­è§‚å¯ŸStageé¡µé¢çš„ä»»åŠ¡æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
- æŸ¥çœ‹Shuffleè¯»å†™æ•°æ®é‡çš„åˆ†å¸ƒæƒ…å†µ

**è§£å†³æ–¹æ¡ˆ**ï¼š

| å€¾æ–œç±»å‹ | è§£å†³æ–¹æ¡ˆ | å®ç°æ–¹å¼ |
|---------|---------|---------|
| **Joinå€¾æ–œ** | å¹¿æ’­Join | `broadcast(smallDF).join(largeDF)` |
| **Joinå€¾æ–œ** | æ‹†åˆ†çƒ­ç‚¹é”® | å¯¹çƒ­ç‚¹é”®æ·»åŠ éšæœºå‰ç¼€ï¼Œæ‰©å¤§Join |
| **èšåˆå€¾æ–œ** | ä¸¤é˜¶æ®µèšåˆ | å±€éƒ¨èšåˆ+å…¨å±€èšåˆ |
| **èšåˆå€¾æ–œ** | è‡ªå®šä¹‰åˆ†åŒº | å®ç°è‡ªå®šä¹‰Partitioner |
| **æ•°æ®æºå€¾æ–œ** | é¢„å¤„ç† | ETLé˜¶æ®µé‡æ–°åˆ†åŒº |

**ä»£ç ç¤ºä¾‹**ï¼š
```scala
// ä¸¤é˜¶æ®µèšåˆç¤ºä¾‹
val result = rdd
  .map(x => (x._1 + "_" + Random.nextInt(10), x._2))  // åŠ ç›
  .reduceByKey(_ + _)  // å±€éƒ¨èšåˆ
  .map(x => (x._1.split("_")(0), x._2))  // å»ç›
  .reduceByKey(_ + _)  // å…¨å±€èšåˆ
```

**5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**

åˆç†çš„ç¼“å­˜ç­–ç•¥å¯ä»¥é¿å…é‡å¤è®¡ç®—ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡ã€‚

**ç¼“å­˜çº§åˆ«é€‰æ‹©**ï¼š

| å­˜å‚¨çº§åˆ« | å†…å­˜ä½¿ç”¨ | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| MEMORY_ONLY | é«˜ | ä½ | é»˜è®¤é€‰æ‹©ï¼Œå†…å­˜å……è¶³ |
| MEMORY_AND_DISK | ä¸­ | ä½ | æ•°æ®é‡å¤§äºå†…å­˜ |
| MEMORY_ONLY_SER | ä½ | é«˜ | å†…å­˜å—é™ï¼Œå¯æ¥å—åºåˆ—åŒ–å¼€é”€ |
| OFF_HEAP | ä½ | ä¸­ | éœ€è¦è·¨åº”ç”¨å…±äº«æ•°æ® |

**ç¼“å­˜ä½¿ç”¨åŸåˆ™**ï¼š
- åªç¼“å­˜é‡å¤ä½¿ç”¨çš„RDD/DataFrame
- åœ¨Shuffleæ“ä½œä¹‹åã€Actionæ“ä½œä¹‹å‰ç¼“å­˜
- åŠæ—¶ä½¿ç”¨`unpersist()`é‡Šæ”¾ä¸å†ä½¿ç”¨çš„ç¼“å­˜

**6. Shuffleä¼˜åŒ–**

Shuffleæ˜¯Sparkä¸­æœ€æ˜‚è´µçš„æ“ä½œï¼Œä¼˜åŒ–Shuffleå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚

**Shuffleä¼˜åŒ–ç­–ç•¥**ï¼š
- **å‡å°‘Shuffle**ï¼šä½¿ç”¨`mapPartitions`æ›¿ä»£`map`+`reduceByKey`
- **æœ¬åœ°èšåˆ**ï¼šä½¿ç”¨`reduceByKey`æ›¿ä»£`groupByKey`
- **å¹¿æ’­å˜é‡**ï¼šå°è¡¨å¹¿æ’­é¿å…Shuffle
- **å‚æ•°è°ƒæ•´**ï¼šè°ƒæ•´ç¼“å†²åŒºå¤§å°ã€å‹ç¼©ç®—æ³•ç­‰

**å…³é”®å‚æ•°**ï¼š
```scala
spark.shuffle.file.buffer = "64k"  // å¢åŠ ç¼“å†²åŒºå‡å°‘ç£ç›˜I/O
spark.shuffle.compress = true      // å¯ç”¨å‹ç¼©
spark.shuffle.io.maxRetries = 6    // å¢åŠ é‡è¯•æ¬¡æ•°
```

**7. SQLä¼˜åŒ–**

å¯¹äºSpark SQLåº”ç”¨ï¼Œå¯ä»¥åº”ç”¨ä»¥ä¸‹ä¼˜åŒ–æŠ€æœ¯ï¼š

**æŸ¥è¯¢ä¼˜åŒ–**ï¼š
- **è°“è¯ä¸‹æ¨**ï¼šå°½æ—©è¿‡æ»¤æ•°æ®
- **åˆ—è£å‰ª**ï¼šåªè¯»å–éœ€è¦çš„åˆ—
- **åˆ†åŒºè£å‰ª**ï¼šåªè¯»å–éœ€è¦çš„åˆ†åŒº
- **è‡ªåŠ¨ä¼˜åŒ–**ï¼šå¯ç”¨AQEã€åŠ¨æ€åˆ†åŒºè£å‰ªç­‰

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.sql.adaptive.enabled = true
// å¯ç”¨åŠ¨æ€åˆ†åŒºè£å‰ª
spark.sql.optimizer.dynamicPartitionPruning.enabled = true
// å¯ç”¨Joiné‡æ’åº
spark.sql.adaptive.optimizeSkewedJoin = true
```

**8. åºåˆ—åŒ–ä¼˜åŒ–**

åºåˆ—åŒ–å½±å“æ•°æ®ä¼ è¾“å’Œå­˜å‚¨æ•ˆç‡ã€‚

**åºåˆ—åŒ–é€‰æ‹©**ï¼š
- **Kryoåºåˆ—åŒ–**ï¼šæ¯”Javaåºåˆ—åŒ–æ›´é«˜æ•ˆ
- **åˆ—å¼æ ¼å¼**ï¼šParquetã€ORCç­‰æ ¼å¼æ›´é«˜æ•ˆ

**é…ç½®ç¤ºä¾‹**ï¼š
```scala
// å¯ç”¨Kryoåºåˆ—åŒ–
spark.serializer = "org.apache.spark.serializer.KryoSerializer"
// æ³¨å†Œè‡ªå®šä¹‰ç±»
spark.kryo.registrator = "com.example.MyRegistrator"
```

**9. ç»¼åˆæ€§èƒ½è°ƒä¼˜æ¡ˆä¾‹**

**å¤§è§„æ¨¡æ•°æ®Joinä¼˜åŒ–**ï¼š
```scala
// ä¼˜åŒ–å‰
val result = largeDF.join(smallDF, Seq("key"))

// ä¼˜åŒ–å
// 1. å¹¿æ’­å°è¡¨
val broadcastDF = broadcast(smallDF)
val result = largeDF.join(broadcastDF, Seq("key"))

// 2. å¯ç”¨AQEå’ŒJoinä¼˜åŒ–
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

**æ•°æ®å€¾æ–œå¤„ç†**ï¼š
```scala
// ä¼˜åŒ–å‰
val result = rdd.reduceByKey(_ + _)

// ä¼˜åŒ–å
// 1. ä¸¤é˜¶æ®µèšåˆ
val result = rdd
  .map(x => ((x._1, Random.nextInt(10)), x._2))  // åŠ ç›
  .reduceByKey(_ + _)  // å±€éƒ¨èšåˆ
  .map(x => (x._1._1, x._2))  // å»ç›
  .reduceByKey(_ + _)  // å…¨å±€èšåˆ
```

æ€§èƒ½è°ƒä¼˜æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ç»“åˆå…·ä½“åº”ç”¨åœºæ™¯ã€æ•°æ®ç‰¹ç‚¹å’Œèµ„æºæƒ…å†µï¼Œé‡‡ç”¨é€‚å½“çš„ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒä¼˜ï¼Œå¯ä»¥æ˜¾è‘—æå‡Sparkåº”ç”¨çš„æ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

**Q7: å¦‚ä½•è¯†åˆ«å’Œè§£å†³Sparkä¸­çš„æ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿè¯·ç»™å‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆå’Œä»£ç ç¤ºä¾‹ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

æ•°æ®å€¾æ–œæ˜¯Sparkåº”ç”¨ä¸­å¸¸è§çš„æ€§èƒ½ç“¶é¢ˆï¼Œè¡¨ç°ä¸ºæŸäº›åˆ†åŒºçš„æ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ—¶é—´ä¸å‡è¡¡ï¼Œæ•´ä½“ä½œä¸šå»¶è¿Ÿã€‚æœ‰æ•ˆè§£å†³æ•°æ®å€¾æ–œé—®é¢˜æ˜¯Sparkæ€§èƒ½ä¼˜åŒ–çš„å…³é”®ç¯èŠ‚ã€‚

**1. æ•°æ®å€¾æ–œçš„è¯†åˆ«**

åœ¨è§£å†³æ•°æ®å€¾æ–œå‰ï¼Œé¦–å…ˆéœ€è¦å‡†ç¡®è¯†åˆ«é—®é¢˜ï¼š

**è¯†åˆ«æ–¹æ³•**ï¼š
- **Spark UI**ï¼šè§‚å¯ŸStageé¡µé¢ä¸­ä»»åŠ¡æ‰§è¡Œæ—¶é—´åˆ†å¸ƒï¼Œå¦‚æœ‰æ˜æ˜¾"é•¿å°¾"ç°è±¡åˆ™å¯èƒ½å­˜åœ¨å€¾æ–œ
- **Shuffleç»Ÿè®¡**ï¼šæ£€æŸ¥Shuffleè¯»å†™æ•°æ®é‡åˆ†å¸ƒæ˜¯å¦å‡è¡¡
- **æ•°æ®é‡‡æ ·**ï¼šå¯¹å¯èƒ½çš„å€¾æ–œé”®è¿›è¡Œé‡‡æ ·åˆ†æï¼Œç¡®å®šçƒ­ç‚¹æ•°æ®

**å€¾æ–œç‰¹å¾**ï¼š
```
Taskæ‰§è¡Œæ—¶é—´åˆ†å¸ƒï¼š
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 12s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 13s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 11s
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 120s  <- æ˜æ˜¾çš„å€¾æ–œä»»åŠ¡
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 14s
```

**2. æ•°æ®å€¾æ–œçš„æ ¹æœ¬åŸå› **

æ•°æ®å€¾æ–œé€šå¸¸ç”±ä»¥ä¸‹åŸå› å¯¼è‡´ï¼š

```mermaid
graph TD
    A[æ•°æ®å€¾æ–œæ ¹å› ] --> B[æ•°æ®æœ¬èº«åˆ†å¸ƒä¸å‡]
    A --> C[ä¸šåŠ¡é€»è¾‘å¯¼è‡´]
    A --> D[æŠ€æœ¯å®ç°é—®é¢˜]
    
    B --> B1[çƒ­ç‚¹é”®/å€¼]
    B --> B2[å¼‚å¸¸æ•°æ®]
    
    C --> C1[æ—¶é—´ç»´åº¦èšåˆ]
    C --> C2[åœ°åŸŸç»´åº¦èšåˆ]
    
    D --> D1[é»˜è®¤åˆ†åŒºå™¨é—®é¢˜]
    D --> D2[å¹¶è¡Œåº¦è®¾ç½®ä¸å½“]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#ffe6cc,stroke:#d79b00
    style D fill:#d5e8d4,stroke:#82b366
```

**3. è§£å†³æ–¹æ¡ˆåˆ†ç±»**

æ ¹æ®å€¾æ–œåœºæ™¯å’ŒåŸå› ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼š

| å€¾æ–œåœºæ™¯ | è§£å†³æ–¹æ¡ˆ | é€‚ç”¨æ¡ä»¶ | ä¼˜ç¼ºç‚¹ |
|---------|---------|---------|--------|
| **Joinå€¾æ–œ** | å¹¿æ’­Join | ä¸€ä¾§æ•°æ®é›†è¾ƒå°(<10GB) | ç®€å•é«˜æ•ˆï¼Œä½†å—å†…å­˜é™åˆ¶ |
| **Joinå€¾æ–œ** | æ‹†åˆ†çƒ­ç‚¹é”® | èƒ½è¯†åˆ«å‡ºçƒ­ç‚¹é”® | é’ˆå¯¹æ€§å¼ºï¼Œä½†å®ç°å¤æ‚ |
| **Joinå€¾æ–œ** | éšæœºå‰ç¼€+æ‰©å®¹ | çƒ­ç‚¹é”®è¾ƒå¤š | é€šç”¨æ€§å¥½ï¼Œä½†å¢åŠ è®¡ç®—é‡ |
| **èšåˆå€¾æ–œ** | ä¸¤é˜¶æ®µèšåˆ | èšåˆæ“ä½œ(å¦‚reduceByKey) | æ•ˆæœå¥½ï¼Œé€‚ç”¨é¢å¹¿ |
| **èšåˆå€¾æ–œ** | è‡ªå®šä¹‰åˆ†åŒº | æ•°æ®åˆ†å¸ƒå·²çŸ¥ | ç²¾ç¡®æ§åˆ¶ï¼Œä½†éœ€å®šåˆ¶å¼€å‘ |
| **æ•°æ®æºå€¾æ–œ** | é¢„å¤„ç†é‡åˆ†åŒº | ETLé˜¶æ®µå¯æ§ | æ²»æœ¬æ–¹æ³•ï¼Œä½†å¢åŠ å‰ç½®å¤„ç† |

**4. è¯¦ç»†è§£å†³æ–¹æ¡ˆ**

**4.1 Joinæ“ä½œå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šå¹¿æ’­Join**
```scala
// ä¼˜åŒ–å‰
val result = largeDF.join(smallDF, "key")

// ä¼˜åŒ–å
import org.apache.spark.sql.functions.broadcast
val result = largeDF.join(broadcast(smallDF), "key")
```

**æ–¹æ¡ˆäºŒï¼šæ‹†åˆ†çƒ­ç‚¹é”®**
```scala
// å‡è®¾å‘ç°"000"æ˜¯çƒ­ç‚¹é”®
// 1. å°†å¤§è¡¨ä¸­çƒ­ç‚¹é”®å¯¹åº”çš„æ•°æ®æ‹†åˆ†å‡ºæ¥
val largeDF_normal = largeDF.filter($"key" =!= "000")
val largeDF_skew = largeDF.filter($"key" === "000")
  .withColumn("key_random", concat($"key", lit("_"), rand()*10))

// 2. å°†å°è¡¨å¯¹åº”çƒ­ç‚¹é”®æ•°æ®æ‰©å®¹
val smallDF_normal = smallDF.filter($"key" =!= "000")
val smallDF_skew = smallDF.filter($"key" === "000")
  .withColumn("key_random", 
    explode(array((0 until 10).map(i => concat($"key", lit("_"), lit(i))): _*)))

// 3. åˆ†åˆ«Joinååˆå¹¶ç»“æœ
val join1 = largeDF_normal.join(smallDF_normal, "key")
val join2 = largeDF_skew.join(smallDF_skew, 
  largeDF_skew("key_random") === smallDF_skew("key_random"))
  .drop("key_random")

val result = join1.union(join2)
```

**æ–¹æ¡ˆä¸‰ï¼šéšæœºå‰ç¼€+æ‰©å®¹Join**
```scala
// 1. å¤§è¡¨æ·»åŠ éšæœºå‰ç¼€
val largeDF_rand = largeDF.withColumn("prefix", (rand()*10).cast("int"))
  .withColumn("key_prefixed", concat(col("prefix").cast("string"), lit("_"), col("key")))

// 2. å°è¡¨æ‰©å®¹10å€
val smallDF_expanded = smallDF.withColumn("prefix", 
  explode(array((0 until 10).map(lit(_)): _*)))
  .withColumn("key_prefixed", concat(col("prefix").cast("string"), lit("_"), col("key")))

// 3. åœ¨prefixed keyä¸ŠJoin
val joinResult = largeDF_rand.join(smallDF_expanded, "key_prefixed")
  .drop("prefix", "key_prefixed")
```

**4.2 èšåˆæ“ä½œå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šä¸¤é˜¶æ®µèšåˆ**
```scala
// ä¼˜åŒ–å‰
val result = rdd.reduceByKey(_ + _)

// ä¼˜åŒ–å
val result = rdd
  // ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨èšåˆï¼ŒåŠ éšæœºå‰ç¼€
  .map(x => ((x._1, Random.nextInt(100)), x._2))
  .reduceByKey(_ + _)
  // ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆï¼Œå»é™¤éšæœºå‰ç¼€
  .map(x => (x._1._1, x._2))
  .reduceByKey(_ + _)
```

**æ–¹æ¡ˆäºŒï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨**
```scala
// å®šä¹‰è‡ªå®šä¹‰åˆ†åŒºå™¨
class BalancedPartitioner(partitions: Int) extends Partitioner {
  def numPartitions: Int = partitions
  
  def getPartition(key: Any): Int = {
    val k = key.toString
    // å¯¹çƒ­ç‚¹é”®ç‰¹æ®Šå¤„ç†
    if (k == "hot_key_1") {
      Math.abs(Random.nextInt() % partitions)
    } else {
      Math.abs(k.hashCode % partitions)
    }
  }
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
val result = rdd
  .partitionBy(new BalancedPartitioner(100))
  .reduceByKey(_ + _)
```

**4.3 æ•°æ®æºå€¾æ–œè§£å†³æ–¹æ¡ˆ**

**æ–¹æ¡ˆä¸€ï¼šé¢„å¤„ç†è¿‡æ»¤å¼‚å¸¸æ•°æ®**
```scala
// è¿‡æ»¤æ‰å¯èƒ½å¯¼è‡´å€¾æ–œçš„å¼‚å¸¸å€¼
val cleanedDF = rawDF.filter($"key".isNotNull && $"key" =!= "")
```

**æ–¹æ¡ˆäºŒï¼šé¢„èšåˆå¤„ç†**
```scala
// åœ¨ETLé˜¶æ®µè¿›è¡Œé¢„èšåˆ
val preAggregatedDF = rawDF
  .repartition(200, $"date", $"region")  // å…ˆæŒ‰éå€¾æ–œç»´åº¦é‡åˆ†åŒº
  .groupBy($"date", $"region", $"user_id")  // ä½ç²’åº¦é¢„èšåˆ
  .agg(sum($"value").as("value"))
```

**5. å®é™…æ¡ˆä¾‹åˆ†æ**

**æ¡ˆä¾‹ï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†æä¸­çš„æ•°æ®å€¾æ–œ**

**é—®é¢˜æè¿°**ï¼š
åœ¨ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æä¸­ï¼Œéœ€è¦ç»Ÿè®¡æ¯ä¸ªå•†å“çš„ç‚¹å‡»æ¬¡æ•°ï¼Œä½†æŸäº›çƒ­é—¨å•†å“çš„ç‚¹å‡»é‡è¿œé«˜äºå…¶ä»–å•†å“ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```scala
// åŸå§‹ä»£ç 
val clickCounts = userClicks
  .groupBy("product_id")
  .count()

// ä¼˜åŒ–åä»£ç 
// 1. æ•°æ®é‡‡æ ·ï¼Œè¯†åˆ«çƒ­ç‚¹å•†å“
val sampleDF = userClicks.sample(0.1)
val hotProducts = sampleDF
  .groupBy("product_id")
  .count()
  .orderBy($"count".desc)
  .limit(10)
  .collect()
  .map(_.getAs[String]("product_id"))
  .toSet
val bcHotProducts = spark.sparkContext.broadcast(hotProducts)

// 2. å¯¹çƒ­ç‚¹å•†å“ç‰¹æ®Šå¤„ç†
val processedClicks = userClicks.mapPartitions(iter => {
  val hotProds = bcHotProducts.value
  iter.map(row => {
    val productId = row.getAs[String]("product_id")
    if (hotProds.contains(productId)) {
      // ä¸ºçƒ­ç‚¹å•†å“æ·»åŠ éšæœºåç¼€
      Row.fromSeq(row.toSeq :+ (productId + "_" + Random.nextInt(100)))
    } else {
      // éçƒ­ç‚¹å•†å“ä¿æŒä¸å˜
      Row.fromSeq(row.toSeq :+ productId)
    }
  })
}, true)

// 3. ä½¿ç”¨å¤„ç†åçš„é”®è¿›è¡Œèšåˆ
val schema = userClicks.schema.add("balanced_key", StringType)
val balancedDF = spark.createDataFrame(processedClicks, schema)

val result = balancedDF
  .groupBy("balanced_key")
  .count()
  // å»é™¤éšæœºåç¼€ï¼Œæ¢å¤åŸå§‹å•†å“ID
  .withColumn("product_id", 
    when($"balanced_key".contains("_"), 
      split($"balanced_key", "_").getItem(0))
    .otherwise($"balanced_key"))
  .groupBy("product_id")
  .sum("count")
  .drop("balanced_key")
```

**6. é¢„é˜²æ•°æ®å€¾æ–œçš„æœ€ä½³å®è·µ**

1. **åˆç†è®¾è®¡é”®**ï¼šé¿å…ä½¿ç”¨å¯èƒ½äº§ç”Ÿçƒ­ç‚¹çš„é”®ï¼ˆå¦‚æ—¶é—´æˆ³ç²¾ç¡®åˆ°ç§’ï¼‰
2. **æå‰é¢„ä¼°**ï¼šåœ¨å¼€å‘å‰è¯„ä¼°æ•°æ®åˆ†å¸ƒæƒ…å†µ
3. **ç›‘æ§æœºåˆ¶**ï¼šå»ºç«‹ä»»åŠ¡ç›‘æ§ï¼ŒåŠæ—¶å‘ç°å€¾æ–œé—®é¢˜
4. **æ•°æ®è´¨é‡**ï¼šåœ¨æ•°æ®æ¥å…¥é˜¶æ®µå¤„ç†å¼‚å¸¸å€¼å’Œç©ºå€¼
5. **å¹¶è¡Œåº¦**ï¼šè®¾ç½®åˆç†çš„å¹¶è¡Œåº¦ï¼Œé¿å…åˆ†åŒºè¿‡å°‘

æœ‰æ•ˆè§£å†³æ•°æ®å€¾æ–œé—®é¢˜éœ€è¦ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯å’Œæ•°æ®ç‰¹ç‚¹ï¼Œçµæ´»è¿ç”¨å„ç§æŠ€æœ¯æ‰‹æ®µï¼Œä»æ ¹æœ¬ä¸Šä¼˜åŒ–æ•°æ®åˆ†å¸ƒï¼Œæé«˜Sparkä½œä¸šçš„æ‰§è¡Œæ•ˆç‡ã€‚

### å®æˆ˜åº”ç”¨é¢˜

**Q8: è¯·ä»‹ç»Spark SQLçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ä½•æé«˜SQLæŸ¥è¯¢æ€§èƒ½ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Spark SQLæ˜¯Sparkç”Ÿæ€ç³»ç»Ÿä¸­çš„é‡è¦ç»„ä»¶ï¼Œå®ƒæä¾›äº†ç»“æ„åŒ–æ•°æ®å¤„ç†èƒ½åŠ›å’ŒSQLæŸ¥è¯¢æ¥å£ã€‚é€šè¿‡ä¸€ç³»åˆ—ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡SQLæŸ¥è¯¢æ€§èƒ½ã€‚

**1. Spark SQLä¼˜åŒ–æŠ€æœ¯æ¦‚è¿°**

Spark SQLä¼˜åŒ–ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
- **Catalystä¼˜åŒ–å™¨**ï¼šåŸºäºè§„åˆ™å’Œæˆæœ¬çš„æŸ¥è¯¢ä¼˜åŒ–
- **Tungstenæ‰§è¡Œå¼•æ“**ï¼šå†…å­˜ç®¡ç†å’Œä»£ç ç”Ÿæˆä¼˜åŒ–
- **å‚æ•°é…ç½®**ï¼šé’ˆå¯¹ç‰¹å®šåœºæ™¯çš„å‚æ•°è°ƒæ•´

**2. å…³é”®ä¼˜åŒ–é…ç½®**

```scala
// å¼€å¯è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ(AQE)
spark.conf.set("spark.sql.adaptive.enabled", "true")
// å¯ç”¨å°åˆ†åŒºåˆå¹¶
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
// è®¾ç½®åˆå¹¶åçš„ç›®æ ‡åˆ†åŒºå¤§å°
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128m")

// å¹¿æ’­Joinä¼˜åŒ–
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")
// å¯ç”¨AQEä¼˜åŒ–çš„Joinç­–ç•¥
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")

// åŠ¨æ€åˆ†åŒºè£å‰ª
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
// å¯ç”¨Joiné‡æ’åº
spark.conf.set("spark.sql.optimizer.joinReorder.enabled", "true")
```

**3. æŸ¥è¯¢ä¼˜åŒ–å®ä¾‹**

```scala
// ä¼˜åŒ–å‰
val result = spark.sql("""
  SELECT c.customer_name, sum(o.order_amount) as total_amount
  FROM orders o
  JOIN customers c ON o.customer_id = c.customer_id
  WHERE o.order_date > '2023-01-01'
  GROUP BY c.customer_name
""")

// ä¼˜åŒ–å
// 1. ä½¿ç”¨å¹¿æ’­Join
val customers = spark.table("customers")
val orders = spark.table("orders").filter($"order_date" > "2023-01-01")
import org.apache.spark.sql.functions.broadcast
val result = orders.join(broadcast(customers), "customer_id")
  .groupBy($"customer_name")
  .agg(sum($"order_amount").as("total_amount"))

// 2. å¯ç”¨AQEå’Œå…¶ä»–ä¼˜åŒ–
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
val result = spark.sql("""
  SELECT /*+ BROADCAST(c) */ 
    c.customer_name, sum(o.order_amount) as total_amount
  FROM orders o
  JOIN customers c ON o.customer_id = c.customer_id
  WHERE o.order_date > '2023-01-01'
  GROUP BY c.customer_name
""")
```

**4. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ**

- **æ•°æ®æ ¼å¼é€‰æ‹©**ï¼šä½¿ç”¨åˆ—å¼å­˜å‚¨æ ¼å¼ï¼ˆParquetã€ORCï¼‰
- **åˆ†åŒºç­–ç•¥**ï¼šæ ¹æ®æŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆé€‚çš„åˆ†åŒºé”®
- **é¢„èšåˆ**ï¼šå¯¹å¸¸ç”¨æŸ¥è¯¢åˆ›å»ºç‰©åŒ–è§†å›¾
- **ç¼“å­˜ç®¡ç†**ï¼šç¼“å­˜é¢‘ç¹ä½¿ç”¨çš„è¡¨æˆ–æŸ¥è¯¢ç»“æœ
- **SQL Hint**ï¼šä½¿ç”¨æŸ¥è¯¢æç¤ºæŒ‡å¯¼ä¼˜åŒ–å™¨

é€šè¿‡ç»¼åˆåº”ç”¨è¿™äº›ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡Spark SQLæŸ¥è¯¢æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚

**Q9: å½“Sparkåº”ç”¨å‡ºç°æ•…éšœæˆ–æ€§èƒ½é—®é¢˜æ—¶ï¼Œå¦‚ä½•è¿›è¡Œæ’æŸ¥å’Œè§£å†³ï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkåº”ç”¨æ•…éšœæ’æŸ¥æ˜¯ä¸€é¡¹ç³»ç»Ÿæ€§å·¥ä½œï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦æ”¶é›†ä¿¡æ¯ï¼Œåˆ†ææ ¹å› ï¼Œå¹¶é‡‡å–ç›¸åº”çš„è§£å†³æªæ–½ã€‚

**1. æ•…éšœæ’æŸ¥æ–¹æ³•è®º**

æœ‰æ•ˆçš„æ•…éšœæ’æŸ¥éœ€è¦éµå¾ªä»¥ä¸‹æ–¹æ³•è®ºï¼š
- **ç³»ç»Ÿæ€§åˆ†æ**ï¼šä»åº”ç”¨ã€é›†ç¾¤åˆ°èµ„æºå…¨é¢è€ƒè™‘
- **æ•°æ®é©±åŠ¨**ï¼šåŸºäºæ—¥å¿—å’Œç›‘æ§æ•°æ®è¿›è¡Œåˆ†æ
- **é€å±‚æ’é™¤**ï¼šä»å¤–åˆ°å†…æˆ–ä»å†…åˆ°å¤–é€å±‚æ’æŸ¥
- **å¤ç°éªŒè¯**ï¼šå°è¯•å¤ç°é—®é¢˜å¹¶éªŒè¯è§£å†³æ–¹æ¡ˆ

**2. æ’æŸ¥æ­¥éª¤è¯¦è§£**

1. **æŸ¥çœ‹Spark UI**
   - åˆ†æå¤±è´¥çš„Stageå’ŒTask
   - æ£€æŸ¥Jobæ‰§è¡Œæ—¶é—´å’Œèµ„æºä½¿ç”¨æƒ…å†µ
   - è¯†åˆ«å¼‚å¸¸çš„æ‰§è¡Œæ¨¡å¼ï¼ˆå¦‚æ•°æ®å€¾æ–œï¼‰

2. **æ£€æŸ¥æ—¥å¿—ä¿¡æ¯**
   - Driveræ—¥å¿—ï¼šåº”ç”¨çº§åˆ«é”™è¯¯å’Œå¼‚å¸¸
   - Executoræ—¥å¿—ï¼šä»»åŠ¡æ‰§è¡Œé”™è¯¯
   - Worker/Masteræ—¥å¿—ï¼šé›†ç¾¤çº§åˆ«é—®é¢˜
   - ç³»ç»Ÿæ—¥å¿—ï¼šèµ„æºå’Œç¯å¢ƒé—®é¢˜

3. **èµ„æºç›‘æ§åˆ†æ**
   - CPUä½¿ç”¨ç‡ï¼šæ˜¯å¦å­˜åœ¨è®¡ç®—ç“¶é¢ˆ
   - å†…å­˜ä½¿ç”¨ï¼šæ˜¯å¦å­˜åœ¨OOMæˆ–GCé—®é¢˜
   - ç£ç›˜I/Oï¼šæ˜¯å¦å­˜åœ¨å­˜å‚¨ç“¶é¢ˆ
   - ç½‘ç»œä¼ è¾“ï¼šæ˜¯å¦å­˜åœ¨ç½‘ç»œç“¶é¢ˆ

4. **å¸¸è§é—®é¢˜è¯Šæ–­**

| é—®é¢˜ç±»å‹ | ç—‡çŠ¶ | è¯Šæ–­æ–¹æ³• | å¯èƒ½è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|------------|
| **OOMé”™è¯¯** | `java.lang.OutOfMemoryError` | æ£€æŸ¥GCæ—¥å¿—ï¼Œå†…å­˜ä½¿ç”¨è¶‹åŠ¿ | å¢åŠ å†…å­˜ï¼Œè°ƒæ•´åˆ†åŒºï¼Œä¼˜åŒ–ä»£ç  |
| **æ•°æ®å€¾æ–œ** | å°‘æ•°ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿œé•¿äºå…¶ä»–ä»»åŠ¡ | æŸ¥çœ‹Stageè¯¦æƒ…ï¼Œåˆ†ææ•°æ®åˆ†å¸ƒ | åŠ ç›å¤„ç†ï¼Œé¢„èšåˆï¼Œè°ƒæ•´åˆ†åŒº |
| **åºåˆ—åŒ–é”™è¯¯** | `java.io.NotSerializableException` | æ£€æŸ¥ç±»çš„åºåˆ—åŒ–å®ç° | å®ç°Serializableæ¥å£ï¼Œä½¿ç”¨@transientæ³¨è§£ |
| **Shuffleå¤±è´¥** | `FetchFailedException` | æ£€æŸ¥Shuffleå†™å…¥å’Œè¯»å–æ—¥å¿— | å¢åŠ å†…å­˜ï¼Œè°ƒæ•´Shuffleå‚æ•° |
| **èµ„æºä¸è¶³** | ä»»åŠ¡æ’é˜Ÿï¼Œæ‰§è¡Œç¼“æ…¢ | æŸ¥çœ‹é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ | å¢åŠ èµ„æºï¼Œä¼˜åŒ–èµ„æºåˆ†é… |

**3. æ€§èƒ½é—®é¢˜æ’æŸ¥å·¥å…·**

```bash
# æŸ¥çœ‹Sparkåº”ç”¨æ—¥å¿—
yarn logs -applicationId application_1234567890_0001

# ä½¿ç”¨jstackæŸ¥çœ‹JVMçº¿ç¨‹çŠ¶æ€
jstack <pid> > thread_dump.txt

# ä½¿ç”¨jmapæŸ¥çœ‹å†…å­˜ä½¿ç”¨
jmap -heap <pid>

# ä½¿ç”¨Spark History ServeræŸ¥çœ‹å†å²åº”ç”¨
http://history-server:18080
```

**4. å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ**

```scala
// è§£å†³OOMé—®é¢˜
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

// è§£å†³Shuffleé—®é¢˜
spark.conf.set("spark.shuffle.file.buffer", "64k")
spark.conf.set("spark.reducer.maxSizeInFlight", "96m")
spark.conf.set("spark.shuffle.io.maxRetries", "10")

// è§£å†³æ•°æ®å€¾æ–œ
// å‚è§æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ
```

**5. é¢„é˜²æªæ–½**

- **ç›‘æ§ç³»ç»Ÿ**ï¼šå»ºç«‹åº”ç”¨å’Œé›†ç¾¤ç›‘æ§
- **æ€§èƒ½æµ‹è¯•**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒå‰è¿›è¡Œå‹åŠ›æµ‹è¯•
- **æ¸è¿›å¼éƒ¨ç½²**ï¼šå…ˆå°è§„æ¨¡æµ‹è¯•ï¼Œå†æ‰©å¤§è§„æ¨¡
- **å®¹é‡è§„åˆ’**ï¼šæ ¹æ®æ•°æ®å¢é•¿é¢„ä¼°èµ„æºéœ€æ±‚

é€šè¿‡ç³»ç»Ÿæ€§çš„æ•…éšœæ’æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–ï¼Œå¯ä»¥æé«˜Sparkåº”ç”¨çš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œå‡å°‘ç”Ÿäº§ç¯å¢ƒä¸­çš„é—®é¢˜å‘ç”Ÿã€‚

### æ·±åº¦æŠ€æœ¯åŸç†é¢˜

**Q10: è¯·è¯¦ç»†è§£é‡ŠSparkçš„Catalystä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†åŠå…¶ä¼˜åŒ–è§„åˆ™ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Catalystä¼˜åŒ–å™¨æ˜¯Spark SQLçš„æ ¸å¿ƒä¼˜åŒ–å¼•æ“ï¼Œå®ƒåŸºäºScalaçš„æ¨¡å¼åŒ¹é…å’Œå‡½æ•°å¼ç¼–ç¨‹ç‰¹æ€§æ„å»ºï¼Œä¸ºSpark SQLæä¾›äº†å¼ºå¤§çš„æŸ¥è¯¢ä¼˜åŒ–èƒ½åŠ›ã€‚ç†è§£Catalystçš„å·¥ä½œåŸç†å¯¹äºç¼–å†™é«˜æ•ˆçš„Spark SQLåº”ç”¨è‡³å…³é‡è¦ã€‚

**1. Catalystä¼˜åŒ–å™¨æ¶æ„**

Catalystä¼˜åŒ–å™¨çš„æ ¸å¿ƒæ¶æ„åŒ…æ‹¬ä»¥ä¸‹ç»„ä»¶ï¼š
- **æ ‘èŠ‚ç‚¹è½¬æ¢æ¡†æ¶**ï¼šåŸºäºScalaæ¨¡å¼åŒ¹é…çš„æ ‘è½¬æ¢æœºåˆ¶
- **è§„åˆ™æ‰§è¡Œå¼•æ“**ï¼šåº”ç”¨ä¼˜åŒ–è§„åˆ™çš„æ‰§è¡Œå™¨
- **æˆæœ¬æ¨¡å‹**ï¼šè¯„ä¼°ä¸åŒæ‰§è¡Œè®¡åˆ’æ€§èƒ½çš„æ¨¡å‹
- **ä»£ç ç”Ÿæˆå¼•æ“**ï¼šå°†ç‰©ç†è®¡åˆ’è½¬æ¢ä¸ºé«˜æ•ˆæ‰§è¡Œä»£ç 

**2. ä¼˜åŒ–æµç¨‹è¯¦è§£**

```mermaid
graph TD
    A[SQL/DataFrame API] --> B[æŠ½è±¡è¯­æ³•æ ‘(AST)]
    B --> C[æœªè§£æé€»è¾‘è®¡åˆ’]
    C --> D[è§£æé€»è¾‘è®¡åˆ’]
    D --> E[ä¼˜åŒ–é€»è¾‘è®¡åˆ’]
    E --> F[ç‰©ç†è®¡åˆ’ç”Ÿæˆ]
    F --> G[ç‰©ç†è®¡åˆ’ä¼˜åŒ–]
    G --> H[é€‰æ‹©æœ€ä¼˜è®¡åˆ’]
    H --> I[ä»£ç ç”Ÿæˆ]
    I --> J[æ‰§è¡Œ]
    
    style A fill:#f9f9f9,stroke:#333
    style B fill:#d4f1f9,stroke:#05a4d1
    style C fill:#d4f1f9,stroke:#05a4d1
    style D fill:#ffe6cc,stroke:#d79b00
    style E fill:#ffe6cc,stroke:#d79b00
    style F fill:#d5e8d4,stroke:#82b366
    style G fill:#d5e8d4,stroke:#82b366
    style H fill:#d5e8d4,stroke:#82b366
    style I fill:#e1d5e7,stroke:#9673a6
    style J fill:#f8cecc,stroke:#b85450
```

**3. ä¼˜åŒ–é˜¶æ®µè¯¦ç»†è¯´æ˜**

1. **è¯­æ³•åˆ†æ**
   - å°†SQLè¯­å¥è§£æä¸ºæŠ½è±¡è¯­æ³•æ ‘(AST)
   - ä½¿ç”¨ANTLRè¯­æ³•è§£æå™¨å¤„ç†SQLè¯­æ³•
   - è½¬æ¢DataFrame/Dataset APIè°ƒç”¨ä¸ºå†…éƒ¨è¡¨ç¤º

2. **é€»è¾‘è®¡åˆ’ç”Ÿæˆä¸è§£æ**
   - å°†ASTè½¬æ¢ä¸ºæœªè§£æé€»è¾‘è®¡åˆ’
   - é€šè¿‡Catalogè§£æè¡¨åã€åˆ—åå’Œå‡½æ•°å
   - è¿›è¡Œç±»å‹æ¨æ–­å’Œç±»å‹æ£€æŸ¥

3. **é€»è¾‘è®¡åˆ’ä¼˜åŒ–**
   - åº”ç”¨åŸºäºè§„åˆ™çš„ä¼˜åŒ–ç­–ç•¥
   - ä¼˜åŒ–è½¬æ¢æ˜¯å£°æ˜å¼çš„ï¼ŒåŸºäºæ¨¡å¼åŒ¹é…
   - å¤šè½®åº”ç”¨è§„åˆ™ç›´è‡³è®¡åˆ’ç¨³å®š

4. **ç‰©ç†è®¡åˆ’ç”Ÿæˆ**
   - å°†é€»è¾‘ç®—å­è½¬æ¢ä¸ºç‰©ç†ç®—å­
   - ä¸ºåŒä¸€é€»è¾‘æ“ä½œç”Ÿæˆå¤šç§ç‰©ç†å®ç°
   - å¦‚Sortå¯å®ç°ä¸ºSortExecæˆ–ExternalSortExec

5. **ç‰©ç†è®¡åˆ’ä¼˜åŒ–ä¸é€‰æ‹©**
   - ä½¿ç”¨åŸºäºæˆæœ¬çš„ä¼˜åŒ–å™¨è¯„ä¼°è®¡åˆ’
   - è€ƒè™‘æ•°æ®å¤§å°ã€æ“ä½œå¤æ‚åº¦ç­‰å› ç´ 
   - é€‰æ‹©æˆæœ¬æœ€ä½çš„æ‰§è¡Œè®¡åˆ’

6. **ä»£ç ç”Ÿæˆ**
   - ä½¿ç”¨Janinoç¼–è¯‘å™¨ç”ŸæˆJavaå­—èŠ‚ç 
   - å°†å¤šä¸ªæ“ä½œèåˆä¸ºå•ä¸ªå‡½æ•°
   - å‡å°‘è™šå‡½æ•°è°ƒç”¨å’Œè§£é‡Šå¼€é”€

**4. æ ¸å¿ƒä¼˜åŒ–è§„åˆ™è¯¦è§£**

| ä¼˜åŒ–è§„åˆ™ | æè¿° | ç¤ºä¾‹ |
|---------|------|------|
| **è°“è¯ä¸‹æ¨** | å°†è¿‡æ»¤æ¡ä»¶å°½æ—©åº”ç”¨ï¼Œå‡å°‘æ•°æ®é‡ | `SELECT * FROM t1 JOIN t2 WHERE t1.id > 100` â†’ å…ˆè¿‡æ»¤t1.id > 100å†Join |
| **åˆ—è£å‰ª** | åªè¯»å–å’Œå¤„ç†æŸ¥è¯¢æ‰€éœ€çš„åˆ— | `SELECT name FROM (SELECT id, name, age FROM users)` â†’ åªè¯»å–nameåˆ— |
| **å¸¸é‡æŠ˜å ** | ç¼–è¯‘æ—¶è®¡ç®—å¸¸é‡è¡¨è¾¾å¼ | `SELECT id + 5 * 10 FROM t` â†’ `SELECT id + 50 FROM t` |
| **Joiné‡æ’åº** | ä¼˜åŒ–å¤šè¡¨Joinçš„é¡ºåº | å°è¡¨å…ˆJoinï¼Œå‡å°‘ä¸­é—´ç»“æœ |
| **Joiné€‰æ‹©** | æ ¹æ®è¡¨å¤§å°é€‰æ‹©Joinç­–ç•¥ | å°è¡¨ä½¿ç”¨BroadcastHashJoinï¼Œå¤§è¡¨ä½¿ç”¨SortMergeJoin |
| **åˆ†åŒºè£å‰ª** | åªè¯»å–åŒ…å«æ‰€éœ€æ•°æ®çš„åˆ†åŒº | `WHERE date='2023-01-01'` â†’ åªè¯»å–è¯¥æ—¥æœŸçš„åˆ†åŒº |
| **èšåˆä¼˜åŒ–** | éƒ¨åˆ†èšåˆ+æœ€ç»ˆèšåˆ | å…ˆåœ¨æ¯ä¸ªåˆ†åŒºå†…èšåˆï¼Œå†å…¨å±€èšåˆ |

**5. ä»£ç ç¤ºä¾‹ï¼šCatalystè½¬æ¢è¿‡ç¨‹**

```scala
// ç¤ºä¾‹æŸ¥è¯¢
val query = spark.sql("""
  SELECT c.name, sum(o.amount) as total
  FROM orders o
  JOIN customers c ON o.customer_id = c.id
  WHERE o.date > '2023-01-01'
  GROUP BY c.name
  HAVING sum(o.amount) > 1000
""")

// æŸ¥çœ‹é€»è¾‘è®¡åˆ’
println("Logical Plan:")
query.queryExecution.logical.explain(true)

// æŸ¥çœ‹ä¼˜åŒ–åçš„é€»è¾‘è®¡åˆ’
println("Optimized Logical Plan:")
query.queryExecution.optimizedPlan.explain(true)

// æŸ¥çœ‹ç‰©ç†è®¡åˆ’
println("Physical Plan:")
query.queryExecution.sparkPlan.explain(true)

// æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
println("Executed Plan:")
query.queryExecution.executedPlan.explain(true)
```

**6. è‡ªå®šä¹‰ä¼˜åŒ–è§„åˆ™**

Catalystå…è®¸å¼€å‘è€…æ‰©å±•ä¼˜åŒ–è§„åˆ™ï¼Œå®ç°è‡ªå®šä¹‰ä¼˜åŒ–ï¼š

```scala
// è‡ªå®šä¹‰ä¼˜åŒ–è§„åˆ™ç¤ºä¾‹
object MyOptimizationRule extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    case Filter(condition, child) if canOptimize(condition) =>
      // è‡ªå®šä¹‰ä¼˜åŒ–é€»è¾‘
      OptimizedFilter(optimizeCondition(condition), child)
  }
}

// æ³¨å†Œè‡ªå®šä¹‰è§„åˆ™
spark.experimental.extraOptimizations = Seq(MyOptimizationRule)
```

Catalystä¼˜åŒ–å™¨æ˜¯Spark SQLæ€§èƒ½ä¼˜è¶Šçš„å…³é”®å› ç´ ï¼Œé€šè¿‡ç†è§£å…¶å·¥ä½œåŸç†å’Œä¼˜åŒ–è§„åˆ™ï¼Œå¯ä»¥ç¼–å†™æ›´é«˜æ•ˆçš„Spark SQLåº”ç”¨ï¼Œå¹¶åœ¨å¿…è¦æ—¶é€šè¿‡è‡ªå®šä¹‰è§„åˆ™è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**Q11: è¯·è¯¦ç»†ä»‹ç»Sparkå†…å­˜ç®¡ç†çš„æ¼”è¿›å†å²ï¼Œæ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkå†…å­˜ç®¡ç†æœºåˆ¶ç»å†äº†é‡è¦çš„æ¼”è¿›è¿‡ç¨‹ï¼Œä»æ—©æœŸçš„é™æ€å†…å­˜ç®¡ç†åˆ°ç°ä»£çš„ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œæ˜¾è‘—æå‡äº†å†…å­˜åˆ©ç”¨ç‡å’Œåº”ç”¨æ€§èƒ½ã€‚

**1. å†…å­˜ç®¡ç†æ¼”è¿›çš„èƒŒæ™¯**

Sparkä½œä¸ºå†…å­˜è®¡ç®—æ¡†æ¶ï¼Œå…¶æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå†…å­˜ç®¡ç†æ•ˆç‡ã€‚æ—©æœŸç‰ˆæœ¬çš„å†…å­˜ç®¡ç†æœºåˆ¶å­˜åœ¨è¯¸å¤šé—®é¢˜ï¼Œå¦‚å†…å­˜åˆ’åˆ†å›ºå®šã€é…ç½®å¤æ‚ã€èµ„æºåˆ©ç”¨ç‡ä½ç­‰ï¼Œè¿™ä¿ƒä½¿Sparkå›¢é˜Ÿä¸æ–­ä¼˜åŒ–å†…å­˜ç®¡ç†æœºåˆ¶ã€‚

**2. é™æ€å†…å­˜ç®¡ç†ï¼ˆStatic Memory Managementï¼‰**

é™æ€å†…å­˜ç®¡ç†æ˜¯Spark 1.6ä¹‹å‰çš„é»˜è®¤æ¨¡å¼ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼š

- **å›ºå®šå†…å­˜åˆ’åˆ†**ï¼šé¢„å…ˆä¸ºä¸åŒç”¨é€”åˆ’åˆ†å›ºå®šæ¯”ä¾‹çš„å†…å­˜
- **ä¸¥æ ¼è¾¹ç•Œ**ï¼šå„å†…å­˜åŒºåŸŸä¹‹é—´ä¸èƒ½åŠ¨æ€è°ƒæ•´
- **æ‰‹åŠ¨é…ç½®**ï¼šéœ€è¦ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå‚æ•°

**é™æ€å†…å­˜åˆ’åˆ†**ï¼š
```
+---------------------------+---------------------------+---------------+
|     Storage Memory        |     Execution Memory      |  Other Memory |
| spark.storage.memoryFraction  | spark.shuffle.memoryFraction |  Remainder    |
|        (é»˜è®¤0.6)          |        (é»˜è®¤0.2)          |    (0.2)      |
+---------------------------+---------------------------+---------------+
```

**å…³é”®å‚æ•°**ï¼š
```scala
// é™æ€å†…å­˜ç®¡ç†å…³é”®å‚æ•°
spark.storage.memoryFraction = 0.6  // ç¼“å­˜RDDæ•°æ®çš„å†…å­˜æ¯”ä¾‹
spark.storage.unrollFraction = 0.2  // ç”¨äºå±•å¼€RDDçš„å†…å­˜æ¯”ä¾‹
spark.shuffle.memoryFraction = 0.2  // Shuffleæ“ä½œçš„å†…å­˜æ¯”ä¾‹
```

**3. ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼ˆUnified Memory Managementï¼‰**

ç»Ÿä¸€å†…å­˜ç®¡ç†æ˜¯Spark 1.6åŠä¹‹åçš„é»˜è®¤æ¨¡å¼ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼š

- **åŠ¨æ€å†…å­˜å…±äº«**ï¼šStorageå’ŒExecutionå†…å­˜å¯ä»¥ç›¸äº’å€Ÿç”¨
- **ç®€åŒ–é…ç½®**ï¼šå‡å°‘é…ç½®å‚æ•°ï¼Œæ›´æ˜“ä½¿ç”¨
- **è‡ªé€‚åº”è°ƒæ•´**ï¼šæ ¹æ®è¿è¡Œæ—¶éœ€æ±‚åŠ¨æ€åˆ†é…å†…å­˜

**ç»Ÿä¸€å†…å­˜åˆ’åˆ†**ï¼š
```
+--------------------+----------------------+---------------+
|      Reserved      |      User Memory     |  Spark Memory |
|      (300MB)       | (1-spark.memory.fraction) | spark.memory.fraction |
+--------------------+----------------------+---------------+
                                            |
                     +----------------------+---------------+
                     |    Storage Memory    | Execution Memory |
                     | spark.memory.storageFraction | Remainder |
                     +----------------------+---------------+
```

**å…³é”®å‚æ•°**ï¼š
```scala
// ç»Ÿä¸€å†…å­˜ç®¡ç†å…³é”®å‚æ•°
spark.memory.fraction = 0.75  // Spark Memoryå JVMå †å†…å­˜çš„æ¯”ä¾‹
spark.memory.storageFraction = 0.5  // Storage Memoryåˆå§‹å æ¯”
```

**4. ä¸¤ç§æ¨¡å¼çš„æ ¸å¿ƒåŒºåˆ«**

| ç‰¹æ€§ | é™æ€å†…å­˜ç®¡ç† | ç»Ÿä¸€å†…å­˜ç®¡ç† |
|------|------------|------------|
| **å†…å­˜åˆ’åˆ†** | å›ºå®šæ¯”ä¾‹ï¼Œä¸å¯è°ƒæ•´ | åŠ¨æ€å…±äº«ï¼Œå¯ç›¸äº’å€Ÿç”¨ |
| **é…ç½®å¤æ‚åº¦** | å¤šå‚æ•°ï¼Œè°ƒä¼˜å¤æ‚ | å°‘é‡å‚æ•°ï¼Œç®€åŒ–é…ç½® |
| **å†…å­˜åˆ©ç”¨ç‡** | è¾ƒä½ï¼Œå¸¸æœ‰æµªè´¹ | è¾ƒé«˜ï¼ŒæŒ‰éœ€åˆ†é… |
| **é€‚ç”¨åœºæ™¯** | è´Ÿè½½ç¨³å®šï¼Œå¯é¢„æµ‹ | å¤šæ ·åŒ–è´Ÿè½½ï¼Œèµ„æºç«äº‰ |
| **æº¢å‡ºå¤„ç†** | ç›´æ¥æº¢å‡ºåˆ°ç£ç›˜ | å…ˆå°è¯•å€Ÿç”¨ï¼Œå†æº¢å‡º |
| **ç‰ˆæœ¬æ”¯æŒ** | 1.6ä¹‹å‰é»˜è®¤ | 1.6åŠä¹‹åé»˜è®¤ |

**5. å†…å­˜å€Ÿç”¨æœºåˆ¶è¯¦è§£**

åœ¨ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å¼ä¸‹ï¼š

1. **Storage Memoryä¸è¶³æ—¶**ï¼š
   - å¦‚æœExecution Memoryæœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨
   - å¦‚æœæ²¡æœ‰ç©ºé—²ï¼Œåˆ™æº¢å‡ºåˆ°ç£ç›˜

2. **Execution Memoryä¸è¶³æ—¶**ï¼š
   - å¦‚æœStorage Memoryæœ‰ç©ºé—²ï¼Œå¯ä»¥å€Ÿç”¨
   - å¦‚æœStorage Memoryä¸­æœ‰éƒ¨åˆ†æ˜¯è¢«Execution Memoryå€Ÿç”¨çš„ï¼Œå¯ä»¥å¼ºåˆ¶æ”¶å›
   - Execution Memoryä¸ä¼šæ·˜æ±°Storage Memoryä¸­çš„æ•°æ®

**6. ä»£ç ç¤ºä¾‹ï¼šé…ç½®å¯¹æ¯”**

```scala
// é™æ€å†…å­˜ç®¡ç†é…ç½®ç¤ºä¾‹
spark.storage.memoryFraction = 0.6
spark.storage.unrollFraction = 0.2
spark.shuffle.memoryFraction = 0.2

// ç»Ÿä¸€å†…å­˜ç®¡ç†é…ç½®ç¤ºä¾‹
spark.memory.fraction = 0.75
spark.memory.storageFraction = 0.5
```

**7. å®é™…åº”ç”¨å»ºè®®**

- **å†…å­˜å¯†é›†å‹è®¡ç®—**ï¼šå¢åŠ `spark.memory.fraction`ï¼Œåˆ†é…æ›´å¤šå†…å­˜ç»™Spark
- **ç¼“å­˜ä¼˜å…ˆåœºæ™¯**ï¼šå¢åŠ `spark.memory.storageFraction`ï¼Œæé«˜ç¼“å­˜å®¹é‡
- **Shuffleå¯†é›†åœºæ™¯**ï¼šé™ä½`spark.memory.storageFraction`ï¼Œæä¾›æ›´å¤šæ‰§è¡Œå†…å­˜
- **ç›‘æ§å†…å­˜ä½¿ç”¨**ï¼šé€šè¿‡Spark UIç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µï¼ŒåŠæ—¶è°ƒæ•´å‚æ•°

Sparkå†…å­˜ç®¡ç†çš„æ¼”è¿›ä½“ç°äº†åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶å¯¹èµ„æºåˆ©ç”¨æ•ˆç‡çš„ä¸æ–­è¿½æ±‚ã€‚ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶æ˜¾è‘—æå‡äº†Sparkçš„å†…å­˜åˆ©ç”¨ç‡ï¼Œå‡å°‘äº†OOMé”™è¯¯ï¼Œç®€åŒ–äº†é…ç½®ï¼Œæ˜¯Sparkæ€§èƒ½ä¼˜åŒ–çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚

**Q12: è¯·è¯¦ç»†å¯¹æ¯”Spark Streamingå’ŒStructured Streamingçš„åŒºåˆ«ï¼Œå¹¶è¯´æ˜å„è‡ªçš„é€‚ç”¨åœºæ™¯ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

Sparkæä¾›äº†ä¸¤ç§æµå¤„ç†å¼•æ“ï¼šSpark Streamingå’ŒStructured Streamingï¼Œå®ƒä»¬åœ¨è®¾è®¡ç†å¿µã€APIã€å¤„ç†æ¨¡å‹ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ·±å…¥ç†è§£è¿™äº›å·®å¼‚å¯¹äºé€‰æ‹©åˆé€‚çš„æµå¤„ç†æŠ€æœ¯è‡³å…³é‡è¦ã€‚

**1. åŸºæœ¬æ¦‚å¿µå¯¹æ¯”**

- **Spark Streaming**ï¼š
  Spark 0.7ç‰ˆæœ¬å¼•å…¥çš„ç¬¬ä¸€ä»£æµå¤„ç†å¼•æ“ï¼ŒåŸºäºRDDæ¨¡å‹ï¼Œé‡‡ç”¨å¾®æ‰¹å¤„ç†æ¶æ„ï¼Œå°†æµæ•°æ®åˆ†å‰²æˆå°æ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

- **Structured Streaming**ï¼š
  Spark 2.0ç‰ˆæœ¬å¼•å…¥çš„ç¬¬äºŒä»£æµå¤„ç†å¼•æ“ï¼ŒåŸºäºSpark SQLå¼•æ“ï¼Œå°†æµæ•°æ®è§†ä¸ºæ— ç•Œè¡¨ï¼Œæä¾›æ›´é«˜çº§çš„APIå’Œä¼˜åŒ–ã€‚

**2. æ ¸å¿ƒç‰¹æ€§å¯¹æ¯”**

| ç‰¹æ€§ | Spark Streaming | Structured Streaming | å·®å¼‚å½±å“ |
|------|----------------|---------------------|---------|
| **å¤„ç†æ¨¡å‹** | å¾®æ‰¹å¤„ç†ï¼ˆDStreamï¼‰ | è¿ç»­å¤„ç†ï¼ˆæ— ç•Œè¡¨ï¼‰ | Structured Streamingå¯å®ç°æ›´ä½å»¶è¿Ÿ |
| **APIæŠ½è±¡** | DStream API | DataFrame/Dataset API | Structured Streamingä½¿ç”¨æ›´ç»Ÿä¸€çš„API |
| **ç¼–ç¨‹æ¨¡å‹** | å‡½æ•°å¼è½¬æ¢ | å£°æ˜å¼æŸ¥è¯¢ | Structured Streamingä»£ç æ›´ç®€æ´ |
| **å®¹é”™æœºåˆ¶** | WAL + Checkpoint | çŠ¶æ€å­˜å‚¨ + Checkpoint | ä¸¤è€…éƒ½æ”¯æŒå®¹é”™ï¼Œä½†å®ç°æ–¹å¼ä¸åŒ |
| **å»¶è¿Ÿ** | ç§’çº§ | æ¯«ç§’çº§ï¼ˆè¿ç»­å¤„ç†æ¨¡å¼ï¼‰ | Structured Streamingå¯å®ç°æ›´ä½å»¶è¿Ÿ |
| **çŠ¶æ€ç®¡ç†** | updateStateByKey/mapWithState | å†…ç½®çŠ¶æ€ç®¡ç† | Structured StreamingçŠ¶æ€ç®¡ç†æ›´å¼ºå¤§ |
| **äº‹ä»¶æ—¶é—´** | æœ‰é™æ”¯æŒ | åŸç”Ÿæ”¯æŒ | Structured Streamingæ›´é€‚åˆäº‹ä»¶æ—¶é—´å¤„ç† |
| **æ°´å°æœºåˆ¶** | ä¸æ”¯æŒ | æ”¯æŒ | Structured Streamingèƒ½æ›´å¥½å¤„ç†ä¹±åºæ•°æ® |
| **è¾“å‡ºæ¨¡å¼** | å›ºå®šæ¨¡å¼ | Complete/Append/Update | Structured Streamingæä¾›æ›´çµæ´»çš„è¾“å‡ºé€‰é¡¹ |
| **ä¼˜åŒ–å™¨** | æ—  | Catalystä¼˜åŒ–å™¨ | Structured StreamingæŸ¥è¯¢æ€§èƒ½æ›´é«˜ |
| **ç«¯åˆ°ç«¯ä¸€è‡´æ€§** | è‡³å°‘ä¸€æ¬¡ | ç²¾ç¡®ä¸€æ¬¡ | Structured Streamingæä¾›æ›´å¼ºçš„ä¸€è‡´æ€§ä¿è¯ |

**3. ä»£ç ç¤ºä¾‹å¯¹æ¯”**

**Spark Streamingç¤ºä¾‹**ï¼š
```scala
// åˆ›å»ºStreamingContext
val ssc = new StreamingContext(sparkContext, Seconds(1))

// ä»Kafkaè¯»å–æ•°æ®
val kafkaStream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// å¤„ç†æ•°æ®
val wordCounts = kafkaStream
  .map(record => record.value)
  .flatMap(_.split(" "))
  .map(word => (word, 1))
  .reduceByKey(_ + _)

// è¾“å‡ºç»“æœ
wordCounts.print()

// å¯åŠ¨æµå¤„ç†
ssc.start()
ssc.awaitTermination()
```

**Structured Streamingç¤ºä¾‹**ï¼š
```scala
// ä»Kafkaè¯»å–æ•°æ®
val kafkaStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host:port")
  .option("subscribe", "topic")
  .load()

// å¤„ç†æ•°æ®
val wordCounts = kafkaStream
  .selectExpr("CAST(value AS STRING)")
  .as[String]
  .flatMap(_.split(" "))
  .groupBy("value")
  .count()

// è¾“å‡ºç»“æœ
val query = wordCounts.writeStream
  .outputMode("complete")
  .format("console")
  .start()

query.awaitTermination()
```

**4. é«˜çº§ç‰¹æ€§å¯¹æ¯”**

**äº‹ä»¶æ—¶é—´å¤„ç†**ï¼š
- **Spark Streaming**ï¼šéœ€è¦æ‰‹åŠ¨å®ç°ï¼Œè¾ƒä¸ºå¤æ‚
- **Structured Streaming**ï¼šå†…ç½®æ”¯æŒ
  ```scala
  // ä½¿ç”¨äº‹ä»¶æ—¶é—´çª—å£
  val windowedCounts = kafkaStream
    .selectExpr("CAST(value AS STRING)", "CAST(timestamp AS TIMESTAMP)")
    .as[(String, Timestamp)]
    .flatMap(record => record._1.split(" ").map((_, record._2)))
    .groupBy(
      window($"_2", "10 minutes", "5 minutes"),
      $"_1"
    )
    .count()
  ```

**æ°´å°æœºåˆ¶**ï¼š
- **Spark Streaming**ï¼šä¸æ”¯æŒ
- **Structured Streaming**ï¼šå†…ç½®æ”¯æŒ
  ```scala
  // æ·»åŠ æ°´å°
  val windowedCounts = kafkaStream
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
      window($"timestamp", "10 minutes", "5 minutes"),
      $"value"
    )
    .count()
  ```

**5. æ€§èƒ½ä¸æ‰©å±•æ€§å¯¹æ¯”**

| æ–¹é¢ | Spark Streaming | Structured Streaming | 
|------|----------------|---------------------|
| **æŸ¥è¯¢ä¼˜åŒ–** | æ— å†…ç½®ä¼˜åŒ–å™¨ | ä½¿ç”¨Catalystä¼˜åŒ–å™¨ | 
| **å†…å­˜ä½¿ç”¨** | åŸºäºRDD | åŸºäºTungsten | 
| **ååé‡** | é«˜ | æ›´é«˜ï¼ˆå¾—ç›Šäºä¼˜åŒ–ï¼‰ | 
| **å»¶è¿Ÿ** | å–å†³äºæ‰¹æ¬¡é—´éš” | å¯é…ç½®ï¼ˆå¾®æ‰¹æˆ–è¿ç»­ï¼‰ | 
| **æ‰©å±•æ€§** | æ°´å¹³æ‰©å±• | æ°´å¹³æ‰©å±• | 

**6. é€‚ç”¨åœºæ™¯å»ºè®®**

**é€‰æ‹©Spark Streamingçš„åœºæ™¯**ï¼š
- ä¸ç°æœ‰RDDä»£ç é›†æˆ
- ç®€å•çš„æµå¤„ç†éœ€æ±‚
- å¯¹APIç¨³å®šæ€§è¦æ±‚é«˜ï¼ˆAPIè¾ƒä¸ºç¨³å®šï¼‰
- éœ€è¦è‡ªå®šä¹‰å¤æ‚çš„è½¬æ¢æ“ä½œ
- ä¸é—ç•™ç³»ç»Ÿé›†æˆ

**é€‰æ‹©Structured Streamingçš„åœºæ™¯**ï¼š
- æ–°é¡¹ç›®å¼€å‘
- éœ€è¦ä½å»¶è¿Ÿå¤„ç†
- éœ€è¦äº‹ä»¶æ—¶é—´å¤„ç†å’Œæ°´å°æ”¯æŒ
- éœ€è¦å¼ºä¸€è‡´æ€§ä¿è¯
- å¤æ‚çš„æµå¤„ç†éœ€æ±‚ï¼ˆå¦‚çª—å£èšåˆã€ä¼šè¯åˆ†æï¼‰
- ä¸Spark SQLç”Ÿæ€ç³»ç»Ÿé›†æˆ

**7. æœªæ¥å‘å±•è¶‹åŠ¿**

Structured Streamingæ˜¯Sparkæµå¤„ç†çš„æœªæ¥å‘å±•æ–¹å‘ï¼ŒSparkå›¢é˜Ÿå°†ä¸»è¦ç²¾åŠ›æŠ•å…¥åˆ°Structured Streamingçš„æ”¹è¿›å’Œä¼˜åŒ–ä¸­ã€‚å¯¹äºæ–°é¡¹ç›®ï¼Œå»ºè®®ä¼˜å…ˆè€ƒè™‘Structured Streamingï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚åªèƒ½ä½¿ç”¨Spark Streamingã€‚

æ€»ä½“è€Œè¨€ï¼ŒStructured Streamingç›¸æ¯”Spark Streamingæä¾›äº†æ›´é«˜çº§çš„APIã€æ›´å¼ºå¤§çš„åŠŸèƒ½å’Œæ›´å¥½çš„æ€§èƒ½ï¼Œæ˜¯Sparkæµå¤„ç†æŠ€æœ¯çš„é‡è¦è¿›æ­¥ã€‚

**Q13: è¯·è¯¦ç»†è§£é‡ŠSparkå¦‚ä½•å®ç°Exactly-Onceè¯­ä¹‰ï¼ŒåŒ…æ‹¬åŸç†å’Œå®ç°æ–¹å¼ã€‚**

**æ ‡å‡†ç­”æ¡ˆï¼š**

åœ¨åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿä¸­ï¼Œæ•°æ®å¤„ç†è¯­ä¹‰æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹èŠ‚ç‚¹æ•…éšœå’Œç½‘ç»œåˆ†åŒºç­‰æƒ…å†µæ—¶ã€‚Exactly-Onceè¯­ä¹‰æ˜¯æŒ‡æ¯æ¡æ•°æ®è¢«ç²¾ç¡®å¤„ç†ä¸€æ¬¡ï¼Œä¸å¤šä¸å°‘ï¼Œè¿™æ˜¯æœ€å¼ºçš„ä¸€è‡´æ€§ä¿è¯ã€‚Sparké€šè¿‡å¤šå±‚æœºåˆ¶å®ç°äº†Exactly-Onceè¯­ä¹‰ã€‚

**1. æ•°æ®å¤„ç†è¯­ä¹‰çº§åˆ«**

é¦–å…ˆç†è§£ä¸‰ç§ä¸»è¦çš„æ•°æ®å¤„ç†è¯­ä¹‰ï¼š

| è¯­ä¹‰çº§åˆ« | æè¿° | å®ç°éš¾åº¦ | æ€§èƒ½å½±å“ |
|---------|------|---------|---------|
| **At-most-once** | æ•°æ®æœ€å¤šå¤„ç†ä¸€æ¬¡ï¼Œå¯èƒ½ä¸¢å¤± | ä½ | å‡ ä¹æ— å½±å“ |
| **At-least-once** | æ•°æ®è‡³å°‘å¤„ç†ä¸€æ¬¡ï¼Œå¯èƒ½é‡å¤ | ä¸­ | è½»å¾®å½±å“ |
| **Exactly-once** | æ•°æ®ç²¾ç¡®å¤„ç†ä¸€æ¬¡ï¼Œä¸ä¸¢å¤±ä¸é‡å¤ | é«˜ | å¯èƒ½æ˜¾è‘—å½±å“ |

**2. Sparkå®ç°Exactly-Onceçš„æ ¸å¿ƒæœºåˆ¶**

Sparké€šè¿‡ä»¥ä¸‹å¤šå±‚æœºåˆ¶å…±åŒä¿è¯Exactly-Onceè¯­ä¹‰ï¼š

**2.1 æ•°æ®æºç«¯ä¿è¯**

- **å¯é‡æ”¾çš„æ•°æ®æº**ï¼šä½¿ç”¨æ”¯æŒåç§»é‡è·Ÿè¸ªçš„æ•°æ®æºï¼ˆå¦‚Kafkaï¼‰
- **åç§»é‡ç®¡ç†**ï¼šç²¾ç¡®è®°å½•å’Œç®¡ç†å·²å¤„ç†æ•°æ®çš„åç§»é‡
- **æ–­ç‚¹ç»­ä¼ **ï¼šä»ä¸Šæ¬¡å¤„ç†çš„ä½ç½®ç»§ç»­å¤„ç†

**2.2 å¤„ç†è¿‡ç¨‹ä¿è¯**

- **ç¡®å®šæ€§æ“ä½œ**ï¼šç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º
- **Checkpointæœºåˆ¶**ï¼šå®šæœŸä¿å­˜è®¡ç®—çŠ¶æ€å’Œè¿›åº¦
- **WALï¼ˆé¢„å†™æ—¥å¿—ï¼‰**ï¼šè®°å½•æ‰€æœ‰çŠ¶æ€å˜æ›´æ“ä½œ
- **å¹‚ç­‰æ€§è½¬æ¢**ï¼šç¡®ä¿é‡å¤æ‰§è¡Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨

**2.3 æ•°æ®è¾“å‡ºä¿è¯**

- **å¹‚ç­‰æ€§å†™å…¥**ï¼šç¡®ä¿é‡å¤å†™å…¥ä¸ä¼šå¯¼è‡´æ•°æ®é‡å¤
- **äº‹åŠ¡æ€§å†™å…¥**ï¼šä½¿ç”¨æ”¯æŒäº‹åŠ¡çš„å­˜å‚¨ç³»ç»Ÿ
- **ä¸¤é˜¶æ®µæäº¤**ï¼šç¡®ä¿æ•°æ®å¤„ç†å’Œç»“æœå†™å…¥çš„åŸå­æ€§
- **è¾“å‡ºæäº¤åè®®**ï¼šå°†è¾“å‡ºæ“ä½œä¸åç§»é‡æäº¤ç»‘å®š

**3. Structured Streamingä¸­çš„Exactly-Onceå®ç°**

Structured Streamingé€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°Exactly-Onceï¼š

```mermaid
graph TD
    A[æ•°æ®æº] --> B[åç§»é‡è·Ÿè¸ª]
    B --> C[å¾®æ‰¹å¤„ç†]
    C --> D[çŠ¶æ€ç®¡ç†]
    D --> E[è¾“å‡ºæ¨¡å¼]
    E --> F[äº‹åŠ¡æ€§Sink]
    
    B -.-> G[WAL/Checkpoint]
    D -.-> G
    F -.-> G
    
    style A fill:#d4f1f9,stroke:#05a4d1
    style B fill:#ffe6cc,stroke:#d79b00
    style C fill:#d5e8d4,stroke:#82b366
    style D fill:#e1d5e7,stroke:#9673a6
    style E fill:#f8cecc,stroke:#b85450
    style F fill:#f8cecc,stroke:#b85450
    style G fill:#fff2cc,stroke:#d6b656
```

**3.1 è¯¦ç»†å®ç°æœºåˆ¶**

1. **åç§»é‡è·Ÿè¸ª**ï¼š
   - è®°å½•æ¯ä¸ªæ•°æ®æºçš„è¯»å–ä½ç½®
   - å°†åç§»é‡ä¸å¤„ç†ç»“æœå…³è”
   - åœ¨Checkpointä¸­ä¿å­˜åç§»é‡ä¿¡æ¯

2. **çŠ¶æ€ç®¡ç†**ï¼š
   - ä½¿ç”¨çŠ¶æ€å­˜å‚¨ä¿å­˜ä¸­é—´çŠ¶æ€
   - å®šæœŸCheckpointçŠ¶æ€åˆ°å¯é å­˜å‚¨
   - æ•…éšœæ¢å¤æ—¶é‡å»ºçŠ¶æ€

3. **è¾“å‡ºæäº¤åè®®**ï¼š
   - å°†ç»“æœå†™å…¥ä¸åç§»é‡æäº¤ç»‘å®š
   - ç¡®ä¿åŸå­æ€§ï¼šè¦ä¹ˆéƒ½æˆåŠŸï¼Œè¦ä¹ˆéƒ½å¤±è´¥
   - ä½¿ç”¨äº‹åŠ¡æ€§Sinkæˆ–å¹‚ç­‰æ€§å†™å…¥

**4. ä»£ç ç¤ºä¾‹ï¼šå®ç°Exactly-Onceè¯­ä¹‰**

**4.1 ä½¿ç”¨äº‹åŠ¡æ€§è¾“å‡º**ï¼š

```scala
// ä½¿ç”¨foreachBatchå®ç°äº‹åŠ¡æ€§è¾“å‡º
val query = inputStream
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    // å¼€å¯äº‹åŠ¡
    val connection = getConnection()  // è·å–æ•°æ®åº“è¿æ¥
    connection.beginTransaction()
    try {
      // ä½¿ç”¨batchIdç¡®ä¿å¹‚ç­‰æ€§
      val outputPath = s"output/batch_$batchId"
      
      // åˆ é™¤å¯èƒ½å­˜åœ¨çš„æ—§æ•°æ®ï¼ˆå¹‚ç­‰æ€§ä¿è¯ï¼‰
      connection.executeUpdate(s"DELETE FROM results WHERE batch_id = $batchId")
      
      // å†™å…¥æ–°æ•°æ®
      batchDF.write
        .format("jdbc")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("url", jdbcUrl)
        .option("dbtable", "results")
        .option("user", user)
        .option("password", password)
        .mode("append")
        .save()
        
      // æäº¤äº‹åŠ¡
      connection.commit()
    } catch {
      case e: Exception =>
        // å›æ»šäº‹åŠ¡
        connection.rollback()
        throw e
    } finally {
      connection.close()
    }
  }
  .option("checkpointLocation", "/checkpoint")
  .start()
```

**4.2 ä½¿ç”¨å¹‚ç­‰æ€§å†™å…¥**ï¼š

```scala
// ä½¿ç”¨å¹‚ç­‰æ€§å†™å…¥
val query = inputStream
  .writeStream
  .foreachBatch { (batchDF, batchId) =>
    // ä½¿ç”¨æ‰¹æ¬¡IDä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œç¡®ä¿å¹‚ç­‰æ€§
    val outputPath = s"output/batch_$batchId"
    
    // è¦†ç›–å†™å…¥ï¼Œç¡®ä¿å¹‚ç­‰æ€§
    batchDF.write
      .mode("overwrite")  // è¦†ç›–æ¨¡å¼ç¡®ä¿å¹‚ç­‰æ€§
      .parquet(outputPath)
      
    // å¯é€‰ï¼šæ›´æ–°å…ƒæ•°æ®è¡¨è®°å½•å·²å¤„ç†çš„æ‰¹æ¬¡
    spark.sql(s"""
      MERGE INTO batch_metadata
      USING (SELECT $batchId as id) AS source
      ON batch_metadata.id = source.id
      WHEN MATCHED THEN UPDATE SET processed_time = current_timestamp()
      WHEN NOT MATCHED THEN INSERT (id, processed_time) VALUES ($batchId, current_timestamp())
    """)
  }
  .option("checkpointLocation", "/checkpoint")
  .start()
```

**5. ä¸åŒåœºæ™¯ä¸‹çš„Exactly-Onceå®ç°**

| åœºæ™¯ | å®ç°æ–¹å¼ | å…³é”®ç‚¹ |
|------|---------|-------|
| **Kafka â†’ HDFS** | WAL + å¹‚ç­‰æ€§å†™å…¥ | ä½¿ç”¨æ‰¹æ¬¡IDä½œä¸ºæ–‡ä»¶åæˆ–è·¯å¾„ |
| **Kafka â†’ æ•°æ®åº“** | ä¸¤é˜¶æ®µæäº¤ | å°†åç§»é‡ä¸æ•°æ®åº“äº‹åŠ¡ç»‘å®š |
| **Kafka â†’ Kafka** | äº‹åŠ¡æ€§API | ä½¿ç”¨Kafkaäº‹åŠ¡API |
| **æœ‰çŠ¶æ€æ“ä½œ** | Checkpoint + çŠ¶æ€å­˜å‚¨ | å®šæœŸCheckpointçŠ¶æ€ |

**6. å®ç°Exactly-Onceçš„æœ€ä½³å®è·µ**

1. **é€‰æ‹©åˆé€‚çš„æ•°æ®æº**ï¼šä½¿ç”¨æ”¯æŒé‡æ”¾å’Œåç§»é‡ç®¡ç†çš„æ•°æ®æº
2. **å¯ç”¨Checkpoint**ï¼šé…ç½®å¯é çš„Checkpointå­˜å‚¨
3. **ä½¿ç”¨å¹‚ç­‰æ€§æ“ä½œ**ï¼šç¡®ä¿é‡å¤æ‰§è¡Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨
4. **äº‹åŠ¡æ€§è¾“å‡º**ï¼šä½¿ç”¨æ”¯æŒäº‹åŠ¡çš„è¾“å‡ºç³»ç»Ÿ
5. **é”™è¯¯å¤„ç†**ï¼šå¦¥å–„å¤„ç†å¼‚å¸¸ï¼Œé¿å…éƒ¨åˆ†æäº¤
6. **ç›‘æ§ä¸éªŒè¯**ï¼šå»ºç«‹ç›‘æ§æœºåˆ¶éªŒè¯Exactly-Onceè¯­ä¹‰

**7. é™åˆ¶ä¸æŒ‘æˆ˜**

- **æ€§èƒ½å¼€é”€**ï¼šå®ç°Exactly-Onceé€šå¸¸ä¼šå¸¦æ¥ä¸€å®šæ€§èƒ½å¼€é”€
- **å¤–éƒ¨ç³»ç»Ÿé™åˆ¶**ï¼šä¾èµ–å¤–éƒ¨ç³»ç»Ÿå¯¹äº‹åŠ¡çš„æ”¯æŒ
- **å¤æ‚æ€§å¢åŠ **ï¼šå®ç°å’Œç»´æŠ¤æ›´ä¸ºå¤æ‚
- **è°ƒè¯•éš¾åº¦**ï¼šé—®é¢˜æ’æŸ¥æ›´å…·æŒ‘æˆ˜æ€§

Sparkçš„Exactly-Onceè¯­ä¹‰å®ç°æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡çš„ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆå¤šç§æœºåˆ¶å…±åŒä¿è¯æ•°æ®å¤„ç†çš„ä¸€è‡´æ€§ã€‚ç†è§£è¿™äº›æœºåˆ¶æœ‰åŠ©äºæ„å»ºå¯é çš„æµå¤„ç†åº”ç”¨ï¼Œå¹¶åœ¨é¢å¯¹æ•…éšœæ—¶ä¿æŒæ•°æ®ä¸€è‡´æ€§ã€‚

### æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜

#### å¦‚ä½•è¯Šæ–­å’Œè§£å†³Sparkåº”ç”¨çš„æ€§èƒ½é—®é¢˜ï¼Ÿ



**è¯Šæ–­æ­¥éª¤**ï¼š
1. **æ”¶é›†æŒ‡æ ‡**ï¼šCPUã€å†…å­˜ã€ç½‘ç»œã€ç£ç›˜ä½¿ç”¨æƒ…å†µ
2. **åˆ†ææ—¥å¿—**ï¼šDriverå’ŒExecutoræ—¥å¿—
3. **æŸ¥çœ‹Spark UI**ï¼šStageæ‰§è¡Œæ—¶é—´ã€Taskåˆ†å¸ƒ
4. **ç›‘æ§GC**ï¼šåƒåœ¾å›æ”¶é¢‘ç‡å’Œæ—¶é—´

**å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**ï¼š

**é—®é¢˜1ï¼šæ•°æ®å€¾æ–œ**
```scala
// æ£€æµ‹æ–¹æ³•
val partitionSizes = rdd.mapPartitionsWithIndex { (index, iter) =>
  Iterator((index, iter.size))
}.collect()

// è§£å†³æ–¹æ¡ˆï¼šåŠ ç›å¤„ç†
val saltedRDD = rdd.map(x => (x._1 + "_" + Random.nextInt(10), x._2))
```

**é—®é¢˜2ï¼šå†…å­˜æº¢å‡º**
```bash
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å†…å­˜é…ç½®
--conf spark.executor.memory=8g
--conf spark.executor.memoryOverhead=2g
--conf spark.memory.offHeap.enabled=true
--conf spark.memory.offHeap.size=4g
```

**é—®é¢˜3ï¼šä»»åŠ¡æ‰§è¡Œç¼“æ…¢**
```scala
// æ£€æŸ¥å¹¶è¡Œåº¦
println(s"åˆ†åŒºæ•°: ${rdd.getNumPartitions}")

// è°ƒæ•´åˆ†åŒºæ•°
val repartitionedRDD = rdd.repartition(optimalPartitions)
```

#### Sparkåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¦‚ä½•è¿›è¡Œç›‘æ§ï¼Ÿ



**ç›‘æ§ç»´åº¦**ï¼š
1. **åº”ç”¨çº§ç›‘æ§**ï¼šä½œä¸šæ‰§è¡ŒçŠ¶æ€ã€æ‰§è¡Œæ—¶é—´
2. **èµ„æºç›‘æ§**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œä½¿ç”¨
3. **æ€§èƒ½ç›‘æ§**ï¼šTaskæ‰§è¡Œæ—¶é—´ã€Shuffleæ•°æ®é‡
4. **é”™è¯¯ç›‘æ§**ï¼šå¤±è´¥ä»»åŠ¡ã€å¼‚å¸¸ç»Ÿè®¡

**ç›‘æ§å·¥å…·**ï¼š
```scala
// 1. è‡ªå®šä¹‰ç›‘å¬å™¨
class CustomSparkListener extends SparkListener {
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
    val stage = stageCompleted.stageInfo
    println(s"Stage ${stage.stageId} completed in ${stage.completionTime.get - stage.submissionTime.get} ms")
  }
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    if (metrics.executorRunTime > 60000) {  // è¶…è¿‡1åˆ†é’Ÿçš„ä»»åŠ¡
      println(s"Long running task: ${taskEnd.taskInfo.taskId}")
    }
  }
}

// æ³¨å†Œç›‘å¬å™¨
spark.sparkContext.addSparkListener(new CustomSparkListener)
```

**ç›‘æ§æŒ‡æ ‡**ï¼š
```bash
# å…³é”®ç›‘æ§æŒ‡æ ‡
- åº”ç”¨æ‰§è¡Œæ—¶é—´
- Taskå¤±è´¥ç‡
- å†…å­˜ä½¿ç”¨ç‡
- GCæ—¶é—´å æ¯”
- Shuffleæ•°æ®é‡
- ç½‘ç»œI/O
- ç£ç›˜I/O
```

#### Sparkä¸Hadoopç”Ÿæ€ç³»ç»Ÿçš„é›†æˆæœ€ä½³å®è·µï¼Ÿ



**é›†æˆè¦ç‚¹**ï¼š
1. **å­˜å‚¨é›†æˆ**ï¼šHDFSã€HBaseã€Hive
2. **èµ„æºç®¡ç†**ï¼šYARNé›†æˆ
3. **å®‰å…¨é›†æˆ**ï¼šKerberosè®¤è¯
4. **ç›‘æ§é›†æˆ**ï¼šHadoopç›‘æ§ä½“ç³»

**æœ€ä½³å®è·µ**ï¼š
```scala
// 1. Hiveé›†æˆ
spark.sql("CREATE TABLE IF NOT EXISTS spark_hive_table (key INT, value STRING) USING HIVE")

// 2. HBaseé›†æˆ
val hbaseConf = HBaseConfiguration.create()
hbaseConf.set("hbase.zookeeper.quorum", "zk1,zk2,zk3")
val hbaseRDD = spark.sparkContext.newAPIHadoopRDD(
  hbaseConf,
  classOf[TableInputFormat],
  classOf[ImmutableBytesWritable],
  classOf[Result]
)

// 3. YARNé…ç½®
val conf = new SparkConf()
conf.set("spark.master", "yarn")
conf.set("spark.submit.deployMode", "cluster")
conf.set("spark.yarn.queue", "production")
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š
```properties
# HDFSä¼˜åŒ–
spark.hadoop.dfs.blocksize=268435456
spark.hadoop.dfs.replication=3

# YARNä¼˜åŒ–
spark.yarn.executor.memoryOverhead=512m
spark.yarn.driver.memoryOverhead=512m
spark.yarn.maxAppAttempts=2
```

### é«˜çº§åº”ç”¨é¢˜

#### å¦‚ä½•åœ¨Sparkä¸­å®ç°æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Ÿ



**MLlib Pipelineæ„å»º**ï¼š
```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature._
import org.apache.spark.ml.classification.LogisticRegression

// 1. æ•°æ®é¢„å¤„ç†
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")

val hashingTF = new HashingTF()
  .setInputCol("words")
  .setOutputCol("features")
  .setNumFeatures(10000)

// 2. ç‰¹å¾ç¼©æ”¾
val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")

// 3. æ¨¡å‹è®­ç»ƒ
val lr = new LogisticRegression()
  .setFeaturesCol("scaledFeatures")
  .setLabelCol("label")

// 4. æ„å»ºPipeline
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, scaler, lr))

// 5. è®­ç»ƒæ¨¡å‹
val model = pipeline.fit(trainingData)

// 6. æ¨¡å‹é¢„æµ‹
val predictions = model.transform(testData)
```

**æ¨¡å‹è¯„ä¼°**ï¼š
```scala
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val evaluator = new BinaryClassificationEvaluator()
  .setLabelCol("label")
  .setRawPredictionCol("rawPrediction")
  .setMetricName("areaUnderROC")

val auc = evaluator.evaluate(predictions)
println(s"AUC: $auc")
```

#### Sparkåœ¨å®æ—¶æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Ÿ



**æ¶æ„è®¾è®¡**ï¼š
```mermaid
graph LR
    A[ç”¨æˆ·è¡Œä¸ºæ•°æ®] --> B[Kafka]
    B --> C[Structured Streaming]
    C --> D[ç‰¹å¾æå–]
    D --> E[æ¨¡å‹é¢„æµ‹]
    E --> F[æ¨èç»“æœ]
    F --> G[Redisç¼“å­˜]
    G --> H[APIæœåŠ¡]
```

**å®ç°ä»£ç **ï¼š
```scala
// 1. å®æ—¶ç‰¹å¾æå–
val userFeatures = kafkaStream
  .select(from_json($"value", schema).as("data"))
  .select("data.*")
  .withWatermark("timestamp", "10 minutes")
  .groupBy($"userId", window($"timestamp", "10 minutes"))
  .agg(
    count("*").as("eventCount"),
    countDistinct("itemId").as("uniqueItems"),
    collect_list("category").as("categories")
  )

// 2. å®æ—¶æ¨è
val recommendations = userFeatures.map { row =>
  val userId = row.getAs[String]("userId")
  val features = extractFeatures(row)
  val items = recommendationModel.recommend(userId, features, 10)
  RecommendationResult(userId, items, System.currentTimeMillis())
}

// 3. ç»“æœè¾“å‡º
recommendations.writeStream
  .foreach(new ForeachWriter[RecommendationResult] {
    override def open(partitionId: Long, epochId: Long): Boolean = true
    
    override def process(rec: RecommendationResult): Unit = {
      // å†™å…¥Redis
      redisClient.setex(s"rec:${rec.userId}", 3600, rec.toJson)
    }
    
    override def close(errorOrNull: Throwable): Unit = {}
  })
  .start()
```