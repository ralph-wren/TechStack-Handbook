# Hive 参考指南

## 目录
- [Hive 参考指南](#hive-参考指南)
  - [目录](#目录)
  - [1. SQL 函数与语法](#1-sql-函数与语法)
    - [1.1 日期与时间处理](#11-日期与时间处理)
    - [1.2 字符串处理](#12-字符串处理)
    - [1.3 聚合函数](#13-聚合函数)
    - [1.4 数组与集合操作](#14-数组与集合操作)
    - [1.5 JSON 处理](#15-json-处理)
    - [1.6 条件与判断](#16-条件与判断)
    - [1.7 分组与聚合](#17-分组与聚合)
    - [1.8 窗口函数](#18-窗口函数)
    - [1.9 WITH 语句](#19-with-语句)
    - [1.10 JOIN 操作](#110-join-操作)
  - [2. 表操作](#2-表操作)
    - [2.1 建表与压缩格式](#21-建表与压缩格式)
    - [2.2 分桶与分区](#22-分桶与分区)
    - [2.3 优化配置](#23-优化配置)
    - [2.4 实用查询示例](#24-实用查询示例)
  - [3. UDF与自定义函数](#3-udf与自定义函数)
    - [3.1 地理位置函数](#31-地理位置函数)
    - [3.2 数据验证函数](#32-数据验证函数)
    - [3.3 字符处理函数](#33-字符处理函数)
    - [3.4 布隆过滤器](#34-布隆过滤器)
  - [4. 参考链接与外部资源](#4-参考链接与外部资源)

## 1. SQL 函数与语法

### 1.1 日期与时间处理

```sql
-- 获取10位时间戳
unix_timestamp()  

-- 转换数据类型
cast(create_time as bigint)  

-- 10位时间戳转为指定格式
from_unixtime(create_time,'yyyyMMdd') 

-- 日期加减操作
date_add(date_column, number_of_days) -- 加几天
date_sub(date_column, number_of_days) -- 减几天

-- 复杂日期操作示例
unix_timestamp(date_sub(from_unixtime(unix_timestamp(),30),'yyyy-MM-dd')*1000

-- 计算两个日期差距(天数)
abs(datediff(to_date(logdate, 'yyyyMMdd'), to_date(day, 'yyyyMMdd'))) > 180
```

### 1.2 字符串处理

```sql
-- 判断字符串是否包含
instr(extend_detail, 'name') > 0  

-- col1 在 col2 里面，不在返回0
instr(col2, col1) > 0  

-- 正则表达式提取
regexp_extract(string subject, string pattern, int index)

-- 正则表达式替换
regexp_replace(extend_detail, '#[a-z0-9]{12}#', '##')

-- 子字符串提取 (start从1开始)
substr(string str, int start, int length)
```

### 1.3 聚合函数

```sql
-- 不同COUNT用法比较
```

| 写法 | 作用说明 | NULL行是否计入 |
|------|---------|--------------|
| COUNT(a) | 统计 a 字段非 NULL 的个数 | ❌ 不计入 |
| COUNT(*) | 统计总行数（包括 NULL） | ✅ 计入 |
| COUNT(DISTINCT a) | 去重后的非 NULL 个数 | ❌ 不计入 |

```sql
-- DataFrame 多字段统计示例
df.agg(count(lit(1)), count(when(length(col("geohash")) > 0, 1))).show(false)

-- 聚合多字段
raw.agg(count(when(length(col("ottlist")) > 0, 1)), count(when(length(col("essid")) > 0, 1))).show(false)
```

### 1.4 数组与集合操作

```sql
-- 判断两个数组有交集
arrays_overlap(split(month_app_name, ' '), split(day_app_name, ' '))

-- 数组展开（行转列）
lateral view explode(split(gid_detail, ',')) tmp as single_gid
```

### 1.5 JSON 处理

```sql
-- JSON解析
get_json_object(json_string, '$.field_name')

-- JSON数组处理
json_array_length(json_array_string)

-- JSON对象转Map
str_to_map(json_string, ',', ':')
```

### 1.6 条件与判断

```sql
-- CASE WHEN语句
CASE 
    WHEN condition1 THEN result1
    WHEN condition2 THEN result2
    ELSE default_result
END

-- IF函数
IF(condition, true_value, false_value)

-- COALESCE函数（返回第一个非NULL值）
COALESCE(value1, value2, value3)
```

### 1.7 分组与聚合

```sql
-- GROUP BY分组
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department

-- HAVING过滤
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department
HAVING AVG(salary) > 5000
```

### 1.8 窗口函数

```sql
-- ROW_NUMBER() 行号
SELECT 
    name,
    salary,
    ROW_NUMBER() OVER (ORDER BY salary DESC) as rank
FROM employees

-- RANK() 排名（相同值相同排名，跳过）
SELECT 
    name,
    salary,
    RANK() OVER (ORDER BY salary DESC) as rank
FROM employees

-- DENSE_RANK() 密集排名（相同值相同排名，不跳过）
SELECT 
    name,
    salary,
    DENSE_RANK() OVER (ORDER BY salary DESC) as rank
FROM employees

-- LAG/LEAD 前后值
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) as prev_sales,
    LEAD(sales, 1) OVER (ORDER BY date) as next_sales
FROM daily_sales
```

### 1.9 WITH 语句

```sql
-- CTE (Common Table Expression)
WITH sales_summary AS (
    SELECT 
        product_id,
        SUM(quantity) as total_quantity,
        SUM(amount) as total_amount
    FROM sales
    GROUP BY product_id
)
SELECT 
    p.name,
    s.total_quantity,
    s.total_amount
FROM products p
JOIN sales_summary s ON p.id = s.product_id
```

### 1.10 JOIN 操作

```sql
-- 内连接
SELECT a.*, b.*
FROM table_a a
INNER JOIN table_b b ON a.id = b.id

-- 左连接
SELECT a.*, b.*
FROM table_a a
LEFT JOIN table_b b ON a.id = b.id

-- 右连接
SELECT a.*, b.*
FROM table_a a
RIGHT JOIN table_b b ON a.id = b.id

-- 全连接
SELECT a.*, b.*
FROM table_a a
FULL OUTER JOIN table_b b ON a.id = b.id

-- 交叉连接
SELECT a.*, b.*
FROM table_a a
CROSS JOIN table_b b
```

## 2. 表操作

### 2.1 建表与压缩格式

```sql
-- 建表示例（ORC格式，无压缩）
create table log_orc_none(
  track_time string,
  url string,
  session_id string,
  referer string,
  ip string,
  end_user_id string,
  city_id string
)
row format delimited fields terminated by '\t'
stored as orc tblproperties ("orc.compress"="NONE");

-- 建表示例（Parquet格式，Snappy压缩）
create table log_parquet_snappy(
  track_time string,
  url string,
  session_id string,
  referer string,
  ip string,
  end_user_id string,
  city_id string
)
stored as parquet
tblproperties ("parquet.compression"="SNAPPY");

-- 外部表
create external table external_log_table(
  track_time string,
  url string
)
row format delimited fields terminated by '\t'
location '/user/hive/warehouse/external_logs';
```

### 2.2 分桶与分区

```sql
-- 分桶表创建
CREATE TABLE bucketed_table (
    id INT,
    name STRING
) CLUSTERED BY (id) INTO 10 BUCKETS;

-- 启用分桶
SET hive.enforce.bucketing=true;

-- 动态分区设置
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

-- 分区表创建
CREATE TABLE partitioned_table (
    id INT,
    name STRING
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS ORC;

-- 添加分区
ALTER TABLE partitioned_table ADD PARTITION (year=2023, month=12, day=25);

-- 删除分区
ALTER TABLE partitioned_table DROP PARTITION (year=2023, month=12, day=25);
```

### 2.3 优化配置

```sql
-- MapJoin配置（小表加载到内存）
SET hive.auto.convert.join=true;
SELECT /*+ MAPJOIN(small_table) */ *
FROM large_table
JOIN small_table ON large_table.id = small_table.id;

-- 压缩中间结果
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress=true;
SET mapreduce.map.output.compress=true;
SET mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;

-- 基于成本的优化器(CBO)
SET hive.cbo.enable=true;

-- MapReduce内存配置
SET mapreduce.map.memory.mb=4096;
SET mapreduce.reduce.memory.mb=8192;

-- 并行执行
SET hive.exec.parallel=true;
SET hive.exec.parallel.thread.number=8;

-- 本地模式
SET hive.exec.mode.local.auto=true;
SET hive.exec.mode.local.auto.inputbytes.max=134217728;
```

### 2.4 实用查询示例

```sql
-- 查看表分区信息
SHOW PARTITIONS table_name;

-- 查看表结构
DESCRIBE table_name;
DESCRIBE EXTENDED table_name;

-- 查看建表语句
SHOW CREATE TABLE table_name;

-- 统计表大小
ANALYZE TABLE table_name COMPUTE STATISTICS;

-- 统计列信息
ANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS column1, column2;
```

## 3. UDF与自定义函数

### 3.1 地理位置函数

```java
// 经纬度转geohash UDF
public class GeohashUDF extends UDF {
    public String evaluate(Double lat, Double lng) {
        if (lat == null || lng == null) {
            return null;
        }
        return GeohashUtil.encode(lat, lng);
    }
}

-- 注册UDF
ADD JAR hdfs://gt-ga-xs/tmp/wuxl/udf/ys-bi-udf-hive-function-0.0.0.jar;
CREATE TEMPORARY FUNCTION encode AS 'com.ys.axe.saas.comn.util.GeohashUtil';

-- 使用UDF
SELECT encode(latitude, longitude) as geohash FROM location_data;

-- 是否在国内判断
geohash_in_china('wku7wp7147f6')
```

### 3.2 数据验证函数

```java
// 合法wifi判断UDF
public class LegalMacUDF extends UDF {
    public Boolean evaluate(String mac) {
        if (mac == null || mac.isEmpty()) {
            return false;
        }
        return LegalMacUtil.isLegalMac(mac);
    }
}

-- 注册UDF
ADD JAR hdfs://path/to/udf.jar;
CREATE TEMPORARY FUNCTION isLegalMac AS 'com.example.LegalMacUDF';

-- 使用UDF
SELECT * FROM wifi_data WHERE isLegalMac(mac_address);
```

### 3.3 字符处理函数

```java
// 不可见字符检测UDF
public class InvisibleCharUDF extends UDF {
    public String evaluate(String input) {
        if (input == null || input.isEmpty()) {
            return "[空]";
        }
        
        StringBuilder result = new StringBuilder();
        for (int i = 0; i < input.length(); i++) {
            char ch = input.charAt(i);
            int code = (int) ch;
            
            if (isInvisible(code)) {
                result.append(String.format("[pos=%d, U+%04X, ASCII=%d], ", i, code, code));
            }
        }
        
        return result.length() > 0 ? result.substring(0, result.length() - 2) : "";
    }
    
    private boolean isInvisible(int code) {
        return (code >= 0x00 && code <= 0x1F) || // 控制字符
               code == 0x7F ||                   // DEL
               code == 0xA0 ||                   // 不换行空格
               code == 0x200B ||                 // 零宽空格
               code == 0xFEFF;                   // BOM
    }
}
```

### 3.4 布隆过滤器

```java
// 布隆过滤器UDF
public class BloomFilterUDF extends UDF {
    private BloomFilter<String> bloomFilter;
    
    public void initialize() {
        // 初始化布隆过滤器
        bloomFilter = BloomFilter.create(
            Funnels.stringFunnel(Charset.defaultCharset()),
            1000000, // 期望元素数量
            0.01     // 误判率
        );
    }
    
    public Boolean evaluate(String value) {
        if (value == null) {
            return false;
        }
        return bloomFilter.mightContain(value);
    }
}
```

## 4. 参考链接与外部资源

- [Hive官方文档](https://hive.apache.org/)
- [Hive SQL语言手册](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
- [UDF 函数文档](https://cf.cloudglab.cn/pages/viewpage.action?pageId=245273514)
- [Hive性能调优指南](https://cwiki.apache.org/confluence/display/Hive/Performance+Optimization) 