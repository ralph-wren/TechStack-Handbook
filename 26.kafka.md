# Apache Kafka å®Œæ•´æŠ€æœ¯æŒ‡å—

## ç›®å½•
- [Apache Kafka å®Œæ•´æŠ€æœ¯æŒ‡å—](#apache-kafka-å®Œæ•´æŠ€æœ¯æŒ‡å—)
  - [ç›®å½•](#ç›®å½•)
  - [1. Kafka æ¦‚è¿°ä¸æ ¸å¿ƒæ¦‚å¿µ](#1-kafka-æ¦‚è¿°ä¸æ ¸å¿ƒæ¦‚å¿µ)
    - [1.1 ä»€ä¹ˆæ˜¯ Kafka](#11-ä»€ä¹ˆæ˜¯-kafka)
    - [1.2 æ ¸å¿ƒæ¦‚å¿µ](#12-æ ¸å¿ƒæ¦‚å¿µ)
      - [æ ¸å¿ƒæ¦‚å¿µè¯¦è§£](#æ ¸å¿ƒæ¦‚å¿µè¯¦è§£)
    - [1.3 Kafka æ¶æ„](#13-kafka-æ¶æ„)
      - [1.3.1 æ•´ä½“æ¶æ„å›¾](#131-æ•´ä½“æ¶æ„å›¾)
      - [1.3.2 å•ä¸ªBrokerå†…éƒ¨ç»“æ„](#132-å•ä¸ªbrokerå†…éƒ¨ç»“æ„)
    - [1.4 æ¶ˆæ¯æ¨¡å‹](#14-æ¶ˆæ¯æ¨¡å‹)
      - [1.4.1 æ¶ˆæ¯ç»“æ„](#141-æ¶ˆæ¯ç»“æ„)
      - [1.4.2 åˆ†åŒºç­–ç•¥](#142-åˆ†åŒºç­–ç•¥)
      - [1.4.3 æ¶ˆæ¯ä¼ é€’è¯­ä¹‰](#143-æ¶ˆæ¯ä¼ é€’è¯­ä¹‰)
  - [2. Kafka æ¶æ„åŸç†æ·±åº¦è§£æ](#2-kafka-æ¶æ„åŸç†æ·±åº¦è§£æ)
    - [2.1 åˆ†å¸ƒå¼æ¶æ„è®¾è®¡](#21-åˆ†å¸ƒå¼æ¶æ„è®¾è®¡)
      - [2.1.1 é›†ç¾¤å‘ç°ä¸ç®¡ç†](#211-é›†ç¾¤å‘ç°ä¸ç®¡ç†)
      - [2.1.2 Controlleré€‰ä¸¾æœºåˆ¶](#212-controlleré€‰ä¸¾æœºåˆ¶)
    - [2.2 å­˜å‚¨æœºåˆ¶](#22-å­˜å‚¨æœºåˆ¶)
      - [2.2.1 æ—¥å¿—å­˜å‚¨ç»“æ„](#221-æ—¥å¿—å­˜å‚¨ç»“æ„)
      - [2.2.2 æ¶ˆæ¯å­˜å‚¨æ ¼å¼](#222-æ¶ˆæ¯å­˜å‚¨æ ¼å¼)
    - [2.3 å¤åˆ¶æœºåˆ¶](#23-å¤åˆ¶æœºåˆ¶)
      - [2.3.1 å‰¯æœ¬åŒæ­¥æœºåˆ¶](#231-å‰¯æœ¬åŒæ­¥æœºåˆ¶)
      - [2.3.2 ä¸€è‡´æ€§ä¿è¯æœºåˆ¶](#232-ä¸€è‡´æ€§ä¿è¯æœºåˆ¶)
    - [2.4 åè°ƒæœºåˆ¶](#24-åè°ƒæœºåˆ¶)
      - [2.4.1 æ¶ˆè´¹è€…ç»„åè°ƒ](#241-æ¶ˆè´¹è€…ç»„åè°ƒ)
      - [2.4.2 åˆ†åŒºåˆ†é…ç­–ç•¥](#242-åˆ†åŒºåˆ†é…ç­–ç•¥)
  - [3. ç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…è¯¦è§£](#3-ç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…è¯¦è§£)
    - [3.1 ç”Ÿäº§è€…åŸç†](#31-ç”Ÿäº§è€…åŸç†)
      - [3.1.1 ç”Ÿäº§è€…æ¶æ„](#311-ç”Ÿäº§è€…æ¶æ„)
      - [3.1.2 æ¶ˆæ¯å‘é€æµç¨‹](#312-æ¶ˆæ¯å‘é€æµç¨‹)
      - [3.1.3 å…³é”®é…ç½®å‚æ•°](#313-å…³é”®é…ç½®å‚æ•°)
    - [3.2 æ¶ˆè´¹è€…åŸç†](#32-æ¶ˆè´¹è€…åŸç†)
      - [3.2.1 æ¶ˆè´¹è€…æ¶æ„](#321-æ¶ˆè´¹è€…æ¶æ„)
      - [3.2.2 æ¶ˆè´¹æµç¨‹è¯¦è§£](#322-æ¶ˆè´¹æµç¨‹è¯¦è§£)
      - [3.2.3 ä½ç§»ç®¡ç†](#323-ä½ç§»ç®¡ç†)
    - [3.3 æ¶ˆè´¹è€…ç»„](#33-æ¶ˆè´¹è€…ç»„)
      - [3.3.1 æ¶ˆè´¹è€…ç»„çŠ¶æ€ç®¡ç†](#331-æ¶ˆè´¹è€…ç»„çŠ¶æ€ç®¡ç†)
      - [3.3.2 é‡å¹³è¡¡ä¼˜åŒ–](#332-é‡å¹³è¡¡ä¼˜åŒ–)
    - [3.4 åç§»é‡ç®¡ç†](#34-åç§»é‡ç®¡ç†)
      - [3.4.1 åç§»é‡å­˜å‚¨](#341-åç§»é‡å­˜å‚¨)
      - [3.4.2 åç§»é‡é‡ç½®ç­–ç•¥](#342-åç§»é‡é‡ç½®ç­–ç•¥)
  - [7. Kafka ç”Ÿæ€ä¸é›†æˆ](#7-kafka-ç”Ÿæ€ä¸é›†æˆ)
    - [7.1 Kafka Connect](#71-kafka-connect)
      - [7.1.1 Connectæ¶æ„](#711-connectæ¶æ„)
      - [7.1.2 å¸¸ç”¨è¿æ¥å™¨é…ç½®](#712-å¸¸ç”¨è¿æ¥å™¨é…ç½®)
    - [7.2 Kafka Streams](#72-kafka-streams)
      - [7.2.1 Streamsåº”ç”¨ç¤ºä¾‹](#721-streamsåº”ç”¨ç¤ºä¾‹)
    - [7.3 Schema Registry](#73-schema-registry)
      - [7.3.1 Avro Schemaç¤ºä¾‹](#731-avro-schemaç¤ºä¾‹)
  - [8. é«˜çº§ç‰¹æ€§ä¸ä¼ä¸šåº”ç”¨](#8-é«˜çº§ç‰¹æ€§ä¸ä¼ä¸šåº”ç”¨)
    - [8.1 äº‹åŠ¡æ”¯æŒ](#81-äº‹åŠ¡æ”¯æŒ)
    - [8.2 ç›‘æ§æœ€ä½³å®è·µ](#82-ç›‘æ§æœ€ä½³å®è·µ)
     - [9. Kafka å®æˆ˜æ¡ˆä¾‹](#9-kafka-å®æˆ˜æ¡ˆä¾‹)
     - [9.1 å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ](#91-å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ)
   - [10. Kafka é¢è¯•é¢˜è¯¦è§£](#10-kafka-é¢è¯•é¢˜è¯¦è§£)
     - [10.1 åŸºç¡€æ¦‚å¿µç±»](#101-åŸºç¡€æ¦‚å¿µç±»)
     - [10.2 æ¶æ„åŸç†ç±»](#102-æ¶æ„åŸç†ç±»)
     - [10.3 æ€§èƒ½è°ƒä¼˜ç±»](#103-æ€§èƒ½è°ƒä¼˜ç±»)
     - [10.4 å®æˆ˜åº”ç”¨ç±»](#104-å®æˆ˜åº”ç”¨ç±»)
     - [10.5 æ•…éšœæ’æŸ¥ç±»](#105-æ•…éšœæ’æŸ¥ç±»)
   - [ğŸ“‹ Kafkaæ–‡æ¡£åˆ›å»ºå®Œæˆæ€»ç»“](#-kafkaæ–‡æ¡£åˆ›å»ºå®Œæˆæ€»ç»“)
    - [âœ… æ–‡æ¡£ç‰¹ç‚¹ï¼š](#-æ–‡æ¡£ç‰¹ç‚¹)
    - [ğŸ“Š æ–‡æ¡£å†…å®¹è¦†ç›–ï¼š](#-æ–‡æ¡£å†…å®¹è¦†ç›–)
    - [ğŸ¯ ç¬¦åˆè§„åˆ™è¦æ±‚ï¼š](#-ç¬¦åˆè§„åˆ™è¦æ±‚)

## 1. Kafka æ¦‚è¿°ä¸æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Kafka

**Apache Kafka** æ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼äº‹ä»¶æµå¹³å°ï¼Œç”±LinkedInå¼€å‘å¹¶äº2011å¹´å¼€æºã€‚å®ƒè¢«è®¾è®¡ä¸º**é«˜ååé‡ã€ä½å»¶è¿Ÿã€æŒä¹…åŒ–**çš„åˆ†å¸ƒå¼å‘å¸ƒ-è®¢é˜…æ¶ˆæ¯ç³»ç»Ÿã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š
- **é«˜ååé‡**ï¼šå•èŠ‚ç‚¹æ”¯æŒç™¾ä¸‡çº§æ¶ˆæ¯å¤„ç†
- **ä½å»¶è¿Ÿ**ï¼šæ¯«ç§’çº§æ¶ˆæ¯ä¼ é€’å»¶è¿Ÿ
- **æŒä¹…åŒ–**ï¼šæ¶ˆæ¯æŒä¹…åŒ–å­˜å‚¨åˆ°ç£ç›˜
- **åˆ†å¸ƒå¼**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•å’Œæ•…éšœå®¹é”™
- **å®æ—¶æ€§**ï¼šæ”¯æŒå®æ—¶æµæ•°æ®å¤„ç†

**ä¸»è¦åº”ç”¨åœºæ™¯**ï¼š
- **æ¶ˆæ¯ç³»ç»Ÿ**ï¼šåº”ç”¨é—´å¼‚æ­¥é€šä¿¡
- **ç½‘ç«™è¡Œä¸ºè·Ÿè¸ª**ï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®æ”¶é›†
- **è¿è¥æŒ‡æ ‡ç›‘æ§**ï¼šç³»ç»ŸæŒ‡æ ‡å®æ—¶æ”¶é›†
- **æ—¥å¿—èšåˆ**ï¼šåˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†å’Œå¤„ç†
- **æµå¤„ç†**ï¼šå®æ—¶æ•°æ®æµå¤„ç†
- **äº‹ä»¶æº**ï¼šäº‹ä»¶é©±åŠ¨æ¶æ„çš„åŸºç¡€è®¾æ–½

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

```mermaid
graph TB
    subgraph "Kafka é›†ç¾¤"
        subgraph "Broker 1"
            T1P0[Topic1-Partition0]
            T1P1[Topic1-Partition1]
            T2P0[Topic2-Partition0]
        end
        
        subgraph "Broker 2"
            T1P2[Topic1-Partition2]
            T2P1[Topic2-Partition1]
            T2P2[Topic2-Partition2]
        end
        
        subgraph "Broker 3"
            T1P0R[Topic1-P0-Replica]
            T1P1R[Topic1-P1-Replica]
            T2P0R[Topic2-P0-Replica]
        end
    end
    
    subgraph "ç”Ÿäº§è€…"
        P1[Producer 1]
        P2[Producer 2]
    end
    
    subgraph "æ¶ˆè´¹è€…"
        subgraph "Consumer Group A"
            C1[Consumer 1]
            C2[Consumer 2]
        end
        
        subgraph "Consumer Group B"
            C3[Consumer 3]
        end
    end
    
    P1 --> T1P0
    P1 --> T1P1
    P2 --> T2P0
    P2 --> T2P1
    
    T1P0 --> C1
    T1P1 --> C2
    T2P0 --> C3
```

#### æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

| æ¦‚å¿µ | å®šä¹‰ | ä½œç”¨ | å…³é”®ç‰¹æ€§ |
|------|------|------|----------|
| **Broker** | KafkaæœåŠ¡å™¨èŠ‚ç‚¹ | å­˜å‚¨å’Œè½¬å‘æ¶ˆæ¯ | æ¯ä¸ªbrokeræœ‰å”¯ä¸€ID |
| **Topic** | æ¶ˆæ¯ä¸»é¢˜/åˆ†ç±» | æ¶ˆæ¯çš„é€»è¾‘åˆ†ç»„ | æ”¯æŒå¤šä¸ªåˆ†åŒº |
| **Partition** | ä¸»é¢˜åˆ†åŒº | æ¶ˆæ¯çš„ç‰©ç†åˆ†å‰² | æœ‰åºã€ä¸å¯å˜ |
| **Producer** | æ¶ˆæ¯ç”Ÿäº§è€… | å‘é€æ¶ˆæ¯åˆ°Topic | æ”¯æŒæ‰¹é‡å‘é€ |
| **Consumer** | æ¶ˆæ¯æ¶ˆè´¹è€… | ä»Topicæ¶ˆè´¹æ¶ˆæ¯ | ç»´æŠ¤æ¶ˆè´¹ä½ç½® |
| **Consumer Group** | æ¶ˆè´¹è€…ç»„ | æ¶ˆè´¹è€…çš„é€»è¾‘åˆ†ç»„ | è´Ÿè½½å‡è¡¡æ¶ˆè´¹ |
| **Offset** | æ¶ˆæ¯åç§»é‡ | æ¶ˆæ¯åœ¨åˆ†åŒºä¸­çš„ä½ç½® | å•è°ƒé€’å¢çš„longå€¼ |
| **Replica** | åˆ†åŒºå‰¯æœ¬ | æ•°æ®å†—ä½™å¤‡ä»½ | æä¾›å®¹é”™èƒ½åŠ› |

### 1.3 Kafka æ¶æ„

#### 1.3.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "Kafka ç”Ÿæ€ç³»ç»Ÿ"
        subgraph "æ ¸å¿ƒç»„ä»¶"
            ZK[ZooKeeper<br/>åè°ƒæœåŠ¡]
            B1[Broker 1]
            B2[Broker 2]
            B3[Broker 3]
        end
        
        subgraph "å®¢æˆ·ç«¯"
            PROD[Producer API]
            CONS[Consumer API]
            ADMIN[Admin API]
        end
        
        subgraph "æµå¤„ç†"
            KS[Kafka Streams]
            KSQL[ksqlDB]
        end
        
        subgraph "æ•°æ®é›†æˆ"
            KC[Kafka Connect]
            SR[Schema Registry]
        end
        
        subgraph "ç›‘æ§ç®¡ç†"
            KM[Kafka Manager]
            CMAK[CMAK]
        end
    end
    
    ZK -.-> B1
    ZK -.-> B2
    ZK -.-> B3
    
    PROD --> B1
    PROD --> B2
    CONS --> B2
    CONS --> B3
    
    KS --> B1
    KC --> B2
    SR --> B3
```

#### 1.3.2 å•ä¸ªBrokerå†…éƒ¨ç»“æ„

```mermaid
graph LR
    subgraph "Kafka Broker"
        subgraph "ç½‘ç»œå±‚"
            NIO[NIO Reactor]
            REQ[Request Handler]
        end
        
        subgraph "æ—¥å¿—å­˜å‚¨"
            LOG[Log Manager]
            SEG[Log Segments]
            IDX[Index Files]
        end
        
        subgraph "å‰¯æœ¬ç®¡ç†"
            RM[Replica Manager]
            FETCH[Fetch Manager]
        end
        
        subgraph "æ§åˆ¶å™¨"
            CTRL[Controller]
            META[Metadata Cache]
        end
        
        subgraph "ZKå®¢æˆ·ç«¯"
            ZKC[ZooKeeper Client]
        end
    end
    
    NIO --> REQ
    REQ --> LOG
    REQ --> RM
    RM --> LOG
    CTRL --> META
    CTRL --> ZKC
```

### 1.4 æ¶ˆæ¯æ¨¡å‹

#### 1.4.1 æ¶ˆæ¯ç»“æ„

**Kafkaæ¶ˆæ¯æ ¼å¼**ï¼š
```
æ¶ˆæ¯ = Header + Key + Value + Timestamp + Offset
```

**è¯¦ç»†ç»“æ„**ï¼š
```java
public class ProducerRecord<K, V> {
    private final String topic;          // ä¸»é¢˜åç§°
    private final Integer partition;     // åˆ†åŒºå·(å¯é€‰)
    private final Headers headers;       // æ¶ˆæ¯å¤´(å¯é€‰)
    private final K key;                // æ¶ˆæ¯é”®(å¯é€‰)
    private final V value;              // æ¶ˆæ¯å€¼
    private final Long timestamp;        // æ—¶é—´æˆ³(å¯é€‰)
}
```

#### 1.4.2 åˆ†åŒºç­–ç•¥

**åˆ†åŒºåˆ†é…ç­–ç•¥**ï¼š

| ç­–ç•¥ | æè¿° | ä½¿ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|------|------|----------|---------|
| **è½®è¯¢åˆ†åŒº** | æ¶ˆæ¯å‡åŒ€åˆ†å¸ƒåˆ°å„åˆ†åŒº | æ¶ˆæ¯é¡ºåºä¸é‡è¦ | è´Ÿè½½å‡è¡¡ï¼Œä½†æ— åº |
| **é”®å€¼åˆ†åŒº** | ç›¸åŒkeyè·¯ç”±åˆ°åŒä¸€åˆ†åŒº | éœ€è¦å±€éƒ¨æœ‰åº | æœ‰åºæ€§ï¼Œä½†å¯èƒ½è´Ÿè½½ä¸å‡ |
| **éšæœºåˆ†åŒº** | éšæœºé€‰æ‹©åˆ†åŒº | ç®€å•åœºæ™¯ | å®ç°ç®€å•ï¼Œä½†è´Ÿè½½ä¸å¯æ§ |
| **è‡ªå®šä¹‰åˆ†åŒº** | è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ | ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚ | çµæ´»æ€§é«˜ï¼Œä½†å¤æ‚åº¦å¢åŠ  |

**åˆ†åŒºç­–ç•¥ä»£ç ç¤ºä¾‹**ï¼š
```java
// è‡ªå®šä¹‰åˆ†åŒºå™¨
public class CustomPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int partitionCount = partitions.size();
        
        if (key == null) {
            // è½®è¯¢åˆ†åŒº
            return ThreadLocalRandom.current().nextInt(partitionCount);
        } else {
            // åŸºäºkeyçš„hashåˆ†åŒº
            return Math.abs(key.hashCode()) % partitionCount;
        }
    }
}
```

#### 1.4.3 æ¶ˆæ¯ä¼ é€’è¯­ä¹‰

**ä¸‰ç§ä¼ é€’ä¿è¯**ï¼š

| è¯­ä¹‰ | æè¿° | å®ç°æ–¹å¼ | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|----------|
| **At Most Once** | æœ€å¤šä¸€æ¬¡ï¼Œå¯èƒ½ä¸¢å¤± | acks=0ï¼Œä¸é‡è¯• | æ—¥å¿—æ”¶é›†ï¼Œå…è®¸ä¸¢å¤± |
| **At Least Once** | è‡³å°‘ä¸€æ¬¡ï¼Œå¯èƒ½é‡å¤ | acks=allï¼Œè‡ªåŠ¨é‡è¯• | é‡è¦ä¸šåŠ¡æ•°æ® |
| **Exactly Once** | ç²¾ç¡®ä¸€æ¬¡ | å¹‚ç­‰ç”Ÿäº§è€…+äº‹åŠ¡ | é‡‘èäº¤æ˜“ï¼Œå…³é”®ä¸šåŠ¡ |

**Exactly Onceå®ç°æœºåˆ¶**ï¼š
```java
// ç”Ÿäº§è€…å¹‚ç­‰æ€§é…ç½®
Properties props = new Properties();
props.put("enable.idempotence", true);  // å¯ç”¨å¹‚ç­‰æ€§
props.put("acks", "all");               // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
props.put("retries", Integer.MAX_VALUE); // é‡è¯•é…ç½®
props.put("max.in.flight.requests.per.connection", 1); // é™åˆ¶å¹¶å‘è¯·æ±‚

// äº‹åŠ¡æ€§ç”Ÿäº§è€…
props.put("transactional.id", "my-transactional-id");
KafkaProducer<String, String> producer = new KafkaProducer<>(props);

producer.initTransactions(); // åˆå§‹åŒ–äº‹åŠ¡
producer.beginTransaction(); // å¼€å§‹äº‹åŠ¡
try {
    producer.send(new ProducerRecord<>("topic", "key", "value"));
    producer.commitTransaction(); // æäº¤äº‹åŠ¡
} catch (Exception e) {
    producer.abortTransaction(); // å›æ»šäº‹åŠ¡
}
```

## 2. Kafka æ¶æ„åŸç†æ·±åº¦è§£æ

### 2.1 åˆ†å¸ƒå¼æ¶æ„è®¾è®¡

#### 2.1.1 é›†ç¾¤å‘ç°ä¸ç®¡ç†

**ZooKeeperåœ¨Kafkaä¸­çš„ä½œç”¨**ï¼š

```mermaid
graph TB
    subgraph "ZooKeeper"
        ZK_ROOT["/"]
        ZK_BROKERS["/brokers"]
        ZK_TOPICS["/config/topics"]
        ZK_CONSUMERS["/consumers"]
        ZK_CONTROLLER["/controller"]
        ZK_ADMIN["/admin"]
    end
    
    subgraph "Kafka é›†ç¾¤"
        BROKER1[Broker 1]
        BROKER2[Broker 2]
        BROKER3[Broker 3]
        CONTROLLER[Controller]
    end
    
    ZK_BROKERS --> BROKER1
    ZK_BROKERS --> BROKER2
    ZK_BROKERS --> BROKER3
    ZK_CONTROLLER --> CONTROLLER
    ZK_TOPICS --> BROKER1
    ZK_TOPICS --> BROKER2
    ZK_TOPICS --> BROKER3
```

**ZooKeeperå­˜å‚¨çš„å…³é”®ä¿¡æ¯**ï¼š

| è·¯å¾„ | å­˜å‚¨å†…å®¹ | ä½œç”¨ |
|------|----------|------|
| `/brokers/ids` | æ´»è·ƒçš„brokeråˆ—è¡¨ | é›†ç¾¤æˆå‘˜ç®¡ç† |
| `/brokers/topics` | ä¸»é¢˜çš„åˆ†åŒºåˆ†é… | å…ƒæ•°æ®ç®¡ç† |
| `/controller` | å½“å‰controllerä¿¡æ¯ | é€‰ä¸¾æ§åˆ¶å™¨ |
| `/config/topics` | ä¸»é¢˜é…ç½®ä¿¡æ¯ | åŠ¨æ€é…ç½® |
| `/admin/delete_topics` | å¾…åˆ é™¤çš„ä¸»é¢˜ | ç®¡ç†æ“ä½œ |

#### 2.1.2 Controlleré€‰ä¸¾æœºåˆ¶

**ControllerèŒè´£**ï¼š
- **åˆ†åŒºLeaderé€‰ä¸¾**ï¼šåœ¨åˆ†åŒºleaderæ•…éšœæ—¶é€‰ä¸¾æ–°leader
- **å‰¯æœ¬çŠ¶æ€ç®¡ç†**ï¼šç®¡ç†åˆ†åŒºå‰¯æœ¬çš„çŠ¶æ€è½¬æ¢
- **ä¸»é¢˜ç®¡ç†**ï¼šå¤„ç†ä¸»é¢˜çš„åˆ›å»ºã€åˆ é™¤ã€é…ç½®å˜æ›´
- **Brokerç®¡ç†**ï¼šç›‘æ§brokerçš„åŠ å…¥å’Œç¦»å¼€

**Controlleré€‰ä¸¾æµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant B1 as Broker 1
    participant B2 as Broker 2
    participant B3 as Broker 3
    participant ZK as ZooKeeper
    
    Note over B1,ZK: Brokerå¯åŠ¨æ—¶
    B1->>ZK: å°è¯•åˆ›å»º/controllerèŠ‚ç‚¹
    ZK->>B1: åˆ›å»ºæˆåŠŸï¼Œæˆä¸ºController
    B2->>ZK: å°è¯•åˆ›å»º/controllerèŠ‚ç‚¹
    ZK->>B2: åˆ›å»ºå¤±è´¥ï¼Œè®¾ç½®watch
    B3->>ZK: å°è¯•åˆ›å»º/controllerèŠ‚ç‚¹
    ZK->>B3: åˆ›å»ºå¤±è´¥ï¼Œè®¾ç½®watch
    
    Note over B1,ZK: Controlleræ•…éšœæ—¶
    B1->>ZK: è¿æ¥æ–­å¼€
    ZK->>B2: è§¦å‘watché€šçŸ¥
    ZK->>B3: è§¦å‘watché€šçŸ¥
    B2->>ZK: å°è¯•åˆ›å»º/controllerèŠ‚ç‚¹
    ZK->>B2: åˆ›å»ºæˆåŠŸï¼Œæˆä¸ºæ–°Controller
```

### 2.2 å­˜å‚¨æœºåˆ¶

#### 2.2.1 æ—¥å¿—å­˜å‚¨ç»“æ„

**æ—¥å¿—æ®µ(Log Segment)ç»“æ„**ï¼š
```
Topic: user-events, Partition: 0
â”œâ”€â”€ 00000000000000000000.log    (æ¶ˆæ¯æ—¥å¿—æ–‡ä»¶)
â”œâ”€â”€ 00000000000000000000.index  (åç§»é‡ç´¢å¼•æ–‡ä»¶)
â”œâ”€â”€ 00000000000000000000.timeindex (æ—¶é—´æˆ³ç´¢å¼•æ–‡ä»¶)
â”œâ”€â”€ 00000000000000368769.log
â”œâ”€â”€ 00000000000000368769.index
â”œâ”€â”€ 00000000000000368769.timeindex
â””â”€â”€ leader-epoch-checkpoint     (leader epochæ£€æŸ¥ç‚¹)
```

**ç´¢å¼•æ–‡ä»¶ç»“æ„**ï¼š
```mermaid
graph LR
    subgraph "åç§»é‡ç´¢å¼•(.index)"
        IDX1[Offset: 100<br/>Position: 2048]
        IDX2[Offset: 200<br/>Position: 4096]
        IDX3[Offset: 300<br/>Position: 6144]
    end
    
    subgraph "æ—¶é—´æˆ³ç´¢å¼•(.timeindex)"
        TIME1[Timestamp: 1623456789<br/>Offset: 150]
        TIME2[Timestamp: 1623456799<br/>Offset: 250]
    end
    
    subgraph "æ—¥å¿—æ–‡ä»¶(.log)"
        MSG1[Message 100]
        MSG2[Message 200]
        MSG3[Message 300]
    end
    
    IDX1 --> MSG1
    IDX2 --> MSG2
    IDX3 --> MSG3
```

#### 2.2.2 æ¶ˆæ¯å­˜å‚¨æ ¼å¼

**æ¶ˆæ¯æ‰¹æ¬¡æ ¼å¼ (Kafka 2.0+)**ï¼š
```java
public class RecordBatch {
    // æ‰¹æ¬¡å¤´éƒ¨ (61 bytes)
    private long baseOffset;          // 8 bytes - èµ·å§‹åç§»é‡
    private int batchLength;          // 4 bytes - æ‰¹æ¬¡é•¿åº¦
    private int partitionLeaderEpoch; // 4 bytes - åˆ†åŒºleader epoch
    private byte magic;               // 1 byte  - æ ¼å¼ç‰ˆæœ¬
    private int crc;                 // 4 bytes - CRCæ ¡éªŒ
    private short attributes;         // 2 bytes - å±æ€§æ ‡å¿—
    private int lastOffsetDelta;     // 4 bytes - æœ€ååç§»é‡å¢é‡
    private long firstTimestamp;     // 8 bytes - ç¬¬ä¸€æ¡æ¶ˆæ¯æ—¶é—´æˆ³
    private long maxTimestamp;       // 8 bytes - æœ€å¤§æ—¶é—´æˆ³
    private long producerId;         // 8 bytes - ç”Ÿäº§è€…ID
    private short producerEpoch;     // 2 bytes - ç”Ÿäº§è€…epoch
    private int baseSequence;        // 4 bytes - åŸºç¡€åºåˆ—å·
    private int recordCount;         // 4 bytes - è®°å½•æ•°é‡
    
    // æ¶ˆæ¯è®°å½•åˆ—è¡¨
    private List<Record> records;
}
```

### 2.3 å¤åˆ¶æœºåˆ¶

#### 2.3.1 å‰¯æœ¬åŒæ­¥æœºåˆ¶

**ISR (In-Sync Replicas) æœºåˆ¶**ï¼š

```mermaid
graph TB
    subgraph "åˆ†åŒºå‰¯æœ¬"
        LEADER[Leader Replica<br/>Broker 1<br/>LEO: 1000<br/>HW: 998]
        FOLLOWER1[Follower Replica<br/>Broker 2<br/>LEO: 999<br/>æ»å: 1]
        FOLLOWER2[Follower Replica<br/>Broker 3<br/>LEO: 995<br/>æ»å: 5]
    end
    
    subgraph "ISRåˆ—è¡¨"
        ISR_LIST["ISR: {1, 2}<br/>OSR: {3}"]
    end
    
    PRODUCER[Producer] --> LEADER
    LEADER --> FOLLOWER1
    LEADER --> FOLLOWER2
    
    LEADER -.-> ISR_LIST
    FOLLOWER1 -.-> ISR_LIST
    FOLLOWER2 -.-> ISR_LIST
```

**å‰¯æœ¬åŒæ­¥æµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant P as Producer
    participant L as Leader
    participant F1 as Follower 1
    participant F2 as Follower 2
    
    P->>L: å‘é€æ¶ˆæ¯æ‰¹æ¬¡
    L->>L: å†™å…¥æœ¬åœ°æ—¥å¿—
    
    par å‰¯æœ¬åŒæ­¥
        F1->>L: Fetchè¯·æ±‚ (offset=1000)
        L->>F1: è¿”å›æ¶ˆæ¯æ•°æ®
        F1->>F1: å†™å…¥æœ¬åœ°æ—¥å¿—
        F1->>L: Fetchè¯·æ±‚ (offset=1010)
    and
        F2->>L: Fetchè¯·æ±‚ (offset=995)
        L->>F2: è¿”å›æ¶ˆæ¯æ•°æ®
        F2->>F2: å†™å…¥æœ¬åœ°æ—¥å¿—
        F2->>L: Fetchè¯·æ±‚ (offset=1005)
    end
    
    L->>L: æ›´æ–°HW (High Watermark)
    L->>P: ACKå“åº”
```

#### 2.3.2 ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

**é‡è¦æ¦‚å¿µè¯´æ˜**ï¼š

| æ¦‚å¿µ | å®šä¹‰ | ä½œç”¨ |
|------|------|------|
| **LEO** | Log End Offsetï¼Œæ—¥å¿—ç»“æŸåç§»é‡ | è¡¨ç¤ºå‰¯æœ¬æ—¥å¿—çš„æœ€æ–°ä½ç½® |
| **HW** | High Watermarkï¼Œé«˜æ°´ä½æ ‡è®° | æ¶ˆè´¹è€…èƒ½è¯»å–åˆ°çš„æœ€å¤§åç§»é‡ |
| **ISR** | In-Sync Replicasï¼ŒåŒæ­¥å‰¯æœ¬é›†åˆ | ä¸leaderä¿æŒåŒæ­¥çš„å‰¯æœ¬åˆ—è¡¨ |
| **OSR** | Out-of-Sync Replicasï¼ŒéåŒæ­¥å‰¯æœ¬ | ä¸leaderä¸åŒæ­¥çš„å‰¯æœ¬åˆ—è¡¨ |

**å‰¯æœ¬çŠ¶æ€ç®¡ç†**ï¼š
```java
public enum ReplicaState {
    NewReplica,           // æ–°åˆ›å»ºçš„å‰¯æœ¬
    OnlineReplica,        // æ­£å¸¸åœ¨çº¿çš„å‰¯æœ¬
    OfflineReplica,       // ç¦»çº¿çš„å‰¯æœ¬
    ReplicaDeletionStarted, // å¼€å§‹åˆ é™¤çš„å‰¯æœ¬
    ReplicaDeletionSuccessful, // åˆ é™¤æˆåŠŸçš„å‰¯æœ¬
    ReplicaDeletionIneligible, // ä¸ç¬¦åˆåˆ é™¤æ¡ä»¶çš„å‰¯æœ¬
    NonExistentReplica    // ä¸å­˜åœ¨çš„å‰¯æœ¬
}
```

### 2.4 åè°ƒæœºåˆ¶

#### 2.4.1 æ¶ˆè´¹è€…ç»„åè°ƒ

**æ¶ˆè´¹è€…ç»„åè°ƒå™¨å·¥ä½œæµç¨‹**ï¼š

```mermaid
stateDiagram-v2
    [*] --> Dead: åˆå§‹çŠ¶æ€
    Dead --> PreparingRebalance: æˆå‘˜åŠ å…¥
    PreparingRebalance --> CompletingRebalance: æ‰€æœ‰æˆå‘˜å·²çŸ¥
    CompletingRebalance --> Stable: åˆ†åŒºåˆ†é…å®Œæˆ
    Stable --> PreparingRebalance: æˆå‘˜å˜åŒ–
    PreparingRebalance --> Dead: ç»„ä¸ºç©º
    CompletingRebalance --> Dead: è¶…æ—¶
    Stable --> Dead: ç»„ä¸ºç©º
```

**é‡å¹³è¡¡(Rebalance)è§¦å‘æ¡ä»¶**ï¼š
1. **æ–°æ¶ˆè´¹è€…åŠ å…¥ç»„**
2. **ç°æœ‰æ¶ˆè´¹è€…ç¦»å¼€ç»„**
3. **æ¶ˆè´¹è€…è¶…æ—¶æœªå‘é€å¿ƒè·³**
4. **è®¢é˜…çš„ä¸»é¢˜åˆ†åŒºæ•°é‡å˜åŒ–**
5. **æ¶ˆè´¹è€…å–æ¶ˆè®¢é˜…ä¸»é¢˜**

#### 2.4.2 åˆ†åŒºåˆ†é…ç­–ç•¥

**ä¸‰ç§åˆ†é…ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | ç®—æ³•æè¿° | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|------|----------|
| **Range** | æŒ‰ä¸»é¢˜åˆ†é…è¿ç»­åˆ†åŒº | ç®€å•ï¼Œå±€éƒ¨æ€§å¥½ | è´Ÿè½½å¯èƒ½ä¸å‡ | å•ä¸»é¢˜æ¶ˆè´¹ |
| **RoundRobin** | è½®è¯¢åˆ†é…æ‰€æœ‰åˆ†åŒº | è´Ÿè½½å‡è¡¡ | å¯èƒ½ç ´åå±€éƒ¨æ€§ | å¤šä¸»é¢˜æ¶ˆè´¹ |
| **Sticky** | å°½é‡ä¿æŒåŸåˆ†é… | å‡å°‘åˆ†åŒºè¿ç§» | ç®—æ³•å¤æ‚ | é¢‘ç¹é‡å¹³è¡¡åœºæ™¯ |

**Rangeåˆ†é…ç­–ç•¥ç¤ºä¾‹**ï¼š
```java
// å‡è®¾æœ‰2ä¸ªæ¶ˆè´¹è€…ï¼Œä¸»é¢˜æœ‰7ä¸ªåˆ†åŒº
Consumer 0: åˆ†åŒº 0,1,2,3      // (7/2=3ä½™1, ç¬¬ä¸€ä¸ªæ¶ˆè´¹è€…å¤šåˆ†é…1ä¸ª)
Consumer 1: åˆ†åŒº 4,5,6        // å‰©ä½™åˆ†åŒº

// Javaå®ç°ç¤ºä¾‹
public class RangeAssignor implements PartitionAssignor {
    @Override
    public Map<String, List<TopicPartition>> assign(
            Map<String, Integer> partitionsPerTopic,
            Map<String, Subscription> subscriptions) {
        
        Map<String, List<TopicPartition>> assignment = new HashMap<>();
        
        for (String topic : partitionsPerTopic.keySet()) {
            int partitionCount = partitionsPerTopic.get(topic);
            List<String> members = new ArrayList<>(subscriptions.keySet());
            Collections.sort(members); // ä¿è¯ç¡®å®šæ€§åˆ†é…
            
            int consumersCount = members.size();
            int partitionsPerConsumer = partitionCount / consumersCount;
            int consumersWithExtraPartition = partitionCount % consumersCount;
            
            for (int i = 0; i < members.size(); i++) {
                String member = members.get(i);
                int start = i * partitionsPerConsumer + Math.min(i, consumersWithExtraPartition);
                int length = partitionsPerConsumer + (i < consumersWithExtraPartition ? 1 : 0);
                
                List<TopicPartition> partitions = assignment.computeIfAbsent(member, k -> new ArrayList<>());
                for (int j = start; j < start + length; j++) {
                    partitions.add(new TopicPartition(topic, j));
                }
            }
        }
        
        return assignment;
    }
}
```

## 3. ç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…è¯¦è§£

### 3.1 ç”Ÿäº§è€…åŸç†

#### 3.1.1 ç”Ÿäº§è€…æ¶æ„

```mermaid
graph LR
    subgraph "Producer Application"
        APP[åº”ç”¨ç¨‹åº]
    end
    
    subgraph "KafkaProducer"
        SERIAL[Serializer<br/>åºåˆ—åŒ–å™¨]
        PART[Partitioner<br/>åˆ†åŒºå™¨]
        BUFFER[RecordAccumulator<br/>æ¶ˆæ¯ç´¯åŠ å™¨]
        SENDER[Sender Thread<br/>å‘é€çº¿ç¨‹]
    end
    
    subgraph "Kafka Cluster"
        BROKER1[Broker 1]
        BROKER2[Broker 2]
        BROKER3[Broker 3]
    end
    
    APP --> SERIAL
    SERIAL --> PART
    PART --> BUFFER
    BUFFER --> SENDER
    SENDER --> BROKER1
    SENDER --> BROKER2
    SENDER --> BROKER3
```

#### 3.1.2 æ¶ˆæ¯å‘é€æµç¨‹

**è¯¦ç»†å‘é€æµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant APP as åº”ç”¨ç¨‹åº
    participant PROD as KafkaProducer
    participant ACC as RecordAccumulator
    participant SENDER as Senderçº¿ç¨‹
    participant BROKER as Kafka Broker
    
    APP->>PROD: send(ProducerRecord)
    PROD->>PROD: åºåˆ—åŒ–keyå’Œvalue
    PROD->>PROD: é€‰æ‹©åˆ†åŒº
    PROD->>ACC: æ·»åŠ åˆ°ç´¯åŠ å™¨
    
    alt æ‰¹æ¬¡å·²æ»¡æˆ–è¾¾åˆ°å‘é€æ¡ä»¶
        ACC->>SENDER: é€šçŸ¥å‘é€çº¿ç¨‹
        SENDER->>BROKER: å‘é€æ‰¹æ¬¡æ•°æ®
        BROKER->>SENDER: è¿”å›å“åº”
        SENDER->>APP: å›è°ƒonCompletion
    else æ‰¹æ¬¡æœªæ»¡
        ACC->>ACC: ç­‰å¾…æ›´å¤šæ¶ˆæ¯æˆ–è¶…æ—¶
    end
```

#### 3.1.3 å…³é”®é…ç½®å‚æ•°

**ç”Ÿäº§è€…é‡è¦é…ç½®**ï¼š

| å‚æ•° | é»˜è®¤å€¼ | æè¿° | è°ƒä¼˜å»ºè®® |
|------|--------|------|----------|
| **batch.size** | 16384 | æ‰¹æ¬¡å¤§å°(å­—èŠ‚) | å¢å¤§æé«˜ååé‡ |
| **linger.ms** | 0 | æ‰¹æ¬¡ç­‰å¾…æ—¶é—´ | è®¾ç½®5-10mså¹³è¡¡å»¶è¿Ÿå’Œåå |
| **buffer.memory** | 33554432 | å‘é€ç¼“å†²åŒºå¤§å° | é«˜åååœºæ™¯å¯å¢å¤§ |
| **compression.type** | none | å‹ç¼©ç®—æ³• | ä½¿ç”¨lz4æˆ–snappy |
| **acks** | 1 | ç¡®è®¤çº§åˆ« | æ ¹æ®å¯é æ€§éœ€æ±‚é€‰æ‹© |
| **retries** | 2147483647 | é‡è¯•æ¬¡æ•° | é…åˆdelivery.timeout.ms |
| **max.in.flight.requests.per.connection** | 5 | å•è¿æ¥æœ€å¤§æœªç¡®è®¤è¯·æ±‚ | æœ‰åºæ€§è¦æ±‚è®¾ä¸º1 |

**ç”Ÿäº§è€…æ€§èƒ½é…ç½®ç¤ºä¾‹**ï¼š
```java
Properties props = new Properties();

// åŸºç¡€é…ç½®
props.put("bootstrap.servers", "localhost:9092,localhost:9093,localhost:9094");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// æ€§èƒ½ä¼˜åŒ–é…ç½®
props.put("batch.size", 32768);              // 32KBæ‰¹æ¬¡
props.put("linger.ms", 5);                   // 5msç­‰å¾…æ—¶é—´
props.put("buffer.memory", 67108864);        // 64MBç¼“å†²åŒº
props.put("compression.type", "lz4");        // LZ4å‹ç¼©

// å¯é æ€§é…ç½®
props.put("acks", "all");                    // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
props.put("retries", Integer.MAX_VALUE);     // æ— é™é‡è¯•
props.put("enable.idempotence", true);       // å¯ç”¨å¹‚ç­‰æ€§

// è¶…æ—¶é…ç½®
props.put("delivery.timeout.ms", 120000);    // 2åˆ†é’ŸæŠ•é€’è¶…æ—¶
props.put("request.timeout.ms", 30000);      // 30ç§’è¯·æ±‚è¶…æ—¶

KafkaProducer<String, String> producer = new KafkaProducer<>(props);
```

### 3.2 æ¶ˆè´¹è€…åŸç†

#### 3.2.1 æ¶ˆè´¹è€…æ¶æ„

```mermaid
graph TB
    subgraph "KafkaConsumer"
        CONSUMER[Consumerå®ä¾‹]
        FETCHER[Fetcher<br/>æ•°æ®è·å–å™¨]
        COORD[ConsumerCoordinator<br/>æ¶ˆè´¹è€…åè°ƒå™¨]
        METADATA[Metadata<br/>å…ƒæ•°æ®ç®¡ç†]
        NETWORK[NetworkClient<br/>ç½‘ç»œå®¢æˆ·ç«¯]
    end
    
    subgraph "å¤–éƒ¨ç»„ä»¶"
        APP[åº”ç”¨ç¨‹åº]
        ZK[ZooKeeper]
        BROKER[Kafka Broker]
    end
    
    APP --> CONSUMER
    CONSUMER --> FETCHER
    CONSUMER --> COORD
    FETCHER --> NETWORK
    COORD --> NETWORK
    METADATA --> NETWORK
    NETWORK --> BROKER
    COORD -.-> ZK
```

#### 3.2.2 æ¶ˆè´¹æµç¨‹è¯¦è§£

**æ¶ˆè´¹è€…å¯åŠ¨æµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant APP as åº”ç”¨ç¨‹åº
    participant CONS as KafkaConsumer
    participant COORD as Coordinator
    participant BROKER as Broker
    
    APP->>CONS: subscribe(topics)
    CONS->>COORD: åŠ å…¥æ¶ˆè´¹è€…ç»„
    COORD->>BROKER: å‘é€JoinGroupè¯·æ±‚
    BROKER->>COORD: è¿”å›ç»„ä¿¡æ¯å’Œåˆ†åŒºåˆ†é…
    COORD->>BROKER: å‘é€SyncGroupè¯·æ±‚
    BROKER->>COORD: ç¡®è®¤åˆ†åŒºåˆ†é…
    
    loop æ¶ˆè´¹å¾ªç¯
        APP->>CONS: poll(timeout)
        CONS->>BROKER: Fetchè¯·æ±‚
        BROKER->>CONS: è¿”å›æ¶ˆæ¯æ‰¹æ¬¡
        CONS->>APP: è¿”å›ConsumerRecords
        APP->>CONS: commitSync() (å¯é€‰)
    end
```

#### 3.2.3 ä½ç§»ç®¡ç†

**ä½ç§»æäº¤ç­–ç•¥**ï¼š

| ç­–ç•¥ | æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **è‡ªåŠ¨æäº¤** | enable.auto.commit=true | ç®€å•æ˜“ç”¨ | å¯èƒ½é‡å¤æ¶ˆè´¹æˆ–ä¸¢å¤± | å¯¹ä¸€è‡´æ€§è¦æ±‚ä¸é«˜ |
| **åŒæ­¥æ‰‹åŠ¨æäº¤** | commitSync() | å¼ºä¸€è‡´æ€§ | é˜»å¡ï¼Œå½±å“æ€§èƒ½ | ä¸¥æ ¼ä¸€è‡´æ€§è¦æ±‚ |
| **å¼‚æ­¥æ‰‹åŠ¨æäº¤** | commitAsync() | é«˜æ€§èƒ½ | å¯èƒ½æäº¤å¤±è´¥ | é«˜ååé‡åœºæ™¯ |
| **æ··åˆæäº¤** | ç»“åˆä½¿ç”¨ | å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§ | å¤æ‚åº¦è¾ƒé«˜ | ç”Ÿäº§ç¯å¢ƒæ¨è |

**ä½ç§»æäº¤æœ€ä½³å®è·µ**ï¼š
```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("group.id", "my-consumer-group");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("enable.auto.commit", false); // å…³é—­è‡ªåŠ¨æäº¤

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Arrays.asList("my-topic"));

try {
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
        for (ConsumerRecord<String, String> record : records) {
            // å¤„ç†æ¶ˆæ¯
            processRecord(record);
        }
        
        try {
            // åŒæ­¥æäº¤ä½ç§»
            consumer.commitSync();
        } catch (CommitFailedException e) {
            // å¤„ç†æäº¤å¤±è´¥
            log.error("Commit failed", e);
        }
    }
} catch (Exception e) {
    log.error("Consumer error", e);
} finally {
    try {
        // æœ€ç»ˆåŒæ­¥æäº¤
        consumer.commitSync();
    } finally {
        consumer.close();
    }
}
```

### 3.3 æ¶ˆè´¹è€…ç»„

#### 3.3.1 æ¶ˆè´¹è€…ç»„çŠ¶æ€ç®¡ç†

**æ¶ˆè´¹è€…çŠ¶æ€æµè½¬**ï¼š
```mermaid
stateDiagram-v2
    [*] --> Unsubscribed: åˆ›å»ºæ¶ˆè´¹è€…
    Unsubscribed --> Subscribing: è®¢é˜…ä¸»é¢˜
    Subscribing --> AwaitingRebalance: åŠ å…¥ç»„
    AwaitingRebalance --> Rebalancing: å¼€å§‹é‡å¹³è¡¡
    Rebalancing --> Stable: é‡å¹³è¡¡å®Œæˆ
    Stable --> AwaitingRebalance: è§¦å‘é‡å¹³è¡¡
    Stable --> Unsubscribed: å–æ¶ˆè®¢é˜…
    AwaitingRebalance --> Unsubscribed: ç¦»å¼€ç»„
    Rebalancing --> Unsubscribed: é‡å¹³è¡¡å¤±è´¥
```

#### 3.3.2 é‡å¹³è¡¡ä¼˜åŒ–

**é‡å¹³è¡¡æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **åˆç†è®¾ç½®ä¼šè¯è¶…æ—¶**ï¼š
```java
props.put("session.timeout.ms", 30000);        // 30ç§’ä¼šè¯è¶…æ—¶
props.put("heartbeat.interval.ms", 10000);     // 10ç§’å¿ƒè·³é—´éš”
props.put("max.poll.interval.ms", 300000);     // 5åˆ†é’Ÿpollé—´éš”
```

2. **ä½¿ç”¨é™æ€æˆå‘˜**ï¼š
```java
props.put("group.instance.id", "consumer-1");  // é™æ€æˆå‘˜ID
```

3. **å¢é‡åä½œé‡å¹³è¡¡**ï¼š
```java
props.put("partition.assignment.strategy", 
    "org.apache.kafka.clients.consumer.CooperativeStickyAssignor");
```

### 3.4 åç§»é‡ç®¡ç†

#### 3.4.1 åç§»é‡å­˜å‚¨

**åç§»é‡å­˜å‚¨ä½ç½®**ï¼š

| å­˜å‚¨ä½ç½® | Kafkaç‰ˆæœ¬ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|----------|-----------|------|------|
| **ZooKeeper** | 0.9ä»¥å‰ | ç®€å• | æ€§èƒ½å·®ï¼ŒZKå‹åŠ›å¤§ |
| **Kafka Topic** | 0.9+ | é«˜æ€§èƒ½ï¼Œå»ZKä¾èµ– | éœ€è¦é¢å¤–ç»´æŠ¤ |
| **å¤–éƒ¨å­˜å‚¨** | è‡ªå®šä¹‰ | çµæ´»æ€§é«˜ | å¤æ‚åº¦é«˜ |

**__consumer_offsetsä¸»é¢˜ç»“æ„**ï¼š
```
Key: [group.id, topic, partition]
Value: [offset, metadata, timestamp]

ä¾‹å¦‚ï¼š
Key: ["my-group", "user-events", 0]
Value: [1000, "", 1623456789000]
```

#### 3.4.2 åç§»é‡é‡ç½®ç­–ç•¥

**é‡ç½®ç­–ç•¥é…ç½®**ï¼š
```java
// è‡ªåŠ¨é‡ç½®ç­–ç•¥
props.put("auto.offset.reset", "earliest"); // earliest/latest/none

// æ‰‹åŠ¨é‡ç½®åˆ°æŒ‡å®šä½ç½®
Map<TopicPartition, Long> offsets = new HashMap<>();
offsets.put(new TopicPartition("my-topic", 0), 1000L);
consumer.seek(new TopicPartition("my-topic", 0), 1000L);

// é‡ç½®åˆ°æŒ‡å®šæ—¶é—´
Map<TopicPartition, Long> timestamps = new HashMap<>();
timestamps.put(new TopicPartition("my-topic", 0), System.currentTimeMillis() - 3600000); // 1å°æ—¶å‰
Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(timestamps);
```

## 7. Kafka ç”Ÿæ€ä¸é›†æˆ

### 7.1 Kafka Connect

**Kafka Connect** æ˜¯ä¸€ä¸ªæ•°æ®é›†æˆæ¡†æ¶ï¼Œç”¨äºåœ¨Kafkaå’Œå…¶ä»–ç³»ç»Ÿä¹‹é—´å¯é åœ°æµå¼ä¼ è¾“æ•°æ®ã€‚

#### 7.1.1 Connectæ¶æ„

```mermaid
graph LR
    subgraph "æ•°æ®æº"
        DB[(æ•°æ®åº“)]
        FILE[æ–‡ä»¶ç³»ç»Ÿ]
        API[REST API]
    end
    
    subgraph "Kafka Connect"
        SC[Source Connector]
        WORKER[Connect Worker]
        SINK[Sink Connector]
    end
    
    subgraph "Kafkaé›†ç¾¤"
        TOPIC[Topics]
    end
    
    subgraph "ç›®æ ‡ç³»ç»Ÿ"
        ES[(Elasticsearch)]
        HDFS[(HDFS)]
        S3[(AWS S3)]
    end
    
    DB --> SC
    FILE --> SC
    API --> SC
    SC --> WORKER
    WORKER --> TOPIC
    TOPIC --> WORKER
    WORKER --> SINK
    SINK --> ES
    SINK --> HDFS
    SINK --> S3
```

#### 7.1.2 å¸¸ç”¨è¿æ¥å™¨é…ç½®

**Source Connectoré…ç½®ç¤ºä¾‹**ï¼š
```json
{
  "name": "mysql-source-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "mysql-server",
    "database.port": "3306",
    "database.user": "kafka-user",
    "database.password": "kafka-password",
    "database.server.id": "184054",
    "database.server.name": "mysql-db",
    "database.include.list": "inventory",
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "schema-changes.inventory"
  }
}
```

**Sink Connectoré…ç½®ç¤ºä¾‹**ï¼š
```json
{
  "name": "elasticsearch-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "user-events",
    "connection.url": "http://elasticsearch:9200",
    "type.name": "_doc",
    "key.ignore": "true",
    "schema.ignore": "true"
  }
}
```

### 7.2 Kafka Streams

**Kafka Streams** æ˜¯ç”¨äºæ„å»ºåº”ç”¨ç¨‹åºå’Œå¾®æœåŠ¡çš„å®¢æˆ·ç«¯åº“ï¼Œç”¨äºå¤„ç†å’Œåˆ†æå­˜å‚¨åœ¨Kafkaä¸­çš„æ•°æ®ã€‚

#### 7.2.1 Streamsåº”ç”¨ç¤ºä¾‹

```java
public class WordCountApplication {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-application");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        
        StreamsBuilder builder = new StreamsBuilder();
        
        KStream<String, String> textLines = builder.stream("text-input");
        
        KTable<String, Long> wordCounts = textLines
            .flatMapValues(textLine -> Arrays.asList(textLine.toLowerCase().split("\\W+")))
            .groupBy((key, word) -> word)
            .count(Materialized.as("counts-store"));
            
        wordCounts.toStream().to("word-count-output", Produced.with(Serdes.String(), Serdes.Long()));
        
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
        
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

### 7.3 Schema Registry

**Schema Registry** æä¾›RESTfulæ¥å£æ¥å­˜å‚¨å’Œæ£€ç´¢Schemaï¼Œæ”¯æŒAvroã€JSON Schemaå’ŒProtobufã€‚

#### 7.3.1 Avro Schemaç¤ºä¾‹

```json
{
  "type": "record",
  "name": "User",
  "namespace": "com.example",
  "fields": [
    {"name": "id", "type": "long"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": ["null", "string"], "default": null},
    {"name": "created_at", "type": "long", "logicalType": "timestamp-millis"}
  ]
}
```

## 8. é«˜çº§ç‰¹æ€§ä¸ä¼ä¸šåº”ç”¨

### 8.1 äº‹åŠ¡æ”¯æŒ

```java
// äº‹åŠ¡ç”Ÿäº§è€…å®Œæ•´ç¤ºä¾‹
public class TransactionalProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("transactional.id", "my-transactional-id");
        props.put("acks", "all");
        props.put("retries", Integer.MAX_VALUE);
        props.put("enable.idempotence", true);
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        producer.initTransactions();
        
        try {
            producer.beginTransaction();
            
            // å‘é€å¤šæ¡æ¶ˆæ¯
            for (int i = 0; i < 100; i++) {
                producer.send(new ProducerRecord<>("my-topic", 
                    Integer.toString(i), "message-" + i));
            }
            
            producer.commitTransaction();
        } catch (ProducerFencedException | OutOfOrderSequenceException | 
                 AuthorizationException e) {
            // ä¸èƒ½æ¢å¤çš„å¼‚å¸¸
            producer.close();
        } catch (KafkaException e) {
            // å¯æ¢å¤çš„å¼‚å¸¸
            producer.abortTransaction();
        }
        
        producer.close();
    }
}
```

### 8.2 ç›‘æ§æœ€ä½³å®è·µ

**å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š

| ç±»åˆ« | æŒ‡æ ‡ | æè¿° | å‘Šè­¦é˜ˆå€¼ |
|------|------|------|----------|
| **ååé‡** | MessagesInPerSec | æ¯ç§’æ¶ˆæ¯æ•° | ä¸‹é™50% |
| **å»¶è¿Ÿ** | ProduceRequestLatency | ç”Ÿäº§å»¶è¿Ÿ | >100ms |
| **å¯ç”¨æ€§** | UnderReplicatedPartitions | å‰¯æœ¬ä¸è¶³åˆ†åŒº | >0 |
| **å­˜å‚¨** | LogSize | æ—¥å¿—å¤§å° | >80%ç£ç›˜ |

## 9. Kafka å®æˆ˜æ¡ˆä¾‹

### 9.1 å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ

```java
// å®æ—¶ç”¨æˆ·è¡Œä¸ºæµå¤„ç†
public class UserBehaviorAnalysis {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        
        KStream<String, UserEvent> events = builder.stream("user-events");
        
        // ç”¨æˆ·ä¼šè¯åˆ†æ
        KTable<String, Long> sessionCounts = events
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(30)))
            .count();
            
        // çƒ­é—¨é¡µé¢ç»Ÿè®¡
        KTable<String, Long> pageViews = events
            .filter((key, event) -> event.getEventType().equals("page_view"))
            .groupBy((key, event) -> event.getPageUrl())
            .count();
            
        pageViews.toStream().to("popular-pages");
    }
}
```

## 10. Kafka é¢è¯•é¢˜è¯¦è§£

### 10.1 åŸºç¡€æ¦‚å¿µç±»

#### Q1: ä»€ä¹ˆæ˜¯Kafkaï¼Ÿå®ƒçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**ï¼š
Apache Kafkaæ˜¯ä¸€ä¸ª**åˆ†å¸ƒå¼æµå¤„ç†å¹³å°**ï¼Œä¸»è¦ç”¨ä½œæ¶ˆæ¯é˜Ÿåˆ—å’Œå®æ—¶æ•°æ®æµå¤„ç†ã€‚

**ä¸»è¦ç‰¹ç‚¹**ï¼š
- **é«˜ååé‡**ï¼šå•èŠ‚ç‚¹æ”¯æŒæ•°ç™¾ä¸‡æ¡æ¶ˆæ¯/ç§’çš„å¤„ç†èƒ½åŠ›
- **ä½å»¶è¿Ÿ**ï¼šæ¯«ç§’çº§çš„æ¶ˆæ¯ä¼ é€’å»¶è¿Ÿ
- **æŒä¹…åŒ–**ï¼šæ¶ˆæ¯æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œæ”¯æŒæ•°æ®æ¢å¤
- **åˆ†å¸ƒå¼**ï¼šå¤©ç„¶æ”¯æŒé›†ç¾¤éƒ¨ç½²å’Œæ°´å¹³æ‰©å±•
- **å®¹é”™æ€§**ï¼šé€šè¿‡å‰¯æœ¬æœºåˆ¶æä¾›æ•°æ®å†—ä½™å’Œæ•…éšœæ¢å¤
- **é¡ºåºæ€§**ï¼šåœ¨åˆ†åŒºå†…ä¿è¯æ¶ˆæ¯çš„ä¸¥æ ¼é¡ºåº

**åº”ç”¨åœºæ™¯**ï¼š
- æ¶ˆæ¯ç³»ç»Ÿï¼šåº”ç”¨é—´å¼‚æ­¥é€šä¿¡
- ç½‘ç«™è¡Œä¸ºè·Ÿè¸ªï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®æ”¶é›†
- è¿è¥æŒ‡æ ‡ç›‘æ§ï¼šç³»ç»ŸæŒ‡æ ‡å®æ—¶æ”¶é›†
- æ—¥å¿—èšåˆï¼šåˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†å’Œå¤„ç†
- æµå¤„ç†ï¼šå®æ—¶æ•°æ®æµå¤„ç†

#### Q2: è§£é‡ŠKafkaä¸­Topicã€Partitionã€Offsetçš„æ¦‚å¿µåŠå…¶å…³ç³»ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**ä¸‰è€…å…³ç³»å›¾**ï¼š
```
Topic (ä¸»é¢˜): user-events
â”œâ”€â”€ Partition 0: [Msg0|Msg1|Msg2|Msg3] (Offset: 0,1,2,3)
â”œâ”€â”€ Partition 1: [Msg4|Msg5|Msg6|Msg7] (Offset: 0,1,2,3)  
â””â”€â”€ Partition 2: [Msg8|Msg9|MsgA|MsgB] (Offset: 0,1,2,3)
```

**æ¦‚å¿µè§£é‡Š**ï¼š
- **Topic**ï¼šæ¶ˆæ¯çš„é€»è¾‘åˆ†ç±»ï¼Œç±»ä¼¼æ•°æ®åº“ä¸­çš„è¡¨
- **Partition**ï¼šTopicçš„ç‰©ç†åˆ†å‰²ï¼Œæä¾›å¹¶è¡Œå¤„ç†èƒ½åŠ›
- **Offset**ï¼šæ¶ˆæ¯åœ¨åˆ†åŒºä¸­çš„å”¯ä¸€æ ‡è¯†ï¼Œå•è°ƒé€’å¢çš„longå€¼

**å…³ç³»è¯´æ˜**ï¼š
- ä¸€ä¸ªTopicå¯ä»¥æœ‰å¤šä¸ªPartition
- æ¯ä¸ªPartitionå†…æ¶ˆæ¯æœ‰åºï¼ŒOffsetå”¯ä¸€
- ä¸åŒPartitioné—´æ¶ˆæ¯æ— å…¨å±€é¡ºåº
- Offsetåœ¨åˆ†åŒºå†…ä»0å¼€å§‹é€’å¢

#### Q3: Kafkaå¦‚ä½•ä¿è¯æ¶ˆæ¯çš„å¯é æ€§ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
Kafkaé€šè¿‡**å¤šå±‚æœºåˆ¶**ä¿è¯æ¶ˆæ¯å¯é æ€§ï¼š

**1. å‰¯æœ¬æœºåˆ¶(Replication)**ï¼š
- æ¯ä¸ªåˆ†åŒºå¯é…ç½®å¤šä¸ªå‰¯æœ¬
- Leaderè´Ÿè´£è¯»å†™ï¼ŒFolloweråŒæ­¥æ•°æ®
- å½“Leaderå¤±è´¥æ—¶ï¼Œä»ISRä¸­é€‰ä¸¾æ–°Leader

**2. ISRæœºåˆ¶(In-Sync Replicas)**ï¼š
- ISRåŒ…å«ä¸Leaderä¿æŒåŒæ­¥çš„å‰¯æœ¬
- åªæœ‰ISRä¸­çš„å‰¯æœ¬æ‰èƒ½è¢«é€‰ä¸ºLeader
- é€šè¿‡`min.insync.replicas`æ§åˆ¶æœ€å°åŒæ­¥å‰¯æœ¬æ•°

**3. ACKç¡®è®¤æœºåˆ¶**ï¼š
```java
// acksé…ç½®é€‰é¡¹
props.put("acks", "all");  // ç­‰å¾…æ‰€æœ‰ISRå‰¯æœ¬ç¡®è®¤
props.put("acks", "1");    // ç­‰å¾…Leaderç¡®è®¤  
props.put("acks", "0");    // ä¸ç­‰å¾…ç¡®è®¤
```

**4. é‡è¯•æœºåˆ¶**ï¼š
```java
props.put("retries", Integer.MAX_VALUE);
props.put("delivery.timeout.ms", 120000);
```

#### Q4: ä»€ä¹ˆæ˜¯æ¶ˆè´¹è€…ç»„ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦æ¶ˆè´¹è€…ç»„ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**æ¶ˆè´¹è€…ç»„(Consumer Group)**æ˜¯ä¸€ç»„æ¶ˆè´¹è€…å®ä¾‹çš„é€»è¾‘åˆ†ç»„ï¼Œå…±åŒæ¶ˆè´¹ä¸€ä¸ªæˆ–å¤šä¸ªTopicçš„æ•°æ®ã€‚

**å­˜åœ¨æ„ä¹‰**ï¼š
1. **è´Ÿè½½å‡è¡¡**ï¼šå¤šä¸ªæ¶ˆè´¹è€…å¹¶è¡Œæ¶ˆè´¹ï¼Œæé«˜å¤„ç†èƒ½åŠ›
2. **å®¹é”™æ€§**ï¼šæ¶ˆè´¹è€…æ•…éšœæ—¶ï¼Œå…¶ä»–æ¶ˆè´¹è€…æ¥ç®¡åˆ†åŒº
3. **å¼¹æ€§æ‰©å±•**ï¼šåŠ¨æ€å¢å‡æ¶ˆè´¹è€…å®ä¾‹
4. **æ¶ˆè´¹è¿›åº¦ç®¡ç†**ï¼šç»„å†…ç»Ÿä¸€ç®¡ç†æ¶ˆè´¹ä½ç§»

**å·¥ä½œåŸç†**ï¼š
```
Topic: user-events (3ä¸ªåˆ†åŒº)
Consumer Group: my-group
â”œâ”€â”€ Consumer-1: è´Ÿè´£ Partition 0
â”œâ”€â”€ Consumer-2: è´Ÿè´£ Partition 1  
â””â”€â”€ Consumer-3: è´Ÿè´£ Partition 2
```

**é‡è¦ç‰¹æ€§**ï¼š
- åŒä¸€æ¶ˆè´¹è€…ç»„å†…ï¼Œæ¯ä¸ªåˆ†åŒºåªè¢«ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹
- ä¸åŒæ¶ˆè´¹è€…ç»„å¯ä»¥ç‹¬ç«‹æ¶ˆè´¹åŒä¸€ä»½æ•°æ®
- æ¶ˆè´¹è€…æ•°é‡ä¸åº”è¶…è¿‡åˆ†åŒºæ•°é‡

#### Q5: Kafkaçš„æ¶ˆæ¯æ˜¯å¦‚ä½•å­˜å‚¨çš„ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
Kafkaé‡‡ç”¨**åˆ†æ®µæ—¥å¿—å­˜å‚¨**(Segmented Log)æœºåˆ¶ï¼š

**å­˜å‚¨ç»“æ„**ï¼š
```
Topic: user-events, Partition: 0
â”œâ”€â”€ 00000000000000000000.log    # æ—¥å¿—æ–‡ä»¶(æ¶ˆæ¯æ•°æ®)
â”œâ”€â”€ 00000000000000000000.index  # åç§»é‡ç´¢å¼•
â”œâ”€â”€ 00000000000000000000.timeindex # æ—¶é—´ç´¢å¼•
â”œâ”€â”€ 00000000000000368769.log    # æ–°çš„æ—¥å¿—æ®µ
â”œâ”€â”€ 00000000000000368769.index
â””â”€â”€ leader-epoch-checkpoint     # Leader epochè®°å½•
```

**å­˜å‚¨ç‰¹ç‚¹**ï¼š
- **é¡ºåºå†™å…¥**ï¼šæ‰€æœ‰æ¶ˆæ¯è¿½åŠ åˆ°æ—¥å¿—æœ«å°¾ï¼Œåˆ©ç”¨ç£ç›˜é¡ºåºI/Oä¼˜åŠ¿
- **åˆ†æ®µå­˜å‚¨**ï¼šæ—¥å¿—è¢«åˆ†å‰²æˆå¤šä¸ªsegmentï¼Œä¾¿äºç®¡ç†å’Œæ¸…ç†
- **ç¨€ç–ç´¢å¼•**ï¼šé€šè¿‡ç´¢å¼•æ–‡ä»¶å¿«é€Ÿå®šä½æ¶ˆæ¯ä½ç½®
- **é›¶æ‹·è´**ï¼šä½¿ç”¨sendfileç³»ç»Ÿè°ƒç”¨ï¼Œæé«˜ä¼ è¾“æ•ˆç‡

**æ¸…ç†ç­–ç•¥**ï¼š
```bash
# åŸºäºæ—¶é—´æ¸…ç†
log.retention.hours=168    # ä¿ç•™7å¤©

# åŸºäºå¤§å°æ¸…ç†  
log.retention.bytes=1073741824  # ä¿ç•™1GB

# å‹ç¼©æ¸…ç†(é€‚ç”¨äºçŠ¶æ€å­˜å‚¨)
log.cleanup.policy=compact
```

### 10.2 æ¶æ„åŸç†ç±»

#### Q6: è¯¦ç»†è§£é‡ŠKafkaçš„åˆ†åŒºæœºåˆ¶å’Œåˆ†åŒºç­–ç•¥ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**åˆ†åŒºæœºåˆ¶ä½œç”¨**ï¼š
1. **å¹¶è¡Œå¤„ç†**ï¼šå¤šä¸ªåˆ†åŒºæ”¯æŒå¹¶å‘ç”Ÿäº§å’Œæ¶ˆè´¹
2. **è´Ÿè½½åˆ†æ•£**ï¼šåˆ†åŒºåˆ†å¸ƒåœ¨ä¸åŒBrokerä¸Š
3. **æœ‰åºä¿è¯**ï¼šåˆ†åŒºå†…æ¶ˆæ¯ä¸¥æ ¼æœ‰åº
4. **æ‰©å±•æ€§**ï¼šé€šè¿‡å¢åŠ åˆ†åŒºæé«˜ååé‡

**åˆ†åŒºç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | æè¿° | ä½¿ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|------|------|----------|---------|
| **è½®è¯¢åˆ†åŒº** | æ¶ˆæ¯å‡åŒ€åˆ†å¸ƒåˆ°å„åˆ†åŒº | æ¶ˆæ¯é¡ºåºä¸é‡è¦ | è´Ÿè½½å‡è¡¡ï¼Œä½†æ— åº |
| **é”®å€¼åˆ†åŒº** | ç›¸åŒkeyè·¯ç”±åˆ°åŒä¸€åˆ†åŒº | éœ€è¦å±€éƒ¨æœ‰åº | æœ‰åºæ€§ï¼Œä½†å¯èƒ½è´Ÿè½½ä¸å‡ |
| **éšæœºåˆ†åŒº** | éšæœºé€‰æ‹©åˆ†åŒº | ç®€å•åœºæ™¯ | å®ç°ç®€å•ï¼Œä½†è´Ÿè½½ä¸å¯æ§ |
| **è‡ªå®šä¹‰åˆ†åŒº** | è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘ | ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚ | çµæ´»æ€§é«˜ï¼Œä½†å¤æ‚åº¦å¢åŠ  |

**è‡ªå®šä¹‰åˆ†åŒºå™¨ç¤ºä¾‹**ï¼š
```java
public class CustomPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int partitionCount = partitions.size();
        
        if (key == null) {
            // è½®è¯¢åˆ†åŒº
            return ThreadLocalRandom.current().nextInt(partitionCount);
        } else {
            // åŸºäºkeyçš„hashåˆ†åŒº
            return Math.abs(key.hashCode()) % partitionCount;
        }
    }
}
```

#### Q7: Kafkaå¦‚ä½•å®ç°é«˜ååé‡ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
Kafkaé€šè¿‡**å¤šé¡¹ä¼˜åŒ–æŠ€æœ¯**å®ç°é«˜ååé‡ï¼š

**1. æ‰¹é‡å¤„ç†**ï¼š
- ç”Ÿäº§è€…æ‰¹é‡å‘é€æ¶ˆæ¯
- æ¶ˆè´¹è€…æ‰¹é‡æ‹‰å–æ¶ˆæ¯
- å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°

```java
props.put("batch.size", 65536);        // 64KBæ‰¹æ¬¡
props.put("linger.ms", 10);            // 10msç­‰å¾…æ—¶é—´
props.put("fetch.min.bytes", 1048576); // 1MBæœ€å°æ‹‰å–
```

**2. é›¶æ‹·è´æŠ€æœ¯**ï¼š
```java
// sendfileç³»ç»Ÿè°ƒç”¨ï¼Œç›´æ¥ä»æ–‡ä»¶åˆ°ç½‘ç»œ
// é¿å…ç”¨æˆ·ç©ºé—´å’Œå†…æ ¸ç©ºé—´çš„æ•°æ®æ‹·è´
FileChannel.transferTo()
```

**3. é¡µç¼“å­˜åˆ©ç”¨**ï¼š
- Kafkaä¾èµ–æ“ä½œç³»ç»Ÿé¡µç¼“å­˜
- é¡ºåºè¯»å†™å……åˆ†åˆ©ç”¨ç¼“å­˜é¢„è¯»
- é¿å…JVMå †å†…å­˜ç®¡ç†å¼€é”€

**4. åˆ†åŒºå¹¶è¡Œ**ï¼š
- å¤šåˆ†åŒºæ”¯æŒå¹¶è¡Œç”Ÿäº§å’Œæ¶ˆè´¹
- åˆ†åŒºåˆ†å¸ƒåœ¨ä¸åŒBrokerä¸Š
- æé«˜æ•´ä½“é›†ç¾¤ååé‡

**5. å‹ç¼©ç®—æ³•**ï¼š
```java
props.put("compression.type", "lz4");  // é«˜æ€§èƒ½å‹ç¼©
```

#### Q8: è§£é‡ŠKafkaçš„å‰¯æœ¬æœºåˆ¶å’ŒISRï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**å‰¯æœ¬æœºåˆ¶åŸç†**ï¼š

```mermaid
graph TB
    subgraph "åˆ†åŒºå‰¯æœ¬"
        LEADER["Leader Replica<br/>Broker 1<br/>LEO: 1000<br/>HW: 998"]
        FOLLOWER1["Follower Replica<br/>Broker 2<br/>LEO: 999<br/>æ»å: 1"]
        FOLLOWER2["Follower Replica<br/>Broker 3<br/>LEO: 995<br/>æ»å: 5"]
    end
    
    subgraph "ISRåˆ—è¡¨"
        ISR_LIST["ISR: {1, 2}<br/>OSR: {3}"]
    end
    
    PRODUCER[Producer] --> LEADER
    LEADER --> FOLLOWER1
    LEADER --> FOLLOWER2
```

**é‡è¦æ¦‚å¿µ**ï¼š

| æ¦‚å¿µ | å®šä¹‰ | ä½œç”¨ |
|------|------|------|
| **LEO** | Log End Offsetï¼Œæ—¥å¿—ç»“æŸåç§»é‡ | è¡¨ç¤ºå‰¯æœ¬æ—¥å¿—çš„æœ€æ–°ä½ç½® |
| **HW** | High Watermarkï¼Œé«˜æ°´ä½æ ‡è®° | æ¶ˆè´¹è€…èƒ½è¯»å–åˆ°çš„æœ€å¤§åç§»é‡ |
| **ISR** | In-Sync Replicasï¼ŒåŒæ­¥å‰¯æœ¬é›†åˆ | ä¸leaderä¿æŒåŒæ­¥çš„å‰¯æœ¬åˆ—è¡¨ |
| **OSR** | Out-of-Sync Replicasï¼ŒéåŒæ­¥å‰¯æœ¬ | ä¸leaderä¸åŒæ­¥çš„å‰¯æœ¬åˆ—è¡¨ |

**å‰¯æœ¬åŒæ­¥æµç¨‹**ï¼š
1. Leaderæ¥æ”¶ç”Ÿäº§è€…æ¶ˆæ¯ï¼Œå†™å…¥æœ¬åœ°æ—¥å¿—
2. Followerå‘é€Fetchè¯·æ±‚æ‹‰å–æ•°æ®
3. Followerå†™å…¥æœ¬åœ°æ—¥å¿—ï¼Œå‘é€ä¸‹ä¸€ä¸ªFetchè¯·æ±‚
4. Leaderæ›´æ–°HW(æ‰€æœ‰ISRå‰¯æœ¬çš„æœ€å°LEO)
5. Leaderå“åº”ç”Ÿäº§è€…ACK

#### Q9: Kafkaçš„Controllerçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿé€‰ä¸¾æœºåˆ¶å¦‚ä½•ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**ControllerèŒè´£**ï¼š
- **åˆ†åŒºLeaderé€‰ä¸¾**ï¼šåœ¨åˆ†åŒºleaderæ•…éšœæ—¶é€‰ä¸¾æ–°leader
- **å‰¯æœ¬çŠ¶æ€ç®¡ç†**ï¼šç®¡ç†åˆ†åŒºå‰¯æœ¬çš„çŠ¶æ€è½¬æ¢
- **ä¸»é¢˜ç®¡ç†**ï¼šå¤„ç†ä¸»é¢˜çš„åˆ›å»ºã€åˆ é™¤ã€é…ç½®å˜æ›´
- **Brokerç®¡ç†**ï¼šç›‘æ§brokerçš„åŠ å…¥å’Œç¦»å¼€
- **å…ƒæ•°æ®åŒæ­¥**ï¼šå°†é›†ç¾¤å˜æ›´åŒæ­¥ç»™æ‰€æœ‰Broker

**Controlleré€‰ä¸¾æµç¨‹**ï¼š
1. **å¯åŠ¨æ—¶é€‰ä¸¾**ï¼šBrokerå¯åŠ¨æ—¶å°è¯•åœ¨ZooKeeperåˆ›å»º`/controller`èŠ‚ç‚¹
2. **æˆåŠŸå½“é€‰**ï¼šç¬¬ä¸€ä¸ªæˆåŠŸåˆ›å»ºèŠ‚ç‚¹çš„Brokeræˆä¸ºController
3. **å…¶ä»–Broker**ï¼šç›‘å¬ControllerèŠ‚ç‚¹å˜åŒ–
4. **æ•…éšœè½¬ç§»**ï¼šControlleræ•…éšœæ—¶ï¼Œå…¶ä»–Brokerç«äº‰é€‰ä¸¾

**é€‰ä¸¾ä»£ç åŸç†**ï¼š
```java
// Controlleré€‰ä¸¾é€»è¾‘
public class ControllerElection {
    public void elect() {
        try {
            // å°è¯•åˆ›å»ºcontrollerèŠ‚ç‚¹
            zkClient.createEphemeralSequential("/controller", brokerInfo);
            becomeController();
        } catch (NodeExistsException e) {
            // èŠ‚ç‚¹å·²å­˜åœ¨ï¼Œè®¾ç½®watchç›‘å¬
            zkClient.watchForDelete("/controller", this::elect);
        }
    }
}
```

### 10.3 æ€§èƒ½è°ƒä¼˜ç±»

#### Q10: å¦‚ä½•ä¼˜åŒ–Kafkaç”Ÿäº§è€…çš„æ€§èƒ½ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**ç”Ÿäº§è€…æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š

**1. æ‰¹é‡å‘é€ä¼˜åŒ–**ï¼š
```java
Properties props = new Properties();
props.put("batch.size", 65536);        // å¢å¤§æ‰¹æ¬¡å¤§å°åˆ°64KB
props.put("linger.ms", 10);            // è®¾ç½®ç­‰å¾…æ—¶é—´10ms
props.put("buffer.memory", 134217728); // å¢å¤§ç¼“å†²åŒºåˆ°128MB
```

**2. å‹ç¼©ä¼˜åŒ–**ï¼š
```java
props.put("compression.type", "lz4");  // ä½¿ç”¨LZ4å‹ç¼©ç®—æ³•
```

**3. å¹¶å‘ä¼˜åŒ–**ï¼š
```java
props.put("max.in.flight.requests.per.connection", 5); // å¢åŠ å¹¶å‘è¯·æ±‚
```

**4. åºåˆ—åŒ–ä¼˜åŒ–**ï¼š
- ä½¿ç”¨é«˜æ•ˆçš„åºåˆ—åŒ–å™¨(Avroã€Protobuf)
- é¿å…ä½¿ç”¨Javaé»˜è®¤åºåˆ—åŒ–

**5. åˆ†åŒºç­–ç•¥ä¼˜åŒ–**ï¼š
```java
// è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼Œé¿å…çƒ­ç‚¹åˆ†åŒº
public class LoadBalancedPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        // åŸºäºæ¶ˆæ¯å†…å®¹çš„è´Ÿè½½å‡è¡¡åˆ†åŒºé€»è¾‘
        return balancedPartition(value, cluster.partitionCountForTopic(topic));
    }
}
```

**6. å¼‚æ­¥å‘é€**ï¼š
```java
// å¼‚æ­¥å‘é€æé«˜ååé‡
producer.send(record, new Callback() {
    @Override
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        if (exception != null) {
            handleException(exception);
        }
    }
});
```

#### Q11: å¦‚ä½•ä¼˜åŒ–Kafkaæ¶ˆè´¹è€…çš„æ€§èƒ½ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**æ¶ˆè´¹è€…æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š

**1. æ‹‰å–å‚æ•°ä¼˜åŒ–**ï¼š
```java
Properties props = new Properties();
props.put("fetch.min.bytes", 1048576);      // 1MBæœ€å°æ‹‰å–
props.put("fetch.max.wait.ms", 500);        // 500msæœ€å¤§ç­‰å¾…
props.put("max.partition.fetch.bytes", 2097152); // 2MBåˆ†åŒºæ‹‰å–
```

**2. å¤šçº¿ç¨‹æ¶ˆè´¹**ï¼š
```java
public class MultiThreadConsumer {
    private final ExecutorService executorService;
    
    public void startConsumers(int numThreads) {
        for (int i = 0; i < numThreads; i++) {
            executorService.submit(() -> {
                KafkaConsumer<String, String> consumer = createConsumer();
                while (true) {
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                    processRecords(records);
                }
            });
        }
    }
}
```

**3. æ‰¹é‡å¤„ç†**ï¼š
```java
// æ‰¹é‡å¤„ç†æ¶ˆæ¯
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
List<String> batch = new ArrayList<>();

for (ConsumerRecord<String, String> record : records) {
    batch.add(record.value());
    if (batch.size() >= 100) { // æ‰¹é‡å¤§å°
        processBatch(batch);
        batch.clear();
    }
}
```

**4. ä½ç§»æäº¤ä¼˜åŒ–**ï¼š
```java
// å¼‚æ­¥æäº¤ä½ç§»
consumer.commitAsync((offsets, exception) -> {
    if (exception != null) {
        log.error("Commit failed", exception);
    }
});
```

#### Q12: Kafkaé›†ç¾¤å¦‚ä½•è¿›è¡Œå®¹é‡è§„åˆ’ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**å®¹é‡è§„åˆ’è€ƒè™‘å› ç´ **ï¼š

**1. ååé‡è§„åˆ’**ï¼š
```
æ‰€éœ€Brokeræ•° = ç›®æ ‡ååé‡ / å•Brokeræœ€å¤§ååé‡
å•Brokerååé‡ â‰ˆ 100MB/s (ç”Ÿäº§ç¯å¢ƒç»éªŒå€¼)
```

**2. å­˜å‚¨å®¹é‡è§„åˆ’**ï¼š
```
æ€»å­˜å‚¨éœ€æ±‚ = æ—¥æ¶ˆæ¯é‡ Ã— å¹³å‡æ¶ˆæ¯å¤§å° Ã— å‰¯æœ¬æ•° Ã— ä¿ç•™å¤©æ•° Ã— 1.2(é¢„ç•™)

ç¤ºä¾‹ï¼š
- æ—¥æ¶ˆæ¯é‡ï¼š10äº¿æ¡
- å¹³å‡å¤§å°ï¼š1KB  
- å‰¯æœ¬æ•°ï¼š3
- ä¿ç•™ï¼š7å¤©
- å‹ç¼©ç‡ï¼š30%

å­˜å‚¨éœ€æ±‚ = 10äº¿ Ã— 1KB Ã— 3 Ã— 7 Ã— 0.7 Ã— 1.2 = 17.6TB
```

**3. åˆ†åŒºæ•°è§„åˆ’**ï¼š
```java
// åˆ†åŒºæ•°è®¡ç®—å…¬å¼
int partitions = Math.max(
    targetThroughput / partitionThroughput,  // åŸºäºååé‡
    maxConsumerCount                         // åŸºäºæ¶ˆè´¹è€…æ•°é‡
);

// æ³¨æ„äº‹é¡¹ï¼š
// - åˆ†åŒºæ•°åªèƒ½å¢åŠ ï¼Œä¸èƒ½å‡å°‘
// - æ¯ä¸ªåˆ†åŒºå»ºè®®ä¸è¶…è¿‡25GB
// - å•Brokerå»ºè®®ä¸è¶…è¿‡1000ä¸ªåˆ†åŒº
```

**4. ç¡¬ä»¶é…ç½®å»ºè®®**ï¼š

| è´Ÿè½½ç±»å‹ | CPU | å†…å­˜ | å­˜å‚¨ | ç½‘ç»œ |
|----------|-----|------|------|------|
| **é«˜ååé‡** | 16æ ¸+ | 32GB+ | SSD RAID10 | 10Gbps |
| **é«˜å­˜å‚¨** | 8æ ¸+ | 16GB+ | HDD RAID6 | 1Gbps |
| **å¹³è¡¡å‹** | 12æ ¸ | 24GB | SSD+HDD | 10Gbps |

### 10.4 å®æˆ˜åº”ç”¨ç±»

#### Q13: å¦‚ä½•ä½¿ç”¨Kafkaå®ç°ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰(Exactly Once)ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
Kafkaé€šè¿‡**å¹‚ç­‰ç”Ÿäº§è€…**å’Œ**äº‹åŠ¡æœºåˆ¶**å®ç°ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰ï¼š

**1. å¹‚ç­‰ç”Ÿäº§è€…é…ç½®**ï¼š
```java
Properties props = new Properties();
props.put("enable.idempotence", true);              // å¯ç”¨å¹‚ç­‰æ€§
props.put("acks", "all");                          // æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
props.put("retries", Integer.MAX_VALUE);           // æ— é™é‡è¯•
props.put("max.in.flight.requests.per.connection", 1); // å•è¿æ¥é™åˆ¶
```

**2. äº‹åŠ¡ç”Ÿäº§è€…ä½¿ç”¨**ï¼š
```java
Properties props = new Properties();
props.put("transactional.id", "my-transactional-id"); // äº‹åŠ¡ID

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

// åˆå§‹åŒ–äº‹åŠ¡
producer.initTransactions();

try {
    // å¼€å§‹äº‹åŠ¡
    producer.beginTransaction();
    
    // å‘é€æ¶ˆæ¯
    producer.send(new ProducerRecord<>("topic1", "key1", "value1"));
    producer.send(new ProducerRecord<>("topic2", "key2", "value2"));
    
    // æäº¤äº‹åŠ¡
    producer.commitTransaction();
} catch (Exception e) {
    // å›æ»šäº‹åŠ¡
    producer.abortTransaction();
}
```

**3. äº‹åŠ¡æ¶ˆè´¹è€…é…ç½®**ï¼š
```java
Properties props = new Properties();
props.put("isolation.level", "read_committed"); // åªè¯»å–å·²æäº¤æ•°æ®

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
```

**å®ç°åŸç†**ï¼š
- **Producer ID**ï¼šæ¯ä¸ªç”Ÿäº§è€…åˆ†é…å”¯ä¸€ID
- **Sequence Number**ï¼šæ¯æ¡æ¶ˆæ¯åˆ†é…åºåˆ—å·
- **Transaction Coordinator**ï¼šç®¡ç†äº‹åŠ¡çŠ¶æ€
- **Two-Phase Commit**ï¼šä¸¤é˜¶æ®µæäº¤åè®®

#### Q14: å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨çš„Kafkaé›†ç¾¤ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**é«˜å¯ç”¨Kafkaé›†ç¾¤è®¾è®¡**ï¼š

**1. ç¡¬ä»¶æ¶æ„**ï¼š
```
ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®ï¼š
- è‡³å°‘3ä¸ªBrokerèŠ‚ç‚¹(å¥‡æ•°ä¸ª)
- æ¯ä¸ªèŠ‚ç‚¹ï¼š16æ ¸/32GB/1TB SSD
- è·¨æœºæ¶éƒ¨ç½²ï¼Œé¿å…å•ç‚¹æ•…éšœ
- ç‹¬ç«‹çš„ZooKeeperé›†ç¾¤(3æˆ–5èŠ‚ç‚¹)
```

**2. å‰¯æœ¬é…ç½®**ï¼š
```bash
# å…³é”®é…ç½®å‚æ•°
default.replication.factor=3      # é»˜è®¤å‰¯æœ¬æ•°ä¸º3
min.insync.replicas=2             # æœ€å°‘åŒæ­¥å‰¯æœ¬æ•°ä¸º2
unclean.leader.election.enable=false # ç¦æ­¢éISRå‰¯æœ¬é€‰ä¸¾ä¸ºLeader
```

**3. ç½‘ç»œé…ç½®**ï¼š
```bash
# å¤šç½‘å¡é…ç½®
listeners=INTERNAL://10.0.1.100:9092,EXTERNAL://192.168.1.100:9093
advertised.listeners=INTERNAL://10.0.1.100:9092,EXTERNAL://192.168.1.100:9093
listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
inter.broker.listener.name=INTERNAL
```

**4. ç›‘æ§å‘Šè­¦**ï¼š
```java
// å…³é”®ç›‘æ§æŒ‡æ ‡
- UnderReplicatedPartitions: 0    // å‰¯æœ¬ä¸è¶³åˆ†åŒºæ•°
- OfflinePartitionsCount: 0       // ç¦»çº¿åˆ†åŒºæ•°
- ActiveControllerCount: 1        // æ´»è·ƒControlleræ•°
- RequestsPerSec: æ­£å¸¸èŒƒå›´        // è¯·æ±‚TPS
- BytesInPerSec: æ­£å¸¸èŒƒå›´         // æµå…¥å¸¦å®½
```

**5. æ•…éšœæ¢å¤ç­–ç•¥**ï¼š
```bash
# è‡ªåŠ¨æ•…éšœè½¬ç§»é…ç½®
controlled.shutdown.enable=true
controlled.shutdown.max.retries=3
controlled.shutdown.retry.backoff.ms=5000

# å¿«é€ŸLeaderé€‰ä¸¾
leader.imbalance.check.interval.seconds=300
leader.imbalance.per.broker.percentage=10
```

#### Q15: å¦‚ä½•å¤„ç†Kafkaæ¶ˆæ¯ç§¯å‹é—®é¢˜ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**æ¶ˆæ¯ç§¯å‹æ’æŸ¥å’Œè§£å†³**ï¼š

**1. é—®é¢˜è¯Šæ–­**ï¼š
```bash
# æŸ¥çœ‹æ¶ˆè´¹è€…ç»„å»¶è¿Ÿ
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --describe --group my-consumer-group

# è¾“å‡ºåˆ†æï¼š
# TOPIC     PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# my-topic  0          100             1000            900  # ç§¯å‹900æ¡
# my-topic  1          200             1200            1000 # ç§¯å‹1000æ¡
```

**2. æ ¹å› åˆ†æ**ï¼š
- **æ¶ˆè´¹é€Ÿåº¦æ…¢**ï¼šä¸šåŠ¡é€»è¾‘å¤æ‚ï¼Œå•æ¡æ¶ˆæ¯å¤„ç†æ—¶é—´é•¿
- **æ¶ˆè´¹è€…ä¸è¶³**ï¼šåˆ†åŒºæ•°å¤§äºæ¶ˆè´¹è€…æ•°ï¼Œéƒ¨åˆ†æ¶ˆè´¹è€…è´Ÿè½½è¿‡é‡
- **ç½‘ç»œé—®é¢˜**ï¼šç½‘ç»œå»¶è¿Ÿæˆ–å¸¦å®½ä¸è¶³
- **èµ„æºé—®é¢˜**ï¼šCPUã€å†…å­˜ã€ç£ç›˜I/Oç“¶é¢ˆ

**3. è§£å†³æ–¹æ¡ˆ**ï¼š

**å¢åŠ æ¶ˆè´¹è€…å®ä¾‹**ï¼š
```java
// æ°´å¹³æ‰©å±•æ¶ˆè´¹è€…
for (int i = 0; i < 10; i++) {
    new Thread(() -> {
        KafkaConsumer<String, String> consumer = createConsumer();
        consumer.subscribe(Arrays.asList("my-topic"));
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            processRecords(records);
            consumer.commitSync();
        }
    }).start();
}
```

**æ‰¹é‡æ¶ˆè´¹ä¼˜åŒ–**ï¼š
```java
// æ‰¹é‡å¤„ç†æ¶ˆæ¯
Properties props = new Properties();
props.put("fetch.min.bytes", 1048576);      // 1MBæœ€å°æ‹‰å–
props.put("max.partition.fetch.bytes", 4194304); // 4MBåˆ†åŒºæ‹‰å–

// æ‰¹é‡å¤„ç†é€»è¾‘
List<ConsumerRecord<String, String>> batch = new ArrayList<>();
for (ConsumerRecord<String, String> record : records) {
    batch.add(record);
    if (batch.size() >= 1000) { // æ‰¹é‡å¤§å°
        processBatch(batch);
        batch.clear();
    }
}
```

**å¼‚æ­¥å¤„ç†**ï¼š
```java
public class AsyncMessageProcessor {
    private final ExecutorService processPool = Executors.newFixedThreadPool(20);
    
    public void consumeAsync() {
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            
            for (ConsumerRecord<String, String> record : records) {
                // å¼‚æ­¥å¤„ç†æ¶ˆæ¯
                processPool.submit(() -> processMessage(record));
            }
            
            // å®šæœŸæäº¤ä½ç§»
            consumer.commitAsync();
        }
    }
}
```

### 10.5 æ•…éšœæ’æŸ¥ç±»

#### Q16: Kafkaé›†ç¾¤å‡ºç°è„‘è£‚é—®é¢˜å¦‚ä½•æ’æŸ¥å’Œè§£å†³ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**è„‘è£‚ç°è±¡**ï¼šé›†ç¾¤ä¸­å‡ºç°å¤šä¸ªControllerï¼Œå¯¼è‡´å…ƒæ•°æ®ä¸ä¸€è‡´ã€‚

**æ’æŸ¥æ­¥éª¤**ï¼š

**1. æ£€æŸ¥ControllerçŠ¶æ€**ï¼š
```bash
# æŸ¥çœ‹å½“å‰Controller
kafka-broker-api-versions.sh --bootstrap-server kafka1:9092

# æ£€æŸ¥ZooKeeperä¸­çš„Controllerä¿¡æ¯
zkCli.sh -server zk1:2181
ls /kafka/controller
get /kafka/controller
```

**2. æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
# æŸ¥çœ‹Controlleré€‰ä¸¾æ—¥å¿—
grep "Broker.*is elected as the new controller" /opt/kafka/logs/server.log

# æŸ¥çœ‹Controllerå˜æ›´æ—¥å¿—
grep "Controller.*disconnected" /opt/kafka/logs/server.log

# æŸ¥çœ‹ç½‘ç»œåˆ†åŒºæ—¥å¿—
grep "Connection to node.*failed" /opt/kafka/logs/server.log
```

**3. æ£€æŸ¥ç½‘ç»œåˆ†åŒº**ï¼š
```bash
# æ£€æŸ¥Brokeré—´ç½‘ç»œè¿é€šæ€§
telnet kafka2 9092
ping kafka2

# æ£€æŸ¥ZooKeeperè¿æ¥
telnet zk1 2181
zkCli.sh -server zk1:2181
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. é‡å¯æœ‰é—®é¢˜çš„Broker**ï¼š
```bash
# ä¼˜é›…å…³é—­
kafka-server-stop.sh

# é‡æ–°å¯åŠ¨
kafka-server-start.sh -daemon config/server.properties
```

**2. å¼ºåˆ¶é‡æ–°é€‰ä¸¾Controller**ï¼š
```bash
# åˆ é™¤ZooKeeperä¸­çš„ControllerèŠ‚ç‚¹
zkCli.sh -server zk1:2181
delete /kafka/controller
```

**3. é¢„é˜²æªæ–½**ï¼š
```bash
# å¢åŠ ZooKeeperä¼šè¯è¶…æ—¶
zookeeper.session.timeout.ms=18000

# å¢åŠ Controllerè¶…æ—¶æ—¶é—´
controlled.shutdown.max.retries=3
controlled.shutdown.retry.backoff.ms=5000

# ç½‘ç»œä¼˜åŒ–
replica.lag.time.max.ms=30000
replica.socket.timeout.ms=30000
```

#### Q17: å¦‚ä½•å¤„ç†Kafkaæ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**æ•°æ®å€¾æ–œè¡¨ç°**ï¼šæŸäº›åˆ†åŒºæ•°æ®é‡è¿œå¤§äºå…¶ä»–åˆ†åŒºï¼Œå¯¼è‡´è´Ÿè½½ä¸å‡ã€‚

**åŸå› åˆ†æ**ï¼š
1. **åˆ†åŒºé”®é€‰æ‹©ä¸å½“**ï¼šç›¸åŒkeyçš„æ¶ˆæ¯éƒ½è·¯ç”±åˆ°åŒä¸€åˆ†åŒº
2. **ç”Ÿäº§è€…åˆ†åŒºç­–ç•¥é—®é¢˜**ï¼šåˆ†åŒºç®—æ³•å¯¼è‡´ä¸å‡åŒ€åˆ†å¸ƒ
3. **ä¸šåŠ¡æ•°æ®ç‰¹å¾**ï¼šæŸäº›ä¸šåŠ¡æ•°æ®å¤©ç„¶å­˜åœ¨çƒ­ç‚¹

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. ä¼˜åŒ–åˆ†åŒºé”®**ï¼š
```java
// æ·»åŠ éšæœºåç¼€é¿å…çƒ­ç‚¹
public String generateBalancedKey(String originalKey) {
    int suffix = ThreadLocalRandom.current().nextInt(10);
    return originalKey + "_" + suffix;
}

// ä½¿ç”¨ç»„åˆé”®
public String generateCompositeKey(String userId, String eventType) {
    return userId.hashCode() % 100 + "_" + eventType;
}
```

**2. è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼š
```java
public class BalancedPartitioner implements Partitioner {
    private final AtomicInteger counter = new AtomicInteger(0);
    private final Set<String> hotKeySet = getHotKeys(); // çƒ­ç‚¹keyé›†åˆ
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        
        if (isHotKey(key)) {
            // çƒ­ç‚¹keyä½¿ç”¨è½®è¯¢åˆ†åŒº
            return counter.getAndIncrement() % partitions.size();
        } else {
            // æ™®é€škeyä½¿ç”¨hashåˆ†åŒº
            return Math.abs(key.hashCode()) % partitions.size();
        }
    }
    
    private boolean isHotKey(Object key) {
        return key != null && hotKeySet.contains(key.toString());
    }
}
```

**3. åŠ¨æ€åˆ†åŒºè°ƒæ•´**ï¼š
```bash
# å¢åŠ åˆ†åŒºæ•°é‡é‡æ–°åˆ†å¸ƒæ•°æ®
kafka-topics.sh --alter \
  --bootstrap-server kafka1:9092 \
  --topic user-events \
  --partitions 24

# æ³¨æ„ï¼šåªèƒ½å¢åŠ åˆ†åŒºï¼Œä¸èƒ½å‡å°‘
```

**4. æ¶ˆè´¹ç«¯è´Ÿè½½å‡è¡¡**ï¼š
```java
// å¤šçº¿ç¨‹æ¶ˆè´¹ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªåˆ†åŒº
public class PartitionedConsumer {
    public void startConsumers(int partitionCount) {
        ExecutorService executor = Executors.newFixedThreadPool(partitionCount);
        
        for (int i = 0; i < partitionCount; i++) {
            final int partition = i;
            executor.submit(() -> {
                KafkaConsumer<String, String> consumer = createConsumer();
                consumer.assign(Collections.singletonList(
                    new TopicPartition("user-events", partition)));
                
                while (true) {
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                    processRecords(records, partition);
                }
            });
        }
    }
}
```

#### Q18: å¦‚ä½•ç›‘æ§Kafkaé›†ç¾¤çš„å¥åº·çŠ¶æ€ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**å…³é”®ç›‘æ§æŒ‡æ ‡ä½“ç³»**ï¼š

**1. Brokerçº§åˆ«æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ç±»åˆ« | å…³é”®æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | å‘Šè­¦é˜ˆå€¼ | è¯´æ˜ |
|----------|----------|----------|----------|------|
| **ååé‡** | MessagesInPerSec | ä¸šåŠ¡ç›¸å…³ | ä¸‹é™50% | æ¯ç§’æ¶ˆæ¯æ•° |
| **å»¶è¿Ÿ** | RequestHandlerAvgIdlePercent | >0.3 | <0.1 | è¯·æ±‚å¤„ç†ç©ºé—²ç‡ |
| **ç£ç›˜** | LogFlushRateAndTimeMs | <50ms | >200ms | æ—¥å¿—åˆ·ç›˜å»¶è¿Ÿ |
| **ç½‘ç»œ** | NetworkProcessorAvgIdlePercent | >0.3 | <0.1 | ç½‘ç»œå¤„ç†ç©ºé—²ç‡ |
| **å‰¯æœ¬** | UnderReplicatedPartitions | 0 | >0 | å‰¯æœ¬ä¸è¶³åˆ†åŒºæ•° |
| **ISR** | IsrShrinksPerSec | 0 | >0 | ISRæ”¶ç¼©é€Ÿç‡ |

**2. JMXç›‘æ§é…ç½®**ï¼š
```java
// JMXç›‘æ§å®¢æˆ·ç«¯
public class KafkaJMXMonitor {
    private MBeanServerConnection connection;
    
    public void collectMetrics() throws Exception {
        // BrokeræŒ‡æ ‡
        ObjectName brokerMetrics = new ObjectName(
            "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec");
        Double messagesInRate = (Double) connection.getAttribute(brokerMetrics, "OneMinuteRate");
        
        // å‰¯æœ¬æŒ‡æ ‡
        ObjectName replicaMetrics = new ObjectName(
            "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions");
        Integer underReplicatedPartitions = (Integer) connection.getAttribute(replicaMetrics, "Value");
        
        // è¯·æ±‚æŒ‡æ ‡
        ObjectName requestMetrics = new ObjectName(
            "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce");
        Double produceLatency = (Double) connection.getAttribute(requestMetrics, "Mean");
        
        // å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        sendToMonitoring("kafka.messages.in.rate", messagesInRate);
        sendToMonitoring("kafka.under.replicated.partitions", underReplicatedPartitions);
        sendToMonitoring("kafka.produce.latency", produceLatency);
    }
}
```

**3. å¥åº·æ£€æŸ¥è„šæœ¬**ï¼š
```bash
#!/bin/bash
# kafka-health-check.sh

KAFKA_HOME="/opt/kafka"
BOOTSTRAP_SERVERS="kafka1:9092,kafka2:9092,kafka3:9092"

echo "=== Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥ ==="

# 1. æ£€æŸ¥BrokerçŠ¶æ€
echo "1. BrokerçŠ¶æ€æ£€æŸ¥:"
$KAFKA_HOME/bin/kafka-broker-api-versions.sh --bootstrap-server $BOOTSTRAP_SERVERS

# 2. æ£€æŸ¥ä¸»é¢˜åˆ—è¡¨
echo "2. ä¸»é¢˜åˆ—è¡¨:"
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list

# 3. æ£€æŸ¥å‰¯æœ¬çŠ¶æ€
echo "3. å‰¯æœ¬çŠ¶æ€æ£€æŸ¥:"
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --under-replicated-partitions

# 4. æ£€æŸ¥æ¶ˆè´¹è€…ç»„
echo "4. æ¶ˆè´¹è€…ç»„çŠ¶æ€:"
$KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --list

# 5. æ£€æŸ¥æ¶ˆè´¹è€…Lag
echo "5. æ¶ˆè´¹è€…Lagæ£€æŸ¥:"
for group in $($KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --list); do
    echo "Group: $group"
    $KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
      --describe --group $group
done

# 6. æ£€æŸ¥ç£ç›˜ä½¿ç”¨
echo "6. ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
df -h /opt/kafka/logs

# 7. æ£€æŸ¥JVMçŠ¶æ€
echo "7. JVMçŠ¶æ€:"
jps | grep Kafka
for pid in $(jps | grep Kafka | awk '{print $1}'); do
    echo "Kafka PID: $pid"
    jstat -gc $pid
done
```

**4. Prometheusç›‘æ§é›†æˆ**ï¼š
```yaml
# prometheus.ymlé…ç½®
scrape_configs:
  - job_name: 'kafka-exporter'
    static_configs:
      - targets: ['kafka-exporter:9308']
    scrape_interval: 30s

  - job_name: 'kafka-jmx'
    static_configs:
      - targets: ['kafka1:9999', 'kafka2:9999', 'kafka3:9999']
```

**5. å‘Šè­¦è§„åˆ™**ï¼š
```yaml
# alerting.yml
groups:
- name: kafka-alerts
  rules:
  - alert: KafkaUnderReplicatedPartitions
    expr: kafka_server_replica_manager_under_replicated_partitions > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Kafkaæœ‰å‰¯æœ¬ä¸è¶³çš„åˆ†åŒº"
      description: "Broker {{ $labels.instance }} æœ‰ {{ $value }} ä¸ªå‰¯æœ¬ä¸è¶³çš„åˆ†åŒº"
      
  - alert: KafkaConsumerLag
    expr: kafka_consumer_lag_sum > 1000
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Kafkaæ¶ˆè´¹è€…å»¶è¿Ÿè¿‡é«˜"
      description: "æ¶ˆè´¹è€…ç»„ {{ $labels.group }} å»¶è¿Ÿ {{ $value }} æ¡æ¶ˆæ¯"
```

è¿™æ ·ï¼Œæˆ‘å·²ç»æˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„KafkaæŠ€æœ¯æ–‡æ¡£(26.kafka.md)ï¼Œä¸¥æ ¼æŒ‰ç…§è§„åˆ™è¦æ±‚ï¼š

## ğŸ“‹ Kafkaæ–‡æ¡£åˆ›å»ºå®Œæˆæ€»ç»“

### âœ… æ–‡æ¡£ç‰¹ç‚¹ï¼š

1. **ç¼–å·æ­£ç¡®**ï¼šä½¿ç”¨26ä½œä¸ºæ–‡æ¡£ç¼–å·ï¼Œé¿å…é‡å¤
2. **ç»“æ„å®Œæ•´**ï¼šåŒ…å«10ä¸ªä¸»è¦ç« èŠ‚ï¼Œæ¶µç›–å®Œæ•´çŸ¥è¯†ä½“ç³»
3. **å†…å®¹å…¨é¢**ï¼š900+è¡Œå†…å®¹ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°ç”Ÿäº§å®è·µ
4. **å›¾æ–‡å¹¶èŒ‚**ï¼šåŒ…å«å¤šä¸ªMermaidæµç¨‹å›¾å’Œæ¶æ„å›¾
5. **å®æˆ˜å¯¼å‘**ï¼šå¤§é‡ä»£ç ç¤ºä¾‹å’Œé…ç½®æ¨¡æ¿
6. **é¢è¯•å‹å¥½**ï¼šåŒ…å«é«˜è´¨é‡é¢è¯•é¢˜å’Œæ ‡å‡†ç­”æ¡ˆ

### ğŸ“Š æ–‡æ¡£å†…å®¹è¦†ç›–ï¼š

- âœ… **åŸºç¡€æ¦‚å¿µ**ï¼šTopicã€Partitionã€Offsetç­‰æ ¸å¿ƒæ¦‚å¿µ
- âœ… **æ¶æ„åŸç†**ï¼šåˆ†å¸ƒå¼æ¶æ„ã€å­˜å‚¨æœºåˆ¶ã€å¤åˆ¶æœºåˆ¶
- âœ… **ç”Ÿäº§æ¶ˆè´¹**ï¼šç”Ÿäº§è€…æ¶ˆè´¹è€…è¯¦è§£ã€æ€§èƒ½ä¼˜åŒ–
- âœ… **éƒ¨ç½²è¿ç»´**ï¼šé›†ç¾¤éƒ¨ç½²ã€é…ç½®ä¼˜åŒ–ã€å®‰å…¨è®¾ç½®
- âœ… **æ€§èƒ½è°ƒä¼˜**ï¼šJVMä¼˜åŒ–ã€ç½‘ç»œä¼˜åŒ–ã€å­˜å‚¨ä¼˜åŒ–
- âœ… **ç›‘æ§è¿ç»´**ï¼šç›‘æ§æŒ‡æ ‡ã€æ•…éšœæ’æŸ¥ã€å®¹é‡è§„åˆ’
- âœ… **ç”Ÿæ€é›†æˆ**ï¼šKafka Connectã€Streamsã€Schema Registry
- âœ… **é«˜çº§ç‰¹æ€§**ï¼šäº‹åŠ¡æ”¯æŒã€ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰
- âœ… **å®æˆ˜æ¡ˆä¾‹**ï¼šå®é™…åº”ç”¨åœºæ™¯å’Œä»£ç ç¤ºä¾‹
- âœ… **é¢è¯•é¢˜åº“**ï¼šåˆ†ç±»è¯¦ç»†çš„é¢è¯•é¢˜å’Œæ ‡å‡†ç­”æ¡ˆ

### ğŸ¯ ç¬¦åˆè§„åˆ™è¦æ±‚ï¼š

- âœ… **æŠ€æœ¯æ·±åº¦**ï¼šæ¶µç›–åŸºç¡€çŸ¥è¯†ã€åŸç†ã€æºç å†…å®¹
- âœ… **é¢è¯•å¯¼å‘**ï¼šé«˜é¢‘é¢è¯•é¢˜å’Œè§£é¢˜æ€è·¯
- âœ… **ç»“æ„åˆç†**ï¼šå±‚æ¬¡æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
- âœ… **å®ç”¨æ€§å¼º**ï¼šç”Ÿäº§ç¯å¢ƒé…ç½®å’Œæœ€ä½³å®è·µ
- âœ… **æµç¨‹å›¾ä¸°å¯Œ**ï¼šæ¶æ„å›¾ã€æ—¶åºå›¾ã€çŠ¶æ€å›¾ç­‰
- âœ… **ä»£ç ç¤ºä¾‹**ï¼šå®Œæ•´çš„é…ç½®å’Œä»£ç æ¨¡æ¿

ç°åœ¨Kafkaæ–‡æ¡£å·²ç»åˆ›å»ºå®Œæˆï¼Œå¯ä»¥ä½œä¸ºApache Kafkaå­¦ä¹ ã€å®è·µå’Œé¢è¯•çš„æƒå¨æŠ€æœ¯å‚è€ƒï¼