# Shell 命令参考手册

## 目录
- [文件操作](#文件操作)
  - [文件查找与过滤](#文件查找与过滤)
  - [文件处理](#文件处理)
  - [文件压缩](#文件压缩)
- [HDFS操作](#hdfs操作)
  - [文件查看](#文件查看)
  - [文件大小统计](#文件大小统计)
- [文本处理](#文本处理)
  - [文件读取](#文件读取)
  - [文本过滤](#文本过滤)
- [进程管理](#进程管理)
- [时间处理](#时间处理)
- [Spark操作](#spark操作)
  - [Spark Shell](#spark-shell)
  - [Spark SQL](#spark-sql)
- [网络请求](#网络请求)
- [实用工具](#实用工具)

## 文件操作

### 文件查找与过滤
```bash
# 查找目录下以8位数字结尾的文件（YYYYMMDD格式）
find /path/to/dir -type f -regextype posix-extended -regex '.*[0-9]{8}$'

# 检查文件是否存在
if [ ! -f "$file" ]; then
    echo "File not found: $file"
    exit 1
fi
```

### 文件处理
```bash
# 分割大文件
split -b 10M large_file.txt output_

# 合并文件
cat output_* > combined_file.txt
```

### 文件压缩
```bash
# 压缩文件
gzip a.txt          # 压缩并删除原文件
gzip -c a.txt       # 压缩并保留原文件

# 解压文件
gunzip your_file.txt.gz
```

## HDFS操作

### 文件查看
```bash
# 列出HDFS目录下以8位数字结尾的文件
hadoop fs -ls /path/to/dir | awk '$NF ~ /[0-9]{8}$/ {print $NF}'

# 使用grep过滤HDFS目录
hadoop fs -ls /data/dept/bi/dim/data | grep -E "[0-9]{8}$"
```

### 文件大小统计
```bash
# 获取目录文件大小并选择最后一行
hadoop fs -du -h $line | tail -n 1 >> file_size_result.txt
```

## 文本处理

### 文件读取
```bash
# 遍历文件每一行
file="example.txt"
while IFS= read -r line; do
    echo "$line"
done < "$file"

# 另一种读取方式
cat "$file" | while read line; do
    echo "$line"
done
```

### 文本过滤
```bash
# 查看grep过滤结果的前100个字符
grep "关键词" file.txt | cut -c 1-100

# 显示匹配行及其后5行
curl -s "http://localhost:8080/hotsearch" | grep -A 5 "wallpaper-menu"
```

## 时间处理
```bash
# 获取当前时间戳（秒）
end_time=$(date +%s)

# 计算时间差
elapsed=$((end_time-start_time))
```

## Spark操作

### Spark Shell
```bash
/opt/spark23/bin/spark-shell --master yarn \
--executor-memory 4G \
--num-executors 300 \
--conf spark.sql.parquet.binaryAsString=true \
--conf spark.default.parallelism=2000 \
--conf spark.sql.shuffle.partitions=2000 \
--driver-memory 5g \
--name app_name \
--queue queue_name << EOF

# Spark代码
for (i <- 1 to 6) {
    val input = s"${hdfs_incr_ck_output}/$i/${log_date}"
    println("输入路径 " + input)
    val output = s"${hdfs_incr_ck_output}_sort/$i/${log_date}"
    println("输出路径 " + output)
    spark.read.parquet(input).sort("gid").write.mode("overwrite").parquet(output)
}

:quit
EOF
```

### Spark SQL
```bash
/opt/spark23/bin/spark-sql \
--master yarn \
--executor-memory 2G \
--driver-memory 2g \
--num-executors 2
```

## 网络请求
```bash
# 循环发送curl请求
while IFS= read -r i; do
    response=$(curl -s -f --location \
        --request POST 'http://xxxx/api/v1/endpoint' \
        --header 'Content-Type: application/json' \
        -d '{
        "able":true
        }')
    
    if [ $? -eq 0 ]; then
        # 使用jq解析JSON
        unique_items=$(echo "$response" | jq -r '.data[].item' | sort -u)
        echo "Unique items for id $i:"
        echo "$unique_items"
        # 统计数量
        count=$(echo "$unique_items" | wc -l)
        echo "Total unique items: $count"
    else
        echo "Error fetching data for id $i" >&2
    fi
done < input.txt
```

## 进程管理
```bash
# 查看特定进程
ps aux | grep pyspark
```

## 实用工具
```bash
# 安装Python kernel
python -m ipykernel install --user --name openai --display-name "openai"
```
