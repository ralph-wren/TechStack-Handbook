-- 1. 常用SQL语法与函数

-- 日期与时间处理
unix_timestamp()  -- 获取10位时间戳
cast(create_time as bigint)  -- 转换数据类型
from_unixtime(create_time,'yyyyMMdd') -- 10位时间戳转为指定格式
date_add(date_column, number_of_days) -- 加几天
date_sub(date_column, number_of_days) -- 减几天
unix_timestamp(date_sub(from_unixtime(unix_timestamp(),30),'yyyy-MM-dd')*1000

-- 字符串处理
instr(extend_detail,'name')>0  -- 判断字符串是否包含
instr(col2, col1) > 0  -- col1 在 col2 里面 ,不在返回0
regexp_extract(string subject, string pattern, int index)
regexp_replace(extend_detail, '#[a-z0-9]{12}#', '##')
substr(string str, int start, int length) -- start从1开始

-- 数组与集合操作
lateral view explode(split(gid_detail,',')) tmp as single_gid
transform 转换数组；array_union 合并数组并去重
scala> df.withColumn("geos",expr("array_union(transform(split(geo_list,','),x->substr(x,1,6)),array(substr(geohash,1,6)))")).show
-- 过滤数组元素 filter
df.withColumn("size",expr("filter(split(geo_list,','),x->length(x)=8))"))

-- JSON解析
spark.sql("select get_json_object(split(value,'\\\\|')[5],'$.data') from logs").show(false)

-- 条件与判断
count()  count(1) 非NULL的行  计算快
count(*) 计算所有行,包括NULL值的行。扫描正整表,算的慢

-- 分组与聚合
-- over开窗函数
over()  -- 获取整个表的总条数  
select "oaid" id_type,oaid_number id_number,count(1) over() gid_sum 
  from gid_to_id
  where query_id in (select query_id  from gid_to_id group by query_id having count(1)=2) 
    and rpc_type='rpc_end'

-- with用法
WITH t1 AS ( SELECT * FROM carinfo), 
     t2 AS ( SELECT * FROM car_blacklist )   --不用写分号
SELECT *
FROM t1, t2

-- having与where区别
-- 1、where在分组前过滤,having在分组后使用,应用于每一组数据
-- 2、having可以使用select里面定义的别名
hive> select job, count(1) cnt from emp group by job having cnt>2;


-- 2. 表操作

-- 建表与压缩格式
create table log_orc_none(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as orc tblproperties ("orc.compress"="NONE");

-- 分桶与分区
CREATE TABLE bucketed_table (
    id INT,
    name STRING
) CLUSTERED BY (id) INTO 10 BUCKETS;
-- 需要在查询时启用分桶。
SET hive.enforce.bucketing=true;

-- 动态分区
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

-- MapJoin与分桶Join
SET hive.auto.convert.join=true;
SELECT /*+ MAPJOIN(small_table) */ *
FROM large_table
JOIN small_table ON large_table.id = small_table.id;

-- 压缩中间结果
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress=true;
SET mapreduce.map.output.compress=true;
SET mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;

-- CBO优化
SET hive.cbo.enable=true;

-- MapReduce内存配置
SET mapreduce.map.memory.mb=4096;
SET mapreduce.reduce.memory.mb=8192;


-- 3. UDF与自定义函数

-- UDF参考
-- https://cf.cloudglab.cn/pages/viewpage.action?pageId=245273514

-- 经纬度转geohash 
--jars hdfs://gt-ga-xs/tmp/wuxl/udf/ys-bi-udf-hive-function-0.0.0.jar
import com.ys.axe.saas.comn.util.GeohashUtil
spark.udf.register("encode",GeohashUtil.encode _)

-- 合法wifi判断
import com.ys.axe.saas.comn.util.LegalMacUtil;
val isLegalMac = udf((arg: String) => LegalMacUtil.isLegalMac(arg))
spark.udf.register("isLegalMac",isLegalMac);

-- 不可见字符检测
import org.apache.spark.sql.{SparkSession, functions => F}
import org.apache.spark.sql.expressions.UserDefinedFunction
  /** 判断是否为不可见字符 */
def isInvisible(code: Int): Boolean = {
  (code >= 0x00 && code <= 0x1F) || // 控制字符
  code == 0x7F ||                   // DEL
  code == 0xA0 ||                   // 不换行空格
  code == 0x200B ||                 // 零宽空格
  code == 0xFEFF                   // BOM
}
  /** 检测不可见字符并打印位置、Unicode等 */
def detectInvisible(input: String): String = {
  if (input == null || input.isEmpty) return "[空]"
  input.zipWithIndex.flatMap {
    case (ch, idx) =>
      val code = ch.toInt
      if (isInvisible(code)) {
        Some(f"[pos=$idx, U+${code}%04X, ASCII=$code]")
      } else None
  }.mkString(", ")
}
    // 注册 UDF
  val detectInvisibleUDF: UserDefinedFunction = F.udf((input: String) => detectInvisible(input))
  spark.udf.register("detectInvisible", detectInvisibleUDF)

-- 4. 参考链接与外部资源
-- UDF 函数
-- https://cf.cloudglab.cn/pages/viewpage.action?pageId=245273514